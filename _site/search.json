[
  {
    "objectID": "02-llms/08-shakespeare-example-colab/index.html",
    "href": "02-llms/08-shakespeare-example-colab/index.html",
    "title": "wordplay üéÆ üí¨: Shakespeare",
    "section": "",
    "text": "‚úçÔ∏è\nSam Foreman ([ALCF](https://alcf.anl.gov/about/people/sam-foreman))\n2025-07-22\nWe will be using the Shakespeare dataset to train a (~ small) 10M param LLM from scratch.\nImage generated from stabilityai/stable-diffusion on ü§ó Spaces.",
    "crumbs": [
      "[02] Large Language Models",
      "[8] Shakespeare Example (Colab)"
    ]
  },
  {
    "objectID": "02-llms/08-shakespeare-example-colab/index.html#install-setup",
    "href": "02-llms/08-shakespeare-example-colab/index.html#install-setup",
    "title": "wordplay üéÆ üí¨: Shakespeare",
    "section": "Install / Setup",
    "text": "Install / Setup\nWarning!\nIF YOU ARE EXECUTING ON GOOGLE COLAB:\nYou will need to restart your runtime (Runtime \\rightarrow\\, Restart runtime)\nafter executing the following cell:\n\n%%bash\n\npython3 -c 'import wordplay; print(wordplay.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has wordplay installed. Nothing to do.\"\nelse\n    echo \"Does not have wordplay installed. Installing...\"\n    git clone 'https://github.com/saforem2/wordplay'\n    python3 wordplay/data/shakespeare_char/prepare.py\n    python3 wordplay/data/shakespeare/prepare.py\n    python3 -m pip install deepspeed\n    python3 -m pip install -e wordplay\nfi\n\n/content/wordplay/src/wordplay/__init__.py\nHas wordplay installed. Nothing to do.",
    "crumbs": [
      "[02] Large Language Models",
      "[8] Shakespeare Example (Colab)"
    ]
  },
  {
    "objectID": "02-llms/08-shakespeare-example-colab/index.html#post-install",
    "href": "02-llms/08-shakespeare-example-colab/index.html#post-install",
    "title": "wordplay üéÆ üí¨: Shakespeare",
    "section": "Post Install",
    "text": "Post Install\nIf installed correctly, you should be able to:\n&gt;&gt;&gt; import wordplay\n&gt;&gt;&gt; wordplay.__file__\n'/path/to/wordplay/src/wordplay/__init__.py'\n\n%load_ext autoreload\n%autoreload 2\nimport os\nimport sys\nimport ezpz\n\nos.environ['COLORTERM'] = 'truecolor'\nif sys.platform == 'darwin':\n    # If running on MacOS:\n    # os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n    os.environ['TORCH_DEVICE'] = 'cpu'\n# -----------------------------------------------\n\nlogger = ezpz.get_logger()\n\nimport wordplay\nlogger.info(wordplay.__file__)\n\n[2025-07-23 17:07:07,066155][I][ezpz/__init__:265:ezpz] Setting logging level to 'INFO' on 'RANK == 0'\n\n\n\n[2025-07-23 17:07:07,072771][I][ezpz/__init__:266:ezpz] Setting logging level to 'CRITICAL' on all others 'RANK != 0'\n\n\n\n[2025-07-23 17:07:07,079375][I][tmp/ipython-input-2-2338663768:17:ezpz.log] /content/wordplay/src/wordplay/__init__.py",
    "crumbs": [
      "[02] Large Language Models",
      "[8] Shakespeare Example (Colab)"
    ]
  },
  {
    "objectID": "02-llms/08-shakespeare-example-colab/index.html#build-trainer",
    "href": "02-llms/08-shakespeare-example-colab/index.html#build-trainer",
    "title": "wordplay üéÆ üí¨: Shakespeare",
    "section": "Build Trainer",
    "text": "Build Trainer\nExplicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\n\n\nimport wordplay\nprint(wordplay.__file__)\n\n/content/wordplay/src/wordplay/__init__.py\n\n\n\nimport os\nimport numpy as np\nfrom ezpz import setup\nfrom hydra.utils import instantiate\nfrom wordplay.configs import get_config, PROJECT_ROOT\nfrom wordplay.trainer import Trainer\n\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nBACKEND = 'DDP'\n\nrank = setup(\n    framework='pytorch',\n    backend=BACKEND,\n    seed=1234,\n)\n\ncfg = get_config(\n    [\n        'data=shakespeare',\n        'model=shakespeare',\n        'model.batch_size=8',\n        'model.block_size=1024',\n        'optimizer=shakespeare',\n        'train=shakespeare',\n        f'train.backend={BACKEND}',\n        'train.compile=false',\n        'train.dtype=bfloat16',\n        'train.max_iters=1000',\n        'train.log_interval=10',\n        'train.eval_interval=100',\n    ]\n)\nconfig = instantiate(cfg)\n\n[2025-07-23 17:07:07,409437][I][wordplay/configs:81] Setting HF_DATASETS_CACHE to /content/wordplay/.cache/huggingface/datasets\n\n\n\n[2025-07-23 17:07:07,435593][I][ezpz/dist:1159] Using fw='ddp' with torch_{device,backend}= {cuda, nccl}\n\n\n\n[2025-07-23 17:07:07,438150][I][ezpz/dist:1026] Caught MASTER_PORT=41765 from environment!\n\n\n\n[2025-07-23 17:07:07,440989][I][ezpz/dist:1042] Using torch.distributed.init_process_group with\n- master_addr='588b3fb1cb70'\n- master_port='41765'\n- world_size=1\n- rank=0\n- local_rank=0\n- timeout=datetime.timedelta(seconds=3600)\n- backend='nccl'\n\n\n\n[2025-07-23 17:07:07,447590][I][ezpz/dist:759] Calling torch.distributed.init_process_group_with: rank=0 world_size=1 backend=nccl\n\n\n\n[2025-07-23 17:07:07,462711][I][ezpz/dist:1377] Using device='cuda' with backend='nccl' + 'nccl' for distributed training.\n\n\n\n[2025-07-23 17:07:07,465933][I][ezpz/dist:1422] ['588b3fb1cb70'][0/0] \n\n\n\n[2025-07-23 17:07:08,215788][I][wordplay/configs:317] Loading val from /content/wordplay/data/shakespeare_char/val.bin\n\n\n\n[2025-07-23 17:07:08,221368][I][wordplay/configs:317] Loading train from /content/wordplay/data/shakespeare_char/train.bin\n\n\n\n[2025-07-23 17:07:08,226696][I][wordplay/configs:442] Tokens per iteration: 8,192\n\n\n\n[2025-07-23 17:07:08,231221][I][wordplay/configs:465] Using self.ptdtype=torch.bfloat16 on self.device_type='cuda'\n\n\n\n[2025-07-23 17:07:08,234866][I][wordplay/configs:471] Initializing a new model from scratch\n\n\n\n\nBuild Trainer object\n\ntrainer = Trainer(config)\n\n[2025-07-23 17:07:08,315621][I][wordplay/trainer:248] Initializing a new model from scratch\n\n\n\n[2025-07-23 17:07:08,654618][I][wordplay/model:255] number of parameters: 10.65M\n\n\n\n[2025-07-23 17:07:08,675995][I][wordplay/trainer:266] Model size: num_params=10646784\n\n\n\n[2025-07-23 17:07:08,686453][I][wordplay/model:445] num decayed parameter tensors: 26, with 11,035,008 parameters\n\n\n\n[2025-07-23 17:07:08,690282][I][wordplay/model:449] num non-decayed parameter tensors: 13, with 4,992 parameters\n\n\n\n[2025-07-23 17:07:08,696244][I][wordplay/model:465] using fused AdamW: True\n\n\n\n[2025-07-23 17:07:08,699647][C][wordplay/trainer:318] \"devid='cuda:0'\"\n\n\n\n[2025-07-23 17:07:08,703940][I][wordplay/trainer:358] ‚Ä¢ self.model=GPT(\n  (transformer): ModuleDict(\n    (wte): Embedding(65, 384)\n    (wpe): Embedding(1024, 384)\n    (drop): Dropout(p=0.2, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n          (attn_dropout): Dropout(p=0.2, inplace=False)\n          (resid_dropout): Dropout(p=0.2, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n          (act_fn): GELU(approximate='none')\n          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): Linear(in_features=384, out_features=65, bias=False)\n)\n\n\n\n[2025-07-23 17:07:08,731597][I][wordplay/trainer:359] ‚Ä¢ self.grad_scaler=&lt;torch.cuda.amp.grad_scaler.GradScaler object at 0x7cbd3c9a85d0&gt;\n\n\n\n[2025-07-23 17:07:08,737375][I][wordplay/trainer:360] ‚Ä¢ self.model_engine=GPT(\n  (transformer): ModuleDict(\n    (wte): Embedding(65, 384)\n    (wpe): Embedding(1024, 384)\n    (drop): Dropout(p=0.2, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n          (attn_dropout): Dropout(p=0.2, inplace=False)\n          (resid_dropout): Dropout(p=0.2, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n          (act_fn): GELU(approximate='none')\n          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): Linear(in_features=384, out_features=65, bias=False)\n)\n\n\n\n[2025-07-23 17:07:08,760469][I][wordplay/trainer:361] ‚Ä¢ self.optimizer=AdamW (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.99)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: True\n    lr: 0.001\n    maximize: False\n    weight_decay: 0.1\n\nParameter Group 1\n    amsgrad: False\n    betas: (0.9, 0.99)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: True\n    lr: 0.001\n    maximize: False\n    weight_decay: 0.0\n)",
    "crumbs": [
      "[02] Large Language Models",
      "[8] Shakespeare Example (Colab)"
    ]
  },
  {
    "objectID": "02-llms/08-shakespeare-example-colab/index.html#prompt-prior-to-training",
    "href": "02-llms/08-shakespeare-example-colab/index.html#prompt-prior-to-training",
    "title": "wordplay üéÆ üí¨: Shakespeare",
    "section": "Prompt (prior to training)",
    "text": "Prompt (prior to training)\n\nquery = \"What is an LLM?\"\noutputs = trainer.evaluate(\n    query,\n    num_samples=1,\n    max_new_tokens=256,\n    top_k=16,\n    display=False\n)\nlogger.info(f\"['prompt']: '{query}'\")\nlogger.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n[2025-07-23 17:07:10,765047][I][tmp/ipython-input-6-3496000222:9:ezpz.log] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:07:10,767795][I][tmp/ipython-input-6-3496000222:10:ezpz.log] ['response']:\n\nWhat is an LLM?ouuu'fU?UUUU-LLlVmoYY;?U$IMwwYDjMYYXSSdIss;I''DPOjHhooooMZtmkoGXjZ\nBDDddZkydVPcM'MAWILMDDP'''!A'Vzl;R\ndtA$ttoXttJJffobJJ;b-vkwwJJOHHwQFccddlobAGGnM'''$kW;kzZlSwZkAoR;wmooo$J-fffoYDd'UBooXYB;JSf?P'MJ..t'hPffID;R.XXo'''SPZkXXXe'VS.JoMdkXSffo''RHQklK''UUUSoMn",
    "crumbs": [
      "[02] Large Language Models",
      "[8] Shakespeare Example (Colab)"
    ]
  },
  {
    "objectID": "02-llms/08-shakespeare-example-colab/index.html#train-model",
    "href": "02-llms/08-shakespeare-example-colab/index.html#train-model",
    "title": "wordplay üéÆ üí¨: Shakespeare",
    "section": "Train Model",
    "text": "Train Model\n\n\n\nname\ndescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization[1]\n\n\n\n^legend: #tbl-legend\n[1] in units of A100 bfloat16 peak FLOPS\n\ntrainer.config.device_type\n\n'cuda'\n\n\n\nfrom rich import print\n\nprint(trainer.model)\n\nGPT(\n  (transformer): ModuleDict(\n    (wte): Embedding(65, 384)\n    (wpe): Embedding(1024, 384)\n    (drop): Dropout(p=0.2, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n          (attn_dropout): Dropout(p=0.2, inplace=False)\n          (resid_dropout): Dropout(p=0.2, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n          (act_fn): GELU(approximate='none')\n          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): Linear(in_features=384, out_features=65, bias=False)\n)",
    "crumbs": [
      "[02] Large Language Models",
      "[8] Shakespeare Example (Colab)"
    ]
  },
  {
    "objectID": "02-llms/08-shakespeare-example-colab/index.html#partial-training",
    "href": "02-llms/08-shakespeare-example-colab/index.html#partial-training",
    "title": "wordplay üéÆ üí¨: Shakespeare",
    "section": "(partial) Training:",
    "text": "(partial) Training:\nWe‚Äôll first train for 500 iterations and then evaluate the models performance on the same prompt:\n\nWhat is an LLM?\n\n\ntrainer.train(train_iters=500)\n\n                Training Legend                 \n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ        abbr ‚îÉ desc                           ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ        step ‚îÇ Current training iteration     ‚îÇ\n‚îÇ        loss ‚îÇ Loss value                     ‚îÇ\n‚îÇ          dt ‚îÇ Elapsed time per training step ‚îÇ\n‚îÇ         dtf ‚îÇ Elapsed time per forward step  ‚îÇ\n‚îÇ         dtb ‚îÇ Elapsed time per backward step ‚îÇ\n‚îÇ         sps ‚îÇ Samples per second             ‚îÇ\n‚îÇ sps_per_gpu ‚îÇ Samples per second (per GPU)   ‚îÇ\n‚îÇ         tps ‚îÇ Tokens per second              ‚îÇ\n‚îÇ tps_per_gpu ‚îÇ Tokens per second (per GPU)    ‚îÇ\n‚îÇ         mfu ‚îÇ Model flops utilization        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n[2025-07-23 17:07:12,567707][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:07:12,572514][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM?ZIoZo-om';-'MAhB,RcOVP!JJhhkkJnnUzI''&D&jH!ddWJJhfUUVkRhZoZ:MoJRtDjkkhhdMM'Sdd-dbUoXXLSfyXXXRb3ZOS''$!o&&jnVJ3MMkjJ'Mffe-cm..J3Oa;'$hooJ3z!jUSDn\n'DqBJtHH;!ozZIZokzoooYlMKLJm.DDmkkXRX'NnhMSccJsH;Ude.tRzDoUtm'JmCd;Jd&j'Qo&'$$DAJTPPVv&j'jjtmmtdls;wNNoooJ3$DDJ\n\n\n\n[2025-07-23 17:08:14,213943][I][wordplay/trainer:894] step=10 loss=3.28901 dt=0.388647 dtf=0.0077605 dtb=0.0102481 sps=2.57303 sps_per_gpu=2.57303 tps=21078.3 tps_per_gpu=21078.3 mfu=0.622837\n\n\n\n[2025-07-23 17:08:18,050755][I][wordplay/trainer:894] step=20 loss=2.82665 dt=0.392386 dtf=0.0123749 dtb=0.0163346 sps=2.54851 sps_per_gpu=2.54851 tps=20877.4 tps_per_gpu=20877.4 mfu=0.622244\n\n\n\n[2025-07-23 17:08:21,869708][I][wordplay/trainer:894] step=30 loss=2.64874 dt=0.379033 dtf=0.00770909 dtb=0.0103789 sps=2.6383 sps_per_gpu=2.6383 tps=21612.9 tps_per_gpu=21612.9 mfu=0.623883\n\n\n\n[2025-07-23 17:08:25,681515][I][wordplay/trainer:894] step=40 loss=2.58119 dt=0.375823 dtf=0.00982569 dtb=0.0116637 sps=2.66083 sps_per_gpu=2.66083 tps=21797.5 tps_per_gpu=21797.5 mfu=0.625904\n\n\n\n[2025-07-23 17:08:29,489842][I][wordplay/trainer:894] step=50 loss=2.5564 dt=0.381329 dtf=0.00818184 dtb=0.0101487 sps=2.6224 sps_per_gpu=2.6224 tps=21482.7 tps_per_gpu=21482.7 mfu=0.626792\n\n\n\n[2025-07-23 17:08:33,295135][I][wordplay/trainer:894] step=60 loss=2.55377 dt=0.37768 dtf=0.00809329 dtb=0.00990252 sps=2.64775 sps_per_gpu=2.64775 tps=21690.3 tps_per_gpu=21690.3 mfu=0.628205\n\n\n\n[2025-07-23 17:08:37,094848][I][wordplay/trainer:894] step=70 loss=2.53792 dt=0.37185 dtf=0.00804143 dtb=0.010255 sps=2.68926 sps_per_gpu=2.68926 tps=22030.4 tps_per_gpu=22030.4 mfu=0.630482\n\n\n\n[2025-07-23 17:08:40,894946][I][wordplay/trainer:894] step=80 loss=2.56441 dt=0.380709 dtf=0.00861202 dtb=0.0100984 sps=2.62668 sps_per_gpu=2.62668 tps=21517.8 tps_per_gpu=21517.8 mfu=0.631016\n\n\n\n[2025-07-23 17:08:44,697477][I][wordplay/trainer:894] step=90 loss=2.5338 dt=0.368932 dtf=0.00809296 dtb=0.00962644 sps=2.71053 sps_per_gpu=2.71053 tps=22204.6 tps_per_gpu=22204.6 mfu=0.633527\n\n\n\n[2025-07-23 17:08:48,500289][I][wordplay/trainer:894] step=100 loss=2.53127 dt=0.376976 dtf=0.00801782 dtb=0.0100192 sps=2.65269 sps_per_gpu=2.65269 tps=21730.8 tps_per_gpu=21730.8 mfu=0.634386\n\n\n\n[2025-07-23 17:08:49,332883][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:08:49,334601][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM?\nAREThe he anghangatr ho misen fave by the t fe wh w onk pe wns w s did s fithe s.\n\nCHather s, t be angenont ofous sts se mathan se.\n\n\nAn s tr be the acice pllll is s anontharanonte as wakar s sthe toore sthe towar thag, tin toullon llly my makndheacove t \n\n\n\n[2025-07-23 17:09:47,060965][I][wordplay/trainer:762] Saving checkpoint to: /content\n\n\n\n[2025-07-23 17:09:47,063008][I][wordplay/trainer:763] Saving model to: /content/model.pth\n\n\n\n[2025-07-23 17:09:47,414828][I][wordplay/configs:141] Appending /content to /content/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 17:09:51,240684][I][wordplay/trainer:894] step=110 loss=2.50749 dt=0.380784 dtf=0.00766138 dtb=0.0102359 sps=2.62616 sps_per_gpu=2.62616 tps=21513.5 tps_per_gpu=21513.5 mfu=0.634517\n\n\n\n[2025-07-23 17:09:55,063291][I][wordplay/trainer:894] step=120 loss=2.5274 dt=0.379459 dtf=0.00809937 dtb=0.010612 sps=2.63533 sps_per_gpu=2.63533 tps=21588.7 tps_per_gpu=21588.7 mfu=0.634857\n\n\n\n[2025-07-23 17:09:58,886616][I][wordplay/trainer:894] step=130 loss=2.54362 dt=0.380395 dtf=0.00779761 dtb=0.00998153 sps=2.62885 sps_per_gpu=2.62885 tps=21535.5 tps_per_gpu=21535.5 mfu=0.635006\n\n\n\n[2025-07-23 17:10:02,708605][I][wordplay/trainer:894] step=140 loss=2.50172 dt=0.381295 dtf=0.00778436 dtb=0.0100367 sps=2.62264 sps_per_gpu=2.62264 tps=21484.7 tps_per_gpu=21484.7 mfu=0.63499\n\n\n\n[2025-07-23 17:10:06,528915][I][wordplay/trainer:894] step=150 loss=2.50335 dt=0.373231 dtf=0.0079468 dtb=0.0108304 sps=2.67931 sps_per_gpu=2.67931 tps=21948.9 tps_per_gpu=21948.9 mfu=0.636348\n\n\n\n[2025-07-23 17:10:10,344712][I][wordplay/trainer:894] step=160 loss=2.48674 dt=0.372652 dtf=0.0117069 dtb=0.0104974 sps=2.68347 sps_per_gpu=2.68347 tps=21983 tps_per_gpu=21983 mfu=0.63767\n\n\n\n[2025-07-23 17:10:14,168118][I][wordplay/trainer:894] step=170 loss=2.47736 dt=0.380656 dtf=0.00807191 dtb=0.0106655 sps=2.62705 sps_per_gpu=2.62705 tps=21520.8 tps_per_gpu=21520.8 mfu=0.637494\n\n\n\n[2025-07-23 17:10:17,988492][I][wordplay/trainer:894] step=180 loss=2.46811 dt=0.380603 dtf=0.0078251 dtb=0.0103172 sps=2.62741 sps_per_gpu=2.62741 tps=21523.8 tps_per_gpu=21523.8 mfu=0.637345\n\n\n\n[2025-07-23 17:10:21,810169][I][wordplay/trainer:894] step=190 loss=2.45376 dt=0.381434 dtf=0.013805 dtb=0.0137897 sps=2.62169 sps_per_gpu=2.62169 tps=21476.9 tps_per_gpu=21476.9 mfu=0.637072\n\n\n\n[2025-07-23 17:10:25,634107][I][wordplay/trainer:894] step=200 loss=2.47938 dt=0.383512 dtf=0.00936293 dtb=0.0101239 sps=2.60748 sps_per_gpu=2.60748 tps=21360.5 tps_per_gpu=21360.5 mfu=0.636483\n\n\n\n[2025-07-23 17:10:26,457547][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:10:26,459401][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM?\nHLUS:\nLII hethin.\nTE: hast seatisurindo wiretyo benin tige, manens, br athetir hyors, blireriarond te me and, f llfes thes thor ists a m thives me windou,\n\n\n\nHA oulince s muce oll sse s avelo the rurd p as aver themes l neas:\nHeratho w ts the o w. thane r\n\n\n\n[2025-07-23 17:11:24,182085][I][wordplay/trainer:762] Saving checkpoint to: /content\n\n\n\n[2025-07-23 17:11:24,184071][I][wordplay/trainer:763] Saving model to: /content/model.pth\n\n\n\n[2025-07-23 17:11:24,514195][I][wordplay/configs:141] Appending /content to /content/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 17:11:28,333182][I][wordplay/trainer:894] step=210 loss=2.45724 dt=0.380321 dtf=0.00789146 dtb=0.00988756 sps=2.62936 sps_per_gpu=2.62936 tps=21539.7 tps_per_gpu=21539.7 mfu=0.636482\n\n\n\n[2025-07-23 17:11:32,159664][I][wordplay/trainer:894] step=220 loss=2.48242 dt=0.383149 dtf=0.00807603 dtb=0.0101043 sps=2.60995 sps_per_gpu=2.60995 tps=21380.7 tps_per_gpu=21380.7 mfu=0.636011\n\n\n\n[2025-07-23 17:11:35,989095][I][wordplay/trainer:894] step=230 loss=2.48992 dt=0.381508 dtf=0.00775943 dtb=0.00976974 sps=2.62117 sps_per_gpu=2.62117 tps=21472.7 tps_per_gpu=21472.7 mfu=0.635859\n\n\n\n[2025-07-23 17:11:39,818287][I][wordplay/trainer:894] step=240 loss=2.45306 dt=0.382383 dtf=0.00783342 dtb=0.0103981 sps=2.61518 sps_per_gpu=2.61518 tps=21423.5 tps_per_gpu=21423.5 mfu=0.635577\n\n\n\n[2025-07-23 17:11:43,651793][I][wordplay/trainer:894] step=250 loss=2.48512 dt=0.381244 dtf=0.00790653 dtb=0.00995927 sps=2.623 sps_per_gpu=2.623 tps=21487.6 tps_per_gpu=21487.6 mfu=0.635512\n\n\n\n[2025-07-23 17:11:47,488905][I][wordplay/trainer:894] step=260 loss=2.45921 dt=0.375016 dtf=0.0110469 dtb=0.0137554 sps=2.66655 sps_per_gpu=2.66655 tps=21844.4 tps_per_gpu=21844.4 mfu=0.636509\n\n\n\n[2025-07-23 17:11:51,323856][I][wordplay/trainer:894] step=270 loss=2.46985 dt=0.38433 dtf=0.00785675 dtb=0.0111291 sps=2.60193 sps_per_gpu=2.60193 tps=21315 tps_per_gpu=21315 mfu=0.635841\n\n\n\n[2025-07-23 17:11:55,157805][I][wordplay/trainer:894] step=280 loss=2.47304 dt=0.38265 dtf=0.00785524 dtb=0.010542 sps=2.61336 sps_per_gpu=2.61336 tps=21408.6 tps_per_gpu=21408.6 mfu=0.635517\n\n\n\n[2025-07-23 17:11:58,985311][I][wordplay/trainer:894] step=290 loss=2.4519 dt=0.38073 dtf=0.0100743 dtb=0.0128665 sps=2.62653 sps_per_gpu=2.62653 tps=21516.5 tps_per_gpu=21516.5 mfu=0.635544\n\n\n\n[2025-07-23 17:12:02,814627][I][wordplay/trainer:894] step=300 loss=2.44979 dt=0.383147 dtf=0.00804455 dtb=0.0103887 sps=2.60996 sps_per_gpu=2.60996 tps=21380.8 tps_per_gpu=21380.8 mfu=0.635167\n\n\n\n[2025-07-23 17:12:03,628924][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:12:03,630654][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM? muroursee aril icalis\n\nWe lal pl mal.\nCIO:\n\nLESTerthe coprideve, y wingrenget mir bue powin ithe an w\nAN:\nINI heshas be, intaly ws avevethay aiourofourthelin wous ans ay ber IUS:\nWh f y have s n t.\nIOLONThaventer the t at tho, I win thounepancke and find \n\n\n\n[2025-07-23 17:13:01,480227][I][wordplay/trainer:762] Saving checkpoint to: /content\n\n\n\n[2025-07-23 17:13:01,482159][I][wordplay/trainer:763] Saving model to: /content/model.pth\n\n\n\n[2025-07-23 17:13:01,816991][I][wordplay/configs:141] Appending /content to /content/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 17:13:05,641415][I][wordplay/trainer:894] step=310 loss=2.45647 dt=0.383054 dtf=0.00785093 dtb=0.00992947 sps=2.6106 sps_per_gpu=2.6106 tps=21386 tps_per_gpu=21386 mfu=0.634844\n\n\n\n[2025-07-23 17:13:09,467371][I][wordplay/trainer:894] step=320 loss=2.45905 dt=0.382875 dtf=0.0081 dtb=0.010746 sps=2.61182 sps_per_gpu=2.61182 tps=21396 tps_per_gpu=21396 mfu=0.634582\n\n\n\n[2025-07-23 17:13:13,297667][I][wordplay/trainer:894] step=330 loss=2.4555 dt=0.38572 dtf=0.0108775 dtb=0.0128777 sps=2.59256 sps_per_gpu=2.59256 tps=21238.2 tps_per_gpu=21238.2 mfu=0.63388\n\n\n\n[2025-07-23 17:13:17,131895][I][wordplay/trainer:894] step=340 loss=2.4634 dt=0.384959 dtf=0.00957926 dtb=0.010189 sps=2.59768 sps_per_gpu=2.59768 tps=21280.2 tps_per_gpu=21280.2 mfu=0.633373\n\n\n\n[2025-07-23 17:13:20,957109][I][wordplay/trainer:894] step=350 loss=2.49212 dt=0.38072 dtf=0.00796532 dtb=0.0103618 sps=2.6266 sps_per_gpu=2.6266 tps=21517.1 tps_per_gpu=21517.1 mfu=0.633616\n\n\n\n[2025-07-23 17:13:24,791303][I][wordplay/trainer:894] step=360 loss=2.42521 dt=0.380351 dtf=0.00941999 dtb=0.0131558 sps=2.62915 sps_per_gpu=2.62915 tps=21538 tps_per_gpu=21538 mfu=0.633897\n\n\n\n[2025-07-23 17:13:28,625122][I][wordplay/trainer:894] step=370 loss=2.46779 dt=0.383116 dtf=0.00759078 dtb=0.0105659 sps=2.61017 sps_per_gpu=2.61017 tps=21382.5 tps_per_gpu=21382.5 mfu=0.63369\n\n\n\n[2025-07-23 17:13:32,456066][I][wordplay/trainer:894] step=380 loss=2.46751 dt=0.384732 dtf=0.00849637 dtb=0.0100098 sps=2.59921 sps_per_gpu=2.59921 tps=21292.8 tps_per_gpu=21292.8 mfu=0.633238\n\n\n\n[2025-07-23 17:13:36,284446][I][wordplay/trainer:894] step=390 loss=2.47132 dt=0.390981 dtf=0.0104592 dtb=0.0141359 sps=2.55767 sps_per_gpu=2.55767 tps=20952.4 tps_per_gpu=20952.4 mfu=0.631826\n\n\n\n[2025-07-23 17:13:40,120231][I][wordplay/trainer:894] step=400 loss=2.50043 dt=0.382461 dtf=0.00788739 dtb=0.011582 sps=2.61465 sps_per_gpu=2.61465 tps=21419.2 tps_per_gpu=21419.2 mfu=0.631935\n\n\n\n[2025-07-23 17:13:40,955053][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:13:40,956742][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM?\nHUSUS:\nWingens thent ndd the se thof heare oupeed s te ase harot anes hant wisthe het clor m at t somy th br his s he, thanononoun heco he bong were asesonor t wearesp\n\n\n\nNUS: th ber d, ay sh thout wo pavavond ay touch the hastrd omer hes ias may perengor\n\n\n\n[2025-07-23 17:14:38,666483][I][wordplay/trainer:762] Saving checkpoint to: /content\n\n\n\n[2025-07-23 17:14:38,673966][I][wordplay/trainer:763] Saving model to: /content/model.pth\n\n\n\n[2025-07-23 17:14:39,050214][I][wordplay/configs:141] Appending /content to /content/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 17:14:42,870655][I][wordplay/trainer:894] step=410 loss=2.48579 dt=0.380458 dtf=0.00763788 dtb=0.00967759 sps=2.62841 sps_per_gpu=2.62841 tps=21531.9 tps_per_gpu=21531.9 mfu=0.632366\n\n\n\n[2025-07-23 17:14:46,696620][I][wordplay/trainer:894] step=420 loss=2.44756 dt=0.389089 dtf=0.0143081 dtb=0.0108978 sps=2.57011 sps_per_gpu=2.57011 tps=21054.3 tps_per_gpu=21054.3 mfu=0.631342\n\n\n\n[2025-07-23 17:14:50,528406][I][wordplay/trainer:894] step=430 loss=2.46498 dt=0.383404 dtf=0.0097532 dtb=0.0132017 sps=2.60821 sps_per_gpu=2.60821 tps=21366.5 tps_per_gpu=21366.5 mfu=0.631343\n\n\n\n[2025-07-23 17:14:54,360775][I][wordplay/trainer:894] step=440 loss=2.46993 dt=0.384899 dtf=0.00866323 dtb=0.0128457 sps=2.59808 sps_per_gpu=2.59808 tps=21283.5 tps_per_gpu=21283.5 mfu=0.631099\n\n\n\n[2025-07-23 17:14:58,197581][I][wordplay/trainer:894] step=450 loss=2.45371 dt=0.383754 dtf=0.00799181 dtb=0.0108706 sps=2.60584 sps_per_gpu=2.60584 tps=21347 tps_per_gpu=21347 mfu=0.631067\n\n\n\n[2025-07-23 17:15:02,033033][I][wordplay/trainer:894] step=460 loss=2.43378 dt=0.379863 dtf=0.0110734 dtb=0.0147297 sps=2.63253 sps_per_gpu=2.63253 tps=21565.6 tps_per_gpu=21565.6 mfu=0.631684\n\n\n\n[2025-07-23 17:15:05,868916][I][wordplay/trainer:894] step=470 loss=2.41934 dt=0.378727 dtf=0.00844342 dtb=0.0111405 sps=2.64043 sps_per_gpu=2.64043 tps=21630.4 tps_per_gpu=21630.4 mfu=0.632431\n\n\n\n[2025-07-23 17:15:09,703796][I][wordplay/trainer:894] step=480 loss=2.45929 dt=0.382927 dtf=0.00844033 dtb=0.0114589 sps=2.61146 sps_per_gpu=2.61146 tps=21393.1 tps_per_gpu=21393.1 mfu=0.632402\n\n\n\n[2025-07-23 17:15:13,538234][I][wordplay/trainer:894] step=490 loss=2.4835 dt=0.383195 dtf=0.0079397 dtb=0.0104966 sps=2.60964 sps_per_gpu=2.60964 tps=21378.1 tps_per_gpu=21378.1 mfu=0.632332\n\n\n\n[2025-07-23 17:15:17,374316][I][wordplay/trainer:894] step=500 loss=2.43789 dt=0.382541 dtf=0.00727845 dtb=0.0100782 sps=2.6141 sps_per_gpu=2.6141 tps=21414.7 tps_per_gpu=21414.7 mfu=0.632376\n\n\n\n\nimport time\n\nquery = \"What is an LLM?\"\nt0 = time.perf_counter()\noutputs = trainer.evaluate(\n    query,\n    num_samples=1,\n    max_new_tokens=256,\n    top_k=16,\n    display=False\n)\nlogger.info(f'took: {time.perf_counter() - t0:.4f}s')\nlogger.info(f\"['prompt']: '{query}'\")\nlogger.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n[2025-07-23 17:15:18,240721][I][tmp/ipython-input-10-1425179755:12:ezpz.log] took: 0.8133s\n\n\n\n[2025-07-23 17:15:18,242822][I][tmp/ipython-input-10-1425179755:13:ezpz.log] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:15:18,245933][I][tmp/ipython-input-10-1425179755:14:ezpz.log] ['response']:\n\nWhat is an LLM? burthilio s in o th twiser mbalilis ar sis alincore tt t mes mpresofo m whe hary ht ourighothast omy pomithe d?\n\n\n\n\nBu le wie IUTore ll ishath tes d fr irme nco s f maksere,\nIAn he ise wicouss s, areatath meangre the, my hare wis pay toth laut athe s,\nAno",
    "crumbs": [
      "[02] Large Language Models",
      "[8] Shakespeare Example (Colab)"
    ]
  },
  {
    "objectID": "02-llms/08-shakespeare-example-colab/index.html#resume-training",
    "href": "02-llms/08-shakespeare-example-colab/index.html#resume-training",
    "title": "wordplay üéÆ üí¨: Shakespeare",
    "section": "Resume Training‚Ä¶",
    "text": "Resume Training‚Ä¶\n\ntrainer.train()\n\n[2025-07-23 17:15:19,128023][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:15:19,129812][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM?\n\nPOOSTOLENETES:\nINIONEO: oft ffan yo pe hous tor ce me s here serste buthe he ase he\n\n\nNENIO:\nWhe arallin hatithofoull the, fousencay yont paris.\nPENTER:\nAn o, that s f lllle ishan be be acer se war tha pe iopre is ore nckat, me my?\n\nWI tofifre he llly po\n\n\n\n[2025-07-23 17:16:16,858986][I][wordplay/trainer:762] Saving checkpoint to: /content\n\n\n\n[2025-07-23 17:16:16,861272][I][wordplay/trainer:763] Saving model to: /content/model.pth\n\n\n\n[2025-07-23 17:16:17,190207][I][wordplay/configs:141] Appending /content to /content/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 17:16:21,016363][I][wordplay/trainer:894] step=510 loss=2.46165 dt=0.380989 dtf=0.00765307 dtb=0.0102919 sps=2.62475 sps_per_gpu=2.62475 tps=21502 tps_per_gpu=21502 mfu=0.635357\n\n\n\n[2025-07-23 17:16:24,851610][I][wordplay/trainer:894] step=520 loss=2.44981 dt=0.383659 dtf=0.00791765 dtb=0.0103253 sps=2.60648 sps_per_gpu=2.60648 tps=21352.3 tps_per_gpu=21352.3 mfu=0.634915\n\n\n\n[2025-07-23 17:16:28,687465][I][wordplay/trainer:894] step=530 loss=2.45632 dt=0.388874 dtf=0.01204 dtb=0.0159266 sps=2.57153 sps_per_gpu=2.57153 tps=21066 tps_per_gpu=21066 mfu=0.633671\n\n\n\n[2025-07-23 17:16:32,526883][I][wordplay/trainer:894] step=540 loss=2.45869 dt=0.38549 dtf=0.00823117 dtb=0.0103854 sps=2.5941 sps_per_gpu=2.5941 tps=21250.9 tps_per_gpu=21250.9 mfu=0.633098\n\n\n\n[2025-07-23 17:16:36,360809][I][wordplay/trainer:894] step=550 loss=2.44677 dt=0.385398 dtf=0.00789234 dtb=0.0121862 sps=2.59472 sps_per_gpu=2.59472 tps=21256 tps_per_gpu=21256 mfu=0.632597\n\n\n\n[2025-07-23 17:16:40,195560][I][wordplay/trainer:894] step=560 loss=2.43464 dt=0.385434 dtf=0.0106042 dtb=0.0129227 sps=2.59448 sps_per_gpu=2.59448 tps=21254 tps_per_gpu=21254 mfu=0.63214\n\n\n\n[2025-07-23 17:16:44,032374][I][wordplay/trainer:894] step=570 loss=2.45685 dt=0.382214 dtf=0.00815606 dtb=0.0103625 sps=2.61633 sps_per_gpu=2.61633 tps=21433 tps_per_gpu=21433 mfu=0.632258\n\n\n\n[2025-07-23 17:16:47,866282][I][wordplay/trainer:894] step=580 loss=2.42042 dt=0.383891 dtf=0.00803656 dtb=0.010343 sps=2.60491 sps_per_gpu=2.60491 tps=21339.4 tps_per_gpu=21339.4 mfu=0.632087\n\n\n\n[2025-07-23 17:16:51,705365][I][wordplay/trainer:894] step=590 loss=2.45867 dt=0.381508 dtf=0.0139744 dtb=0.0143725 sps=2.62118 sps_per_gpu=2.62118 tps=21472.7 tps_per_gpu=21472.7 mfu=0.632328\n\n\n\n[2025-07-23 17:16:55,543539][I][wordplay/trainer:894] step=600 loss=2.42416 dt=0.391623 dtf=0.0130454 dtb=0.0146926 sps=2.55347 sps_per_gpu=2.55347 tps=20918.1 tps_per_gpu=20918.1 mfu=0.630905\n\n\n\n[2025-07-23 17:16:56,372045][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:16:56,373698][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM?\n\n\n\nKILINGSBRK:\nYe oinot ath lord nous cke, iat ckin and;\nYor te, wad caco aver h\nTow, tom harrds, wer ow coon nalilllll th m thol s s heree, an sus alleris malatetoung ty nd mimarssin myeayelof f my bungrentind's bee and oulodo oter hendin ndind at\nIfowar\n\n\n\n[2025-07-23 17:17:54,112942][I][wordplay/trainer:762] Saving checkpoint to: /content\n\n\n\n[2025-07-23 17:17:54,116645][I][wordplay/trainer:763] Saving model to: /content/model.pth\n\n\n\n[2025-07-23 17:17:54,560498][I][wordplay/configs:141] Appending /content to /content/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 17:17:58,382321][I][wordplay/trainer:894] step=610 loss=2.40125 dt=0.380516 dtf=0.00768699 dtb=0.0103504 sps=2.62801 sps_per_gpu=2.62801 tps=21528.7 tps_per_gpu=21528.7 mfu=0.63143\n\n\n\n[2025-07-23 17:18:02,217685][I][wordplay/trainer:894] step=620 loss=2.38897 dt=0.382149 dtf=0.00761661 dtb=0.00966352 sps=2.61678 sps_per_gpu=2.61678 tps=21436.7 tps_per_gpu=21436.7 mfu=0.631629\n\n\n\n[2025-07-23 17:18:06,047977][I][wordplay/trainer:894] step=630 loss=2.38868 dt=0.378137 dtf=0.00969834 dtb=0.0128937 sps=2.64454 sps_per_gpu=2.64454 tps=21664.1 tps_per_gpu=21664.1 mfu=0.632481\n\n\n\n[2025-07-23 17:18:09,883308][I][wordplay/trainer:894] step=640 loss=2.4127 dt=0.382373 dtf=0.00796208 dtb=0.0101229 sps=2.61525 sps_per_gpu=2.61525 tps=21424.1 tps_per_gpu=21424.1 mfu=0.632539\n\n\n\n[2025-07-23 17:18:13,722090][I][wordplay/trainer:894] step=650 loss=2.41445 dt=0.385077 dtf=0.00783048 dtb=0.0110297 sps=2.59688 sps_per_gpu=2.59688 tps=21273.7 tps_per_gpu=21273.7 mfu=0.632146\n\n\n\n[2025-07-23 17:18:17,557001][I][wordplay/trainer:894] step=660 loss=2.38916 dt=0.397191 dtf=0.0126378 dtb=0.0280523 sps=2.51768 sps_per_gpu=2.51768 tps=20624.8 tps_per_gpu=20624.8 mfu=0.629875\n\n\n\n[2025-07-23 17:18:21,395377][I][wordplay/trainer:894] step=670 loss=2.40125 dt=0.37982 dtf=0.00799165 dtb=0.0102509 sps=2.63282 sps_per_gpu=2.63282 tps=21568.1 tps_per_gpu=21568.1 mfu=0.630619\n\n\n\n[2025-07-23 17:18:25,229485][I][wordplay/trainer:894] step=680 loss=2.36815 dt=0.367467 dtf=0.00798743 dtb=0.0101859 sps=2.72133 sps_per_gpu=2.72133 tps=22293.2 tps_per_gpu=22293.2 mfu=0.633431\n\n\n\n[2025-07-23 17:18:29,069577][I][wordplay/trainer:894] step=690 loss=2.40319 dt=0.379338 dtf=0.00789747 dtb=0.0107017 sps=2.63617 sps_per_gpu=2.63617 tps=21595.5 tps_per_gpu=21595.5 mfu=0.6339\n\n\n\n[2025-07-23 17:18:32,902179][I][wordplay/trainer:894] step=700 loss=2.4019 dt=0.382542 dtf=0.00746426 dtb=0.0101071 sps=2.61409 sps_per_gpu=2.61409 tps=21414.6 tps_per_gpu=21414.6 mfu=0.633787\n\n\n\n[2025-07-23 17:18:33,732336][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:18:33,734156][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM?\n\nThile than bat ton dor nong mur,\nNO belll lit lop gereing ichth ts heas fopoo l s fowis the\n\nWofores pis wiceris chithith d concofabththesthis t me t t of sis meagoury.\n\nARO:\nWhe my m bo ar f s yourel s f ther thindusolofe s m s le iserangofothin thesith\n\n\n\n[2025-07-23 17:19:31,494673][I][wordplay/trainer:762] Saving checkpoint to: /content\n\n\n\n[2025-07-23 17:19:31,499399][I][wordplay/trainer:763] Saving model to: /content/model.pth\n\n\n\n[2025-07-23 17:19:31,956960][I][wordplay/configs:141] Appending /content to /content/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 17:19:35,780584][I][wordplay/trainer:894] step=710 loss=2.41346 dt=0.378216 dtf=0.00835439 dtb=0.0104115 sps=2.64399 sps_per_gpu=2.64399 tps=21659.6 tps_per_gpu=21659.6 mfu=0.63441\n\n\n\n[2025-07-23 17:19:39,611784][I][wordplay/trainer:894] step=720 loss=2.39009 dt=0.383217 dtf=0.00772173 dtb=0.010444 sps=2.60949 sps_per_gpu=2.60949 tps=21376.9 tps_per_gpu=21376.9 mfu=0.634135\n\n\n\n[2025-07-23 17:19:43,450301][I][wordplay/trainer:894] step=730 loss=2.38395 dt=0.38477 dtf=0.0103028 dtb=0.0132564 sps=2.59896 sps_per_gpu=2.59896 tps=21290.6 tps_per_gpu=21290.6 mfu=0.633633\n\n\n\n[2025-07-23 17:19:47,286173][I][wordplay/trainer:894] step=740 loss=2.35507 dt=0.382978 dtf=0.00775962 dtb=0.00999175 sps=2.61112 sps_per_gpu=2.61112 tps=21390.3 tps_per_gpu=21390.3 mfu=0.633475\n\n\n\n[2025-07-23 17:19:51,122311][I][wordplay/trainer:894] step=750 loss=2.34116 dt=0.385881 dtf=0.00818335 dtb=0.0122375 sps=2.59147 sps_per_gpu=2.59147 tps=21229.3 tps_per_gpu=21229.3 mfu=0.632858\n\n\n\n[2025-07-23 17:19:54,958706][I][wordplay/trainer:894] step=760 loss=2.35229 dt=0.395003 dtf=0.0133316 dtb=0.0176366 sps=2.53163 sps_per_gpu=2.53163 tps=20739.1 tps_per_gpu=20739.1 mfu=0.630854\n\n\n\n[2025-07-23 17:19:58,793260][I][wordplay/trainer:894] step=770 loss=2.34521 dt=0.381653 dtf=0.00799117 dtb=0.0100162 sps=2.62018 sps_per_gpu=2.62018 tps=21464.5 tps_per_gpu=21464.5 mfu=0.631194\n\n\n\n[2025-07-23 17:20:02,627603][I][wordplay/trainer:894] step=780 loss=2.31829 dt=0.384113 dtf=0.00808119 dtb=0.0106393 sps=2.6034 sps_per_gpu=2.6034 tps=21327.1 tps_per_gpu=21327.1 mfu=0.631093\n\n\n\n[2025-07-23 17:20:06,463581][I][wordplay/trainer:894] step=790 loss=2.31021 dt=0.383535 dtf=0.00812252 dtb=0.0103508 sps=2.60732 sps_per_gpu=2.60732 tps=21359.2 tps_per_gpu=21359.2 mfu=0.631098\n\n\n\n[2025-07-23 17:20:10,293805][I][wordplay/trainer:894] step=800 loss=2.30534 dt=0.376394 dtf=0.00790557 dtb=0.0103412 sps=2.65679 sps_per_gpu=2.65679 tps=21764.4 tps_per_gpu=21764.4 mfu=0.632299\n\n\n\n[2025-07-23 17:20:11,127431][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:20:11,129261][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM?\n\nHESSTY OMy MONN:\nThe as a thestop skin cof or we or bines best busplo cothe.\n\nFORCAMPHY:\nANaracapat there t cathe dyou toraron\n\nAnd ndinis aca t t dis tir.\n\n\nSTRENIO:\nNo ano or, where my sint stthe bllos t ho sow the the,\nTise sigan t.\n\nYCLES:\nMatacou f \n\n\n\n[2025-07-23 17:21:08,807775][I][wordplay/trainer:762] Saving checkpoint to: /content\n\n\n\n[2025-07-23 17:21:08,810267][I][wordplay/trainer:763] Saving model to: /content/model.pth\n\n\n\n[2025-07-23 17:21:09,289617][I][wordplay/configs:141] Appending /content to /content/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 17:21:13,120320][I][wordplay/trainer:894] step=810 loss=2.31587 dt=0.381621 dtf=0.00781348 dtb=0.0120922 sps=2.6204 sps_per_gpu=2.6204 tps=21466.3 tps_per_gpu=21466.3 mfu=0.6325\n\n\n\n[2025-07-23 17:21:16,950119][I][wordplay/trainer:894] step=820 loss=2.32552 dt=0.378177 dtf=0.00779952 dtb=0.0102359 sps=2.64426 sps_per_gpu=2.64426 tps=21661.8 tps_per_gpu=21661.8 mfu=0.633258\n\n\n\n[2025-07-23 17:21:20,780635][I][wordplay/trainer:894] step=830 loss=2.27354 dt=0.387149 dtf=0.0106936 dtb=0.0140346 sps=2.58298 sps_per_gpu=2.58298 tps=21159.8 tps_per_gpu=21159.8 mfu=0.632457\n\n\n\n[2025-07-23 17:21:24,610506][I][wordplay/trainer:894] step=840 loss=2.26241 dt=0.383837 dtf=0.00787966 dtb=0.0108706 sps=2.60527 sps_per_gpu=2.60527 tps=21342.4 tps_per_gpu=21342.4 mfu=0.632275\n\n\n\n[2025-07-23 17:21:28,446417][I][wordplay/trainer:894] step=850 loss=2.26027 dt=0.383713 dtf=0.00800034 dtb=0.0100456 sps=2.60611 sps_per_gpu=2.60611 tps=21349.3 tps_per_gpu=21349.3 mfu=0.632132\n\n\n\n[2025-07-23 17:21:32,273517][I][wordplay/trainer:894] step=860 loss=2.25673 dt=0.382741 dtf=0.0083715 dtb=0.0101342 sps=2.61273 sps_per_gpu=2.61273 tps=21403.5 tps_per_gpu=21403.5 mfu=0.632164\n\n\n\n[2025-07-23 17:21:36,109224][I][wordplay/trainer:894] step=870 loss=2.21383 dt=0.381168 dtf=0.00781913 dtb=0.0098429 sps=2.62351 sps_per_gpu=2.62351 tps=21491.8 tps_per_gpu=21491.8 mfu=0.632453\n\n\n\n[2025-07-23 17:21:39,941412][I][wordplay/trainer:894] step=880 loss=2.21413 dt=0.380526 dtf=0.00772047 dtb=0.00999847 sps=2.62794 sps_per_gpu=2.62794 tps=21528.1 tps_per_gpu=21528.1 mfu=0.632821\n\n\n\n[2025-07-23 17:21:43,768114][I][wordplay/trainer:894] step=890 loss=2.21783 dt=0.370921 dtf=0.00774233 dtb=0.0108925 sps=2.69599 sps_per_gpu=2.69599 tps=22085.6 tps_per_gpu=22085.6 mfu=0.634799\n\n\n\n[2025-07-23 17:21:47,604118][I][wordplay/trainer:894] step=900 loss=2.20972 dt=0.389311 dtf=0.0136295 dtb=0.0109 sps=2.56864 sps_per_gpu=2.56864 tps=21042.3 tps_per_gpu=21042.3 mfu=0.633497\n\n\n\n[2025-07-23 17:21:48,462679][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:21:48,464365][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM?\n\nDURENCK:\nMe so my nou, hou ward thes ler noms he he,\nOxt my the my de is by beperd.\n\nHARY ORK:\nWhe tho su win th ars at herd pedis.\n\nKING RICHARD II:\nThat we sco arre,\nThade so frener sheran may or tot tremedonght oness.\nGLUCER:\nHe le inest soul mok, son\n\n\n\n[2025-07-23 17:22:46,157466][I][wordplay/trainer:762] Saving checkpoint to: /content\n\n\n\n[2025-07-23 17:22:46,159878][I][wordplay/trainer:763] Saving model to: /content/model.pth\n\n\n\n[2025-07-23 17:22:46,625786][I][wordplay/configs:141] Appending /content to /content/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 17:22:50,449927][I][wordplay/trainer:894] step=910 loss=2.17491 dt=0.379023 dtf=0.00754135 dtb=0.0100832 sps=2.63836 sps_per_gpu=2.63836 tps=21613.5 tps_per_gpu=21613.5 mfu=0.634012\n\n\n\n[2025-07-23 17:22:54,284268][I][wordplay/trainer:894] step=920 loss=2.1536 dt=0.383239 dtf=0.0075398 dtb=0.00996356 sps=2.60934 sps_per_gpu=2.60934 tps=21375.7 tps_per_gpu=21375.7 mfu=0.633773\n\n\n\n[2025-07-23 17:22:58,116915][I][wordplay/trainer:894] step=930 loss=2.15065 dt=0.381936 dtf=0.00785014 dtb=0.0114434 sps=2.61824 sps_per_gpu=2.61824 tps=21448.6 tps_per_gpu=21448.6 mfu=0.633774\n\n\n\n[2025-07-23 17:23:01,953658][I][wordplay/trainer:894] step=940 loss=2.12782 dt=0.38311 dtf=0.00824185 dtb=0.0105607 sps=2.61022 sps_per_gpu=2.61022 tps=21382.9 tps_per_gpu=21382.9 mfu=0.633581\n\n\n\n[2025-07-23 17:23:05,787479][I][wordplay/trainer:894] step=950 loss=2.18616 dt=0.38379 dtf=0.00788715 dtb=0.0103477 sps=2.60559 sps_per_gpu=2.60559 tps=21345 tps_per_gpu=21345 mfu=0.633295\n\n\n\n[2025-07-23 17:23:09,621436][I][wordplay/trainer:894] step=960 loss=2.11422 dt=0.384061 dtf=0.00771515 dtb=0.00979936 sps=2.60376 sps_per_gpu=2.60376 tps=21330 tps_per_gpu=21330 mfu=0.632993\n\n\n\n[2025-07-23 17:23:13,455949][I][wordplay/trainer:894] step=970 loss=2.05699 dt=0.383695 dtf=0.00807108 dtb=0.0107169 sps=2.60624 sps_per_gpu=2.60624 tps=21350.3 tps_per_gpu=21350.3 mfu=0.632781\n\n\n\n[2025-07-23 17:23:17,284032][I][wordplay/trainer:894] step=980 loss=2.15509 dt=0.376189 dtf=0.00803431 dtb=0.0109163 sps=2.65824 sps_per_gpu=2.65824 tps=21776.3 tps_per_gpu=21776.3 mfu=0.633849\n\n\n\n[2025-07-23 17:23:21,114368][I][wordplay/trainer:894] step=990 loss=2.1031 dt=0.393959 dtf=0.0123796 dtb=0.0165355 sps=2.53833 sps_per_gpu=2.53833 tps=20794 tps_per_gpu=20794 mfu=0.631908\n\n\n\n[2025-07-23 17:23:24,949152][I][wordplay/trainer:894] step=1000 loss=2.05209 dt=0.371632 dtf=0.00834119 dtb=0.0110242 sps=2.69083 sps_per_gpu=2.69083 tps=22043.3 tps_per_gpu=22043.3 mfu=0.633853\n\n\n\n[2025-07-23 17:23:25,760378][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:23:25,762149][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM?\n\n\nWAMILLY:\nAnd I tucke thimbok have doorcent mone,\nWavert mus of me the han hat the deant.\nDEORK:\nFar thall is coors sited not de ind,\nBut theat to ad coftitest fort sthengers,\nThey my thous sor was to yourte mee.\nTARK:\nI leer, men you, wit the by the the\n\n\n\n[2025-07-23 17:24:23,438968][I][wordplay/trainer:762] Saving checkpoint to: /content\n\n\n\n[2025-07-23 17:24:23,441364][I][wordplay/trainer:763] Saving model to: /content/model.pth\n\n\n\n[2025-07-23 17:24:23,918157][I][wordplay/configs:141] Appending /content to /content/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 17:24:27,738089][I][wordplay/trainer:894] step=1010 loss=2.07161 dt=0.367348 dtf=0.00773713 dtb=0.0102199 sps=2.72222 sps_per_gpu=2.72222 tps=22300.4 tps_per_gpu=22300.4 mfu=0.636362\n\n\n\n[2025-07-23 17:24:31,569441][I][wordplay/trainer:894] step=1020 loss=2.04552 dt=0.383316 dtf=0.00751949 dtb=0.01008 sps=2.60881 sps_per_gpu=2.60881 tps=21371.4 tps_per_gpu=21371.4 mfu=0.635876\n\n\n\n[2025-07-23 17:24:35,396411][I][wordplay/trainer:894] step=1030 loss=2.03231 dt=0.384257 dtf=0.00816572 dtb=0.0102516 sps=2.60243 sps_per_gpu=2.60243 tps=21319.1 tps_per_gpu=21319.1 mfu=0.635284\n\n\n\n[2025-07-23 17:24:39,228505][I][wordplay/trainer:894] step=1040 loss=2.05762 dt=0.383646 dtf=0.00790242 dtb=0.00997257 sps=2.60657 sps_per_gpu=2.60657 tps=21353 tps_per_gpu=21353 mfu=0.634851\n\n\n\n[2025-07-23 17:24:43,061324][I][wordplay/trainer:894] step=1050 loss=2.03493 dt=0.378067 dtf=0.00783631 dtb=0.00984342 sps=2.64504 sps_per_gpu=2.64504 tps=21668.1 tps_per_gpu=21668.1 mfu=0.635392\n\n\n\n[2025-07-23 17:24:46,898059][I][wordplay/trainer:894] step=1060 loss=1.99328 dt=0.383855 dtf=0.00812065 dtb=0.0102131 sps=2.60515 sps_per_gpu=2.60515 tps=21341.4 tps_per_gpu=21341.4 mfu=0.634914\n\n\n\n[2025-07-23 17:24:50,734315][I][wordplay/trainer:894] step=1070 loss=2.02538 dt=0.38352 dtf=0.00975553 dtb=0.00995462 sps=2.60743 sps_per_gpu=2.60743 tps=21360.1 tps_per_gpu=21360.1 mfu=0.634539\n\n\n\n[2025-07-23 17:24:54,571713][I][wordplay/trainer:894] step=1080 loss=1.98803 dt=0.383255 dtf=0.00790832 dtb=0.0101534 sps=2.60923 sps_per_gpu=2.60923 tps=21374.8 tps_per_gpu=21374.8 mfu=0.634245\n\n\n\n[2025-07-23 17:24:58,396586][I][wordplay/trainer:894] step=1090 loss=2.05368 dt=0.379503 dtf=0.00809327 dtb=0.0106979 sps=2.63503 sps_per_gpu=2.63503 tps=21586.1 tps_per_gpu=21586.1 mfu=0.634605\n\n\n\n[2025-07-23 17:25:02,230324][I][wordplay/trainer:894] step=1100 loss=1.99345 dt=0.386284 dtf=0.0115638 dtb=0.0162085 sps=2.58877 sps_per_gpu=2.58877 tps=21207.2 tps_per_gpu=21207.2 mfu=0.633809\n\n\n\n[2025-07-23 17:25:03,086185][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:25:03,088005][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM? Godeel we ye the live courerd, mare you the sill:\nThis bent the do we shre yeat pert\nSo but yerter the him theely?\n\nKING EDWARD IV:\nYis past whis to is witer gor miny,\nTo the corts a have could heret\nThis the the deears, so your cers tee a be.\n\nCLESTER:\nM\n\n\n\n[2025-07-23 17:26:00,773544][I][wordplay/trainer:762] Saving checkpoint to: /content\n\n\n\n[2025-07-23 17:26:00,776016][I][wordplay/trainer:763] Saving model to: /content/model.pth\n\n\n\n[2025-07-23 17:26:01,284390][I][wordplay/configs:141] Appending /content to /content/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 17:26:05,114000][I][wordplay/trainer:894] step=1110 loss=1.95124 dt=0.377492 dtf=0.00786764 dtb=0.0100368 sps=2.64906 sps_per_gpu=2.64906 tps=21701.1 tps_per_gpu=21701.1 mfu=0.634553\n\n\n\n[2025-07-23 17:26:08,948083][I][wordplay/trainer:894] step=1120 loss=1.98738 dt=0.381927 dtf=0.00748538 dtb=0.00989547 sps=2.6183 sps_per_gpu=2.6183 tps=21449.1 tps_per_gpu=21449.1 mfu=0.634477\n\n\n\n[2025-07-23 17:26:12,776837][I][wordplay/trainer:894] step=1130 loss=1.89314 dt=0.374098 dtf=0.008244 dtb=0.0108253 sps=2.67309 sps_per_gpu=2.67309 tps=21898 tps_per_gpu=21898 mfu=0.635735\n\n\n\n[2025-07-23 17:26:16,611706][I][wordplay/trainer:894] step=1140 loss=1.92855 dt=0.393585 dtf=0.0130297 dtb=0.014002 sps=2.54075 sps_per_gpu=2.54075 tps=20813.8 tps_per_gpu=20813.8 mfu=0.633664\n\n\n\n[2025-07-23 17:26:20,447771][I][wordplay/trainer:894] step=1150 loss=1.83626 dt=0.384681 dtf=0.00807674 dtb=0.0107471 sps=2.59955 sps_per_gpu=2.59955 tps=21295.6 tps_per_gpu=21295.6 mfu=0.633223\n\n\n\n[2025-07-23 17:26:24,282285][I][wordplay/trainer:894] step=1160 loss=1.90146 dt=0.383857 dtf=0.0082585 dtb=0.0104392 sps=2.60514 sps_per_gpu=2.60514 tps=21341.3 tps_per_gpu=21341.3 mfu=0.632962\n\n\n\n[2025-07-23 17:26:28,116716][I][wordplay/trainer:894] step=1170 loss=1.88228 dt=0.382931 dtf=0.00886622 dtb=0.0120676 sps=2.61144 sps_per_gpu=2.61144 tps=21392.9 tps_per_gpu=21392.9 mfu=0.632879\n\n\n\n[2025-07-23 17:26:31,951239][I][wordplay/trainer:894] step=1180 loss=1.88628 dt=0.381804 dtf=0.00750252 dtb=0.0100093 sps=2.61914 sps_per_gpu=2.61914 tps=21456 tps_per_gpu=21456 mfu=0.632991\n\n\n\n[2025-07-23 17:26:35,788827][I][wordplay/trainer:894] step=1190 loss=1.91094 dt=0.383424 dtf=0.00741282 dtb=0.0106785 sps=2.60808 sps_per_gpu=2.60808 tps=21365.4 tps_per_gpu=21365.4 mfu=0.632824\n\n\n\n[2025-07-23 17:26:39,625719][I][wordplay/trainer:894] step=1200 loss=1.90239 dt=0.388221 dtf=0.015037 dtb=0.01746 sps=2.57585 sps_per_gpu=2.57585 tps=21101.4 tps_per_gpu=21101.4 mfu=0.631894\n\n\n\n[2025-07-23 17:26:40,480918][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:26:40,482683][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM?\n\nLADY GLOUCESTES:\nAnd when there to my liker of mady the:\nIt will the shall contre fature he\nthe day thengery'd one died me meanty:\nWhy, which ime dished your wind the oblod thus hemes,\nI the conte the caition, fortuse whiches faings,\nI her far will there\n\n\n\n[2025-07-23 17:27:38,165528][I][wordplay/trainer:762] Saving checkpoint to: /content\n\n\n\n[2025-07-23 17:27:38,167913][I][wordplay/trainer:763] Saving model to: /content/model.pth\n\n\n\n[2025-07-23 17:27:38,675275][I][wordplay/configs:141] Appending /content to /content/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 17:27:42,506973][I][wordplay/trainer:894] step=1210 loss=1.83644 dt=0.379808 dtf=0.00813016 dtb=0.0106532 sps=2.63291 sps_per_gpu=2.63291 tps=21568.8 tps_per_gpu=21568.8 mfu=0.632438\n\n\n\n[2025-07-23 17:27:46,341248][I][wordplay/trainer:894] step=1220 loss=1.85 dt=0.384616 dtf=0.00783149 dtb=0.0105751 sps=2.6  sps_per_gpu=2.6  tps=21299.2 tps_per_gpu=21299.2 mfu=0.63213\n\n\n\n[2025-07-23 17:27:50,174132][I][wordplay/trainer:894] step=1230 loss=1.85794 dt=0.384468 dtf=0.00796023 dtb=0.0101979 sps=2.60099 sps_per_gpu=2.60099 tps=21307.3 tps_per_gpu=21307.3 mfu=0.631878\n\n\n\n[2025-07-23 17:27:53,996352][I][wordplay/trainer:894] step=1240 loss=1.86443 dt=0.381407 dtf=0.00777995 dtb=0.00996514 sps=2.62187 sps_per_gpu=2.62187 tps=21478.3 tps_per_gpu=21478.3 mfu=0.632156\n\n\n\n[2025-07-23 17:27:57,829111][I][wordplay/trainer:894] step=1250 loss=1.76382 dt=0.382476 dtf=0.00785835 dtb=0.0100383 sps=2.61454 sps_per_gpu=2.61454 tps=21418.3 tps_per_gpu=21418.3 mfu=0.632229\n\n\n\n[2025-07-23 17:28:01,663291][I][wordplay/trainer:894] step=1260 loss=1.74205 dt=0.385531 dtf=0.00776372 dtb=0.0138436 sps=2.59382 sps_per_gpu=2.59382 tps=21248.6 tps_per_gpu=21248.6 mfu=0.631793\n\n\n\n[2025-07-23 17:28:05,497559][I][wordplay/trainer:894] step=1270 loss=1.86381 dt=0.395746 dtf=0.0125432 dtb=0.0178912 sps=2.52688 sps_per_gpu=2.52688 tps=20700.2 tps_per_gpu=20700.2 mfu=0.62978\n\n\n\n[2025-07-23 17:28:09,331924][I][wordplay/trainer:894] step=1280 loss=1.85107 dt=0.382921 dtf=0.0081101 dtb=0.00997405 sps=2.61151 sps_per_gpu=2.61151 tps=21393.5 tps_per_gpu=21393.5 mfu=0.630017\n\n\n\n[2025-07-23 17:28:13,161160][I][wordplay/trainer:894] step=1290 loss=1.84071 dt=0.382439 dtf=0.00762057 dtb=0.0106278 sps=2.6148 sps_per_gpu=2.6148 tps=21420.4 tps_per_gpu=21420.4 mfu=0.630311\n\n\n\n[2025-07-23 17:28:16,996729][I][wordplay/trainer:894] step=1300 loss=1.82688 dt=0.383368 dtf=0.0123784 dtb=0.0184451 sps=2.60846 sps_per_gpu=2.60846 tps=21368.5 tps_per_gpu=21368.5 mfu=0.630421\n\n\n\n[2025-07-23 17:28:17,833682][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:28:17,835402][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM?\n\nGood my RICHARD III:\nHe you will distent, I may\nIs like pret to fort,\nTo some that fold my part they lok.\nA farther's to consonce which sater,\nAnd fater and him in the shall it them do her this,\nThe a my navin his more the with of haver,\nBut me and the a\n\n\n\n[2025-07-23 17:29:15,475172][I][wordplay/trainer:762] Saving checkpoint to: /content\n\n\n\n[2025-07-23 17:29:15,477113][I][wordplay/trainer:763] Saving model to: /content/model.pth\n\n\n\n[2025-07-23 17:29:15,945199][I][wordplay/configs:141] Appending /content to /content/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 17:29:19,772665][I][wordplay/trainer:894] step=1310 loss=1.83877 dt=0.380515 dtf=0.00832942 dtb=0.00994461 sps=2.62802 sps_per_gpu=2.62802 tps=21528.7 tps_per_gpu=21528.7 mfu=0.630994\n\n\n\n[2025-07-23 17:29:23,597414][I][wordplay/trainer:894] step=1320 loss=1.79997 dt=0.380789 dtf=0.00753653 dtb=0.0100344 sps=2.62613 sps_per_gpu=2.62613 tps=21513.2 tps_per_gpu=21513.2 mfu=0.631463\n\n\n\n[2025-07-23 17:29:27,425373][I][wordplay/trainer:894] step=1330 loss=1.84227 dt=0.383599 dtf=0.00811679 dtb=0.0102277 sps=2.60689 sps_per_gpu=2.60689 tps=21355.6 tps_per_gpu=21355.6 mfu=0.63142\n\n\n\n[2025-07-23 17:29:31,259289][I][wordplay/trainer:894] step=1340 loss=1.77032 dt=0.381153 dtf=0.00731168 dtb=0.00972694 sps=2.62362 sps_per_gpu=2.62362 tps=21492.7 tps_per_gpu=21492.7 mfu=0.631787\n\n\n\n[2025-07-23 17:29:35,088601][I][wordplay/trainer:894] step=1350 loss=1.8076 dt=0.384321 dtf=0.00808188 dtb=0.0116733 sps=2.60199 sps_per_gpu=2.60199 tps=21315.5 tps_per_gpu=21315.5 mfu=0.631593\n\n\n\n[2025-07-23 17:29:38,914972][I][wordplay/trainer:894] step=1360 loss=1.79383 dt=0.383019 dtf=0.00830957 dtb=0.0104623 sps=2.61084 sps_per_gpu=2.61084 tps=21388 tps_per_gpu=21388 mfu=0.631632\n\n\n\n[2025-07-23 17:29:42,746913][I][wordplay/trainer:894] step=1370 loss=1.73757 dt=0.377326 dtf=0.009339 dtb=0.0118509 sps=2.65023 sps_per_gpu=2.65023 tps=21710.7 tps_per_gpu=21710.7 mfu=0.632622\n\n\n\n[2025-07-23 17:29:46,582929][I][wordplay/trainer:894] step=1380 loss=1.74524 dt=0.373365 dtf=0.00773357 dtb=0.0100906 sps=2.67835 sps_per_gpu=2.67835 tps=21941 tps_per_gpu=21941 mfu=0.634193\n\n\n\n[2025-07-23 17:29:50,410901][I][wordplay/trainer:894] step=1390 loss=1.75995 dt=0.382166 dtf=0.00797486 dtb=0.0104627 sps=2.61667 sps_per_gpu=2.61667 tps=21435.7 tps_per_gpu=21435.7 mfu=0.634113\n\n\n\n[2025-07-23 17:29:54,241756][I][wordplay/trainer:894] step=1400 loss=1.81278 dt=0.391504 dtf=0.0126958 dtb=0.0182819 sps=2.55425 sps_per_gpu=2.55425 tps=20924.4 tps_per_gpu=20924.4 mfu=0.632531\n\n\n\n[2025-07-23 17:29:55,175194][I][wordplay/trainer:827] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:29:55,177068][I][wordplay/trainer:831] ['response']:\n\nWhat is an LLM?\n\nROHUMERS:\nCitizen:\nThe's no worth bold of I heave is the port art.\n\nSICINIUS:\nAlay, sir, thou away the perfored,\nBelie a hard set the of to your pakial;\nSirt are a a shall in thee.\nYet come, I chould cound thy king will.\n\nBRATUS:\nThe good is heart thou t\n\n\n\n[2025-07-23 17:30:52,849338][I][wordplay/trainer:762] Saving checkpoint to: /content\n\n\n\n[2025-07-23 17:30:52,851168][I][wordplay/trainer:763] Saving model to: /content/model.pth\n\n\n\n[2025-07-23 17:30:53,184812][I][wordplay/configs:141] Appending /content to /content/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 17:30:57,012134][I][wordplay/trainer:894] step=1410 loss=1.79791 dt=0.381525 dtf=0.00881983 dtb=0.0102005 sps=2.62106 sps_per_gpu=2.62106 tps=21471.7 tps_per_gpu=21471.7 mfu=0.632724\n\n\n\n[2025-07-23 17:31:00,841188][I][wordplay/trainer:894] step=1420 loss=1.74375 dt=0.381039 dtf=0.00761951 dtb=0.0101972 sps=2.6244 sps_per_gpu=2.6244 tps=21499.1 tps_per_gpu=21499.1 mfu=0.632979\n\n\n\n[2025-07-23 17:31:04,675786][I][wordplay/trainer:894] step=1430 loss=1.73401 dt=0.388151 dtf=0.00959453 dtb=0.0123491 sps=2.57631 sps_per_gpu=2.57631 tps=21105.2 tps_per_gpu=21105.2 mfu=0.632045\n\n\n\n[2025-07-23 17:31:08,511906][I][wordplay/trainer:894] step=1440 loss=1.72673 dt=0.380442 dtf=0.00765078 dtb=0.00993138 sps=2.62852 sps_per_gpu=2.62852 tps=21532.8 tps_per_gpu=21532.8 mfu=0.632467\n\n\n\n[2025-07-23 17:31:12,350823][I][wordplay/trainer:894] step=1450 loss=1.75055 dt=0.384587 dtf=0.00793686 dtb=0.0107903 sps=2.60019 sps_per_gpu=2.60019 tps=21300.8 tps_per_gpu=21300.8 mfu=0.632162\n\n\n\n[2025-07-23 17:31:16,189335][I][wordplay/trainer:894] step=1460 loss=1.68073 dt=0.381957 dtf=0.00771424 dtb=0.00991214 sps=2.6181 sps_per_gpu=2.6181 tps=21447.4 tps_per_gpu=21447.4 mfu=0.63232\n\n\n\n[2025-07-23 17:31:20,023731][I][wordplay/trainer:894] step=1470 loss=1.71749 dt=0.389038 dtf=0.0123934 dtb=0.016246 sps=2.57045 sps_per_gpu=2.57045 tps=21057.1 tps_per_gpu=21057.1 mfu=0.631309\n\n\n\n[2025-07-23 17:31:23,858642][I][wordplay/trainer:894] step=1480 loss=1.72494 dt=0.380766 dtf=0.00802833 dtb=0.0109163 sps=2.62629 sps_per_gpu=2.62629 tps=21514.5 tps_per_gpu=21514.5 mfu=0.631751\n\n\n\n[2025-07-23 17:31:27,693442][I][wordplay/trainer:894] step=1490 loss=1.72521 dt=0.384513 dtf=0.00979085 dtb=0.0104102 sps=2.60069 sps_per_gpu=2.60069 tps=21304.9 tps_per_gpu=21304.9 mfu=0.631529\n\n\n\n[2025-07-23 17:31:31,528345][I][wordplay/trainer:894] step=1500 loss=1.70409 dt=0.385203 dtf=0.0109562 dtb=0.0163935 sps=2.59604 sps_per_gpu=2.59604 tps=21266.7 tps_per_gpu=21266.7 mfu=0.631217",
    "crumbs": [
      "[02] Large Language Models",
      "[8] Shakespeare Example (Colab)"
    ]
  },
  {
    "objectID": "02-llms/08-shakespeare-example-colab/index.html#evaluate-model",
    "href": "02-llms/08-shakespeare-example-colab/index.html#evaluate-model",
    "title": "wordplay üéÆ üí¨: Shakespeare",
    "section": "Evaluate Model",
    "text": "Evaluate Model\n\nimport time\n\nquery = \"What is an LLM?\"\nt0 = time.perf_counter()\noutputs = trainer.evaluate(\n    query,\n    num_samples=1,\n    max_new_tokens=256,\n    top_k=2,\n    display=False\n)\nlogger.info(f'took: {time.perf_counter() - t0:.4f}s')\nlogger.info(f\"['prompt']: '{query}'\")\nlogger.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n[2025-07-23 17:31:32,597792][I][tmp/ipython-input-12-582817405:12:ezpz.log] took: 0.9968s\n\n\n\n[2025-07-23 17:31:32,599918][I][tmp/ipython-input-12-582817405:13:ezpz.log] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 17:31:32,601844][I][tmp/ipython-input-12-582817405:14:ezpz.log] ['response']:\n\nWhat is an LLM? What, that the wild my lord,\nAnd the shal to may so shal that the shall thee.\n\nRICHARD:\nWhat that there thee shal the const the shall so thine.\n\nRICHARD:\nThe wil thee the shal shal that that the should.\n\nRICHARD:\nThen the shal too the show shal to thee.",
    "crumbs": [
      "[02] Large Language Models",
      "[8] Shakespeare Example (Colab)"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#outline",
    "href": "02-llms/01-hands-on-llms/index.html#outline",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "Outline",
    "text": "Outline\nAlthough the name ‚Äúlanguage models‚Äù is derived from Natural Language Processing, the models used in these approaches can be applied to diverse scientific applications as illustrated below.\nDuring this session I will cover:\n\nScientific applications for language models\nGeneral overview of Transformers\nTokenization\nModel Architecture\nPipeline using HuggingFace\nModel loading",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#modeling-sequential-data",
    "href": "02-llms/01-hands-on-llms/index.html#modeling-sequential-data",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "Modeling Sequential Data",
    "text": "Modeling Sequential Data\nSequences are variable-length lists with data in subsequent iterations that depends on previous iterations (or tokens).\nMathematically:\nA sequence is a list of tokens:\nT = [T_1, T_2, T_3,...,T_N]\nwhere each token within the list depends on the others with a particular probability:\nP(t_N | t_{N-1}, ..., t_3, t_2, t_1)\nThe purpose of sequential modeling is to learn these probabilities for possible tokens in a distribution to perform various tasks including:\n\nSequence generation based on a prompt\nLanguage translation (e.g.¬†English ‚Äì&gt; French)\nProperty prediction (predicting a property based on an entire sequence)\nIdentifying mistakes or missing elements in sequential data",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#scientific-sequential-data-modeling-examples",
    "href": "02-llms/01-hands-on-llms/index.html#scientific-sequential-data-modeling-examples",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "Scientific sequential data modeling examples",
    "text": "Scientific sequential data modeling examples\n\nNucleic acid sequences + genomic data\n\n\n\n\n\n\nFigure¬†1: RNA Codons\n\n\n\nNucleic acid sequences can be used to predict translation of proteins, mutations, and gene expression levels.\nHere is an image of GenSLM. This is a language model developed by Argonne researchers that can model genomic information in a single model. It was shown to model the evolution of SARS-COV2 without expensive experiments.\n\n\n\n\n\n\nFigure¬†2: Genomic Scale Language Models (GenSLM) Zvyagin et. al 2022. BioRXiv\n\n\n\n\n\nProtein sequences\nProtein sequences can be used to predict folding structure, protein-protein interactions, chemical/binding properties, protein function and many more properties.\n\n\n\n\n\n\nFigure¬†3: Protein Structure\n\n\n\n\n\n\n\n\n\nFigure¬†4: ESMFold Lin et. al.¬†2023. Science\n\n\n\n\n\nOther applications\n\nBiomedical text\nSMILES strings\nWeather predictions\nInterfacing with simulations such as molecular dynamics simulation",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#overview-of-language-models",
    "href": "02-llms/01-hands-on-llms/index.html#overview-of-language-models",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "Overview of Language models",
    "text": "Overview of Language models\nWe will now briefly talk about the progression of language models.\n\nTransformers\nThe most common LMs base their design on the Transformer architecture that was introduced in 2017 in the ‚ÄúAttention is all you need‚Äù paper.\n\n\n\n\n\n\nFigure¬†5: Attention is all you need Vaswani 2017. Advances in Neural Information Processing Systems\n\n\n\nSince then a multitude of LLM architectures have been designed.\n\n\n\n\n\n\nFigure¬†6: Transformers, chronologically\n\n\n\nHuggingFace NLP Course",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#coding-example-of-llms-in-action",
    "href": "02-llms/01-hands-on-llms/index.html#coding-example-of-llms-in-action",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "Coding example of LLMs in action!",
    "text": "Coding example of LLMs in action!\nLet‚Äôs look at an example of running inference with a LLM as a block box to generate text given a prompt and we will also initiate a training loop for an LLM\nHere, we will use the transformers library which is as part of HuggingFace, a repository of different models, tokenizers and information on how to apply these models\n\n\n\n\n\n\nWarningü¶ú Stochastic Parrots\n\n\n\n\n\nWarning: Large Language Models are only as good as their training data.\nThey have no ethics, judgement, or editing ability.\nWe will be using some pretrained models from Hugging Face which used wide samples of internet hosted text.\nThe datasets have not been strictly filtered to restrict all malign content so the generated text may be surprisingly dark or questionable.\nThey do not reflect our core values and are only used for demonstration purposes.\n\n\n\n\n\n\nüèÉ‚Äç‚ôÇÔ∏è Running @ ALCF\n\n\n\nIf running this notebook on any of the ALCF machines, be sure to:\nimport os\nos.environ[\"HTTP_PROXY\"]=\"proxy.alcf.anl.gov:3128\"\nos.environ[\"HTTPS_PROXY\"]=\"proxy.alcf.anl.gov:3128\"\nos.environ[\"http_proxy\"]=\"proxy.alcf.anl.gov:3128\"\nos.environ[\"https_proxy\"]=\"proxy.alcf.anl.gov:3128\"\nos.environ[\"ftp_proxy\"]=\"proxy.alcf.anl.gov:3128\"",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#hands-on",
    "href": "02-llms/01-hands-on-llms/index.html#hands-on",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "üßë‚Äçüíª Hands On",
    "text": "üßë‚Äçüíª Hands On\n\n#!pip install transformers\n#!pip install pandas\n#!pip install torch\n\n\n%load_ext autoreload\n%autoreload 2",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#logging-tests",
    "href": "02-llms/01-hands-on-llms/index.html#logging-tests",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "Logging Tests",
    "text": "Logging Tests\n\nimport logging\nimport sys\n\n#logging.basicConfig(\n#    level=logging.INFO\n#    # format='[{%(filename)s:%(lineno)d} %(levelname)s - %(message)s',\n#    # handlers=[\n#    #     logging.FileHandler(filename='tmp5a.log'),\n#    #     logging.StreamHandler(sys.stdout)\n#    # ]\n#)\n\n# Test\n#logger = logging.getLogger('NORMAL')\n#logger.debug('This message should go to the log file and to the console')\n#logger.info('So should this')\n#logger.warning('And this, too')\nbasic_logger = logging.getLogger('basic')\nbasic_logger.propagate = False\nbasic_logger.addHandler(logging.StreamHandler(sys.stdout))\nbasic_logger.setLevel(\"INFO\")\nbasic_logger.info(\"Basic logger\")\n\nBasic logger\n\n\n\nimport logging\n# import ezpz\nfrom rich.console import Console\nfrom rich.logging import RichHandler\n\n\n# logging.basicConfig(level=logging.INFO, handlers=[RichHandler()])\nrich_logger = logging.getLogger(\"rich\")\nrich_logger.propagate = False\nconsole = Console(color_system=\"truecolor\", soft_wrap=True, log_time=True, log_path=False)  # , theme=ezpz.log.get_theme())\nrich_handler = RichHandler(level=\"INFO\", console=console, show_path=False, )\nrich_logger.handlers.clear()\nrich_logger.addHandler(rich_handler)\nrich_logger.setLevel(\"INFO\")\nrich_logger.info(\"Rich logger\")\n\n[07/26/25 17:19:08] INFO     Rich logger                                                                           \n\n\n\n\nfrom rich.text import Text\n\nt = Text(\"abc\")\n\n\nt\n\nabc\n\n\n\n\nfrom ezpz.log.handler import EzpzHandler\nezpz_logger = logging.getLogger(\"ezpz\")\nezpz_logger.propagate = False\nezpz_logger.handlers.clear()\n#ezpz_console = ezpz.log.get_console()\nezpz_handler = EzpzHandler()\nezpz_logger.setLevel(\"INFO\")\nezpz_logger.addHandler(ezpz_handler)\nezpz_logger.info(\"ezpz_logger\")\n\n[07/26/25 17:19:18] INFO     Setting logging level to 'INFO' on 'RANK == 0'                         __init__.py:265\n\n\n\n                    INFO     Setting logging level to 'CRITICAL' on all others 'RANK != 0'          __init__.py:266\n\n\n\n[2025-07-26 17:19:18,068891][I][ipykernel_92725/3256268672:9:ezpz]ezpz_logger                                      \n\n\n\n\ndef test_logger(logger):\n    for i in range(20):\n        logger.info(f\"i: {i}\")\n\n\nfor l in [basic_logger, rich_logger, ezpz_logger]:\n    print('\\n\\n' + 80 * '-' +'\\n\\n')\n    print(f\"Using logger={l}\")\n    test_logger(l)\n\n\n\n--------------------------------------------------------------------------------\n\n\nUsing logger=&lt;Logger basic (INFO)&gt;\ni: 0g logger=&lt;Logger basic (INFO)&gt;\n\n1g logger=&lt;Logger basic (INFO)&gt;\n\n\ni: 2ogger=&lt;Logger basic (INFO)&gt;\n\n\n\n3ogger=&lt;Logger basic (INFO)&gt;\n\n\n\n\ni: 4er=&lt;Logger basic (INFO)&gt;\n\n\n\n\n\n5er=&lt;Logger basic (INFO)&gt;\n\n\n\n\n\n\ni: 6&lt;Logger basic (INFO)&gt;\n\n\n\n\n\n\n\n7&lt;Logger basic (INFO)&gt;\n\n\n\n\n\n\n\n\ni: 8gger basic (INFO)&gt;\n\n\n\n\n\n\n\n\n\n9gger basic (INFO)&gt;\n\n\n\n\n\n\n\n\n\n\ni: 10 basic (INFO)&gt;\n\n\n\n\n\n\n\n\n\n\n\n1 basic (INFO)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\ni: 12ic (INFO)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n3ic (INFO)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ni: 14INFO)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5INFO)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ni: 16)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ni: 18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--------------------------------------------------------------------------------\n\n\nUsing logger=&lt;Logger rich (INFO)&gt;\n\n\n[07/26/25 16:27:41] INFO     i: 0                                                                                  \n\n\n\n                    INFO     i: 1                                                                                  \n\n\n\n                    INFO     i: 2                                                                                  \n\n\n\n                    INFO     i: 3                                                                                  \n\n\n\n                    INFO     i: 4                                                                                  \n\n\n\n                    INFO     i: 5                                                                                  \n\n\n\n                    INFO     i: 6                                                                                  \n\n\n\n                    INFO     i: 7                                                                                  \n\n\n\n                    INFO     i: 8                                                                                  \n\n\n\n                    INFO     i: 9                                                                                  \n\n\n\n                    INFO     i: 10                                                                                 \n\n\n\n                    INFO     i: 11                                                                                 \n\n\n\n                    INFO     i: 12                                                                                 \n\n\n\n                    INFO     i: 13                                                                                 \n\n\n\n                    INFO     i: 14                                                                                 \n\n\n\n                    INFO     i: 15                                                                                 \n\n\n\n                    INFO     i: 16                                                                                 \n\n\n\n                    INFO     i: 17                                                                                 \n\n\n\n                    INFO     i: 18                                                                                 \n\n\n\n                    INFO     i: 19                                                                                 \n\n\n\n\n\n--------------------------------------------------------------------------------\n\n\n\n\n\nUsing logger=&lt;Logger ezpz (INFO)&gt;\n\n[2025-07-26 16:27:41,548374][I][ipykernel_70809/243331313:3:ezpz] i: 0\n\n819][I][ipykernel_70809/243331313:3:ezpz] i: 1\n\n\n\n[2025-07-26 16:27:41,549277][I][ipykernel_70809/243331313:3:ezpz] i: 2\n\n818][I][ipykernel_70809/243331313:3:ezpz] i: 3\n\n\n\n[2025-07-26 16:27:41,550301][I][ipykernel_70809/243331313:3:ezpz] i: 4\n\n781][I][ipykernel_70809/243331313:3:ezpz] i: 5\n\n\n\n[2025-07-26 16:27:41,551264][I][ipykernel_70809/243331313:3:ezpz] i: 6\n\n717][I][ipykernel_70809/243331313:3:ezpz] i: 7\n\n\n\n[2025-07-26 16:27:41,552144][I][ipykernel_70809/243331313:3:ezpz] i: 8\n\n617][I][ipykernel_70809/243331313:3:ezpz] i: 9\n\n\n\n[2025-07-26 16:27:41,553104][I][ipykernel_70809/243331313:3:ezpz] i: 10\n\n608][I][ipykernel_70809/243331313:3:ezpz] i: 11\n\n\n\n[2025-07-26 16:27:41,554204][I][ipykernel_70809/243331313:3:ezpz] i: 12\n\n660][I][ipykernel_70809/243331313:3:ezpz] i: 13\n\n\n\n[2025-07-26 16:27:41,555125][I][ipykernel_70809/243331313:3:ezpz] i: 14\n\n591][I][ipykernel_70809/243331313:3:ezpz] i: 15\n\n\n\n[2025-07-26 16:27:41,555991][I][ipykernel_70809/243331313:3:ezpz] i: 16\n\n6384][I][ipykernel_70809/243331313:3:ezpz] i: 17\n\n\n\n[2025-07-26 16:27:41,556719][I][ipykernel_70809/243331313:3:ezpz] i: 18\n\n7197][I][ipykernel_70809/243331313:3:ezpz] i: 19",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#back-to-our-regularly-scheduled-program",
    "href": "02-llms/01-hands-on-llms/index.html#back-to-our-regularly-scheduled-program",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "Back to our regularly scheduled program‚Ä¶",
    "text": "Back to our regularly scheduled program‚Ä¶\n\nimport logging\nfrom ezpz.log.handler import EzpzHandler\nlogger = logging.getLogger(\"ezpz\")\nlogger.propagate = False\nlogger.handlers.clear()\nlogger.setLevel(\"INFO\")\nlogger.addHandler(EzpzHandler())\nlogger.info(\"logger\")\n\n[07/26/25 19:12:43] INFO     Setting logging level to 'INFO' on 'RANK == 0'                         __init__.py:265\n\n\n\n                    INFO     Setting logging level to 'CRITICAL' on all others 'RANK != 0'          __init__.py:266\n\n\n\n[2025-07-26 19:12:43,356366][I][ipykernel_18750/1863777421:8:ezpz]logger                                           \n\n\n\n\nimport ambivalent\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport ezpz\n#console = ezpz.log.get_console()\n# ezpz_logger = ezpz.get_logger(\"root\")\n# ezpz_logger = logging.getLogger(\"ezpz\")\n# ezpz_logger.setLevel(\"INFO\")\n# ezpz_logger.info(\"INFO\")\n#logger = ezpz.get_logger(\"root\")\n\nplt.style.use(ambivalent.STYLES['ambivalent'])\nsns.set_context(\"notebook\")\nplt.rcParams[\"figure.figsize\"] = [6.4, 4.8]\n\n\nfrom transformers import AutoTokenizer,AutoModelForCausalLM, AutoConfig\nfrom transformers import pipeline\n\ninput_text = \"What is the ALCF?\"\ngenerator = pipeline(\"text-generation\", model=\"gpt2\")\n\npipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-3B\")\noutput = generator(input_text, max_length=20, num_return_sequences=2)\n\n\n\n\n\nimport rich\nfor i, response in enumerate([i[\"generated_text\"] for i in output]):\n    # logger.info(f\"Response {i}:\\n\\t{' '.join(response.split('\\n'))}\")\n    #rich.print(f\"Response {i}:\\n\\t{' '.join(response.split('\\n'))}\")\n    logger.info(f\"Response {i}:\\n\\t{' '.join(response.split('\\n'))}\")\n#logger.info(\"\\n\".join([f\"Response {i}: {c}\"] for i, c in enumerate(outputs)))\n\n[2025-07-26 19:13:16,383194][I][ipykernel_18750/348035257:5:ezpz]Response 0:                                       \n        What is the ALCF?  The ALCF is a device that allows you to control a laptop's internal power. That power   \ncan then be transferred from your computer to the laptop (depending on the device that you use). This means that   \nyou can power up your laptop while it is in use, and it can be controlled by a variety of devices or on your mobile\nphone. In order to use the device, you will need to connect an external power source (such as a wired or wireless  \ncharger), which is installed on the laptop.  How can I turn on and off ALCF?  The ALCF is a great way to turn on or\noff ALCF. The ALCF will begin a clock cycle that will start when you turn on the computer, and will continue until \nyou stop. The ALCF will stop while the computer is on.  Can I turn on and off the computer's ALCF?  The ALCF can   \nonly be turned on and off when you are on the computer. However, the ALCF does have a few features on its side. The\nALCF can be turned on and off by just holding the ALCF key on the keyboard.  Can I use the ALCF in                 \n\n\n\n[2025-07-26 19:13:16,386237][I][ipykernel_18750/348035257:5:ezpz]Response 1:                                       \n        What is the ALCF?  ALCF is a class that assists individuals with the ALCF problem. It is a tool for the    \ndevelopment of skills and understanding of ALCF. The purpose of ALCF is to help individuals to develop a specific  \nALCF problem.  What are the main objectives of ALCF?  ALCF aims to bring together different groups of people. To do\nthis, it tries to educate and strengthen individuals with the problems of ALCF.  The major goal of ALCF is to      \nenable them to achieve their own goals.  What are the main methods for developing a problem?  It requires that the \norganization and the individual use the appropriate skills and tools.  What do the main tasks of ALCF look like?   \nAt first, ALCF has a few objectives. The first is to provide a means for developing the problem problem.  The      \nsecond is to develop new solutions to the problem problem.  The third is to achieve the goal of developing the ALCF\nproblem at a higher level. The ALCF problem can be solved in a more efficient manner.  ALCF has several other aims \nsuch as:  Developing skills/tools for the",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#whats-going-on-under-the-hood",
    "href": "02-llms/01-hands-on-llms/index.html#whats-going-on-under-the-hood",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "What‚Äôs going on under the hood?",
    "text": "What‚Äôs going on under the hood?\nThere are two components that are ‚Äúblack-boxes‚Äù here:\n\nThe method for tokenization\nThe model that generates novel text.",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#tokenization-and-embedding-of-sequential-data",
    "href": "02-llms/01-hands-on-llms/index.html#tokenization-and-embedding-of-sequential-data",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "Tokenization and embedding of sequential data",
    "text": "Tokenization and embedding of sequential data\nHumans can inherently understand language data because they previously learned phonetic sounds.\nMachines don‚Äôt have phonetic knowledge so they need to be told how to break text into standard units to process it.\nThey use a system called ‚Äútokenization‚Äù, where sequences of text are broken into smaller parts, or ‚Äútokens‚Äù, and then fed as input.\n\n\n\n\n\n\nFigure¬†7\n\n\n\nTokenization is a data preprocessing step which transforms the raw text data into a format suitable for machine learning models. Tokenizers break down raw text into smaller units called tokens. These tokens are what is fed into the language models. Based on the type and configuration of the tokenizer, these tokens can be words, subwords, or characters.\nTypes of tokenizers:\n\nCharacter Tokenizers: Split text into individual characters.\nWord Tokenizers: Split text into words based on whitespace or punctuation.\nSubword Tokenizers: Split text into subword units, such as morphemes or character n-grams. Common subword tokenization algorithms include:\n\nByte-Pair Encoding (BPE),\nSentencePiece,\nWordPiece.\n\n\n\n\n\n\n\n\nFigure¬†8\n\n\n\nnlpiation\n\nExample of tokenization\nLet‚Äôs look at an example of tokenization using byte-pair encoding.\n\nfrom transformers import AutoTokenizer\n\ndef tokenization_summary(tokenizer, sequence):\n    # get the vocabulary\n    vocab = tokenizer.vocab\n    # Number of entries to print\n    n = 10\n    # print subset of the vocabulary\n    logger.info(\"Subset of tokenizer.vocab:\")\n    for i, (token, index) in enumerate(tokenizer.vocab.items()):\n        logger.info(f\"{token}: {index}\")\n        if i &gt;= n - 1:\n            break\n    logger.info(f\"Vocab size of the tokenizer = {len(vocab)}\")\n    logger.info(\"------------------------------------------\")\n    # .tokenize chunks the existing sequence into different tokens based on the rules and vocab of the tokenizer.\n    tokens = tokenizer.tokenize(sequence)\n    logger.info(f\"Tokens : {tokens}\")\n    logger.info(\"------------------------------------------\")\n\n    # .convert_tokens_to_ids or .encode or .tokenize converts the tokens to their corresponding numerical representation.\n    #  .convert_tokens_to_ids has a 1-1 mapping between tokens and numerical representation\n    # ids = tokenizer.convert_tokens_to_ids(tokens)\n    # logger.info(\"encoded Ids: \", ids)\n\n    # .encode also adds additional information like Start of sequence tokens and End of sequene\n    logger.info(f\"tokenized sequence : {tokenizer.encode(sequence)}\")\n\n    # .tokenizer has additional information about attention_mask.\n    # encode = tokenizer(sequence)\n    # logger.info(\"Encode sequence : \", encode)\n    # logger.info(\"------------------------------------------\")\n\n    # .decode decodes the ids to raw text\n    ids = tokenizer.convert_tokens_to_ids(tokens)\n    decode = tokenizer.decode(ids)\n    logger.info(f\"Decode sequence {decode}\")\n\n\ntokenizer_1  =  AutoTokenizer.from_pretrained(\"gpt2\") # GPT-2 uses \"Byte-Pair Encoding (BPE)\"\n\nsequence = \"Counselor, please adjust your Zoom filter to appear as a human, rather than as a cat\"\n\ntokenization_summary(tokenizer_1, sequence)\n\n[2025-07-26 19:14:50,874178][I][ipykernel_18750/3808680097:9:ezpz]Subset of tokenizer.vocab:                       \n\n\n\n[2025-07-26 19:14:50,887844][I][ipykernel_18750/3808680097:11:ezpz]iates: 32820                                    \n\n\n\n[2025-07-26 19:14:50,889017][I][ipykernel_18750/3808680097:11:ezpz]ƒ†calcium: 19700                                 \n\n\n\n[2025-07-26 19:14:50,889795][I][ipykernel_18750/3808680097:11:ezpz]ƒ†buyers: 14456                                  \n\n\n\n[2025-07-26 19:14:50,890517][I][ipykernel_18750/3808680097:11:ezpz]ƒ†Ripple: 40303                                  \n\n\n\n[2025-07-26 19:14:50,891199][I][ipykernel_18750/3808680097:11:ezpz]ƒ†murdering: 33217                               \n\n\n\n[2025-07-26 19:14:50,892065][I][ipykernel_18750/3808680097:11:ezpz]ƒ†Inventory: 35772                               \n\n\n\n[2025-07-26 19:14:50,892898][I][ipykernel_18750/3808680097:11:ezpz]Pi: 38729                                       \n\n\n\n[2025-07-26 19:14:50,893530][I][ipykernel_18750/3808680097:11:ezpz]ƒ†Honolulu: 43296                                \n\n\n\n[2025-07-26 19:14:50,894128][I][ipykernel_18750/3808680097:11:ezpz]ƒ†indic: 2699                                    \n\n\n\n[2025-07-26 19:14:50,894867][I][ipykernel_18750/3808680097:11:ezpz]ƒ†Cerberus: 42593                                \n\n\n\n[2025-07-26 19:14:50,896272][I][ipykernel_18750/3808680097:14:ezpz]Vocab size of the tokenizer = 50257             \n\n\n\n[2025-07-26 19:14:50,896922][I][ipykernel_18750/3808680097:15:ezpz]------------------------------------------      \n\n\n\n[2025-07-26 19:14:50,898056][I][ipykernel_18750/3808680097:18:ezpz]Tokens : ['Coun', 'sel', 'or', ',', 'ƒ†please',  \n'ƒ†adjust', 'ƒ†your', 'ƒ†Zoom', 'ƒ†filter', 'ƒ†to', 'ƒ†appear', 'ƒ†as', 'ƒ†a', 'ƒ†human', ',', 'ƒ†rather', 'ƒ†than', 'ƒ†as',   \n'ƒ†a', 'ƒ†cat']                                                                                                      \n\n\n\n[2025-07-26 19:14:50,899146][I][ipykernel_18750/3808680097:19:ezpz]------------------------------------------      \n\n\n\n[2025-07-26 19:14:50,899998][I][ipykernel_18750/3808680097:27:ezpz]tokenized sequence : [31053, 741, 273, 11, 3387,\n4532, 534, 40305, 8106, 284, 1656, 355, 257, 1692, 11, 2138, 621, 355, 257, 3797]                                  \n\n\n\n[2025-07-26 19:14:50,901216][I][ipykernel_18750/3808680097:37:ezpz]Decode sequence Counselor, please adjust your   \nZoom filter to appear as a human, rather than as a cat                                                             \n\n\n\n\n\nToken embedding:\nWords are turned into vectors based on their location within a vocabulary.\nThe strategy of choice for learning language structure from tokenized text is to find a clever way to map each token into a moderate-dimension vector space, adjusting the mapping so that\nSimilar, or associated tokens take up residence nearby each other, and different regions of the space correspond to different position in the sequence. Such a mapping from token ID to a point in a vector space is called a token embedding. The dimension of the vector space is often high (e.g.¬†1024-dimensional), but much smaller than the vocabulary size (30,000‚Äì500,000).\nVarious approaches have been attempted for generating such embeddings, including static algorithms that operate on a corpus of tokenized data as preprocessors for NLP tasks. Transformers, however, adjust their embeddings during training.",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#transformer-model-architecture",
    "href": "02-llms/01-hands-on-llms/index.html#transformer-model-architecture",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "Transformer Model Architecture",
    "text": "Transformer Model Architecture\nNow let‚Äôs look at the base elements that make up a Transformer by dissecting the popular GPT2 model\n\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nlogger.info(model)\n\n[2025-07-26 19:15:02,094361][I][ipykernel_18750/865827860:3:ezpz]GPT2LMHeadModel(                                  \n  (transformer): GPT2Model(                                                                                        \n    (wte): Embedding(50257, 768)                                                                                   \n    (wpe): Embedding(1024, 768)                                                                                    \n    (drop): Dropout(p=0.1, inplace=False)                                                                          \n    (h): ModuleList(                                                                                               \n      (0-11): 12 x GPT2Block(                                                                                      \n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)                                              \n        (attn): GPT2Attention(                                                                                     \n          (c_attn): Conv1D(nf=2304, nx=768)                                                                        \n          (c_proj): Conv1D(nf=768, nx=768)                                                                         \n          (attn_dropout): Dropout(p=0.1, inplace=False)                                                            \n          (resid_dropout): Dropout(p=0.1, inplace=False)                                                           \n        )                                                                                                          \n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)                                              \n        (mlp): GPT2MLP(                                                                                            \n          (c_fc): Conv1D(nf=3072, nx=768)                                                                          \n          (c_proj): Conv1D(nf=768, nx=3072)                                                                        \n          (act): NewGELUActivation()                                                                               \n          (dropout): Dropout(p=0.1, inplace=False)                                                                 \n        )                                                                                                          \n      )                                                                                                            \n    )                                                                                                              \n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)                                                  \n  )                                                                                                                \n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)                                               \n)                                                                                                                  \n\n\n\nGPT2 is an example of a Transformer Decoder which is used to generate novel text.\nDecoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models. The pretraining of decoder models usually revolves around predicting the next word in the sentence.\nThese models are best suited for tasks involving text generation.\nThe architecture of GPT-2 is inspired by the paper: ‚ÄúGenerating Wikipedia by Summarizing Long Sequences‚Äù which is another arrangement of the transformer block that can do language modeling. This model threw away the encoder and thus is known as the ‚ÄúTransformer-Decoder‚Äù.\n\n\n\nIllustrated GPT2\nKey components of the transformer architecture include:\n\nInput Embeddings: Word embedding or word vectors help us represent words or text as a numeric vector where words with similar meanings have the similar representation.\nPositional Encoding: Injects information about the position of words in a sequence, helping the model understand word order.\nSelf-Attention Mechanism: Allows the model to weigh the importance of different words in a sentence, enabling it to effectively capture contextual information.\nFeedforward Neural Networks: Process information from self-attention layers to generate output for each word/token.\nLayer Normalization and Residual Connections: Aid in stabilizing training and mitigating the vanishing gradient problem.\nTransformer Blocks: Comprised of multiple layers of self-attention and feedforward neural networks, stacked together to form the model.\n\n\nAttention mechanisms\nSince attention mechanisms are arguably the most powerful component of the Transformer, let‚Äôs discuss this in a little more detail.\nSuppose the following sentence is an input sentence we want to translate using an LLM:\n‚ÄùThe animal didn't cross the street because it was too tired‚Äù\nTo understand a full sentence, the model needs to understand what each word means in relation to other words.\nFor example, when we read the sentence: ‚ÄùThe animal didn't cross the street because it was too tired‚Äù we know intuitively that the word \"it\" refers to \"animal\", the state for \"it\" is \"tired\", and the associated action is \"didn't cross\".\nHowever, the model needs a way to learn all of this information in a simple yet generalizable way. What makes Transformers particularly powerful compared to earlier sequential architectures is how it encodes context with the self-attention mechanism.\nAs the model processes each word in the input sequence, attention looks at other positions in the input sequence for clues to a better understanding for this word.\n\n\n\nThe Illustrated Transformer\n\nMulti-head attention\nIn practice, multiple attention heads are used simultaneously.\nThis: * Expands the model‚Äôs ability to focus on different positions. * Prevents the attention to be dominated by the word itself.\n\n\nLet‚Äôs see multi-head attention mechanisms in action!\nWe are going to use the powerful visualization tool bertviz, which allows an interactive experience of the attention mechanisms. Normally these mechanisms are abstracted away but this will allow us to inspect our model in more detail.\n\n!pip install bertviz\n\nLet‚Äôs load in the model, GPT2 and look at the attention mechanisms.\nHint‚Ä¶ click on the different blocks in the visualization to see the attention\n\nfrom transformers import AutoTokenizer, AutoModel, utils, AutoModelForCausalLM\n\nfrom bertviz import model_view\nutils.logging.set_verbosity_error()  # Suppress standard warnings\n\nmodel_name = 'openai-community/gpt2'\ninput_text = \"No, I am your father\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ninputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\noutputs = model(inputs)  # Run model\nattention = outputs[-1]  # Retrieve attention from model outputs\ntokens = tokenizer.convert_ids_to_tokens(inputs[0])  # Convert input ids to token strings\nmodel_view(attention, tokens)  # Display model view",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#pipeline-using-huggingface",
    "href": "02-llms/01-hands-on-llms/index.html#pipeline-using-huggingface",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "Pipeline using HuggingFace",
    "text": "Pipeline using HuggingFace\nNow, let‚Äôs see a practical application of LLMs using a HuggingFace pipeline for classification.\nThis involves a few steps including: 1. Setting up a prompt 2. Loading in a pretrained model 3. Loading in the tokenizer and tokenizing input text 4. Performing model inference 5. Interpreting inference output\n\n# STEP 0 : Installations and imports\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\nimport torch\nimport torch.nn.functional as F\n\n\n1. Setting up a prompt\nA ‚Äúprompt‚Äù refers to a specific input or query provided to a language model. They guide the text processing and generation by providing the context for the model to generate coherent and relevant text based on the given input.\nThe choice and structure of the prompt depends on the specific task, the context and desired output. Prompts can be ‚Äúdiscrete‚Äù or ‚Äúinstructive‚Äù where they are explicit instructions or questions directed to the language model. They can also be more nuanced by more providing suggestions, directions and contexts to the model.\nWe will use very simple prompts in this tutorial section, but we will learn more about prompt engineering and how it helps in optimizing the performance of the model for a given use case in the following tutorials.\n\n# STEP 1 : Set up the prompt\ninput_text = \"The panoramic view of the ocean was breathtaking.\"\n\n\n\n2. Loading Pretrained Models\nThe AutoModelForSequenceClassification from_pretrained() method instantiates a sequence classification model.\nRefer to https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodels for the list of model classes supported.\n‚Äúfrom_pretrained‚Äù method downloads the pre-trained weights from the Hugging Face Model Hub or the specified URL if the model is not already cached locally. It then loads the weights into the instantiated model, initializing the model parameters with the pre-trained values.\nThe model cache contains:\n\nmodel configuration (config.json)\npretrained model weights (model.safetensors)\ntokenizer information (tokenizer.json, vocab.json, merges.txt, tokenizer.model)\n\n\n# STEP 2 : Load the pretrained model.\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\nconfig = AutoConfig.from_pretrained(model_name)\nlogger.info(config)\n\n[2025-07-26 19:16:13,577577][I][ipykernel_18750/2259886585:5:ezpz]DistilBertConfig {                               \n  \"activation\": \"gelu\",                                                                                            \n  \"architectures\": [                                                                                               \n    \"DistilBertForSequenceClassification\"                                                                          \n  ],                                                                                                               \n  \"attention_dropout\": 0.1,                                                                                        \n  \"dim\": 768,                                                                                                      \n  \"dropout\": 0.1,                                                                                                  \n  \"finetuning_task\": \"sst-2\",                                                                                      \n  \"hidden_dim\": 3072,                                                                                              \n  \"id2label\": {                                                                                                    \n    \"0\": \"NEGATIVE\",                                                                                               \n    \"1\": \"POSITIVE\"                                                                                                \n  },                                                                                                               \n  \"initializer_range\": 0.02,                                                                                       \n  \"label2id\": {                                                                                                    \n    \"NEGATIVE\": 0,                                                                                                 \n    \"POSITIVE\": 1                                                                                                  \n  },                                                                                                               \n  \"max_position_embeddings\": 512,                                                                                  \n  \"model_type\": \"distilbert\",                                                                                      \n  \"n_heads\": 12,                                                                                                   \n  \"n_layers\": 6,                                                                                                   \n  \"output_past\": true,                                                                                             \n  \"pad_token_id\": 0,                                                                                               \n  \"qa_dropout\": 0.1,                                                                                               \n  \"seq_classif_dropout\": 0.2,                                                                                      \n  \"sinusoidal_pos_embds\": false,                                                                                   \n  \"tie_weights_\": true,                                                                                            \n  \"transformers_version\": \"4.53.3\",                                                                                \n  \"vocab_size\": 30522                                                                                              \n}                                                                                                                  \n                                                                                                                   \n\n\n\n\n\n3. Loading in the tokenizer and tokenizing input text\nHere, we load in a pretrained tokenizer associated with this model.\n\n#STEP 3 : Load the tokenizer and tokenize the input text\ntokenizer  =  AutoTokenizer.from_pretrained(model_name)\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"]\nlogger.info(input_ids)\n\n[2025-07-26 19:16:14,437902][I][ipykernel_18750/1325198429:4:ezpz]tensor([[  101,  1996,  6090,  6525,  7712,      \n3193,  1997,  1996,  4153,  2001,                                                                                  \n          3052, 17904,  1012,   102]])                                                                             \n\n\n\n\n\n4. Performing inference and interpreting\nHere, we: * load data into the model, * perform inference to obtain logits, * Convert logits into probabilities * According to probabilities assign label\nThe end result is that we can predict whether the input phrase is positive or negative.\n\n# STEP 5 : Perform inference\noutputs = model(input_ids)\nresult = outputs.logits\nlogger.info(result)\n\n# STEP 6 :  Interpret the output.\nprobabilities = F.softmax(result, dim=-1)\nlogger.info(probabilities)\npredicted_class = torch.argmax(probabilities, dim=-1).item()\nlabels = [\"NEGATIVE\", \"POSITIVE\"]\nout_string = \"[{'label': '\" + str(labels[predicted_class]) + \"', 'score': \" + str(probabilities[0][predicted_class].tolist()) + \"}]\"\nlogger.info(out_string)\n\n[2025-07-26 19:16:15,627310][I][ipykernel_18750/4091457049:4:ezpz]tensor([[-4.2767,  4.5486]],                     \ngrad_fn=&lt;AddmmBackward0&gt;)                                                                                          \n\n\n\n[2025-07-26 19:16:15,632337][I][ipykernel_18750/4091457049:8:ezpz]tensor([[1.4695e-04, 9.9985e-01]],               \ngrad_fn=&lt;SoftmaxBackward0&gt;)                                                                                        \n\n\n\n[2025-07-26 19:16:15,634736][I][ipykernel_18750/4091457049:12:ezpz][{'label': 'POSITIVE', 'score':                 \n0.9998530149459839}]                                                                                               \n\n\n\n\n\nSaving and loading models\nModel can be saved and loaded to and from a local model directory.\n\nfrom transformers import AutoModel, AutoModelForCausalLM\n\n# Instantiate and train or fine-tune a model\nmodel = AutoModelForCausalLM.from_pretrained(\"bert-base-uncased\")\n\n# Train or fine-tune the model...\n\n# Save the model to a local directory\ndirectory = \"my_local_model\"\nmodel.save_pretrained(directory)\n\n# Load a pre-trained model from a local directory\nloaded_model = AutoModel.from_pretrained(directory)",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#model-hub",
    "href": "02-llms/01-hands-on-llms/index.html#model-hub",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "Model Hub",
    "text": "Model Hub\nThe Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing.\n\nDownload pre-trained models with the huggingface_hub client library, with Transformers for fine-tuning.\nMake use of Inference API to use models in production settings.\nYou can filter for different models for different tasks, frameworks used, datasets used, and many more.\nYou can select any model, that will show the model card.\nModel card contains information of the model, including the description, usage, limitations etc. Some models also have inference API‚Äôs that can be used directly.\n\nModel Hub Link : https://huggingface.co/docs/hub/en/models-the-hub\nExample of a model card : https://huggingface.co/bert-base-uncased/tree/main",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#recommended-reading",
    "href": "02-llms/01-hands-on-llms/index.html#recommended-reading",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "Recommended reading",
    "text": "Recommended reading\n\n‚ÄúThe Illustrated Transformer‚Äù by Jay Alammar\n‚ÄúVisualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)‚Äù by Jay Alammar\n‚ÄúThe Illustrated GPT-2 (Visualizing Transformer Language Models)‚Äù\n‚ÄúA gentle introduction to positional encoding‚Äù\n‚ÄúLLM Tutorial Workshop (Argonne National Laboratory)‚Äù\n‚ÄúLLM Tutorial Workshop Part 2 (Argonne National Laboratory)‚Äù",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/01-hands-on-llms/index.html#homework",
    "href": "02-llms/01-hands-on-llms/index.html#homework",
    "title": "Hands On: Introduction to Large Language Models (LLMs)",
    "section": "Homework",
    "text": "Homework\n\nLoad in a generative model using the HuggingFace pipeline and generate text using a batch of prompts.\n\n\nPlay with generative parameters such as temperature, max_new_tokens, and the model itself and explain the effect on the legibility of the model response. Try at least 4 different parameter/model combinations.\nModels that can be used include:\n\ngoogle/gemma-2-2b-it\nmicrosoft/Phi-3-mini-4k-instruct\nmeta-llama/Llama-3.2-1B\nAny model from this list: Text-generation models\ngpt2 if having trouble loading these models in\n\nThis guide should help! Text-generation strategies\n\n\nLoad in 2 models of different parameter size (e.g.¬†GPT2, meta-llama/Llama-2-7b-chat-hf, or distilbert/distilgpt2) and analyze the BertViz for each. How does the attention mechanisms change depending on model size?",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "01-neural-networks/3-conv-nets/index.html",
    "href": "01-neural-networks/3-conv-nets/index.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "Up until transformers, convolutions were the state of the art in computer vision.\nIn many ways and applications they still are!\nLarge Language Models, which are what we‚Äôll focus on the rest of the series after this lecture, are really good at ordered, *tokenized data. But there is lots of data that isn‚Äôt implicitly ordered like images, and their more general cousins graphs.\nToday‚Äôs lecture focuses on computer vision models, and particularly on convolutional neural networks. There are a ton of applications you can do with these, and not nearly enough time to get into them. Check out the extra references file to see some publications to get you started if you want to learn more.\nTip: this notebook is much faster on the GPU!",
    "crumbs": [
      "[01] Neural Networks",
      "[3] Conv. Nets"
    ]
  },
  {
    "objectID": "01-neural-networks/3-conv-nets/index.html#convolutional-networks-a-brief-historical-context",
    "href": "01-neural-networks/3-conv-nets/index.html#convolutional-networks-a-brief-historical-context",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Networks: A brief historical context",
    "text": "Convolutional Networks: A brief historical context\nPerformance on ImageNet over time1\n\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n# settings for jupyter book: svg for html version, high-resolution png for pdf\nimport matplotlib_inline.backend_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats('retina', 'svg', 'png')\nimport matplotlib as mpl\n# mpl.rcParams['figure.dpi'] = 400\n\n\nimport ambivalent\nimport ezpz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nlogger = ezpz.get_logger(__name__)\nif logger.hasHandlers():\n    logger.handlers.clear()\n\nfor child in logger.getChildren():\n    if child.hasHandlers():\n        child.handlers.clear()\n\nlogger.propagate = True\n\nsns.set_context(\"notebook\")\nsns.set(rc={\"figure.dpi\": 400, \"savefig.dpi\": 400})\nplt.style.use(ambivalent.STYLES[\"ambivalent\"])\nplt.rcParams[\"figure.figsize\"] = [6.4, 4.8]\nplt.rcParams[\"figure.facecolor\"] = \"none\"\n\n\n[2025-07-25 15:26:34,499021][I][ezpz/__init__:265:ezpz] Setting logging level to 'INFO' on 'RANK == 0'\n\n[2025-07-25 15:26:34,500844][I][ezpz/__init__:266:ezpz] Setting logging level to 'CRITICAL' on all others 'RANK != 0'\n\n\n\n\n\n# Data\ndata = {2010: 28, 2011: 26, 2012: 16, 2013: 12, 2014: 7, 2015: 3, 2016: 2.3, 2017: 2.1}\nhuman_error_rate = 5\n\n# Create bar plot\nplt.bar(list(data.keys()), list(data.values()), color=\"blue\")\n\n# Add human error rate line\nplt.axhline(y=human_error_rate, color=\"red\", linestyle=\"--\", label=\"Human error rate\")\n\n# Labels and title\nplt.xlabel(\"Year\")\nplt.ylabel(\"ImageNet Visual Recognition Error Rate (%)\")\nplt.title(\"ImageNet Error Rates Over Time\")\nplt.legend()\n\n# Display plot\nplt.show()",
    "crumbs": [
      "[01] Neural Networks",
      "[3] Conv. Nets"
    ]
  },
  {
    "objectID": "01-neural-networks/3-conv-nets/index.html#convolutional-building-blocks",
    "href": "01-neural-networks/3-conv-nets/index.html#convolutional-building-blocks",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Building Blocks",
    "text": "Convolutional Building Blocks\n\nimport torch\nimport torchvision\n\nWe‚Äôre going to go through some examples of building blocks for convolutional networks. To help illustate some of these, let‚Äôs use an image for examples:\n\nfrom PIL import Image\n\n# wget line useful in Google Colab\n! wget https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/03_advanced_neural_networks/ALCF-Staff.jpg 2&gt;&1 &gt;/dev/null\nalcf_image = Image.open(\"ALCF-Staff.jpg\")\n\n\nfrom matplotlib import pyplot as plt\n\nfx, fy = plt.rcParamsDefault[\"figure.figsize\"]\nfigure = plt.figure(figsize=(1.5 * fx, 1.5 * fy))\n_ = plt.imshow(alcf_image)\n\n\n\n\n\n\n\n\n\nConvolutions\nConvolutions are a restriction of - and a specialization of - dense linear layers. A convolution of an image produces another image, and each output pixel is a function of only it‚Äôs local neighborhood of points. This is called an inductive bias and is a big reason why convolutions work for image data: neighboring pixels are correlated and you can operate on just those pixels at a time.\nSee examples of convolutions here\n\n\n\nimage-2.png\n\n\n\n\n\nimage.png\n\n\n\n# Let's apply a convolution to the ALCF Staff photo:\nalcf_tensor = torchvision.transforms.ToTensor()(alcf_image)\n\n# Reshape the tensor to have a batch size of 1:\nalcf_tensor = alcf_tensor.reshape((1,) + alcf_tensor.shape)\n\n# Create a random convolution:\n# shape is: (channels_in, channels_out, kernel_x, kernel_y)\nconv_random = torch.rand((3, 3, 15, 15))\n\nalcf_rand = torch.nn.functional.conv2d(alcf_tensor, conv_random)\nalcf_rand = (1.0 / alcf_rand.max()) * alcf_rand\nlogger.info(alcf_rand.shape)\nalcf_rand = alcf_rand.reshape(alcf_rand.shape[1:])\n\nlogger.info(alcf_tensor.shape)\n\nrand_image = alcf_rand.permute((1, 2, 0)).cpu()\nfx, fy = plt.rcParamsDefault[\"figure.figsize\"]\nfigure = plt.figure(figsize=(1.5 * fx, 1.5 * fy))\n_ = plt.imshow(rand_image)\n\n\n[2025-07-25 15:31:07,476785][I][ipykernel_67898/3575795754:13:__main__] torch.Size([1, 3, 1111, 1986])\n\n[2025-07-25 15:31:07,480626][I][ipykernel_67898/3575795754:16:__main__] torch.Size([1, 3, 1125, 2000])\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormalization\n Reference: Normalizations\nNormalization is the act of transforming the mean and moment of your data to standard values (usually 0.0 and 1.0). It‚Äôs particularly useful in machine learning since it stabilizes training, and allows higher learning rates.\n\n\n\nBatch Normalization accelerates training\n\n\nReference: Batch Norm\n\n# Let's apply a normalization to the ALCF Staff photo:\nalcf_tensor = torchvision.transforms.ToTensor()(alcf_image)\n\n# Reshape the tensor to have a batch size of 1:\nalcf_tensor = alcf_tensor.reshape((1,) + alcf_tensor.shape)\n\n\nalcf_rand = torch.nn.functional.normalize(alcf_tensor)\nalcf_rand = alcf_rand.reshape(alcf_rand.shape[1:])\n\nlogger.info(alcf_tensor.shape)\n\nrand_image = alcf_rand.permute((1, 2, 0)).cpu()\n\nfx, fy = plt.rcParamsDefault[\"figure.figsize\"]\nfigure = plt.figure(figsize=(1.5 * fx, 1.5 * fy))\n_ = plt.imshow(rand_image)\n\n\n[2025-07-25 15:31:11,181565][I][ipykernel_67898/378276982:11:__main__] torch.Size([1, 3, 1125, 2000])\n\n\n\n\n\n\n\n\n\n\n\n\n\nDownsampling (And upsampling)\nDownsampling is a critical component of convolutional and many vision models. Because of the local-only nature of convolutional filters, learning large-range features can be too slow for convergence. Downsampling of layers can bring information from far away closer, effectively changing what it means to be ‚Äúlocal‚Äù as the input to a convolution.\n\n\n\nConvolutional Pooling\n\n\nReference\n\n# Let's apply a normalization to the ALCF Staff photo:\nalcf_tensor = torchvision.transforms.ToTensor()(alcf_image)\n\n# Reshape the tensor to have a batch size of 1:\nalcf_tensor = alcf_tensor.reshape((1,) + alcf_tensor.shape)\n\n\nalcf_rand = torch.nn.functional.max_pool2d(alcf_tensor, 2)\nalcf_rand = alcf_rand.reshape(alcf_rand.shape[1:])\n\nlogger.info(alcf_tensor.shape)\n\nrand_image = alcf_rand.permute((1, 2, 0)).cpu()\n\nfx, fy = plt.rcParamsDefault[\"figure.figsize\"]\nfigure = plt.figure(figsize=(1.5 * fx, 1.5 * fy))\n_ = plt.imshow(rand_image)\n\n\n[2025-07-25 15:31:14,206298][I][ipykernel_67898/2241832520:11:__main__] torch.Size([1, 3, 1125, 2000])\n\n\n\n\n\n\n\n\n\n\n\n\n\nResidual Connections\nOne issue, quickly encountered when making convolutional networks deeper and deeper, is the ‚ÄúVanishing Gradients‚Äù problem. As layers were stacked on top of each other, the size of updates dimished at the earlier layers of a convolutional network. The paper ‚ÄúDeep Residual Learning for Image Recognition‚Äù solved this by introduction ‚Äúresidual connections‚Äù as skip layers.\nReference: Deep Residual Learning for Image Recognition\n\n\n\nResidual Layer\n\n\nCompare the performance of the models before and after the introduction of these layers:\n\n\n\nResnet Performance vs.¬†Plain network performance\n\n\nIf you have time to read only one paper on computer vision, make it this one! Resnet was the first model to beat human accuracy on ImageNet and is one of the most impactful papers in AI ever published.",
    "crumbs": [
      "[01] Neural Networks",
      "[3] Conv. Nets"
    ]
  },
  {
    "objectID": "01-neural-networks/3-conv-nets/index.html#building-a-convnet",
    "href": "01-neural-networks/3-conv-nets/index.html#building-a-convnet",
    "title": "Convolutional Neural Networks",
    "section": "Building a ConvNet",
    "text": "Building a ConvNet\nIn this section we‚Äôll build and apply a conv net to the mnist dataset. The layers here are loosely based off of the ConvNext architecture. Why? Because we‚Äôre getting into LLM‚Äôs soon, and this ConvNet uses LLM features. ConvNext is an update to the ResNet architecture that outperforms it.\nConvNext\nThe dataset here is CIFAR-10 - slightly harder than MNIST but still relatively easy and computationally tractable.\n\nbatch_size = 256\n\n\nfrom torchvision import transforms\n\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ],\n)\n\ntraining_data = torchvision.datasets.CIFAR10(\n    # Polaris: root=\"/lus/eagle/projects/datasets/CIFAR-10/\",\n    # Polaris: download=False,\n    root=\"data\",\n    download=True,\n    train=True,\n    transform=transform,\n)\n\ntest_data = torchvision.datasets.CIFAR10(\n    # Polaris: root=\"/lus/eagle/projects/datasets/CIFAR-10/\",\n    # Polaris: download=False,\n    root=\"data\",\n    download=True,\n    train=False,\n    transform=transform,\n    # transform=v2.Compose(\n    #     [\n    #         torchvision.transforms.ToTensor(),\n    #         v2.ConvertImageDtype(torch.bfloat16),\n    #     ]\n    # )\n)\n\ntraining_data, validation_data = torch.utils.data.random_split(\n    training_data,\n    [0.8, 0.2],\n    generator=torch.Generator().manual_seed(55)\n)\n\n\n# The dataloader makes our dataset iterable\ntrain_dataloader = torch.utils.data.DataLoader(\n    training_data,\n    batch_size=batch_size,\n    pin_memory=True,\n    shuffle=True,\n    num_workers=0,\n)\nval_dataloader = torch.utils.data.DataLoader(\n    validation_data,\n    batch_size=batch_size,\n    pin_memory=True,\n    shuffle=False,\n    num_workers=0,\n)\nclasses = (\n    'plane',\n    'car',\n    'bird',\n    'cat',\n    'deer',\n    'dog',\n    'frog',\n    'horse',\n    'ship',\n    'truck'\n)\n\n\nbatch, (X, Y) = next(enumerate(train_dataloader))\nplt.imshow(X[0].cpu().permute((1, 2, 0)))\nplt.show()\n\n\n[2025-07-25 16:03:17,843841][W][matplotlib/image:661] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.6..0.52156866].\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# get some random training images\nimages, labels = next(iter(train_dataloader))\n\nfx, fy= plt.rcParamsDefault['figure.figsize']\nfig = plt.figure(figsize=(2 * fx, 4 * fy))\n# show images\nimshow(torchvision.utils.make_grid(images))\n# print labels\nlogger.info(\"\\n\" + \" \".join(f\"{classes[labels[j]]:5s}\" for j in range(batch_size)))\n\n\n\n\n\n\n\n\n\n[2025-07-25 16:03:26,019789][I][ipykernel_67898/2917342340:18:__main__] \n\ncar   car   bird  ship  horse ship  truck horse horse deer  dog   horse truck cat   ship  frog  ship  bird  dog   horse horse deer  car   ship  plane horse bird  plane ship  deer  frog  ship  truck bird  ship  bird  ship  truck dog   horse ship  plane deer  truck cat   ship  horse dog   horse ship  bird  truck ship  frog  deer  dog   frog  plane car   car   horse dog   cat   deer  horse frog  deer  deer  deer  frog  dog   dog   frog  car   truck dog   car   cat   frog  horse frog  horse plane truck plane deer  dog   horse cat   frog  deer  cat   truck truck plane bird  cat   dog   horse car   truck deer  car   bird  truck truck car   ship  horse ship  plane dog   plane frog  deer  bird  plane ship  cat   car   cat   deer  cat   truck deer  cat   car   horse dog   horse bird  truck bird  cat   horse plane bird  car   plane horse ship  frog  cat   plane truck truck car   truck frog  horse car   cat   plane frog  horse horse cat   cat   truck bird  cat   bird  ship  ship  dog   frog  frog  deer  car   horse truck plane cat   plane deer  ship  dog   frog  frog  cat   bird  ship  dog   truck frog  dog   horse bird  deer  bird  car   dog   ship  horse ship  car   ship  deer  car   truck truck dog   deer  horse deer  plane cat   plane cat   ship  truck cat   truck cat   bird  horse cat   ship  dog   horse horse truck deer  frog  ship  plane dog   cat   car   truck horse frog  horse plane ship  truck frog  horse plane car   plane cat   dog   bird  car   ship  ship  deer  horse truck deer  horse plane deer  dog   horse\n\n\n\n\nThis code below is important as our models get bigger: this is wrapping the pytorch data loaders to put the data onto the GPU!\n\ndev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\ndef preprocess(x, y):\n    # CIFAR-10 is *color* images so 3 layers!\n    x = x.view(-1, 3, 32, 32)\n    #  y = y.to(dtype)\n    return (\n        x.to(dev),\n        y.to(dev)\n    )\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        for b in self.dl:\n            yield (self.func(*b))\n\ntrain_dataloader = WrappedDataLoader(train_dataloader, preprocess)\nval_dataloader = WrappedDataLoader(val_dataloader, preprocess)\n\n\nfrom typing import Optional\n\nfrom torch import nn\n\nclass Downsampler(nn.Module):\n    def __init__(self, in_channels, out_channels, shape, stride=2):\n        super(Downsampler, self).__init__()\n        self.norm = nn.LayerNorm([in_channels, *shape])\n        self.downsample = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=stride,\n            stride=stride,\n        )\n\n    def forward(self, inputs):\n        return self.downsample(self.norm(inputs))\n\n\nclass ConvNextBlock(nn.Module):\n    \"\"\"This block of operations is loosely based on this paper:\"\"\"\n\n    def __init__(\n        self, in_channels, shape, kernel_size: Optional[None] = None,\n    ):\n        super(ConvNextBlock, self).__init__()\n        # Depthwise, seperable convolution with a large number of output filters:\n        kernel_size = [7, 7] if kernel_size is None else kernel_size\n        self.conv1 = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=in_channels,\n            groups=in_channels,\n            kernel_size=kernel_size,\n            padding=\"same\",\n        )\n        self.norm = nn.LayerNorm([in_channels, *shape])\n        # Two more convolutions:\n        self.conv2 = nn.Conv2d(\n            in_channels=in_channels, out_channels=4 * in_channels, kernel_size=1\n        )\n        self.conv3 = nn.Conv2d(\n            in_channels=4 * in_channels, out_channels=in_channels, kernel_size=1\n        )\n\n    def forward(self, inputs):\n        x = self.conv1(inputs)\n        # The normalization layer:\n        x = self.norm(x)\n        x = self.conv2(x)\n        # The non-linear activation layer:\n        x = torch.nn.functional.gelu(x)\n        x = self.conv3(x)\n        # This makes it a residual network:\n        return x + inputs\n\n\nclass Classifier(nn.Module):\n    def __init__(\n        self,\n        n_initial_filters,\n        n_stages,\n        blocks_per_stage,\n        kernel_size: Optional[None] = None,\n    ):\n        super(Classifier, self).__init__()\n        # This is a downsampling convolution that will produce patches of output.\n        # This is similar to what vision transformers do to tokenize the images.\n        self.stem = nn.Conv2d(\n            in_channels=3,\n            out_channels=n_initial_filters,\n            kernel_size=1,\n            stride=1\n        )\n        current_shape = [32, 32]\n        self.norm1 = nn.LayerNorm([n_initial_filters, *current_shape])\n        # self.norm1 = WrappedLayerNorm()\n        current_n_filters = n_initial_filters\n        self.layers = nn.Sequential()\n        for i, n_blocks in enumerate(range(n_stages)):\n            # Add a convnext block series:\n            for _ in range(blocks_per_stage):\n                self.layers.append(\n                    ConvNextBlock(\n                        in_channels=current_n_filters,\n                        shape=current_shape,\n                        kernel_size=kernel_size,\n                    )\n                )\n            # Add a downsampling layer:\n            if i != n_stages - 1:\n                # Skip downsampling if it's the last layer!\n                self.layers.append(\n                    Downsampler(\n                        in_channels=current_n_filters,\n                        out_channels=2 * current_n_filters,\n                        shape=current_shape,\n                    )\n                )\n                # Double the number of filters:\n                current_n_filters = 2 * current_n_filters\n                # Cut the shape in half:\n                current_shape = [cs // 2 for cs in current_shape]\n        self.head = nn.Sequential(\n            nn.Flatten(),\n            nn.LayerNorm(current_n_filters),\n            nn.Linear(current_n_filters, 10),\n        )\n        # self.norm2 = nn.InstanceNorm2d(current_n_filters)\n        # # This brings it down to one channel / class\n        # self.bottleneck = nn.Conv2d(in_channels=current_n_filters, out_channels=10,\n        #                                   kernel_size=1, stride=1)\n\n    def forward(self, x):\n        x = self.stem(x)\n        # Apply a normalization after the initial patching:\n        x = self.norm1(x)\n        # Apply the main chunk of the network:\n        x = self.layers(x)\n        # Normalize and readout:\n        x = nn.functional.avg_pool2d(x, x.shape[2:])\n        x = self.head(x)\n        return x\n\n        # x = self.norm2(x)\n        # x = self.bottleneck(x)\n\n        # # Average pooling of the remaining spatial dimensions (and reshape) makes this label-like:\n        # return nn.functional.avg_pool2d(x, kernel_size=x.shape[-2:]).reshape((-1,10))\n\n\n!pip install torchinfo # if not on Polaris\n\n\nmodel = Classifier(32, 4, 2, kernel_size=(4, 4))\n#model = model.to(dtype)\nmodel.to(device=dev)\n# model.to(torch.bfloat16)\n\nfrom torchinfo import summary\n\nlogger.info(f\"\\n{summary(model, input_size=(batch_size, 3, 32, 32))}\")\n\n\n[2025-07-25 16:08:28,414533][I][ipykernel_67898/1906784419:8:__main__] \n\n==========================================================================================\n\nLayer (type:depth-idx)                   Output Shape              Param #\n\n==========================================================================================\n\nClassifier                               [256, 10]                 --\n\n‚îú‚îÄConv2d: 1-1                            [256, 32, 32, 32]         128\n\n‚îú‚îÄLayerNorm: 1-2                         [256, 32, 32, 32]         65,536\n\n‚îú‚îÄSequential: 1-3                        [256, 256, 4, 4]          --\n\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-1                [256, 32, 32, 32]         --\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-1                  [256, 32, 32, 32]         544\n\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-2               [256, 32, 32, 32]         65,536\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-3                  [256, 128, 32, 32]        4,224\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-4                  [256, 32, 32, 32]         4,128\n\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-2                [256, 32, 32, 32]         --\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-5                  [256, 32, 32, 32]         544\n\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-6               [256, 32, 32, 32]         65,536\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-7                  [256, 128, 32, 32]        4,224\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-8                  [256, 32, 32, 32]         4,128\n\n‚îÇ    ‚îî‚îÄDownsampler: 2-3                  [256, 64, 16, 16]         --\n\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-9               [256, 32, 32, 32]         65,536\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-10                 [256, 64, 16, 16]         8,256\n\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-4                [256, 64, 16, 16]         --\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-11                 [256, 64, 16, 16]         1,088\n\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-12              [256, 64, 16, 16]         32,768\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-13                 [256, 256, 16, 16]        16,640\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-14                 [256, 64, 16, 16]         16,448\n\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-5                [256, 64, 16, 16]         --\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-15                 [256, 64, 16, 16]         1,088\n\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-16              [256, 64, 16, 16]         32,768\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-17                 [256, 256, 16, 16]        16,640\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-18                 [256, 64, 16, 16]         16,448\n\n‚îÇ    ‚îî‚îÄDownsampler: 2-6                  [256, 128, 8, 8]          --\n\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-19              [256, 64, 16, 16]         32,768\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-20                 [256, 128, 8, 8]          32,896\n\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-7                [256, 128, 8, 8]          --\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-21                 [256, 128, 8, 8]          2,176\n\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-22              [256, 128, 8, 8]          16,384\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-23                 [256, 512, 8, 8]          66,048\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-24                 [256, 128, 8, 8]          65,664\n\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-8                [256, 128, 8, 8]          --\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-25                 [256, 128, 8, 8]          2,176\n\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-26              [256, 128, 8, 8]          16,384\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-27                 [256, 512, 8, 8]          66,048\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-28                 [256, 128, 8, 8]          65,664\n\n‚îÇ    ‚îî‚îÄDownsampler: 2-9                  [256, 256, 4, 4]          --\n\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-29              [256, 128, 8, 8]          16,384\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-30                 [256, 256, 4, 4]          131,328\n\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-10               [256, 256, 4, 4]          --\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-31                 [256, 256, 4, 4]          4,352\n\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-32              [256, 256, 4, 4]          8,192\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-33                 [256, 1024, 4, 4]         263,168\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-34                 [256, 256, 4, 4]          262,400\n\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-11               [256, 256, 4, 4]          --\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-35                 [256, 256, 4, 4]          4,352\n\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-36              [256, 256, 4, 4]          8,192\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-37                 [256, 1024, 4, 4]         263,168\n\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-38                 [256, 256, 4, 4]          262,400\n\n‚îú‚îÄSequential: 1-4                        [256, 10]                 --\n\n‚îÇ    ‚îî‚îÄFlatten: 2-12                     [256, 256]                --\n\n‚îÇ    ‚îî‚îÄLayerNorm: 2-13                   [256, 256]                512\n\n‚îÇ    ‚îî‚îÄLinear: 2-14                      [256, 10]                 2,570\n\n==========================================================================================\n\nTotal params: 2,015,434\n\nTrainable params: 2,015,434\n\nNon-trainable params: 0\n\nTotal mult-adds (Units.GIGABYTES): 19.63\n\n==========================================================================================\n\nInput size (MB): 3.15\n\nForward/backward pass size (MB): 2072.53\n\nParams size (MB): 8.06\n\nEstimated Total Size (MB): 2083.74\n\n==========================================================================================\n\n\n\n\n\ndef evaluate(dataloader, model, loss_fn, val_bar):\n    # Set the model to evaluation mode - some NN pieces behave differently during training\n    # Unnecessary in this situation but added for best practices\n    model.eval()\n    size = len(dataloader)\n    num_batches = len(dataloader)\n    loss, correct = 0, 0\n\n    # We can save computation and memory by not calculating gradients here - we aren't optimizing\n    with torch.no_grad():\n        # loop over all of the batches\n        for X, y in dataloader:\n            pred = model(X.to(DTYPE))\n            loss += loss_fn(pred, y).item()\n            # how many are correct in this batch? Tracking for accuracy\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n            val_bar.update()\n\n    loss /= num_batches\n    correct /= size * batch_size\n\n    accuracy = 100 * correct\n    return accuracy, loss\n\n\nimport time\nfrom torch import nn\n\nDTYPE = torch.bfloat16\nDEVICE = ezpz.get_torch_device_type()\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=2.5e-4)\n\ndef eval_step(x, y):\n    with torch.no_grad():\n        t0 = time.perf_counter()\n        pred = model(x.to(DTYPE))\n        t1 = time.perf_counter()\n        loss = loss_fn(pred, y).item()\n        correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n        t2 = time.perf_counter()\n    return {\n        \"loss\": loss,\n        \"acc\": correct / y.shape[0],\n        \"dtf\": t1 - t0,\n        \"dtm\": t2 - t1,\n    }\n\ndef train_step(x, y):\n    t0 = time.perf_counter()\n    # Forward pass\n    with torch.autocast(dtype=DTYPE, device_type=DEVICE):\n        pred = model(x.to(DTYPE))\n    loss = loss_fn(pred, y)\n    t1 = time.perf_counter()\n\n    # Backward pass\n    loss.backward()\n    t2 = time.perf_counter()\n\n    # Update weights\n    optimizer.step()\n    t3 = time.perf_counter()\n\n    # Reset gradients\n    optimizer.zero_grad()\n    t4 = time.perf_counter()\n\n    return loss.item(), {\n        \"dtf\": t1 - t0,\n        \"dtb\": t2 - t1,\n        \"dtu\": t3 - t2,\n        \"dtz\": t4 - t3,\n    }\n\n\ndef train_one_epoch(\n    dataloader, model, loss_fn, optimizer, progress_bar, history: ezpz.History | None\n):\n    model.train()\n    t0 = time.perf_counter()\n    batch_metrics = {}\n    for batch, (X, y) in enumerate(dataloader):\n        loss, metrics = train_step(x, y)\n        progress_bar.update()\n        metrics = {\"bidx\": batch, \"loss\": loss, **metrics}\n        batch_metrics[batch] = metrics\n        if history is not None:\n            logger.info(history.update(metrics))\n    t1 = time.perf_counter()\n    batch_metrics |= {\"dt_batch\": t1 - t0}\n    # if history is not None:\n    #     _ = history.update({\"dt_batch\": t1 - t0})\n    return batch_metrics\n\ndef train_one_epoch1(\n    dataloader, model, loss_fn, optimizer, progress_bar, history: ezpz.History | None\n):\n    model.train()\n    t0 = time.perf_counter()\n    batch_metrics = {}\n    for batch, (X, y) in enumerate(dataloader):\n        _t0 = time.perf_counter()\n        # forward pass\n        pred = model(X)\n        _t1 = time.perf_counter()\n        loss = loss_fn(pred, y)\n        _t2 = time.perf_counter()\n        # backward pass calculates gradients\n        loss.backward()\n        _t3 = time.perf_counter()\n        # take one step with these gradients\n        optimizer.step()\n        _t4 = time.perf_counter()\n        # resets the gradients\n        optimizer.zero_grad()\n        _t5 = time.perf_counter()\n        progress_bar.update()\n        metrics = {\n            \"bidx\": batch,\n            \"loss\": loss.item(),\n            \"dtf\": (_t1 - _t0),\n            \"dtl\": (_t2 - _t1),\n            \"dtb\": (_t3 - _t2),\n            \"dto\": (_t4 - _t3),\n            \"dtz\": (_t5 - _t4),\n        }\n        batch_metrics[batch] = metrics\n        if history is not None:\n            summary = history.update(metrics)\n    t1 = time.perf_counter()\n    batch_metrics |= {\n        \"dt_batch\": t1 - t0,\n    }\n    return batch_metrics\n\n\nmodel.to(DTYPE)\n\nClassifier(\n  (stem): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n  (norm1): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)\n  (layers): Sequential(\n    (0): ConvNextBlock(\n      (conv1): Conv2d(32, 32, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=32)\n      (norm): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)\n      (conv2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n      (conv3): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (1): ConvNextBlock(\n      (conv1): Conv2d(32, 32, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=32)\n      (norm): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)\n      (conv2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n      (conv3): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (2): Downsampler(\n      (norm): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)\n      (downsample): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (3): ConvNextBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=64)\n      (norm): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)\n      (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n      (conv3): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (4): ConvNextBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=64)\n      (norm): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)\n      (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n      (conv3): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (5): Downsampler(\n      (norm): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)\n      (downsample): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (6): ConvNextBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=128)\n      (norm): LayerNorm((128, 8, 8), eps=1e-05, elementwise_affine=True)\n      (conv2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n      (conv3): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (7): ConvNextBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=128)\n      (norm): LayerNorm((128, 8, 8), eps=1e-05, elementwise_affine=True)\n      (conv2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n      (conv3): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (8): Downsampler(\n      (norm): LayerNorm((128, 8, 8), eps=1e-05, elementwise_affine=True)\n      (downsample): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (9): ConvNextBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=256)\n      (norm): LayerNorm((256, 4, 4), eps=1e-05, elementwise_affine=True)\n      (conv2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n      (conv3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (10): ConvNextBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=256)\n      (norm): LayerNorm((256, 4, 4), eps=1e-05, elementwise_affine=True)\n      (conv2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n      (conv3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (head): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    (2): Linear(in_features=256, out_features=10, bias=True)\n  )\n)\n\n\n\n_x, _y = next(iter(val_dataloader))\nlogger.info(f\"{eval_step(_x.to(DTYPE), _y)}\")\n\n\n[2025-07-25 16:10:48,954772][I][ipykernel_67898/1550865217:2:__main__] {'loss': 2.4375, 'acc': 0.08984375, 'dtf': 1.3937692500185221, 'dtm': 0.006061082938686013}\n\n\n\n\n\nmods = list(model.modules())\nm = mods[0]\nm.layers[0].conv1.weight.dtype\n\ntorch.bfloat16\n\n\n\nimport ezpz\nfrom tqdm.notebook import tqdm\n\nhistory = ezpz.History()\nmodel.train()\nfor i in range(50):\n    t0 = time.perf_counter()\n    # with torch.autocast(dtype=dtype, device_type=ezpz.get_torch_device_type()):\n    x, y = next(iter(train_dataloader))\n    t1 = time.perf_counter()\n    loss, dt = train_step(x, y)\n    if i % 1 == 0:\n        \n        logger.info(\"\\n\" +\n            history.update(\n                {\n                    \"train/iter\": i,\n                    \"train/loss\": loss,\n                    \"train/dtd\": t1 - t0,\n                    **{f\"train/{k}\": v for k, v in dt.items()},\n                },\n            ).replace(\"/\", \".\")\n        )\n\n\n[2025-07-25 16:12:56,128498][I][ipykernel_67898/587244271:13:__main__] \n\ntrain.iter=0 train.loss=2.484375 train.dtd=0.024490 train.dtf=1.432345 train.dtb=11.115532 train.dtu=0.019517 train.dtz=0.000435\n\n[2025-07-25 16:13:07,680308][I][ipykernel_67898/587244271:13:__main__] \n\ntrain.iter=1 train.loss=2.531250 train.dtd=0.028146 train.dtf=1.602640 train.dtb=9.906646 train.dtu=0.009997 train.dtz=0.000779\n\n19,460064][I][ipykernel_67898/587244271:13:__main__] \n\ntrain.iter=1 train.loss=2.531250 train.dtd=0.028146 train.dtf=1.602640 train.dtb=9.906646 train.dtu=0.009997 train.dtz=0.000779\n\n\n\ntrain.iter=2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n[2025-07-25 16:13:31,484096][I][ipykernel_67898/587244271:13:__main__] =2.531250 train.dtd=0.028146 train.dtf=1.602640 train.dtb=9.906646 train.dtu=0.009997 train.dtz=0.000779\n\n\n\ntrain.iter=2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n42,388669][I][ipykernel_67898/587244271:13:__main__] =2.531250 train.dtd=0.028146 train.dtf=1.602640 train.dtb=9.906646 train.dtu=0.009997 train.dtz=0.000779\n\n\n\ntrain.iter=2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n[2025-07-25 16:13:53,324214][I][ipykernel_67898/587244271:13:__main__] [0m=0.028146 train.dtf=1.602640 train.dtb=9.906646 train.dtu=0.009997 train.dtz=0.000779\n\n\n\ntrain.iter=2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n4:04,443205][I][ipykernel_67898/587244271:13:__main__] [0m=0.028146 train.dtf=1.602640 train.dtb=9.906646 train.dtu=0.009997 train.dtz=0.000779\n\n\n\ntrain.iter=2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n[2025-07-25 16:14:15,397988][I][ipykernel_67898/587244271:13:__main__] =1.602640 train.dtb=9.906646 train.dtu=0.009997 train.dtz=0.000779\n\n\n\ntrain.iter=2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n26,269775][I][ipykernel_67898/587244271:13:__main__] =1.602640 train.dtb=9.906646 train.dtu=0.009997 train.dtz=0.000779\n\n\n\ntrain.iter=2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n[2025-07-25 16:14:37,302913][I][ipykernel_67898/587244271:13:__main__] [0m=9.906646 train.dtu=0.009997 train.dtz=0.000779\n\n\n\ntrain.iter=2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n48,335749][I][ipykernel_67898/587244271:13:__main__] [0m=9.906646 train.dtu=0.009997 train.dtz=0.000779\n\n\n\ntrain.iter=2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n[2025-07-25 16:14:59,458915][I][ipykernel_67898/587244271:13:__main__] 0m=0.009997 train.dtz=0.000779\n\n\n\ntrain.iter=2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n5:10,366421][I][ipykernel_67898/587244271:13:__main__] 0m=0.009997 train.dtz=0.000779\n\n\n\ntrain.iter=2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n[2025-07-25 16:15:22,336703][I][ipykernel_67898/587244271:13:__main__] [0m=0.000779\n\n\n\ntrain.iter=2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n34,200932][I][ipykernel_67898/587244271:13:__main__] [0m=0.000779\n\n\n\ntrain.iter=2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n[2025-07-25 16:15:45,068578][I][ipykernel_67898/587244271:13:__main__] =2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n56,031400][I][ipykernel_67898/587244271:13:__main__] =2 train.loss=2.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n[2025-07-25 16:16:07,229913][I][ipykernel_67898/587244271:13:__main__] [36m.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n18,246357][I][ipykernel_67898/587244271:13:__main__] [36m.390625 train.dtd=0.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n[2025-07-25 16:16:29,267349][I][ipykernel_67898/587244271:13:__main__] 36m.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n40,058906][I][ipykernel_67898/587244271:13:__main__] 36m.017980 train.dtf=1.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n[2025-07-25 16:16:50,778712][I][ipykernel_67898/587244271:13:__main__] 6m.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n7:01,794252][I][ipykernel_67898/587244271:13:__main__] 6m.301329 train.dtb=10.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n[2025-07-25 16:17:12,741823][I][ipykernel_67898/587244271:13:__main__] [36m.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n24,838310][I][ipykernel_67898/587244271:13:__main__] [36m.419813 train.dtu=0.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n[2025-07-25 16:17:36,602936][I][ipykernel_67898/587244271:13:__main__] 36m.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n47,577169][I][ipykernel_67898/587244271:13:__main__] 36m.035567 train.dtz=0.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n[2025-07-25 16:17:58,641258][I][ipykernel_67898/587244271:13:__main__] 6m.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n8:09,968734][I][ipykernel_67898/587244271:13:__main__] 6m.002696\n\n\n\ntrain.iter=3 train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n[2025-07-25 16:18:21,124662][I][ipykernel_67898/587244271:13:__main__]  train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n32,230294][I][ipykernel_67898/587244271:13:__main__]  train.loss=2.234375 train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n[2025-07-25 16:18:43,092193][I][ipykernel_67898/587244271:13:__main__]  train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n54,004146][I][ipykernel_67898/587244271:13:__main__]  train.dtd=0.023278 train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n[2025-07-25 16:19:05,031366][I][ipykernel_67898/587244271:13:__main__] train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n15,936302][I][ipykernel_67898/587244271:13:__main__] train.dtf=1.456378 train.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n[2025-07-25 16:19:27,897373][I][ipykernel_67898/587244271:13:__main__] rain.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n39,727101][I][ipykernel_67898/587244271:13:__main__] rain.dtb=10.517263 train.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n\n\ntrain.iter=36 train.loss=1.906250 train.dtd=0.015865 train.dtf=1.937747 train.dtb=9.859601 train.dtu=0.014571 train.dtz=0.000627\n\n[2025-07-25 16:19:50,602219][I][ipykernel_67898/587244271:13:__main__] rain.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n\n\ntrain.iter=36 train.loss=1.906250 train.dtd=0.015865 train.dtf=1.937747 train.dtb=9.859601 train.dtu=0.014571 train.dtz=0.000627\n\n\n\ntrain.iter=37 train.loss=1.968750 train.dtd=0.017821 train.dtf=1.378094 train.dtb=9.465782 train.dtu=0.009037 train.dtz=0.001293\n\n20:01,733536][I][ipykernel_67898/587244271:13:__main__] rain.dtu=0.013112 train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n\n\ntrain.iter=36 train.loss=1.906250 train.dtd=0.015865 train.dtf=1.937747 train.dtb=9.859601 train.dtu=0.014571 train.dtz=0.000627\n\n\n\ntrain.iter=37 train.loss=1.968750 train.dtd=0.017821 train.dtf=1.378094 train.dtb=9.465782 train.dtu=0.009037 train.dtz=0.001293\n\n\n\ntrain.iter=38 train.loss=1.898438 train.dtd=0.016044 train.dtf=1.354977 train.dtb=9.746925 train.dtu=0.010405 train.dtz=0.001879\n\n[2025-07-25 16:20:12,683577][I][ipykernel_67898/587244271:13:__main__]  train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n\n\ntrain.iter=36 train.loss=1.906250 train.dtd=0.015865 train.dtf=1.937747 train.dtb=9.859601 train.dtu=0.014571 train.dtz=0.000627\n\n\n\ntrain.iter=37 train.loss=1.968750 train.dtd=0.017821 train.dtf=1.378094 train.dtb=9.465782 train.dtu=0.009037 train.dtz=0.001293\n\n\n\ntrain.iter=38 train.loss=1.898438 train.dtd=0.016044 train.dtf=1.354977 train.dtb=9.746925 train.dtu=0.010405 train.dtz=0.001879\n\n\n\ntrain.iter=39 train.loss=1.968750 train.dtd=0.015668 train.dtf=1.364354 train.dtb=9.558674 train.dtu=0.008612 train.dtz=0.001614\n\n23,749045][I][ipykernel_67898/587244271:13:__main__]  train.dtz=0.000596\n\n\n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n\n\ntrain.iter=36 train.loss=1.906250 train.dtd=0.015865 train.dtf=1.937747 train.dtb=9.859601 train.dtu=0.014571 train.dtz=0.000627\n\n\n\ntrain.iter=37 train.loss=1.968750 train.dtd=0.017821 train.dtf=1.378094 train.dtb=9.465782 train.dtu=0.009037 train.dtz=0.001293\n\n\n\ntrain.iter=38 train.loss=1.898438 train.dtd=0.016044 train.dtf=1.354977 train.dtb=9.746925 train.dtu=0.010405 train.dtz=0.001879\n\n\n\ntrain.iter=39 train.loss=1.968750 train.dtd=0.015668 train.dtf=1.364354 train.dtb=9.558674 train.dtu=0.008612 train.dtz=0.001614\n\n\n\ntrain.iter=40 train.loss=1.953125 train.dtd=0.016078 train.dtf=1.376093 train.dtb=9.660357 train.dtu=0.010320 train.dtz=0.001362\n\n[2025-07-25 16:20:34,986814][I][ipykernel_67898/587244271:13:__main__] \n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n\n\ntrain.iter=36 train.loss=1.906250 train.dtd=0.015865 train.dtf=1.937747 train.dtb=9.859601 train.dtu=0.014571 train.dtz=0.000627\n\n\n\ntrain.iter=37 train.loss=1.968750 train.dtd=0.017821 train.dtf=1.378094 train.dtb=9.465782 train.dtu=0.009037 train.dtz=0.001293\n\n\n\ntrain.iter=38 train.loss=1.898438 train.dtd=0.016044 train.dtf=1.354977 train.dtb=9.746925 train.dtu=0.010405 train.dtz=0.001879\n\n\n\ntrain.iter=39 train.loss=1.968750 train.dtd=0.015668 train.dtf=1.364354 train.dtb=9.558674 train.dtu=0.008612 train.dtz=0.001614\n\n\n\ntrain.iter=40 train.loss=1.953125 train.dtd=0.016078 train.dtf=1.376093 train.dtb=9.660357 train.dtu=0.010320 train.dtz=0.001362\n\n\n\ntrain.iter=41 train.loss=1.945312 train.dtd=0.017464 train.dtf=1.438228 train.dtb=9.770692 train.dtu=0.009360 train.dtz=0.000157\n\n45,847946][I][ipykernel_67898/587244271:13:__main__] \n\ntrain.iter=4 train.loss=2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n\n\ntrain.iter=36 train.loss=1.906250 train.dtd=0.015865 train.dtf=1.937747 train.dtb=9.859601 train.dtu=0.014571 train.dtz=0.000627\n\n\n\ntrain.iter=37 train.loss=1.968750 train.dtd=0.017821 train.dtf=1.378094 train.dtb=9.465782 train.dtu=0.009037 train.dtz=0.001293\n\n\n\ntrain.iter=38 train.loss=1.898438 train.dtd=0.016044 train.dtf=1.354977 train.dtb=9.746925 train.dtu=0.010405 train.dtz=0.001879\n\n\n\ntrain.iter=39 train.loss=1.968750 train.dtd=0.015668 train.dtf=1.364354 train.dtb=9.558674 train.dtu=0.008612 train.dtz=0.001614\n\n\n\ntrain.iter=40 train.loss=1.953125 train.dtd=0.016078 train.dtf=1.376093 train.dtb=9.660357 train.dtu=0.010320 train.dtz=0.001362\n\n\n\ntrain.iter=41 train.loss=1.945312 train.dtd=0.017464 train.dtf=1.438228 train.dtb=9.770692 train.dtu=0.009360 train.dtz=0.000157\n\n\n\ntrain.iter=42 train.loss=1.929688 train.dtd=0.016539 train.dtf=1.368649 train.dtb=9.463489 train.dtu=0.010483 train.dtz=0.000792\n\n[2025-07-25 16:20:56,879577][I][ipykernel_67898/587244271:13:__main__] =2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n\n\ntrain.iter=36 train.loss=1.906250 train.dtd=0.015865 train.dtf=1.937747 train.dtb=9.859601 train.dtu=0.014571 train.dtz=0.000627\n\n\n\ntrain.iter=37 train.loss=1.968750 train.dtd=0.017821 train.dtf=1.378094 train.dtb=9.465782 train.dtu=0.009037 train.dtz=0.001293\n\n\n\ntrain.iter=38 train.loss=1.898438 train.dtd=0.016044 train.dtf=1.354977 train.dtb=9.746925 train.dtu=0.010405 train.dtz=0.001879\n\n\n\ntrain.iter=39 train.loss=1.968750 train.dtd=0.015668 train.dtf=1.364354 train.dtb=9.558674 train.dtu=0.008612 train.dtz=0.001614\n\n\n\ntrain.iter=40 train.loss=1.953125 train.dtd=0.016078 train.dtf=1.376093 train.dtb=9.660357 train.dtu=0.010320 train.dtz=0.001362\n\n\n\ntrain.iter=41 train.loss=1.945312 train.dtd=0.017464 train.dtf=1.438228 train.dtb=9.770692 train.dtu=0.009360 train.dtz=0.000157\n\n\n\ntrain.iter=42 train.loss=1.929688 train.dtd=0.016539 train.dtf=1.368649 train.dtb=9.463489 train.dtu=0.010483 train.dtz=0.000792\n\n\n\ntrain.iter=43 train.loss=1.898438 train.dtd=0.015760 train.dtf=1.381780 train.dtb=9.619256 train.dtu=0.012786 train.dtz=0.000868\n\n1:08,672025][I][ipykernel_67898/587244271:13:__main__] =2.234375 train.dtd=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n\n\ntrain.iter=36 train.loss=1.906250 train.dtd=0.015865 train.dtf=1.937747 train.dtb=9.859601 train.dtu=0.014571 train.dtz=0.000627\n\n\n\ntrain.iter=37 train.loss=1.968750 train.dtd=0.017821 train.dtf=1.378094 train.dtb=9.465782 train.dtu=0.009037 train.dtz=0.001293\n\n\n\ntrain.iter=38 train.loss=1.898438 train.dtd=0.016044 train.dtf=1.354977 train.dtb=9.746925 train.dtu=0.010405 train.dtz=0.001879\n\n\n\ntrain.iter=39 train.loss=1.968750 train.dtd=0.015668 train.dtf=1.364354 train.dtb=9.558674 train.dtu=0.008612 train.dtz=0.001614\n\n\n\ntrain.iter=40 train.loss=1.953125 train.dtd=0.016078 train.dtf=1.376093 train.dtb=9.660357 train.dtu=0.010320 train.dtz=0.001362\n\n\n\ntrain.iter=41 train.loss=1.945312 train.dtd=0.017464 train.dtf=1.438228 train.dtb=9.770692 train.dtu=0.009360 train.dtz=0.000157\n\n\n\ntrain.iter=42 train.loss=1.929688 train.dtd=0.016539 train.dtf=1.368649 train.dtb=9.463489 train.dtu=0.010483 train.dtz=0.000792\n\n\n\ntrain.iter=43 train.loss=1.898438 train.dtd=0.015760 train.dtf=1.381780 train.dtb=9.619256 train.dtu=0.012786 train.dtz=0.000868\n\n\n\ntrain.iter=44 train.loss=1.835938 train.dtd=0.015739 train.dtf=1.678473 train.dtb=10.078814 train.dtu=0.016395 train.dtz=0.001432\n\n[2025-07-25 16:21:20,408389][I][ipykernel_67898/587244271:13:__main__] d=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n\n\ntrain.iter=36 train.loss=1.906250 train.dtd=0.015865 train.dtf=1.937747 train.dtb=9.859601 train.dtu=0.014571 train.dtz=0.000627\n\n\n\ntrain.iter=37 train.loss=1.968750 train.dtd=0.017821 train.dtf=1.378094 train.dtb=9.465782 train.dtu=0.009037 train.dtz=0.001293\n\n\n\ntrain.iter=38 train.loss=1.898438 train.dtd=0.016044 train.dtf=1.354977 train.dtb=9.746925 train.dtu=0.010405 train.dtz=0.001879\n\n\n\ntrain.iter=39 train.loss=1.968750 train.dtd=0.015668 train.dtf=1.364354 train.dtb=9.558674 train.dtu=0.008612 train.dtz=0.001614\n\n\n\ntrain.iter=40 train.loss=1.953125 train.dtd=0.016078 train.dtf=1.376093 train.dtb=9.660357 train.dtu=0.010320 train.dtz=0.001362\n\n\n\ntrain.iter=41 train.loss=1.945312 train.dtd=0.017464 train.dtf=1.438228 train.dtb=9.770692 train.dtu=0.009360 train.dtz=0.000157\n\n\n\ntrain.iter=42 train.loss=1.929688 train.dtd=0.016539 train.dtf=1.368649 train.dtb=9.463489 train.dtu=0.010483 train.dtz=0.000792\n\n\n\ntrain.iter=43 train.loss=1.898438 train.dtd=0.015760 train.dtf=1.381780 train.dtb=9.619256 train.dtu=0.012786 train.dtz=0.000868\n\n\n\ntrain.iter=44 train.loss=1.835938 train.dtd=0.015739 train.dtf=1.678473 train.dtb=10.078814 train.dtu=0.016395 train.dtz=0.001432\n\n\n\ntrain.iter=45 train.loss=1.992188 train.dtd=0.018277 train.dtf=1.579293 train.dtb=10.125529 train.dtu=0.008637 train.dtz=0.001102\n\n33,730158][I][ipykernel_67898/587244271:13:__main__] d=0.016922 train.dtf=1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n\n\ntrain.iter=36 train.loss=1.906250 train.dtd=0.015865 train.dtf=1.937747 train.dtb=9.859601 train.dtu=0.014571 train.dtz=0.000627\n\n\n\ntrain.iter=37 train.loss=1.968750 train.dtd=0.017821 train.dtf=1.378094 train.dtb=9.465782 train.dtu=0.009037 train.dtz=0.001293\n\n\n\ntrain.iter=38 train.loss=1.898438 train.dtd=0.016044 train.dtf=1.354977 train.dtb=9.746925 train.dtu=0.010405 train.dtz=0.001879\n\n\n\ntrain.iter=39 train.loss=1.968750 train.dtd=0.015668 train.dtf=1.364354 train.dtb=9.558674 train.dtu=0.008612 train.dtz=0.001614\n\n\n\ntrain.iter=40 train.loss=1.953125 train.dtd=0.016078 train.dtf=1.376093 train.dtb=9.660357 train.dtu=0.010320 train.dtz=0.001362\n\n\n\ntrain.iter=41 train.loss=1.945312 train.dtd=0.017464 train.dtf=1.438228 train.dtb=9.770692 train.dtu=0.009360 train.dtz=0.000157\n\n\n\ntrain.iter=42 train.loss=1.929688 train.dtd=0.016539 train.dtf=1.368649 train.dtb=9.463489 train.dtu=0.010483 train.dtz=0.000792\n\n\n\ntrain.iter=43 train.loss=1.898438 train.dtd=0.015760 train.dtf=1.381780 train.dtb=9.619256 train.dtu=0.012786 train.dtz=0.000868\n\n\n\ntrain.iter=44 train.loss=1.835938 train.dtd=0.015739 train.dtf=1.678473 train.dtb=10.078814 train.dtu=0.016395 train.dtz=0.001432\n\n\n\ntrain.iter=45 train.loss=1.992188 train.dtd=0.018277 train.dtf=1.579293 train.dtb=10.125529 train.dtu=0.008637 train.dtz=0.001102\n\n\n\ntrain.iter=46 train.loss=1.929688 train.dtd=0.015585 train.dtf=1.558680 train.dtb=11.728913 train.dtu=0.015986 train.dtz=0.000877\n\n[2025-07-25 16:21:45,848456][I][ipykernel_67898/587244271:13:__main__] =1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n\n\ntrain.iter=36 train.loss=1.906250 train.dtd=0.015865 train.dtf=1.937747 train.dtb=9.859601 train.dtu=0.014571 train.dtz=0.000627\n\n\n\ntrain.iter=37 train.loss=1.968750 train.dtd=0.017821 train.dtf=1.378094 train.dtb=9.465782 train.dtu=0.009037 train.dtz=0.001293\n\n\n\ntrain.iter=38 train.loss=1.898438 train.dtd=0.016044 train.dtf=1.354977 train.dtb=9.746925 train.dtu=0.010405 train.dtz=0.001879\n\n\n\ntrain.iter=39 train.loss=1.968750 train.dtd=0.015668 train.dtf=1.364354 train.dtb=9.558674 train.dtu=0.008612 train.dtz=0.001614\n\n\n\ntrain.iter=40 train.loss=1.953125 train.dtd=0.016078 train.dtf=1.376093 train.dtb=9.660357 train.dtu=0.010320 train.dtz=0.001362\n\n\n\ntrain.iter=41 train.loss=1.945312 train.dtd=0.017464 train.dtf=1.438228 train.dtb=9.770692 train.dtu=0.009360 train.dtz=0.000157\n\n\n\ntrain.iter=42 train.loss=1.929688 train.dtd=0.016539 train.dtf=1.368649 train.dtb=9.463489 train.dtu=0.010483 train.dtz=0.000792\n\n\n\ntrain.iter=43 train.loss=1.898438 train.dtd=0.015760 train.dtf=1.381780 train.dtb=9.619256 train.dtu=0.012786 train.dtz=0.000868\n\n\n\ntrain.iter=44 train.loss=1.835938 train.dtd=0.015739 train.dtf=1.678473 train.dtb=10.078814 train.dtu=0.016395 train.dtz=0.001432\n\n\n\ntrain.iter=45 train.loss=1.992188 train.dtd=0.018277 train.dtf=1.579293 train.dtb=10.125529 train.dtu=0.008637 train.dtz=0.001102\n\n\n\ntrain.iter=46 train.loss=1.929688 train.dtd=0.015585 train.dtf=1.558680 train.dtb=11.728913 train.dtu=0.015986 train.dtz=0.000877\n\n\n\ntrain.iter=47 train.loss=1.812500 train.dtd=0.023439 train.dtf=1.832702 train.dtb=10.247002 train.dtu=0.010712 train.dtz=0.000785\n\n57,324623][I][ipykernel_67898/587244271:13:__main__] =1.299828 train.dtb=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n\n\ntrain.iter=36 train.loss=1.906250 train.dtd=0.015865 train.dtf=1.937747 train.dtb=9.859601 train.dtu=0.014571 train.dtz=0.000627\n\n\n\ntrain.iter=37 train.loss=1.968750 train.dtd=0.017821 train.dtf=1.378094 train.dtb=9.465782 train.dtu=0.009037 train.dtz=0.001293\n\n\n\ntrain.iter=38 train.loss=1.898438 train.dtd=0.016044 train.dtf=1.354977 train.dtb=9.746925 train.dtu=0.010405 train.dtz=0.001879\n\n\n\ntrain.iter=39 train.loss=1.968750 train.dtd=0.015668 train.dtf=1.364354 train.dtb=9.558674 train.dtu=0.008612 train.dtz=0.001614\n\n\n\ntrain.iter=40 train.loss=1.953125 train.dtd=0.016078 train.dtf=1.376093 train.dtb=9.660357 train.dtu=0.010320 train.dtz=0.001362\n\n\n\ntrain.iter=41 train.loss=1.945312 train.dtd=0.017464 train.dtf=1.438228 train.dtb=9.770692 train.dtu=0.009360 train.dtz=0.000157\n\n\n\ntrain.iter=42 train.loss=1.929688 train.dtd=0.016539 train.dtf=1.368649 train.dtb=9.463489 train.dtu=0.010483 train.dtz=0.000792\n\n\n\ntrain.iter=43 train.loss=1.898438 train.dtd=0.015760 train.dtf=1.381780 train.dtb=9.619256 train.dtu=0.012786 train.dtz=0.000868\n\n\n\ntrain.iter=44 train.loss=1.835938 train.dtd=0.015739 train.dtf=1.678473 train.dtb=10.078814 train.dtu=0.016395 train.dtz=0.001432\n\n\n\ntrain.iter=45 train.loss=1.992188 train.dtd=0.018277 train.dtf=1.579293 train.dtb=10.125529 train.dtu=0.008637 train.dtz=0.001102\n\n\n\ntrain.iter=46 train.loss=1.929688 train.dtd=0.015585 train.dtf=1.558680 train.dtb=11.728913 train.dtu=0.015986 train.dtz=0.000877\n\n\n\ntrain.iter=47 train.loss=1.812500 train.dtd=0.023439 train.dtf=1.832702 train.dtb=10.247002 train.dtu=0.010712 train.dtz=0.000785\n\n\n\ntrain.iter=48 train.loss=1.921875 train.dtd=0.017242 train.dtf=1.450544 train.dtb=9.988680 train.dtu=0.016540 train.dtz=0.001258\n\n[2025-07-25 16:22:09,123239][I][ipykernel_67898/587244271:13:__main__] [0m=9.575038 train.dtu=0.009346 train.dtz=0.001203\n\n\n\ntrain.iter=5 train.loss=2.234375 train.dtd=0.017019 train.dtf=1.308216 train.dtb=9.597561 train.dtu=0.010874 train.dtz=0.000742\n\n\n\ntrain.iter=6 train.loss=2.171875 train.dtd=0.015302 train.dtf=1.309560 train.dtb=9.780431 train.dtu=0.011015 train.dtz=0.001556\n\n\n\ntrain.iter=7 train.loss=2.187500 train.dtd=0.017555 train.dtf=1.306062 train.dtb=9.618384 train.dtu=0.010146 train.dtz=0.001433\n\n\n\ntrain.iter=8 train.loss=2.171875 train.dtd=0.016442 train.dtf=1.302922 train.dtb=9.540729 train.dtu=0.009466 train.dtz=0.000812\n\n\n\ntrain.iter=9 train.loss=2.078125 train.dtd=0.016107 train.dtf=1.477175 train.dtb=9.528106 train.dtu=0.009435 train.dtz=0.000633\n\n\n\ntrain.iter=10 train.loss=2.078125 train.dtd=0.016117 train.dtf=1.462501 train.dtb=9.544228 train.dtu=0.008660 train.dtz=0.000227\n\n\n\ntrain.iter=11 train.loss=2.093750 train.dtd=0.017514 train.dtf=1.317774 train.dtb=9.775683 train.dtu=0.010217 train.dtz=0.000928\n\n\n\ntrain.iter=12 train.loss=2.062500 train.dtd=0.015937 train.dtf=1.357692 train.dtb=9.522867 train.dtu=0.009089 train.dtz=0.000926\n\n\n\ntrain.iter=13 train.loss=2.062500 train.dtd=0.018012 train.dtf=1.348779 train.dtb=10.585765 train.dtu=0.015131 train.dtz=0.001114\n\n\n\ntrain.iter=14 train.loss=2.062500 train.dtd=0.020446 train.dtf=1.653757 train.dtb=10.175246 train.dtu=0.010203 train.dtz=0.001276\n\n\n\ntrain.iter=15 train.loss=2.078125 train.dtd=0.016014 train.dtf=1.296542 train.dtb=9.541296 train.dtu=0.009571 train.dtz=0.002962\n\n\n\ntrain.iter=16 train.loss=2.046875 train.dtd=0.016319 train.dtf=1.339488 train.dtb=9.595085 train.dtu=0.010588 train.dtz=0.000178\n\n\n\ntrain.iter=17 train.loss=2.000000 train.dtd=0.018822 train.dtf=1.523093 train.dtb=9.643459 train.dtu=0.010476 train.dtz=0.000863\n\n\n\ntrain.iter=18 train.loss=2.000000 train.dtd=0.016758 train.dtf=1.326725 train.dtb=9.660324 train.dtu=0.010885 train.dtz=0.000636\n\n\n\ntrain.iter=19 train.loss=2.093750 train.dtd=0.017480 train.dtf=1.315961 train.dtb=9.676368 train.dtu=0.008834 train.dtz=0.001277\n\n\n\ntrain.iter=20 train.loss=1.960938 train.dtd=0.015773 train.dtf=1.307810 train.dtb=9.456932 train.dtu=0.009677 train.dtz=0.000304\n\n\n\ntrain.iter=21 train.loss=2.062500 train.dtd=0.015496 train.dtf=1.287431 train.dtb=9.405478 train.dtu=0.009586 train.dtz=0.000779\n\n\n\ntrain.iter=22 train.loss=1.992188 train.dtd=0.015095 train.dtf=1.327778 train.dtb=9.659989 train.dtu=0.009307 train.dtz=0.002135\n\n\n\ntrain.iter=23 train.loss=1.992188 train.dtd=0.017967 train.dtf=1.325965 train.dtb=9.591390 train.dtu=0.009080 train.dtz=0.001406\n\n\n\ntrain.iter=24 train.loss=1.937500 train.dtd=0.017526 train.dtf=1.327373 train.dtb=10.707327 train.dtu=0.041676 train.dtz=0.001022\n\n\n\ntrain.iter=25 train.loss=1.960938 train.dtd=0.030242 train.dtf=1.549227 train.dtb=10.169439 train.dtu=0.010300 train.dtz=0.000944\n\n\n\ntrain.iter=26 train.loss=2.000000 train.dtd=0.016525 train.dtf=1.431249 train.dtb=9.513670 train.dtu=0.010391 train.dtz=0.001095\n\n\n\ntrain.iter=27 train.loss=1.929688 train.dtd=0.016541 train.dtf=1.315263 train.dtb=9.718378 train.dtu=0.010303 train.dtz=0.001848\n\n\n\ntrain.iter=28 train.loss=1.960938 train.dtd=0.016112 train.dtf=1.383241 train.dtb=9.915920 train.dtu=0.010128 train.dtz=0.000915\n\n\n\ntrain.iter=29 train.loss=1.984375 train.dtd=0.017622 train.dtf=1.365658 train.dtb=9.755264 train.dtu=0.013589 train.dtz=0.002384\n\n\n\ntrain.iter=30 train.loss=1.960938 train.dtd=0.028296 train.dtf=1.358164 train.dtb=9.698871 train.dtu=0.010066 train.dtz=0.001086\n\n\n\ntrain.iter=31 train.loss=1.960938 train.dtd=0.016636 train.dtf=1.306409 train.dtb=9.526738 train.dtu=0.010004 train.dtz=0.001049\n\n\n\ntrain.iter=32 train.loss=1.937500 train.dtd=0.015903 train.dtf=1.330479 train.dtb=9.551765 train.dtu=0.010755 train.dtz=0.001982\n\n\n\ntrain.iter=33 train.loss=1.906250 train.dtd=0.014864 train.dtf=1.340296 train.dtb=9.658271 train.dtu=0.011499 train.dtz=0.001063\n\n\n\ntrain.iter=34 train.loss=1.898438 train.dtd=0.015336 train.dtf=1.352045 train.dtb=9.523989 train.dtu=0.011362 train.dtz=0.000749\n\n\n\ntrain.iter=35 train.loss=1.945312 train.dtd=0.016147 train.dtf=1.347030 train.dtb=10.585912 train.dtu=0.009167 train.dtz=0.001000\n\n\n\ntrain.iter=36 train.loss=1.906250 train.dtd=0.015865 train.dtf=1.937747 train.dtb=9.859601 train.dtu=0.014571 train.dtz=0.000627\n\n\n\ntrain.iter=37 train.loss=1.968750 train.dtd=0.017821 train.dtf=1.378094 train.dtb=9.465782 train.dtu=0.009037 train.dtz=0.001293\n\n\n\ntrain.iter=38 train.loss=1.898438 train.dtd=0.016044 train.dtf=1.354977 train.dtb=9.746925 train.dtu=0.010405 train.dtz=0.001879\n\n\n\ntrain.iter=39 train.loss=1.968750 train.dtd=0.015668 train.dtf=1.364354 train.dtb=9.558674 train.dtu=0.008612 train.dtz=0.001614\n\n\n\ntrain.iter=40 train.loss=1.953125 train.dtd=0.016078 train.dtf=1.376093 train.dtb=9.660357 train.dtu=0.010320 train.dtz=0.001362\n\n\n\ntrain.iter=41 train.loss=1.945312 train.dtd=0.017464 train.dtf=1.438228 train.dtb=9.770692 train.dtu=0.009360 train.dtz=0.000157\n\n\n\ntrain.iter=42 train.loss=1.929688 train.dtd=0.016539 train.dtf=1.368649 train.dtb=9.463489 train.dtu=0.010483 train.dtz=0.000792\n\n\n\ntrain.iter=43 train.loss=1.898438 train.dtd=0.015760 train.dtf=1.381780 train.dtb=9.619256 train.dtu=0.012786 train.dtz=0.000868\n\n\n\ntrain.iter=44 train.loss=1.835938 train.dtd=0.015739 train.dtf=1.678473 train.dtb=10.078814 train.dtu=0.016395 train.dtz=0.001432\n\n\n\ntrain.iter=45 train.loss=1.992188 train.dtd=0.018277 train.dtf=1.579293 train.dtb=10.125529 train.dtu=0.008637 train.dtz=0.001102\n\n\n\ntrain.iter=46 train.loss=1.929688 train.dtd=0.015585 train.dtf=1.558680 train.dtb=11.728913 train.dtu=0.015986 train.dtz=0.000877\n\n\n\ntrain.iter=47 train.loss=1.812500 train.dtd=0.023439 train.dtf=1.832702 train.dtb=10.247002 train.dtu=0.010712 train.dtz=0.000785\n\n\n\ntrain.iter=48 train.loss=1.921875 train.dtd=0.017242 train.dtf=1.450544 train.dtb=9.988680 train.dtu=0.016540 train.dtz=0.001258\n\n\n\ntrain.iter=49 train.loss=1.859375 train.dtd=0.021252 train.dtf=1.741768 train.dtb=10.019653 train.dtu=0.009834 train.dtz=0.001659\n\n\n\n\n\nlogger.propagate = True\n\n\nimport rich\nimport rich.diagnose\nrich.diagnose.report()\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ &lt;class 'rich.console.Console'&gt; ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ A high level console interface.                                         ‚îÇ\n‚îÇ                                                                         ‚îÇ\n‚îÇ ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ ‚îÇ\n‚îÇ ‚îÇ &lt;console width=115 ColorSystem.TRUECOLOR&gt;                           ‚îÇ ‚îÇ\n‚îÇ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ ‚îÇ\n‚îÇ                                                                         ‚îÇ\n‚îÇ     color_system = 'truecolor'                                          ‚îÇ\n‚îÇ         encoding = 'utf-8'                                              ‚îÇ\n‚îÇ             file = &lt;ipykernel.iostream.OutStream object at 0x103fb3f10&gt; ‚îÇ\n‚îÇ           height = 100                                                  ‚îÇ\n‚îÇ    is_alt_screen = False                                                ‚îÇ\n‚îÇ is_dumb_terminal = False                                                ‚îÇ\n‚îÇ   is_interactive = False                                                ‚îÇ\n‚îÇ       is_jupyter = True                                                 ‚îÇ\n‚îÇ      is_terminal = False                                                ‚îÇ\n‚îÇ   legacy_windows = False                                                ‚îÇ\n‚îÇ         no_color = False                                                ‚îÇ\n‚îÇ          options = ConsoleOptions(                                      ‚îÇ\n‚îÇ                        size=ConsoleDimensions(width=115, height=100),   ‚îÇ\n‚îÇ                        legacy_windows=False,                            ‚îÇ\n‚îÇ                        min_width=1,                                     ‚îÇ\n‚îÇ                        max_width=115,                                   ‚îÇ\n‚îÇ                        is_terminal=False,                               ‚îÇ\n‚îÇ                        encoding='utf-8',                                ‚îÇ\n‚îÇ                        max_height=100,                                  ‚îÇ\n‚îÇ                        justify=None,                                    ‚îÇ\n‚îÇ                        overflow=None,                                   ‚îÇ\n‚îÇ                        no_wrap=False,                                   ‚îÇ\n‚îÇ                        highlight=None,                                  ‚îÇ\n‚îÇ                        markup=None,                                     ‚îÇ\n‚îÇ                        height=None                                      ‚îÇ\n‚îÇ                    )                                                    ‚îÇ\n‚îÇ            quiet = False                                                ‚îÇ\n‚îÇ           record = False                                                ‚îÇ\n‚îÇ         safe_box = True                                                 ‚îÇ\n‚îÇ             size = ConsoleDimensions(width=115, height=100)             ‚îÇ\n‚îÇ        soft_wrap = False                                                ‚îÇ\n‚îÇ           stderr = False                                                ‚îÇ\n‚îÇ            style = None                                                 ‚îÇ\n‚îÇ         tab_size = 8                                                    ‚îÇ\n‚îÇ            width = 115                                                  ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\n‚ï≠‚îÄ‚îÄ‚îÄ &lt;class 'rich._windows.WindowsConsoleFeatures'&gt; ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ Windows features available.                           ‚îÇ\n‚îÇ                                                       ‚îÇ\n‚îÇ ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ ‚îÇ\n‚îÇ ‚îÇ WindowsConsoleFeatures(vt=False, truecolor=False) ‚îÇ ‚îÇ\n‚îÇ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ ‚îÇ\n‚îÇ                                                       ‚îÇ\n‚îÇ truecolor = False                                     ‚îÇ\n‚îÇ        vt = False                                     ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Environment Variables ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ {                                  ‚îÇ\n‚îÇ     'CLICOLOR': '1',               ‚îÇ\n‚îÇ     'COLORTERM': 'truecolor',      ‚îÇ\n‚îÇ     'COLUMNS': '80',               ‚îÇ\n‚îÇ     'JPY_PARENT_PID': '53753',     ‚îÇ\n‚îÇ     'JUPYTER_COLUMNS': None,       ‚îÇ\n‚îÇ     'JUPYTER_LINES': None,         ‚îÇ\n‚îÇ     'LINES': None,                 ‚îÇ\n‚îÇ     'NO_COLOR': None,              ‚îÇ\n‚îÇ     'TERM_PROGRAM': None,          ‚îÇ\n‚îÇ     'TERM': 'xterm-color',         ‚îÇ\n‚îÇ     'TTY_COMPATIBLE': None,        ‚îÇ\n‚îÇ     'TTY_INTERACTIVE': None,       ‚îÇ\n‚îÇ     'VSCODE_VERBOSE_LOGGING': None ‚îÇ\n‚îÇ }                                  ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\nplatform=\"Darwin\"\n\n\n\n\nmodel.eval()\n\nwith tqdm(\n    total=len(val_dataloader), position=0, leave=True, desc=f\"Validate Epoch {i}\"\n) as val_bar:\n    for bidx, (x, y) in enumerate(val_dataloader):\n        metrics = eval_step(x, y)\n        logger.info(history.update({f\"val/{k}\": v for k, v in metrics.items()}).replace(\"/\", \".\"))\n    #acc_val, loss_val = evaluate(val_dataloader, model, loss_fn, val_bar)\n    #logger.info(\n    #    history.update(\n    #        {\n    #            \"eval\": {\n    #                \"bidx\": i,\n    #                \"acc\": acc_val,\n    #                \"loss\": loss_val\n    #            }\n    #        }\n    #    )\n    #)\n\nmodel.train()\n\n\n\n\n\n[2025-07-25 16:28:32,670941][I][ipykernel_67898/811502054:8:__main__] val.loss=1.882812 val.acc=0.296875 val.dtf=1.420222 val.dtm=0.000187\n\n[2025-07-25 16:28:33,990227][I][ipykernel_67898/811502054:8:__main__] val.loss=1.765625 val.acc=0.343750 val.dtf=1.300859 val.dtm=0.000121\n\n5,310532][I][ipykernel_67898/811502054:8:__main__] val.loss=1.914062 val.acc=0.312500 val.dtf=1.302293 val.dtm=0.000124\n\n\n\n[2025-07-25 16:28:36,669237][I][ipykernel_67898/811502054:8:__main__] val.loss=1.953125 val.acc=0.300781 val.dtf=1.342132 val.dtm=0.000138\n\n7,949766][I][ipykernel_67898/811502054:8:__main__] val.loss=1.867188 val.acc=0.351562 val.dtf=1.263595 val.dtm=0.000139\n\n\n\n[2025-07-25 16:28:39,325906][I][ipykernel_67898/811502054:8:__main__] val.loss=1.976562 val.acc=0.312500 val.dtf=1.359982 val.dtm=0.000147\n\n40,664261][I][ipykernel_67898/811502054:8:__main__] val.loss=1.851562 val.acc=0.320312 val.dtf=1.321372 val.dtm=0.000162\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[47], line 7\n      3 with tqdm(\n      4     total=len(val_dataloader), position=0, leave=True, desc=f\"Validate Epoch {i}\"\n      5 ) as val_bar:\n      6     for bidx, (x, y) in enumerate(val_dataloader):\n----&gt; 7         metrics = eval_step(x, y)\n      8         logger.info(history.update({f\"val/{k}\": v for k, v in metrics.items()}).replace(\"/\", \".\"))\n      9     #acc_val, loss_val = evaluate(val_dataloader, model, loss_fn, val_bar)\n     10     #logger.info(\n     11     #    history.update(\n   (...)     19     #    )\n     20     #)\n\nCell In[35], line 13, in eval_step(x, y)\n     11 with torch.no_grad():\n     12     t0 = time.perf_counter()\n---&gt; 13     pred = model(x.to(DTYPE))\n     14     t1 = time.perf_counter()\n     15     loss = loss_fn(pred, y).item()\n\nFile ~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1749     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1750 else:\n-&gt; 1751     return self._call_impl(*args, **kwargs)\n\nFile ~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762, in Module._call_impl(self, *args, **kwargs)\n   1757 # If we don't have any hooks, we want to skip the rest of the logic in\n   1758 # this function, and just call forward.\n   1759 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1760         or _global_backward_pre_hooks or _global_backward_hooks\n   1761         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1762     return forward_call(*args, **kwargs)\n   1764 result = None\n   1765 called_always_called_hooks = set()\n\nCell In[16], line 118, in Classifier.forward(self, x)\n    116 x = self.norm1(x)\n    117 # Apply the main chunk of the network:\n--&gt; 118 x = self.layers(x)\n    119 # Normalize and readout:\n    120 x = nn.functional.avg_pool2d(x, x.shape[2:])\n\nFile ~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1749     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1750 else:\n-&gt; 1751     return self._call_impl(*args, **kwargs)\n\nFile ~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762, in Module._call_impl(self, *args, **kwargs)\n   1757 # If we don't have any hooks, we want to skip the rest of the logic in\n   1758 # this function, and just call forward.\n   1759 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1760         or _global_backward_pre_hooks or _global_backward_hooks\n   1761         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1762     return forward_call(*args, **kwargs)\n   1764 result = None\n   1765 called_always_called_hooks = set()\n\nFile ~/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:240, in Sequential.forward(self, input)\n    238 def forward(self, input):\n    239     for module in self:\n--&gt; 240         input = module(input)\n    241     return input\n\nFile ~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1749     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1750 else:\n-&gt; 1751     return self._call_impl(*args, **kwargs)\n\nFile ~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762, in Module._call_impl(self, *args, **kwargs)\n   1757 # If we don't have any hooks, we want to skip the rest of the logic in\n   1758 # this function, and just call forward.\n   1759 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1760         or _global_backward_pre_hooks or _global_backward_hooks\n   1761         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1762     return forward_call(*args, **kwargs)\n   1764 result = None\n   1765 called_always_called_hooks = set()\n\nCell In[16], line 46, in ConvNextBlock.forward(self, inputs)\n     45 def forward(self, inputs):\n---&gt; 46     x = self.conv1(inputs)\n     47     # The normalization layer:\n     48     x = self.norm(x)\n\nFile ~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1749     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1750 else:\n-&gt; 1751     return self._call_impl(*args, **kwargs)\n\nFile ~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762, in Module._call_impl(self, *args, **kwargs)\n   1757 # If we don't have any hooks, we want to skip the rest of the logic in\n   1758 # this function, and just call forward.\n   1759 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1760         or _global_backward_pre_hooks or _global_backward_hooks\n   1761         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1762     return forward_call(*args, **kwargs)\n   1764 result = None\n   1765 called_always_called_hooks = set()\n\nFile ~/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:554, in Conv2d.forward(self, input)\n    553 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 554     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:549, in Conv2d._conv_forward(self, input, weight, bias)\n    537 if self.padding_mode != \"zeros\":\n    538     return F.conv2d(\n    539         F.pad(\n    540             input, self._reversed_padding_repeated_twice, mode=self.padding_mode\n   (...)    547         self.groups,\n    548     )\n--&gt; 549 return F.conv2d(\n    550     input, weight, bias, self.stride, self.padding, self.dilation, self.groups\n    551 )\n\nKeyboardInterrupt: \n\n\n\n\nehist = history.history.pop('eval')\n\n\nhistory.plot_dataset()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhistory.history.keys()\n\ndict_keys(['iter', 'loss', 'dtd', 'dtf', 'dtb', 'dtu', 'dtz', 'eval', 'val/loss', 'val/acc', 'val/dtf', 'val/dtm'])\n\n\n\nhistory.history['eval']\n\n[{'bidx': 49, 'acc': 33.53515625, 'loss': 1.8208984375},\n {'loss': 1.8046875,\n  'acc': 0.33984375,\n  'dtf': 1.5576423341408372,\n  'dtm': 0.00017420784570276737}]\n\n\n\n# plt.style.use(ambivalent.STYLES[\"ambivalent\"])\nwith plt.rc_context(ambivalent.STYLES[\"ambivalent\"]):\n    history.plot_all()\n\n[autoreload of rich.style failed: Traceback (most recent call last):\n  File \"/Users/samforeman/.venv/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 325, in check\n    superreload(m, reload, self.old_objects)\n  File \"/Users/samforeman/.venv/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 621, in superreload\n    update_generic(old_obj, new_obj)\n  File \"/Users/samforeman/.venv/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 447, in update_generic\n    update(a, b)\n  File \"/Users/samforeman/.venv/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 380, in update_class\n    old_obj = getattr(old, key)\n              ^^^^^^^^^^^^^^^^^\n  File \"/Users/samforeman/.venv/lib/python3.12/site-packages/rich/style.py\", line 25, in __get__\n    if obj._set_attributes & self.bit:\n       ^^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute '_set_attributes'. Did you mean: '__getattribute__'?\n]\n[autoreload of rich.console failed: Traceback (most recent call last):\n  File \"/Users/samforeman/.venv/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 325, in check\n    superreload(m, reload, self.old_objects)\n  File \"/Users/samforeman/.venv/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 621, in superreload\n    update_generic(old_obj, new_obj)\n  File \"/Users/samforeman/.venv/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 447, in update_generic\n    update(a, b)\n  File \"/Users/samforeman/.venv/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 415, in update_class\n    update_instances(old, new)\n  File \"/Users/samforeman/.venv/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 373, in update_instances\n    object.__setattr__(ref, \"__class__\", new)\nTypeError: can't apply this __setattr__ to ConsoleThreadLocals object\n]\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[40], line 2\n      1 # plt.style.use(ambivalent.STYLES[\"ambivalent\"])\n----&gt; 2 with plt.rc_context(ambivalent.STYLES[\"ambivalent\"]):\n      3     history.plot_all()\n\nFile ~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/contextlib.py:137, in _GeneratorContextManager.__enter__(self)\n    135 del self.args, self.kwds, self.func\n    136 try:\n--&gt; 137     return next(self.gen)\n    138 except StopIteration:\n    139     raise RuntimeError(\"generator didn't yield\") from None\n\nFile ~/.venv/lib/python3.12/site-packages/matplotlib/__init__.py:1216, in rc_context(rc, fname)\n   1214         rc_file(fname)\n   1215     if rc:\n-&gt; 1216         rcParams.update(rc)\n   1217     yield\n   1218 finally:\n\nFile ~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/_collections_abc.py:987, in MutableMapping.update(self, other, **kwds)\n    985         self[key] = other[key]\n    986 else:\n--&gt; 987     for key, value in other:\n    988         self[key] = value\n    989 for key, value in kwds.items():\n\nValueError: not enough values to unpack (expected 2, got 1)\n\n\n\n\nlogger.info(history.history['loss'])\n\n\n[2025-07-25 11:09:45,498960][I][ipykernel_68449/1607943937:1:__main__] [2.421875, 2.1875, 2.078125, 1.9921875, 2.0, 1.84375, 1.921875, 1.8828125, 1.9609375, 1.7890625]\n\n\n\n\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[32], line 8\n      3 with tqdm(\n      4     total=len(val_dataloader), position=0, leave=True, desc=f\"Validate Epoch {i}\"\n      5 ) as val_bar:\n      6     acc_val, loss_val = evaluate(val_dataloader, model, loss_fn, val_bar)\n      7     logger.info(\n----&gt; 8         history.update(\n      9             {\n     10                 \"eval\": {\n     11                     \"bidx\": i,\n     12                     \"acc\": acc_val,\n     13                     \"loss\": loss_val\n     14                 }\n     15             }\n     16         )\n     17     )\n     19 model.train()\n\nFile ~/projects/saforem2/ezpz/src/ezpz/dist.py:142, in timeitlogit.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)\n    140 t0 = time.perf_counter()\n    141 assert isinstance(rank, int)\n--&gt; 142 result = func(*args, **kwargs)\n    143 dt = time.perf_counter() - t0\n    144 fname = getattr(\n    145     func, \"__qualname__\", getattr(func, \"__name__\", \"unknown\")\n    146 )\n\nFile ~/projects/saforem2/ezpz/src/ezpz/history.py:202, in History.update(self, metrics, precision, use_wandb, commit, summarize)\n    200     wandb.log(metrics, commit=commit)\n    201 if summarize:\n--&gt; 202     return summarize_dict(metrics, precision=precision)\n    203 return \"\"\n\nFile ~/projects/saforem2/ezpz/src/ezpz/utils.py:103, in summarize_dict(d, precision)\n     91 def summarize_dict(d: dict, precision: int = 6) -&gt; str:\n     92     \"\"\"\n     93     Summarize a dictionary into a string with formatted key-value pairs.\n     94 \n   (...)    100         str: A string representation of the dictionary with formatted key-value pairs.\n    101     \"\"\"\n    102     return \" \".join(\n--&gt; 103         [format_pair(k, v, precision=precision) for k, v in d.items()]\n    104     )\n\nFile ~/projects/saforem2/ezpz/src/ezpz/utils.py:88, in format_pair(k, v, precision)\n     86     return f\"{k}={v}\"\n     87 # return f'{k}={v:&lt;3.4f}'\n---&gt; 88 return f\"{k}={v:&lt;.{precision}f}\"\n\nTypeError: unsupported format string passed to dict.__format__\n\n\n\n\nmodel.train()\n\nfrom tqdm.notebook import tqdm\n\nfor epoch in range(1):\n    model.train()\n    t0 = time.perf_counter()\n    with tqdm(\n        total=len(train_dataloader),\n        position=0,\n        leave=True,\n        desc=f\"Train Epoch {epoch}\"\n    ) as train_bar:\n        # x, y = next(iter(train_dataloader))\n        for batch, (x, y) in enumerate(train_dataloader):\n            loss, metrics = train_step(x, y)\n            metrics = {\n                \"train/bidx\": batch,\n                \"train/loss\": loss,\n                **{f\"train/{k}\": v for k, v in metrics.items()},\n            }\n            logger.info(history.update(metrics).replace(\"train/\", \"\"))\n            train_bar.update()\n    t1 = time.perf_counter()\n    model.eval()\n    with tqdm(\n        total=len(val_dataloader), position=0, leave=True, desc=f\"Validate Epoch {j}\"\n    ) as val_bar:\n        acc_val, loss_val = evaluate(val_dataloader, model, loss_fn, val_bar)\n        logger.info(history.update({\"eval\": {\"bidx\": bidx, \"acc\": acc_val, \"loss\": loss_val}}))\n        # logger.info(\n        #     f\"Epoch {j}: validation loss: {loss_val:.3f}, accuracy: {acc_val:.3f}\"\n        #)\n\n\nhistory.plot_all()",
    "crumbs": [
      "[01] Neural Networks",
      "[3] Conv. Nets"
    ]
  },
  {
    "objectID": "01-neural-networks/3-conv-nets/index.html#training-for-multiple-epochs",
    "href": "01-neural-networks/3-conv-nets/index.html#training-for-multiple-epochs",
    "title": "Convolutional Neural Networks",
    "section": "Training for Multiple Epochs",
    "text": "Training for Multiple Epochs\nepochs = 1\ntrain_history = ezpz.History()\nfor j in range(epochs):\n    with tqdm(\n        total=len(train_dataloader), position=0, leave=True, desc=f\"Train Epoch {j}\"\n    ) as train_bar:\n        bmetrics = train_one_epoch(\n            train_dataloader,\n            model,\n            loss_fn,\n            optimizer,\n            train_bar,\n            history=train_history,\n        )\n\n    # checking on the training & validation loss & accuracy\n    # for training data - only once every 5 epochs (takes a while)\n    if j % 5 == 0:\n        with tqdm(\n            total=len(train_dataloader),\n            position=0,\n            leave=True,\n            desc=f\"Validate (train) Epoch {j}\",\n        ) as train_eval:\n            acc, loss = evaluate(train_dataloader, model, loss_fn, train_eval)\n            logger.info(f\"Epoch {j}: training loss: {loss:.3f}, accuracy: {acc:.3f}\")\n\n    with tqdm(\n        total=len(val_dataloader), position=0, leave=True, desc=f\"Validate Epoch {j}\"\n    ) as val_bar:\n        acc_val, loss_val = evaluate(val_dataloader, model, loss_fn, val_bar)\n        logger.info(\n            f\"Epoch {j}: validation loss: {loss_val:.3f}, accuracy: {acc_val:.3f}\"\n        )",
    "crumbs": [
      "[01] Neural Networks",
      "[3] Conv. Nets"
    ]
  },
  {
    "objectID": "01-neural-networks/3-conv-nets/index.html#homework-1",
    "href": "01-neural-networks/3-conv-nets/index.html#homework-1",
    "title": "Convolutional Neural Networks",
    "section": "Homework 1:",
    "text": "Homework 1:\nIn this notebook, we‚Äôve learned about some basic convolutional networks and trained one on CIFAR-10 images. It did ‚Ä¶ OK. There is significant overfitting of this model. There are some ways to address that, but we didn‚Äôt have time to get into that in this session.\nMeanwhile, your homework (part 1) for this week is to try to train the model again but with a different architecture. Change one or more of the following: - The number of convolutions between downsampling - The number of filters in each layer - The initial ‚Äúpatchify‚Äù layer - Another hyper-parameter of your choosing\nAnd compare your final validation accuracy to the accuracy shown here. Can you beat the validation accuracy shown?\nFor full credit on the homework, you need to show (via text, or make a plot) the training and validation data sets‚Äô performance (loss and accuracy) for all the epochs you train. You also need to explain, in several sentences, what you changed in the network and why you think it makes a difference.",
    "crumbs": [
      "[01] Neural Networks",
      "[3] Conv. Nets"
    ]
  },
  {
    "objectID": "01-neural-networks/3-conv-nets/index.html#footnotes",
    "href": "01-neural-networks/3-conv-nets/index.html#footnotes",
    "title": "Convolutional Neural Networks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n| A Roadmap for Foundational Research on Artificial Intelligence in Medical Imaging From the 2018 NIHRSNAACR‚Ü©Ô∏é",
    "crumbs": [
      "[01] Neural Networks",
      "[3] Conv. Nets"
    ]
  },
  {
    "objectID": "01-neural-networks/4-representation-learning/index.html#this-notebook-is-not-officially-part-of-the-course.",
    "href": "01-neural-networks/4-representation-learning/index.html#this-notebook-is-not-officially-part-of-the-course.",
    "title": "Representation Learning with Neural Networks",
    "section": "This notebook is not officially part of the course.",
    "text": "This notebook is not officially part of the course.\nBut you are welcome to look through it anyways, you can send questions on slack, and we are happy to talk about it.\nThe previous notebook trained a classifier network which did ok. But what if we didn‚Äôt have a lot of data? In this notebook, we‚Äôll apply that model in a new way with representation learning.\n\nimport os\nimport sys\nif sys.platform == \"darwin\":\n    os.environ[\"TORCH_DEVICE\"] = \"cpu\"\n    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = \"1\"\n\n\nimport numpy\nimport random\nimport torch\nimport torchvision\nfrom torchvision.transforms import v2\nfrom ezpz.log.config import STYLES\nfrom rich.console import Console\nfrom rich.theme import Theme\n\nconsole = Console(theme=Theme(STYLES))\n\n\n\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nbatch_size = 128\n\nHere‚Äôs the Convolutional Neural Network Again:\n\nfrom torch import nn\n\nclass Downsampler(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=2):\n        super(Downsampler, self).__init__()\n        self.norm = nn.InstanceNorm2d(in_channels)\n        self.downsample = nn.Conv2d(\n            in_channels=in_channels, \n            out_channels=out_channels,\n            kernel_size = stride,\n            stride = stride,\n        )\n\n    def forward(self, inputs):\n        return self.downsample(self.norm(inputs))\n\n\nclass ConvNextBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(ConvNextBlock, self).__init__()\n        # Depthwise, seperable convolution with a large number of output filters:\n        self.conv1 = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=in_channels,\n            groups=in_channels,\n            kernel_size=(7, 7),\n            padding='same'\n        )\n        self.norm = nn.InstanceNorm2d(in_channels)\n        self.conv2 = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=4*in_channels,\n            kernel_size=1\n        )\n        self.conv3 = nn.Conv2d(\n            in_channels=4*in_channels,\n            out_channels=in_channels,\n            kernel_size=1\n        )\n\n\n    def forward(self, inputs):\n        # x = self.conv1(inputs)\n        # The normalization layer:\n        # x = self.norm(x)\n        # x = self.conv2(x)\n        # The non-linear activation layer:\n        # x = torch.nn.functional.gelu(x)\n        # x = self.conv3(x)\n        # This makes it a residual network:\n        return inputs + self.conv3(\n            torch.nn.functional.gelu(\n                self.conv2(\n                    self.norm(\n                        self.conv1(inputs)\n                    )\n                )\n            )\n        )\n\n\nclass Classifier(nn.Module):\n\n\n    def __init__(self, n_initial_filters, n_stages, blocks_per_stage, n_outputs):\n        super(Classifier, self).__init__()\n        # This is a downsampling convolution that will produce patches of output.\n        # This is similar to what vision transformers do to tokenize the images.\n        self.stem = nn.Conv2d(in_channels=3,\n                                    out_channels=n_initial_filters,\n                                    kernel_size=1,\n                                    stride=1)\n        self.norm1 = nn.InstanceNorm2d(n_initial_filters)\n\n        current_n_filters = n_initial_filters\n        self.layers = nn.Sequential()\n        for n_blocks in range(n_stages):\n            # Add a convnext block series:\n            for _ in range(blocks_per_stage):\n                self.layers.append(ConvNextBlock(in_channels=current_n_filters))\n            # Add a downsampling layer:\n            self.layers.append(Downsampler(in_channels=current_n_filters, out_channels=2*current_n_filters))\n            # Double the number of filters:\n            current_n_filters = 2*current_n_filters\n        self.head = nn.Sequential(\n            nn.Flatten(),\n            nn.LayerNorm(current_n_filters),\n            nn.Linear(current_n_filters, n_outputs)\n        )\n\n    def forward(self, inputs):\n        x = self.stem(inputs)\n        # Apply a normalization after the initial patching:\n        x = self.norm1(x)\n        # Apply the main chunk of the network:\n        x = self.layers(x)\n        # Normalize and readout:\n        x = nn.functional.avg_pool2d(x, x.shape[2:])\n        x = self.head(x)\n        return x\n\n\ndef create_representation_model(n_features, rank, size):\n    model = Classifier(32, 2, 2, n_features)\n    model.to(torch.get_default_device())\n    return model\n\nmodel = create_representation_model(256, 0, 1)\nhead = torch.nn.Sequential(\n    nn.Linear(256,128),\n)\n\nhead.to(torch.get_default_device())\n\nfrom torchinfo import summary\nconsole.log(summary(model, input_size=(batch_size, 3, 32, 32)))\nconsole.log(summary(head, input_size=(batch_size, 256)))\n\nThis will download the data if needed:\nWe‚Äôre going to train this on Polaris nodes which have 4 A100s (But only using one node at a time). So, the following helper functions will automatically distribute the code and model to use all 4 GPUs at once:\n(They are all from the DDP Tutorial )\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\nimport ezpz\n\ndef create_data_loaders(transforms, batch_size, rank, seed):\n    # Start up the data loader:\n    # dev = torch.device(\n    #     f\"cuda:{rank}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    dev = ezpz.get_torch_device_type()\n\n    training_data = torchvision.datasets.CIFAR10(\n        root=\"data\",\n        train=True,\n        download=True,\n        transform=transforms\n    )\n\n    training_data, validation_data = torch.utils.data.random_split(\n        training_data,\n        [0.8, 0.2],\n        generator=torch.Generator().manual_seed(55))\n\n    # The dataloader makes our dataset iterable \n    train_dataloader = torch.utils.data.DataLoader(training_data,\n                                                   batch_size=batch_size,\n                                                   shuffle=True,\n                                                   num_workers=8)\n\n    val_dataloader = torch.utils.data.DataLoader(validation_data,\n                                                 batch_size=batch_size,\n                                                 shuffle=True,\n                                                 num_workers=8)\n\n\n    def preprocess(x, y):\n        # CIFAR-10 is *color* images so 3 layers!\n        return x.view(-1, 3, 32, 32).to(dev), y.to(dev)\n\n\n    class WrappedDataLoader:\n        def __init__(self, dl, func):\n            self.dl = dl\n            self.func = func\n\n        def __len__(self):\n            return len(self.dl)\n\n        def __iter__(self):\n            for b in self.dl:\n                yield (self.func(*b))\n\n\n    train_dataloader = WrappedDataLoader(train_dataloader, preprocess)\n    val_dataloader = WrappedDataLoader(val_dataloader, preprocess)\n\n    return train_dataloader, val_dataloader\n\n\n# def demo_basic(rank, world_size, n_epochs):\n#     console.log(f\"Running basic DDP example on rank {rank}.\")\n#     setup(rank, world_size)\n\n\n#     # create model and move it to GPU with id rank\n#     model = ToyModel().to(rank)\n#     ddp_model = DDP(model, device_ids=[rank])\n\n#     loss_fn = nn.MSELoss()\n#     optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n#     optimizer.zero_grad()\n#     outputs = ddp_model(torch.randn(20, 10))\n#     labels = torch.randn(20, 5).to(rank)\n#     loss_fn(outputs, labels).backward()\n#     optimizer.step()\n\n#     cleanup()\n\n\n# def run_demo(demo_fn, world_size):\n#     mp.spawn(demo_fn,\n#              args=(world_size,5),\n#              nprocs=world_size,\n#              join=True)\n\n\n# import sys, os\n# from multiprocessing import Pool\n# from multiprocessing.reduction import ForkingPickler\n# from types import FunctionType\n# import cloudpickle\n\n# assert sys.version_info &gt;= (3, 8), 'python3.8 or greater required to use reducer_override'\n\n# def reducer_override(obj):\n#     if type(obj) is FunctionType:\n#         return (cloudpickle.loads, (cloudpickle.dumps(obj),))\n#     else:\n#         return NotImplemented\n\n# # Monkeypatch our function reducer into the pickler for multiprocessing.\n# # Without this line, the main block will not work on windows or macOS.\n# # Alterntively, moving the defintionn of foo outside of the if statement\n# # would make the main block work on windows or macOS (when run from\n# # the command line).\n# ForkingPickler.reducer_override = staticmethod(reducer_override)\n\n\n# This method is from the pytorch implementation of SimCLR:\n# https://github.com/sthalles/SimCLR/blob/master/data_aug/contrastive_learning_dataset.py\n\ndef get_simclr_pipeline_transform(size, s=1):\n    \"\"\"Return a set of data augmentation transformations as described in the SimCLR paper.\"\"\"\n    color_jitter = v2.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n    data_transforms = v2.Compose([v2.RandomResizedCrop(size=size, scale=[0.85,1.0]),\n                                          v2.RandomHorizontalFlip(),\n                                          v2.RandomApply([color_jitter], p=0.8),\n                                          v2.RandomGrayscale(p=0.2),\n                                          v2.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n                                          # v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                                          # v2.ToTensor()\n                                        ])\n    return data_transforms\n\n\ntransforms1 = get_simclr_pipeline_transform((32,32))\ntransforms2 = get_simclr_pipeline_transform((32,32))\n\n\ntrain, val = create_data_loaders(v2.ToTensor(), batch_size, 0, seed = 1234)\n\n\nbatch, (X, Y) = next(enumerate(train))\n\n\nX1 = transforms1(X); X2 = transforms2(X)\n\nconsole.log(type(X1))\nconsole.log(type(X2))\n\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n\nplt.imshow(X[0].cpu().permute((1,2,0))); plt.show()\nplt.imshow(X1[0].cpu().permute((1,2,0))); plt.show()\nplt.imshow(X2[0].cpu().permute((1,2,0))); plt.show()\n\n\ndef contrastive_loss(first_images, second_images, rank, world_size = 1, temperature=0.1):\n        # Each image is represented with k parameters,\n        # Assume the batch size is N, so the\n        # inputs have shape (N, k)\n\n        # These are pre-distributed shapes:\n        N = first_images.shape[0]\n        k = first_images.shape[1]\n\n\n        first_images = first_images / torch.norm(first_images,dim=1).reshape((-1,1))\n        second_images = second_images / torch.norm(second_images,dim=1).reshape((-1,1))\n\n        # Take the two tuples, and concatenate them.\n        # Then, reshape into Y = (1, 2N, k) and Z = (2N, 1, k)\n\n        c = torch.concat([first_images, second_images], dim=0)\n\n        # Gather all the c up if the world size &gt; 1:\n        if world_size &gt; 1:\n            gathered_c = torch.distributed.all_gather(tensor=c)\n            gathered_c = gathered_c.reshape((-1, first_images.shape[-1]))\n        else:\n            gathered_c = c\n\n        # Each rank computes only a slice of the global loss matrix, or\n        # the memory usage gets out of control.\n\n        # We calculate the dot product between the local and global tensors:\n        local_reps = c.reshape((c.shape[0], 1, c.shape[1]))\n        all_reps   = gathered_c.reshape((1, gathered_c.shape[0], gathered_c.shape[1]))\n\n\n        # Assume we have n images per rank, for N global images with N = n * world_size\n        # Compute the product of these tensors, which gives shape\n        # (2n, 2N, k)\n        mat =  local_reps*all_reps\n\n        # We need to compute the function (sim(x,y)) for each element in the 2N sequent.\n        # Since the are normalized, we're computing x^T . Y / (||x||*||y||),\n        # but the norms are equal to 1.\n        # So, summing the matrix over the dim = 0 and dim = 1 computes this for each pair.\n\n        sim = torch.sum(mat, dim=-1) / temperature\n\n\n\n        # Now, sim is of shape [2*n, 2*N]\n\n        # This yields a symmetric matrix, diagonal entries equal 1.  Off diagonal are symmetrics and &lt; 1.\n\n        # sim = torch.exp(sim / temperature)\n        # Now, for every entry i in C (concat of both batches), the sum of sim[i] - sim[i][i] is the denominator\n\n        device = sim.device\n\n        # Since we have a non-symmetric matrix, need to build a non-symmetric index:\n        positive = torch.zeros(sim.shape, device=device)\n\n        # We concatenated all the local examples, and compute symmetric positive pairs\n        # So for the first N entries, the index of the positive pair is i + N  (locally)\n        # For the second N entries, the index of the positive pair is i - N (locally)\n        # with a distributed run, we've squashed all the similarity scores together.\n        # to a shape of [2*N, 2*N*Size]\n        # Each 2*N by 2*N block is the local positive indexes, all others are negative.\n        # That means that the index is shifted by global_rank*2*N\n\n        access_index_x = torch.arange(2*N)\n        # For the first N, the y-index is equal to x + 2*N\n        # For the second N\n        access_index_y = torch.arange(2*N)\n        # Shift by +/- N:\n        access_index_y[0:N] = access_index_y[0:N] + N\n        access_index_y[N:]  = access_index_y[N:] - N\n\n        access_index_y +=  rank * 2*N\n\n        # console.log(\"access_index_y: \", access_index_y, flush=True)\n\n        positive[access_index_x, access_index_y] = 1\n\n        # For the negative, we invert the positive and have to 0 out the self-index entries\n        negative = 1 - positive\n\n        # THESE WORK IF IT'S NOT DISTRIBUTED\n        # positive = torch.tile(torch.eye(N, device=device), (2,2))\n        # # Unsure if this line is needed?\n        # positive = positive - torch.eye(2*N, device=device)\n        #\n        # negative = - (torch.eye(2*N, device=device) - 1)\n\n        with torch.no_grad():\n            # Here, we can compute the top-k metrics for this batch, since we have the global state:\n            # We want the top 5 entries but the self-sim is obviously perfect.\n            # So take the top 6 and reject the first.\n            topk = torch.topk(sim, k=6, dim=-1, sorted=True)\n\n            # Top 1 is just an equality check:\n            top1_acc = topk.indices[:,1] == access_index_y.to(topk.indices.device)\n            top1_acc = torch.mean(top1_acc.to(torch.float))\n          \n            # Top 5 is a little more complicated:\n            # Compute the index distance to the correct index, abs value:\n            top5_acc_dist = torch.abs(topk.indices[:,1:] - access_index_y.to(topk.indices.device).reshape(-1,1))\n            # Get the minumum value, and see if it is less than 5:\n            min_values, _ = torch.min(top5_acc_dist, dim=-1)\n            top5_acc =  min_values &lt; 5.\n            # Average over the batch dimension:\n            top5_acc = torch.mean(top5_acc.to(torch.float))\n\n\n        negative_examples = sim * negative\n        positive_examples = sim * positive\n\n        # Now, positive/negative examples is the temperature normalized similarity.\n        # we need to sum across the whole batch dimension to compute it per-example:\n\n\n        # Compute the alignment, summed over the entire global batch:\n        alignment = torch.sum(positive_examples, dim=-1)\n\n        # Compute the exp, which we'll eventually sum and log:\n        exp = torch.sum(torch.exp(negative_examples), dim=-1)\n\n        # console.log(\"Alignment: \", alignment, flush=True)\n        # console.log(\"exp: \",       exp, flush=True)\n\n\n        # And compute the logsumexp of the negative examples:\n        log_sum_exp = torch.log(exp )\n\n\n        # Additionally, we can compute the \"floor\" of the loss at this batch size:\n        # floor = torch.log(1.*N) - 1.\n\n        loss_metrics = {\n            \"alignment\"   : torch.mean(alignment),\n            \"log_sum_exp\" : torch.mean(log_sum_exp),\n            \"top1\"        : top1_acc,\n            \"top5\"        : top5_acc,\n            # \"floor\"       : floor,\n        }\n\n        loss = torch.mean( - alignment + log_sum_exp)\n        return loss, loss_metrics\n\n\ndef train_one_epoch(dataloader, t1, t2, model, head, loss_fn, optimizer, rank, size, progress_bar):\n    model.train()\n    head.train()\n    for (batch, (X, _)) in enumerate(dataloader):\n        # forward pass\n        X1 = t1(X); X2 = t2(X)\n        pred1 = head(model(X1))\n        pred2 = head(model(X2))\n        loss, metrics = loss_fn(pred1, pred2, rank, size)\n\n        # console.log(metrics)\n        \n        # backward pass calculates gradients\n        loss.backward()\n        \n        # take one step with these gradients\n        optimizer.step()\n        \n        # resets the gradients \n        optimizer.zero_grad()\n\n        # progress_bar.refresh()\n        cpu_metrics = { key : f\"{metrics[key].detach().cpu().numpy():.2f}\" for key in metrics.keys()}\n        cpu_metrics[\"loss\"] = f\"{loss.detach().cpu().numpy():.2f}\"\n        progress_bar.update()\n        progress_bar.set_postfix(cpu_metrics)\n        # progress_bar.description = f\"Train loss: {loss.cpu():.2f} top5: {metrics['top5'].cpu():.2f}\"\n        # break\n\n\ndef validate_one_epoch(dataloader, t1, t2, model, head, loss_fn, rank, size, progress_bar):\n    model.train()\n    head.train()\n    n = 0.\n    sum_metrics = None\n    for (batch, (X, _)) in enumerate(dataloader):\n        # forward pass\n        X1 = t1(X); X2 = t2(X)\n        pred1 = head(model(X1))\n        pred2 = head(model(X2))\n        loss, metrics = loss_fn(pred1, pred2, rank, size)\n\n        # console.log(metrics)\n        \n        # backward pass calculates gradients\n        loss.backward()\n        \n        # take one step with these gradients\n        optimizer.step()\n        \n        # resets the gradients \n        optimizer.zero_grad()\n\n        # progress_bar.refresh()\n        cpu_metrics = { key : metrics[key].detach().cpu().numpy() for key in metrics.keys()}\n        if sum_metrics is None: \n            sum_metrics = cpu_metrics\n        else:\n            for key in sum_metrics.keys():\n                sum_metrics[key] += cpu_metrics[key]\n        progress_bar.update()\n        n += 1.\n        # progress_bar.description = f\"Train loss: {loss.cpu():.2f} top5: {sum_metrics['top5'].cpu():.2f}\"\n        # break\n    \n    for key in sum_metrics:\n        sum_metrics[key] = sum_metrics[key] / n\n    return sum_metrics\n\n\noptimizer = torch.optim.AdamW(list(model.parameters()) + list(head.parameters()), lr=0.001)\n\n\nfrom tqdm.notebook import tqdm\n\n\n# for j in range(1):\n#     # with tqdm(total=len(train), position=0, leave=True, desc=f\"Train Epoch {j}\") as train_bar1:\n#\n#     #     train_one_epoch(train, transforms1, transforms2, model, head, contrastive_loss, optimizer, 0, 1, train_bar1)\n#\n#     with tqdm(total=len(val), position=0, leave=True, desc=f\"Validate Epoch {j}\") as val_bar:\n#         metrics = validate_one_epoch(val, transforms1, transforms2, model, head, contrastive_loss, 0, 1, val_bar)\n#         console.log_metrics = {\n#             key : f\"{key}={metrics[key]:.2f}\" for key in metrics.keys()\n#         }\n#         console.log_metrics = \"; \".join(console.log_metrics.values())\n#         console.log(f\"Validate epoch {j}: \", console.log_metrics)\n\n\n# Now, we retrain the classification head without touching the representation. This is called fine tuning.\n\n\ndef evaluate(dataloader, model, head, loss_fn, val_bar):\n    # Set the model to evaluation mode - some NN pieces behave differently during training\n    # Unnecessary in this situation but added for best practices\n    model.eval()\n    size = len(dataloader)\n    num_batches = len(dataloader)\n    loss, correct = 0, 0\n\n    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n    with torch.no_grad():\n        # loop over all of the batches\n        for X, y in dataloader:\n\n            pred = head(model(X))\n            loss += loss_fn(pred, y).item()\n            # how many are correct in this batch? Tracking for accuracy \n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n            val_bar.update()\n            \n    loss /= num_batches\n    correct /= (size*batch_size)\n    \n    accuracy = 100*correct\n    return accuracy, loss\n\n\ndef fine_tune(dataloader, rep_model, head, loss_fn, optimizer, progress_bar):\n    head.train()\n    model.eval()\n    for batch1, (X, Y) in enumerate(dataloader):\n        # forward pass\n        # Calling detach blocks all gradients into the representation model!\n        rep = rep_model(X).detach()\n        pred = head(rep)\n        loss = loss_fn(pred, Y)\n        \n        \n        # backward pass calculates gradients\n        loss.backward()\n        \n        # take one step with these gradients\n        optimizer.step()\n        \n        # resets the gradients \n        optimizer.zero_grad()\n\n        correct = (pred.argmax(1) == Y).type(torch.float).mean().item()\n                    \n        # progress_bar.refresh()\n        cpu_metrics = {}\n        cpu_metrics[\"acc\"] = f\"{correct:.2f}\"\n        cpu_metrics[\"loss\"] = f\"{loss.detach().cpu().numpy():.2f}\"\n        progress_bar.update()\n        progress_bar.set_postfix(cpu_metrics)\n        # progress_bar.description = f\"Train loss: {loss.cpu():.2f} top5: {metrics['top5'].cpu():.2f}\"\n        # break\n\n\nclassification_head = nn.Linear(256, 10)  # .cuda()\nclassification_loss = loss_fn = nn.CrossEntropyLoss()\nfine_tune_optimizer = torch.optim.AdamW(classification_head.parameters(), lr=0.01)\nconsole.log(fine_tune_optimizer)\n\n\n# for j in range(5):\n#     with tqdm(total=len(train), position=0, leave=True, desc=f\"Fine Tune Epoch {j}\") as train_bar1:\n#\n#         fine_tune(train, model, classification_head, classification_loss, fine_tune_optimizer, train_bar1)\n#     with tqdm(total=len(val), position=0, leave=True, desc=f\"Validate Epoch {j}\") as val_bar:\n#         acc, loss = evaluate(val, model, classification_head, classification_loss, val_bar)\n#         console.log(f\"Epoch {j}: validation loss: {loss:.3f}, accuracy: {acc:.3f}\")",
    "crumbs": [
      "[01] Neural Networks",
      "[4] Representation Learning"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html",
    "href": "01-neural-networks/1-mnist/index.html",
    "title": "Intro to NNs: MNIST",
    "section": "",
    "text": "This tutorial will serve as a gentle introduction to neural networks and deep learning through a hands-on classification problem using the MNIST dataset.\nIn particular, we will introduce neural networks and how to train and improve their learning capabilities. We will use the PyTorch Python library.\nThe MNIST dataset contains thousands of examples of handwritten numbers, with each digit labeled 0-9.\nimport ambivalent\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport ezpz\n# console = ezpz.log.get_console()\nlogger = ezpz.get_logger('mnist')\n\nplt.style.use(ambivalent.STYLES['ambivalent'])\nsns.set_context(\"notebook\")\nplt.rcParams[\"figure.figsize\"] = [6.4, 4.8]\n\n[07/27/25 09:55:45] INFO     Setting logging level to 'INFO' on 'RANK == 0'                         __init__.py:265\n\n\n\n                    INFO     Setting logging level to 'CRITICAL' on all others 'RANK != 0'          __init__.py:266\n# %matplotlib inline\n\nimport torch\nimport torchvision\nfrom torch import nn\n\nimport numpy \nimport matplotlib.pyplot as plt\nimport time",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#the-mnist-dataset",
    "href": "01-neural-networks/1-mnist/index.html#the-mnist-dataset",
    "title": "Intro to NNs: MNIST",
    "section": "The MNIST dataset",
    "text": "The MNIST dataset\nWe will now download the dataset that contains handwritten digits. MNIST is a popular dataset, so we can download it via the PyTorch library.\nNote:\n\nx is for the inputs (images of handwritten digits)\ny is for the labels or outputs (digits 0-9)\nWe are given ‚Äútraining‚Äù and ‚Äútest‚Äù datasets.\n\nTraining datasets are used to fit the model.\nTest datasets are saved until the end, when we are satisfied with our model, to estimate how well our model generalizes to new data.\n\n\nNote that downloading it the first time might take some time.\nThe data is split as follows:\n\n60,000 training examples, 10,000 test examples\ninputs: 1 x 28 x 28 pixels\noutputs (labels): one integer per example\n\n\ntraining_data = torchvision.datasets.MNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=torchvision.transforms.ToTensor()\n)\n\ntest_data = torchvision.datasets.MNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=torchvision.transforms.ToTensor()\n)\n\n\ntrain_size = int(0.8 * len(training_data))  # 80% for training\nval_size = len(training_data) - train_size  # Remaining 20% for validation\ntraining_data, validation_data = torch.utils.data.random_split(\n    training_data,\n    [train_size, val_size],\n    generator=torch.Generator().manual_seed(55)\n)\n\n\nlogger.info(\n    \" \".join([\n        f\"MNIST data loaded:\",\n        f\"train={len(training_data)} examples\",\n        f\"validation={len(validation_data)} examples\",\n        f\"test={len(test_data)} examples\",\n        f\"input shape={training_data[0][0].shape}\" \n    ])\n)\n# logger.info(f'Input shape', training_data[0][0].shape)\n\n[07/27/25 09:55:46] INFO     MNIST data loaded: train=48000 examples validation=12000 examples      3921772995.py:1\n                             test=10000 examples input shape=torch.Size([1, 28, 28])                               \n\n\n\nLet‚Äôs take a closer look. Here are the first 10 training digits:\n\npltsize=1\n# plt.figure(figsize=(10*pltsize, pltsize))\n\nfor i in range(10):\n    plt.subplot(1,10,i+1)\n    plt.axis('off')\n    # x, y = training_data[i]\n    # plt.imshow(x.reshape(28, 28), cmap=\"gray\")\n    # x[0] is the image, x[1] is the label\n    plt.imshow(\n        numpy.reshape(\n            training_data[i][0],\n            (28, 28)\n        ),\n        cmap=\"gray\"\n    )\n    plt.title(f\"{training_data[i][1]}\")",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#generalities",
    "href": "01-neural-networks/1-mnist/index.html#generalities",
    "title": "Intro to NNs: MNIST",
    "section": "Generalities:",
    "text": "Generalities:\nTo train our classifier, we need (besides the data):\n\nA model that depend on parameters \\mathbf{\\theta}. Here we are going to use neural networks.\nA loss function J(\\mathbf{\\theta}) to measure the capabilities of the model.\nAn optimization method.",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#linear-model",
    "href": "01-neural-networks/1-mnist/index.html#linear-model",
    "title": "Intro to NNs: MNIST",
    "section": "Linear Model",
    "text": "Linear Model\nLet‚Äôs begin with a simple linear model: linear regression, like last week.\nWe add one complication: each example is a vector (flattened image), so the ‚Äúslope‚Äù multiplication becomes a dot product. If the target output is a vector as well, then the multiplication becomes matrix multiplication.\nNote, like before, we consider multiple examples at once, adding another dimension to the input.\n\n\n\n\n\n\nFigure¬†2: Fully connected linear net\n\n\n\nThe linear layers in PyTorch perform a basic xW + b.\nThese ‚Äúfully connected‚Äù layers connect each input to each output with some weight parameter.\nWe wouldn‚Äôt expect a simple linear model f(x) = xW+b directly outputting the class label and minimizing mean squared error to work well - the model would output labels like 3.55 and 2.11 instead of skipping to integers.\nWe now need:\n\nA loss function J(\\theta) where \\theta is the list of parameters (here W and b). Last week, we used mean squared error (MSE), but this week let‚Äôs make two changes that make more sense for classification:\n\nChange the output to be a length-10 vector of class probabilities (0 to 1, adding to 1).\nCross entropy as the loss function, which is typical for classification. You can read more here.\n\nAn optimization method or optimizer such as the stochastic gradient descent (sgd) method, the Adam optimizer, RMSprop, Adagrad etc. Let‚Äôs start with stochastic gradient descent (sgd), like last week. For far more information about more advanced optimizers than basic SGD, with some cool animations, see https://ruder.io/optimizing-gradient-descent/ or https://distill.pub/2017/momentum/.\nA learning rate. As we learned last week, the learning rate controls how far we move during each step.\n\n\nclass LinearClassifier(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # First, we need to convert the input image to a vector by using \n        # nn.Flatten(). For MNIST, it means the second dimension 28*28 becomes 784.\n        self.flatten = nn.Flatten()\n        # Here, we add a fully connected (\"dense\") layer that has 28 x 28 = 784 input nodes \n        #(one for each pixel in the input image) and 10 output nodes (for probabilities of each class).\n        self.layer_1 = nn.Linear(28*28, 10)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.layer_1(x)\n        return x\n\n\nlinear_model = LinearClassifier()\nlogger.info(linear_model)\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(linear_model.parameters(), lr=0.05)\n\n                    INFO     LinearClassifier(                                                      2844520859.py:2\n                               (flatten): Flatten(start_dim=1, end_dim=-1)                                         \n                               (layer_1): Linear(in_features=784, out_features=10, bias=True)                      \n                             )",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#learning",
    "href": "01-neural-networks/1-mnist/index.html#learning",
    "title": "Intro to NNs: MNIST",
    "section": "Learning",
    "text": "Learning\nNow we are ready to train our first model.\nA training step is comprised of:\n\nA forward pass: the input is passed through the network\nBackpropagation: A backward pass to compute the gradient \\frac{\\partial J}{\\partial \\mathbf{W}} of the loss function with respect to the parameters of the network.\nWeight updates \\mathbf{W} = \\mathbf{W} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W}} where \\alpha is the learning rate.\n\nHow many steps do we take?\n\nThe batch size corresponds to the number of training examples in one pass (forward + backward).\n\nA smaller batch size allows the model to learn from individual examples but takes longer to train.\nA larger batch size requires fewer steps but may result in the model not capturing the nuances in the data.\n\nThe higher the batch size, the more memory you will require.\nAn epoch means one pass through the whole training data (looping over the batches). Using few epochs can lead to underfitting and using too many can lead to overfitting.\nThe choice of batch size and learning rate are important for performance, generalization and accuracy in deep learning.\n\n\nbatch_size = 128\n\n# The dataloader makes our dataset iterable \ntrain_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\nval_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n\n\ndef train_one_epoch(dataloader, model, loss_fn, optimizer):\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # forward pass\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        # backward pass calculates gradients\n        loss.backward()\n        # take one step with these gradients\n        optimizer.step()\n        # resets the gradients \n        optimizer.zero_grad()\n\n\ndef evaluate(dataloader, model, loss_fn):\n    # Set the model to evaluation mode - some NN pieces behave differently during training\n    # Unnecessary in this situation but added for best practices\n    model.eval()\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    loss, correct = 0, 0\n\n    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n    with torch.no_grad():\n        # loop over all of the batches\n        for X, y in dataloader:\n            pred = model(X)\n            loss += loss_fn(pred, y).item()\n            # how many are correct in this batch? Tracking for accuracy \n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    loss /= num_batches\n    correct /= size\n\n    accuracy = 100*correct\n    return accuracy, loss\n\n\n%%time\n\nepochs = 5\ntrain_acc_all = []\nval_acc_all = []\nfor j in range(epochs):\n    train_one_epoch(train_dataloader, linear_model, loss_fn, optimizer)\n\n    # checking on the training loss and accuracy once per epoch\n    acc, loss = evaluate(train_dataloader, linear_model, loss_fn)\n    train_acc_all.append(acc)\n    logger.info(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n\n    # checking on the validation loss and accuracy once per epoch\n    val_acc, val_loss = evaluate(val_dataloader, linear_model, loss_fn)\n    val_acc_all.append(val_acc)\n    logger.info(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n\n[07/27/25 09:55:48] INFO     Epoch 0: training loss: 0.5037873729864756, accuracy:                  &lt;timed exec&gt;:10\n                             87.56041666666667                                                                     \n\n\n\n[07/27/25 09:55:49] INFO     Epoch 0: val. loss: 0.49559599478194055, val. accuracy:                &lt;timed exec&gt;:15\n                             87.44166666666666                                                                     \n\n\n\n[07/27/25 09:55:51] INFO     Epoch 1: training loss: 0.4227330549955368, accuracy:                  &lt;timed exec&gt;:10\n                             89.04791666666667                                                                     \n\n\n\n                    INFO     Epoch 1: val. loss: 0.413072535966305, val. accuracy:                  &lt;timed exec&gt;:15\n                             88.76666666666667                                                                     \n\n\n\n[07/27/25 09:55:54] INFO     Epoch 2: training loss: 0.38845182700951897, accuracy:                 &lt;timed exec&gt;:10\n                             89.69166666666666                                                                     \n\n\n\n                    INFO     Epoch 2: val. loss: 0.37828945035630085, val. accuracy: 89.4           &lt;timed exec&gt;:15\n\n\n\n[07/27/25 09:55:56] INFO     Epoch 3: training loss: 0.36831341989835104, accuracy: 90.06875        &lt;timed exec&gt;:10\n\n\n\n[07/27/25 09:55:57] INFO     Epoch 3: val. loss: 0.3580227758973203, val. accuracy:                 &lt;timed exec&gt;:15\n                             89.96666666666667                                                                     \n\n\n\n[07/27/25 09:55:59] INFO     Epoch 4: training loss: 0.35462561865647635, accuracy: 90.4125         &lt;timed exec&gt;:10\n\n\n\n                    INFO     Epoch 4: val. loss: 0.3443837852237072, val. accuracy: 90.25           &lt;timed exec&gt;:15\n\n\n\nCPU times: user 12.2 s, sys: 1.74 s, total: 13.9 s\nWall time: 13.6 s\n\n\n\nplt.figure()\nplt.plot(range(epochs), train_acc_all, label='Training Acc.' )\nplt.plot(range(epochs), val_acc_all, label='Validation Acc.' )\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\n\n\n\n\n\n\n\n\n# Visualize how the model is doing on the first 10 examples\npltsize=1\nplt.figure(figsize=(10*pltsize, pltsize))\nlinear_model.eval()\nbatch = next(iter(train_dataloader))\npredictions = linear_model(batch[0])\n\nfor i in range(10):\n    plt.subplot(1,10,i+1)\n    plt.axis('off')\n    plt.imshow(batch[0][i,0,:,:], cmap=\"gray\")\n    plt.title('%d' % predictions[i,:].argmax())\n\n\n\n\n\n\n\n\nExercise: How can you improve the accuracy? Some things you might consider: increasing the number of epochs, changing the learning rate, etc.",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#prediction",
    "href": "01-neural-networks/1-mnist/index.html#prediction",
    "title": "Intro to NNs: MNIST",
    "section": "Prediction",
    "text": "Prediction\nLet‚Äôs see how our model generalizes to the unseen test data.\n\n#For HW: cell to change batch size\n#create dataloader for test data\n# The dataloader makes our dataset iterable\n\nbatch_size_test = 256 \ntest_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test)\n\n\nacc_test, loss_test = evaluate(test_dataloader, linear_model, loss_fn)\nlogger.info(f\"Test loss: {loss_test}, test accuracy: {acc_test}\")\n# logger.info(\"Test loss: %.4f, test accuracy: %.2f%%\" % (loss_test, acc_test))\n\n[07/27/25 09:56:00] INFO     Test loss: 0.33322220724076035, test accuracy: 90.85                    372756021.py:2\n\n\n\nWe can now take a closer look at the results.\nLet‚Äôs define a helper function to show the failure cases of our classifier.\n\ndef show_failures(model, dataloader, maxtoshow=10):\n    model.eval()\n    batch = next(iter(dataloader))\n    predictions = model(batch[0])\n\n    rounded = predictions.argmax(1)\n    errors = rounded!=batch[1]\n    logger.info(\n        f\"Showing max {maxtoshow} first failures.\"\n    )\n    logger.info(\"The predicted class is shown first and the correct class in parentheses.\")\n    ii = 0\n    plt.figure(figsize=(maxtoshow, 1))\n    for i in range(batch[0].shape[0]):\n        if ii&gt;=maxtoshow:\n            break\n        if errors[i]:\n            plt.subplot(1, maxtoshow, ii+1)\n            plt.axis('off')\n            plt.imshow(batch[0][i,0,:,:], cmap=\"gray\")\n            plt.title(\"%d (%d)\" % (rounded[i], batch[1][i]))\n            ii = ii + 1\n\nHere are the first 10 images from the test data that this small model classified to a wrong class:\n\nshow_failures(linear_model, test_dataloader)\n\n                    INFO     Showing max 10 first failures.                                         2368214845.py:8\n\n\n\n                    INFO     The predicted class is shown first and the correct class in           2368214845.py:11\n                             parentheses.",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#multilayer-model",
    "href": "01-neural-networks/1-mnist/index.html#multilayer-model",
    "title": "Intro to NNs: MNIST",
    "section": "Multilayer Model",
    "text": "Multilayer Model\nOur linear model isn‚Äôt enough for high accuracy on this dataset. To improve the model, we often need to add more layers and nonlinearities.\n\n\n\n\n\n\nFigure¬†3: Shallow neural network\n\n\n\nThe output of this NN can be written as\n\n\\begin{equation}\n  \\hat{u}(x) = \\sigma_2(\\sigma_1(\\mathbf{x}\\mathbf{W}_1 + \\mathbf{b}_1)\\mathbf{W}_2 + \\mathbf{b}_2),\n\\end{equation}\n\nwhere \\mathbf{x} is the input, \\mathbf{W}_j are the weights of the neural network, \\sigma_j the (nonlinear) activation functions, and \\mathbf{b}_j its biases. The activation function introduces the nonlinearity and makes it possible to learn more complex tasks. Desirable properties in an activation function include being differentiable, bounded, and monotonic.\nImage source: PragatiBaheti\n\n\n\n\n\n\nFigure¬†4: Activation functions\n\n\n\nAdding more layers to obtain a deep neural network:\n\n\n\n\n\n\nFigure¬†5",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#important-things-to-know",
    "href": "01-neural-networks/1-mnist/index.html#important-things-to-know",
    "title": "Intro to NNs: MNIST",
    "section": "Important things to know",
    "text": "Important things to know\nDeep Neural networks can be overly flexible/complicated and ‚Äúoverfit‚Äù your data, just like fitting overly complicated polynomials:\n\n\n\n\n\n\nFigure¬†6: Bias-variance tradeoff\n\n\n\nVizualization wrt to the accuracy and loss (Image source: Baeldung):\n\n\n\n\n\n\nFigure¬†7: Visualization of accuracy and loss\n\n\n\nTo improve the generalization of our model on previously unseen data, we employ a technique known as regularization, which constrains our optimization problem in order to discourage complex models.\n\nDropout is the commonly used regularization technique. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\nPenalizing the loss function by adding a term such as \\lambda ||\\mathbf{W}||^2 is alsp a commonly used regularization technique. This helps ‚Äúcontrol‚Äù the magnitude of the weights of the network.\n\n\nVanishing gradients\n\nGradients become small as they propagate backward through the layers.\n\n\nSquashing activation functions like sigmoid or tanh could cause this.\n\nExploding gradients\n\nGradients grow exponentially usually due to ‚Äúpoor‚Äù weight initialization.\n\n\nWe can now implement a deep network in PyTorch.\nnn.Dropout() performs the Dropout operation mentioned earlier:\n\n#For HW: cell to change activation\nclass NonlinearClassifier(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.layers_stack = nn.Sequential(\n            nn.Linear(28*28, 50),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(50, 50),\n            nn.ReLU(),\n           # nn.Dropout(0.2),\n            nn.Linear(50, 50),\n            nn.ReLU(),\n           # nn.Dropout(0.2),\n            nn.Linear(50, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.layers_stack(x)\n\n        return x\n\n\n#### For HW: cell to change learning rate\nnonlinear_model = NonlinearClassifier()\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.05)\n\n\n%%time\n\nepochs = 5\ntrain_acc_all = []\nval_acc_all = []\nfor j in range(epochs):\n    train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)\n\n    # checking on the training loss and accuracy once per epoch\n    acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n    train_acc_all.append(acc)\n    logger.info(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n\n    # checking on the validation loss and accuracy once per epoch\n    val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n    val_acc_all.append(val_acc)\n    logger.info(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n\n[07/27/25 09:56:03] INFO     Epoch 0: training loss: 0.7723248308499654, accuracy:                  &lt;timed exec&gt;:10\n                             78.19791666666667                                                                     \n\n\n\n                    INFO     Epoch 0: val. loss: 0.7633352856686775, val. accuracy:                 &lt;timed exec&gt;:15\n                             77.96666666666667                                                                     \n\n\n\n[07/27/25 09:56:05] INFO     Epoch 1: training loss: 0.3956672025918961, accuracy:                  &lt;timed exec&gt;:10\n                             88.78333333333333                                                                     \n\n\n\n[07/27/25 09:56:06] INFO     Epoch 1: val. loss: 0.39052720383760775, val. accuracy:                &lt;timed exec&gt;:15\n                             88.69166666666666                                                                     \n\n\n\n[07/27/25 09:56:08] INFO     Epoch 2: training loss: 0.29944101695219677, accuracy:                 &lt;timed exec&gt;:10\n                             91.47291666666668                                                                     \n\n\n\n[07/27/25 09:56:09] INFO     Epoch 2: val. loss: 0.2925776110050526, val. accuracy: 91.325          &lt;timed exec&gt;:15\n\n\n\n[07/27/25 09:56:11] INFO     Epoch 3: training loss: 0.2519585817058881, accuracy:                  &lt;timed exec&gt;:10\n                             92.67916666666667                                                                     \n\n\n\n                    INFO     Epoch 3: val. loss: 0.2485892482577486, val. accuracy:                 &lt;timed exec&gt;:15\n                             92.39166666666667                                                                     \n\n\n\n[07/27/25 09:56:14] INFO     Epoch 4: training loss: 0.21475721516211826, accuracy: 93.71875        &lt;timed exec&gt;:10\n\n\n\n                    INFO     Epoch 4: val. loss: 0.21536820817818034, val. accuracy:                &lt;timed exec&gt;:15\n                             93.53333333333333                                                                     \n\n\n\nCPU times: user 12.7 s, sys: 1.84 s, total: 14.5 s\nWall time: 14 s\n\n\n\n# pltsize=1\n# plt.figure(figsize=(10*pltsize, 10 * pltsize))\nplt.figure()\nplt.plot(range(epochs), train_acc_all,label = 'Training Acc.' )\nplt.plot(range(epochs), val_acc_all, label = 'Validation Acc.' )\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\n\n\n\n\n\n\n\n\nshow_failures(nonlinear_model, test_dataloader)\n\n                    INFO     Showing max 10 first failures.                                         2368214845.py:8\n\n\n\n                    INFO     The predicted class is shown first and the correct class in           2368214845.py:11\n                             parentheses.",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#recap",
    "href": "01-neural-networks/1-mnist/index.html#recap",
    "title": "Intro to NNs: MNIST",
    "section": "Recap",
    "text": "Recap\nTo train and validate a neural network model, you need:\n\nData split into training/validation/test sets,\nA model with parameters to learn\nAn appropriate loss function\nAn optimizer (with tunable parameters such as learning rate, weight decay etc.) used to learn the parameters of the model.",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#homework",
    "href": "01-neural-networks/1-mnist/index.html#homework",
    "title": "Intro to NNs: MNIST",
    "section": "Homework",
    "text": "Homework\n\nCompare the quality of your model when using different:\n\n\nbatch sizes\nlearning rates\nactivation functions\n\n\nBonus: What is a learning rate scheduler?\n\nIf you have time, experiment with how to improve the model.\nNote: training and validation data can be used to compare models, but test data should be saved until the end as a final check of generalization.",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#homework-solution",
    "href": "01-neural-networks/1-mnist/index.html#homework-solution",
    "title": "Intro to NNs: MNIST",
    "section": "Homework solution",
    "text": "Homework solution\nMake the following changes to the cells with the comment ‚Äú#For HW‚Äù\n\n#####################To modify the batch size##########################\nbatch_size = 32 # 64, 128, 256, 512\n\n# The dataloader makes our dataset iterable \ntrain_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\nval_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n##############################################################################\n\n\n##########################To change the learning rate##########################\noptimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.01) #modify the value of lr\n##############################################################################\n\n\n##########################To change activation##########################\n###### Go to https://pytorch.org/docs/main/nn.html#non-linear-activations-weighted-sum-nonlinearity for more activations ######\nclass NonlinearClassifier(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.layers_stack = nn.Sequential(\n            nn.Linear(28*28, 50),\n            nn.Sigmoid(), #nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(50, 50),\n            nn.Tanh(), #nn.ReLU(),\n           # nn.Dropout(0.2),\n            nn.Linear(50, 50),\n            nn.ReLU(),\n           # nn.Dropout(0.2),\n            nn.Linear(50, 10)\n        )\n        \n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.layers_stack(x)\n\n        return x\n##############################################################################\n\nBonus question: A learning rate scheduler is an essential deep learning technique used to dynamically adjust the learning rate during training. This strategic can significantly impact the convergence speed and overall performance of a neural network. See below on how to incorporate it to your training.\n\nnonlinear_model = NonlinearClassifier()\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.1)\n\n# Step learning rate scheduler: reduce by a factor of 0.1 every 2 epochs (only for illustrative purposes)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\n\n%%time\n\nepochs = 6\ntrain_acc_all = []\nval_acc_all = []\nfor j in range(epochs):\n    train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)\n    #step the scheduler\n    scheduler.step()\n\n    # logger.info the current learning rate\n    current_lr = optimizer.param_groups[0]['lr']\n    logger.info(f\"Epoch {j+1}/{epochs}, Learning Rate: {current_lr}\")\n\n    # checking on the training loss and accuracy once per epoch\n    acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n    train_acc_all.append(acc)\n    logger.info(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n\n    # checking on the validation loss and accuracy once per epoch\n    val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n    val_acc_all.append(val_acc)\n    logger.info(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n\n[07/27/25 09:56:16] INFO     Epoch 1/6, Learning Rate: 0.1                                          &lt;timed exec&gt;:11\n\n\n\n[07/27/25 09:56:18] INFO     Epoch 0: training loss: 0.37463608242571356, accuracy:                 &lt;timed exec&gt;:16\n                             89.01666666666667                                                                     \n\n\n\n                    INFO     Epoch 0: val. loss: 0.36558178035418193, val. accuracy:                &lt;timed exec&gt;:21\n                             89.08333333333334                                                                     \n\n\n\n[07/27/25 09:56:20] INFO     Epoch 2/6, Learning Rate: 0.010000000000000002                         &lt;timed exec&gt;:11\n\n\n\n[07/27/25 09:56:21] INFO     Epoch 1: training loss: 0.24827922082692386, accuracy: 92.54375        &lt;timed exec&gt;:16\n\n\n\n[07/27/25 09:56:22] INFO     Epoch 1: val. loss: 0.23980822887519995, val. accuracy:                &lt;timed exec&gt;:21\n                             92.63333333333334                                                                     \n\n\n\n[07/27/25 09:56:24] INFO     Epoch 3/6, Learning Rate: 0.010000000000000002                         &lt;timed exec&gt;:11\n\n\n\n[07/27/25 09:56:25] INFO     Epoch 2: training loss: 0.23335267795125644, accuracy:                 &lt;timed exec&gt;:16\n                             93.01458333333333                                                                     \n\n\n\n                    INFO     Epoch 2: val. loss: 0.22703989148139953, val. accuracy: 93.075         &lt;timed exec&gt;:21\n\n\n\n[07/27/25 09:56:27] INFO     Epoch 4/6, Learning Rate: 0.0010000000000000002                        &lt;timed exec&gt;:11\n\n\n\n[07/27/25 09:56:29] INFO     Epoch 3: training loss: 0.22608824720854562, accuracy:                 &lt;timed exec&gt;:16\n                             93.22916666666666                                                                     \n\n\n\n                    INFO     Epoch 3: val. loss: 0.2204117145339648, val. accuracy:                 &lt;timed exec&gt;:21\n                             93.19166666666666                                                                     \n\n\n\n[07/27/25 09:56:31] INFO     Epoch 5/6, Learning Rate: 0.0010000000000000002                        &lt;timed exec&gt;:11\n\n\n\n[07/27/25 09:56:32] INFO     Epoch 4: training loss: 0.2247082859997948, accuracy: 93.25625         &lt;timed exec&gt;:16\n\n\n\n[07/27/25 09:56:33] INFO     Epoch 4: val. loss: 0.21926352487504483, val. accuracy:                &lt;timed exec&gt;:21\n                             93.21666666666667                                                                     \n\n\n\n[07/27/25 09:56:35] INFO     Epoch 6/6, Learning Rate: 0.00010000000000000003                       &lt;timed exec&gt;:11\n\n\n\n[07/27/25 09:56:36] INFO     Epoch 5: training loss: 0.2240890618575116, accuracy:                  &lt;timed exec&gt;:16\n                             93.25208333333333                                                                     \n\n\n\n                    INFO     Epoch 5: val. loss: 0.2188318810860316, val. accuracy: 93.25           &lt;timed exec&gt;:21\n\n\n\nCPU times: user 18.6 s, sys: 5.65 s, total: 24.2 s\nWall time: 22.1 s",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/index.html#contents",
    "href": "00-intro-AI-HPC/index.html#contents",
    "title": "Introduction to AI on Supercomputers",
    "section": "Contents",
    "text": "Contents\n\nüìÇ 00-intro-AI-HPC/\n\nüìÑ 0-compute-systems\nüìÑ 1-shared-resources\nüìó 2-jupyter-notebooks\nüìÑ 3-homework\nüìÑ 4-nersc\n\n\nLink to original slides: https://drive.google.com/file/d/1PH6HlXPhsVB1wDcEkfRSZrDQTqWBi7aH/view?usp=sharing",
    "crumbs": [
      "[00] Intro to AI and HPC"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html",
    "title": "Artificial intelligence in a nutshell",
    "section": "",
    "text": "In this notebook, we will talk about:",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#how-to-run-this-notebook-on-polaris",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#how-to-run-this-notebook-on-polaris",
    "title": "Artificial intelligence in a nutshell",
    "section": "How to run this notebook on Polaris",
    "text": "How to run this notebook on Polaris\n\nGo to https://jupyter.alcf.anl.gov, and click ‚ÄúLogin Polaris‚Äù\nAfter login, select ALCFAITP project and ALCFAITP queue during the lecture (use debug queue outside of the lecture)\nLoad the notebook and select ‚Äúdatascience/conda-2023-01-10‚Äù python kernel\n\n\n\n\n\n\n\nFigure¬†1\n\n\n\nHow to run this notebook on Google Colab\n\nGo to https://colab.research.google.com/, sign in or sign up\n‚ÄúFile‚Äù-&gt; ‚Äúopen notebook‚Äù\nChoose 01_intro_AI_on_Supercomputer/01_linear_regression_sgd.ipynb from the list\n\n\n\nGoogle Colab",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#what-is-ai-training",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#what-is-ai-training",
    "title": "Artificial intelligence in a nutshell",
    "section": "What is AI training?",
    "text": "What is AI training?\nTwo ways of learning English:\n\nthrough learning rules;\nthrough hearing a lot of speakings\n\n\n\n\n\n\n\nFigure¬†2: Data Driven Learning\n\n\n\nI learned English in my middle school, and memorized a lot of grammar rules in my mind. Every time when I speak, I try to follow the grammar rules as much as I can. But I always break the rules.\nHowever, my daugher learned English differently. She learns speaking by hearing a lot of speaking from TV, teachers, classmates, and her older brother. The fact is that, she seldomly breaks grammar rules. This way of learning by observing patterns is very powerful! This is the essence of AI or data driven science.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#how-does-large-language-model-work",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#how-does-large-language-model-work",
    "title": "Artificial intelligence in a nutshell",
    "section": "How does large language model work?",
    "text": "How does large language model work?\nLarge Language Models, like GPT, function by pre-training on extensive datasets to learn language patterns, utilizing transformer architecture for contextual understanding, and can be fine-tuned for specific tasks, enabling them to generate coherent and contextually relevant text based on provided inputs.\n\n\n\n\n\n\nFigure¬†3\n\n\n\nMore complicated example:\n\n\n\n\n\nPoem about Albert Einstein\n\n\n\n\n\n\n\nPoem about Martin Luther\n\n\n\n\nYou can do this on https://chat.openai.com",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#simplest-ai-model-example-linear-regression",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#simplest-ai-model-example-linear-regression",
    "title": "Artificial intelligence in a nutshell",
    "section": "Simplest AI model example: linear regression",
    "text": "Simplest AI model example: linear regression\nThis example is adopted from Bethany Lusch, ALCF.\nLinear regression is the simplest example learning from existing data for future prediction.\n\n\n\n\n\n\nFigure¬†4: Linear regression in Excel\n\n\n\nWe‚Äôre going to review the math involved in this process to help understand how training an AI works.\nFirst we will load some tools that others wrote and we can use to help us work.\n\nPandas: a toolkit for working with row vs. column data, like excel sheets, and CSV (Comma Seperated Values) files.\nNumpy: a toolkit for managing arrays, vectors, matrices, etc, doing math with them, slicing them up, and many other handy things.\nMatplotlib: a toolkit for plotting data\n\n\nimport os\nos.environ[\"FORCE_COLOR\"] = \"1\"\nos.environ[\"TTY_INTERACTIVE\"] = \"1\"\nimport ambivalent\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use(ambivalent.STYLES['ambivalent'])\nsns.set_context(\"notebook\")\nplt.rcParams[\"figure.figsize\"] = [6.4, 4.8]\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport IPython.display as ipydis\nimport time\n\nimport ezpz\nlogger = ezpz.get_logger('linear-regression')\n\n[07/27/25 09:52:27] INFO     Setting logging level to 'INFO' on 'RANK == 0'                         __init__.py:265\n\n\n\n                    INFO     Setting logging level to 'CRITICAL' on all others 'RANK != 0'          __init__.py:266\n\n\n\n\nDataset\nWe used a realestate dataset from Kaggle to produce this reduced dataset.\nThis dataset contains the sale price and above ground square feet of many houses. We can use this data for our linear regression.\nWe use Pandas to read the data file which is stored as Comma Separated Values (CSV) and logger.info the column labels.\nCSV files are similar to excel sheets.\n\n! [ -e ./slimmed_realestate_data.csv ] || wget https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/01_intro_AI_on_Supercomputer/slimmed_realestate_data.csv\ndata = pd.read_csv('slimmed_realestate_data.csv')\nlogger.info(data.columns)\n\n[07/27/25 09:52:28] INFO     Index(['Unnamed: 0', 'SalePrice', 'GrLivArea'], dtype='object')        1606159476.py:3\n\n\n\nNow pandas provides some helpful tools for us to inspect our data.\nIt provides a plot() function that, behind the scenes, is calling into the Matplotlib library and calling the function matplotlib.pyplot.plot().\nIn this case, we simply tell it the names of the columns we want as our x and y values and the style ('.' tells matplotlib to use a small dot to represent each data point).\n\ndata.plot(x='GrLivArea', y='SalePrice',style='o', alpha=0.8, markeredgecolor=\"#222\")\n\n\n\n\n\n\n\n\nor, even better yet, use seaborn to plot the data:\n\nsns.jointplot(\n    x=\"GrLivArea\",\n    y=\"SalePrice\",\n    data=data,\n    kind='reg',\n    color=(216 / 255.0, 100 / 255.0, 50 / 255.0, 0.33),\n)\n\n\n\n\n\n\n\n\n\n\nTheory of linear regression\nThe goal of learning regression is to find a line that is closest to all the points.\nThe slope and intercept of such a line y = m x + b can be found as:\nm = { n (\\Sigma xy) - (\\Sigma x) (\\Sigma y) \\over n (\\Sigma x^2) - (\\Sigma x)^2 } \nb = { (\\Sigma y) (\\Sigma x^2) - (\\Sigma x) (\\Sigma xy) \\over n (\\Sigma x^2) - (\\Sigma x)^2 } \nDetails derivation of this can be found here.\nWe‚Äôll break this calculation into a few steps to help make it easier.\nFirst lets define x and y. x will be our above ground square footage and y will be sale price. In our equations we have a few different values we need, such as n which is just the number of points we have:\n\nn = len(data)\n\nThen we need our x and y by selecting only the column we care about for each one. Note about data formats: data is a Pandas DataFrame object which has rows and columns; data['GrLivArea'] is a Pandas Series object which only has rows; then we also convert from Pandas data formats (in this case a Series) to Numpy data formats using the to_numpy() function which is part of the Pandas Series object.\n\nx = data['GrLivArea'].to_numpy()\ny = data['SalePrice'].to_numpy()\n\nNow we will calculate \\Sigma xy, \\Sigma x, \\Sigma y, and \\Sigma x^2:\n\nsum_xy = np.sum(x*y)\nsum_x = np.sum(x)\nsum_y = np.sum(y)\nsum_x2 = np.sum(x*x)\n\nThe denominator in the equation for m and b are the same so we can calculate that once:\n\ndenominator = n * sum_x2 - sum_x * sum_x\n\nThen we can calculate our fit values:\n\nm = (n * sum_xy - sum_x * sum_y) / denominator\nb = (sum_y * sum_x2 - sum_x * sum_xy) / denominator\nlogger.info('y = %f * x + %f' % (m,b))\n\n# saving these for later comparison\nm_calc = m\nb_calc = b\n\n                    INFO     y = 87.688145 * x + 34754.077892                                       1833436152.py:3\n\n\n\nNow we can plot the fit results with our data to see how we did.\nFirst we define a plotting function because we‚Äôre going to do this often and we want to reuse our code:\n\ndef plot_data(x,y,m,b,plt = plt):\n   # plot our data points with 'bo' = blue circles\n   plt.plot(x, y, 'o', alpha=0.8, markeredgecolor=\"#222\")\n   # create the line based on our linear fit\n   # first we need to make x points\n   # the 'arange' function generates points between two limits (min,max)\n   linear_x = np.arange(x.min(),x.max())\n   # now we use our fit parameters to calculate the y points based on our x points\n   linear_y = linear_x * m + b\n   # plot the linear points using 'r-' = red line\n   plt.plot(linear_x, linear_y, 'r-', label='fit')\n\nNow can use this function to plot our results:\n\nplot_data(x,y,m,b)\n\n\n\n\n\n\n\n\n\n\nTraining through Stochastic Gradient Descent (SGD)\nSGD is a common method in AI for training deep neural networks on large datasets. It is an iterative method for optimizing a loss function that we get to define. We will use this simple linear regression to demonstrate how it works.\n\nThe model\nIn AI, neural networks are often referred to as a model because, once fully trained, they should model (AKA predict) the behavior of our system. In our example, the system is how house prices vary based on house size. We know our system is roughly driven by a linear function:\n\\hat{y_i}(x_i) = m * x_i + b \nWe just need to figure out m and b. Let‚Äôs create a function that calculates our model given x, m, and b.\n\ndef model(x,m,b):\n   return m * x + b\n\n\n\nThe Loss Function\nA loss function, or objective function, is something we define and is based on what we want to achieve. In the method of SGD, it is our goal to minimize (or make close to zero) the values calculated from the loss function. In our example, we ideally want the prediction of our model to be equal to the actual data, though we will settle for ‚Äúas close as possible‚Äù.\nSo we will select our loss function to be the Mean Squared Error function:\n L(y_i,\\hat{y_i}) = (y_i - \\hat{y_i}(x_i))^2 \nwhere y_i is our i^{th} entry in the data['SalePrice'] vector and \\hat{y_i} is the prediction based on evaluting m * x_i + b.\nThis function looks like the figure below when we plot it with x=y_i - \\hat{y_i}(x_i) and we we want to be down near y_i - \\hat{y_i}(x_i) = 0 which indicates that our y_i is as close as possible to \\hat{y_i}.\n\n\n\n\n\n\nFigure¬†5: Loss function for linear regression\n\n\n\nHere we crate a function that calculates this for us.\n\ndef loss(x,y,m,b):\n   y_predicted = model(x,m,b)\n   return np.power( y - y_predicted, 2 )\n\n\n\nMinimizing the Loss Function\nWe want to use the loss function in order to guide how to update m and b to better model our system. In calculus we learn to minimize a function with respect to a variable you calculate the partial derivative with respect to the variable you want to vary.\n { \\partial L \\over \\partial m } = 0 \nThe location of the solution to this is the minimum as shown in the figure above. We can write down the partial derivative of the loss function as:\n { \\partial L \\over \\partial m } = -2 x_i (y_i - \\hat{y_i}(x_i))   { \\partial L \\over \\partial b } = -2 (y_i - \\hat{y_i}(x_i)) \nWe can use this to calculate an adjustment to m and b that will reduce the loss function, effectively improving our fitting parameters. This is done using this equation:\n m' = m - \\eta { \\partial L \\over \\partial m }  b' = b - \\eta { \\partial L \\over \\partial b }\nHere our original m and b are adjusted by the partial derivative multiplied by some small factor, \\eta, called the learning rate. This learning rate is very important in our process and must be tuned for every problem.\nIn our example, the selection of the learning rate essentially defines how close we can get to the minimum, AKA the best fit solution. This figure shows what happens when we pick a large learning rate. We first select a starting point in our loss function (typically randomly), then every update from m/b to m'/b' results in a shift to somewhere else on our loss function (following the red arrows). In this example, our learning rate (\\eta) has been selected too large such that we bounce back and forth around the minimum, never reaching it.\n\n\n\n\n\n\nFigure¬†6: Large LR\n\n\n\nIf we select a smaller learning we can see better behavior in the next figure.\n\n\n\n\n\n\nFigure¬†7: Small LR\n\n\n\nThough, keep in mind, too small a learning rate results is so little progress toward the minimum that you may never reach it!\nA pit fall of SGD that one must be aware of is when your loss function is complex, with many minima. The next figure shows such a case, in which we select a small learning rate and our starting point happens to be near a local minimum that is not the lowest minimum. As shown, we do reach a minimum, but it isn‚Äôt the lowest minimum in our loss function. It could be that we randomly select a starting point near the minimum we care about, but we should build methods that are more robust against randomly getting the right answer.\n\n\n\n\n\n\nFigure¬†8: Local minimal with small LR\n\n\n\nThen, if we increase our learning rate too much, we bounce around again.\n\n\n\n\n\n\nFigure¬†9: Local minimal with large LR\n\n\n\nWhat we want to do in this situation is start with a large learning rate and slowly reduce its size as we progress. That is shown in this next figure.\n\n\n\n\n\n\nFigure¬†10: Local min with variable LR\n\n\n\nAs you can see, this process is not perfect and could still land in a local minimum, but it is important to be aware of these behaviors as you utilize SGD in machine learning.\nSo let‚Äôs continue, we‚Äôll build functions we can use to update our fit parameters, m and b.\n\ndef updated_m(x,y,m,b,learning_rate):\n   dL_dm = - 2 * x * (y - model(x,m,b))\n   dL_dm = np.mean(dL_dm)\n   return m - learning_rate * dL_dm\n\ndef updated_b(x,y,m,b,learning_rate):\n   dL_db = - 2 * (y - model(x,m,b))\n   dL_db = np.mean(dL_db)\n   return b - learning_rate * dL_db",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#putting-it-together",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#putting-it-together",
    "title": "Artificial intelligence in a nutshell",
    "section": "Putting it together",
    "text": "Putting it together\nWe can now randomly select our initial slope and intercept:\n\nm = 5.\nb = 1000.\nlogger.info(f\"y_i = {m:.2f} * x + {b:.2f}\")\n# logger.info('y_i = %.2f * x + %.2f' % (m,b))\n\n                    INFO     y_i = 5.00 * x + 1000.00                                               1397955390.py:3\n\n\n\nThen we can calculate our Loss function:\n\nl = loss(x,y,m,b)\nlogger.info(f'first 10 loss values: {l[:10]}')\n\n                    INFO     first 10 loss values: [3.03421561e+10 3.55511025e+10 1.24579082e+10    3769949415.py:2\n                             1.91656336e+10                                                                        \n                              1.60604929e+10 2.04432804e+10 1.72410030e+10 1.76517796e+10                          \n                              1.52769600e+10 2.18152900e+10]                                                       \n\n\n\n\nlearning_rate = 1e-9\nm = updated_m(x,y,m,b,learning_rate)\nb = updated_b(x,y,m,b,learning_rate)\nlogger.info('y_i = %.2f * x + %.2f     previously calculated: y_i = %.2f * x + %.2f' % (m,b,m_calc,b_calc))\nplot_data(x,y,m,b)\n\n                    INFO     y_i = 5.47 * x + 1000.00     previously calculated: y_i = 87.69 * x +   963106386.py:4\n                             34754.08                                                                              \n\n\n\n\n\n\n\n\n\n\n\n# set our initial slope and intercept\nm = 5.\nb = 1000.\n# batch_size = 60\n# set a learning rate for each parameter\nlearning_rate_m = 1e-7\nlearning_rate_b = 1e-1\n# use these to plot our progress over time\nloss_history = []\n# convert panda data to numpy arrays, one for the \"Ground Living Area\" and one for \"Sale Price\"\ndata_x = data['GrLivArea'].to_numpy()\ndata_y = data['SalePrice'].to_numpy()\n# we run our loop N times\nloop_N = 30\nfor i in range(loop_N):\n    # update our slope and intercept based on the current values\n    m = updated_m(data_x,data_y,m,b,learning_rate_m)\n    b = updated_b(data_x,data_y,m,b,learning_rate_b)\n\n    # calculate the loss value\n    loss_value = np.mean(loss(data_x,data_y,m,b))\n\n    # keep a history of our loss values\n    loss_history.append(loss_value)\n\n    # logger.info our progress\n    mstr = \" \".join([\n        f\"[{i:03d}]\",\n        f\"dy_i = {m:.2f} * x + {b:.2f}\",\n        f\"previously calculated: y_i = {m_calc:.2f} * x + {b_calc:.2f}\",\n        f\"loss: {loss_value:.2f}\",\n    ])\n    logger.info(mstr)\n    # logger.info(\n    #         '[%03d]  dy_i = %.2f * x + %.2f     previously calculated: y_i = %.2f * x + %.2f    loss: %f' % (i,m,b,m_calc,b_calc,loss_value))\n\n    # close/delete previous plots\n    plt.close('all')\n\n    dfigsize = plt.rcParams['figure.figsize']\n    # create a 1 by 2 plot grid\n    fig,ax = plt.subplots(1,2, figsize=(dfigsize[0]*2,dfigsize[1]))\n    # lot our usual output\n    plot_data(data_x,data_y,m,b,ax[0])\n\n    # here we also plot the calculated linear fit for comparison\n    line_x = np.arange(data_x.min(),data_x.max())\n    line_y = line_x * m_calc + b_calc\n    ax[0].plot(line_x,line_y, color=\"#000\", linestyle=\":\" ,label='calculated')\n    # add a legend to the plot and x/y labels\n    ax[0].legend()\n    ax[0].set_xlabel('square footage')\n    ax[0].set_ylabel('sale price')\n\n    # plot the loss \n    loss_x = np.arange(0,len(loss_history))\n    loss_y = np.asarray(loss_history)\n    ax[1].plot(loss_x,loss_y, 'o-')\n    ax[1].set_yscale('log')\n    ax[1].set_xlabel('loop step')\n    ax[1].set_ylabel('loss')\n    plt.show()\n    # gives us time to see the plot\n    time.sleep(2.5)\n    # clears the plot when the next plot is ready to show.\n    ipydis.clear_output(wait=True)\n\n[07/27/25 09:53:46] INFO     [029] dy_i = 88.89 * x + 32912.24 previously calculated: y_i = 87.69  3358379817.py:33\n                             * x + 34754.08 loss: 1478200827.64",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#homework",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#homework",
    "title": "Artificial intelligence in a nutshell",
    "section": "Homework",
    "text": "Homework\n\nMini Batch Training\nIn AI, datasets are often very large and cannot be processed all at once as is done in the loop above. The data is instead randomly sampled in smaller batches where each batch contains batch_size inputs.\nHow can you change the loop above to sample the dataset in smaller batches?\nHint: Our data variable is a Pandas DataFrame object, search for ‚Äúhow to sample a DataFrame‚Äù.\nInstead of using the entire dataset like:\ndata_x = data['GrLivArea'].to_numpy()\ndata_y = data['SalePrice'].to_numpy()\nUse\ndata_batch = data.sample(batch_size)\ndata_x = data_batch['GrLivArea'].to_numpy()\ndata_y = data_batch['SalePrice'].to_numpy()\nYou also have to adjust the loop_N accordingly to make sure that it loop over the entire datasets the same number of times.\nloop_N = 30*len(data)//batch_size\nPlease plot your learning curve for different batch size, such as 32, 64, 128, 256, 512.\n\n\nLearning rate issue (Bonus)\nAs described above, if the learning rate is too large, it will affect the convergence. Do your training with (batch_size = 64, learning_rate_m = 1e-7, learning_rate_b = 1e-1). Then linearly increase the batch size and learning rate until you see the training does not converge.\n(64, 1e-7, 1e-1)*1\n(64, 1e-7, 1e-1)*2\n(64, 1e-7, 1e-1)*4\n(64, 1e-7, 1e-1)*8\n...\nHow to submit your homework\n\nFork the github repo to your personal github\nMake change to the 01_linear_regression_sgd.ipynb, and then push to your personal github\nProvide the link of 01_linear_regression_sgd in the personal github.\n\n\n\n\nHomework Answer\n\n\nLet us define a train function which allow us to try different hyperparameter setups.\n\nx = data['GrLivArea'].to_numpy()\ny = data['SalePrice'].to_numpy()\n\ndef train(batch_size, epochs=30, learning_rate_m = 1e-7, learning_rate_b = 1e-1):\n    loss_history = []\n    num_batches = len(data)//batch_size\n    loop_N = epochs*num_batches\n    m = 5.\n    b = 1000.\n    for i in range(loop_N):\n        data_batch = data.sample(batch_size)\n        data_x = data_batch['GrLivArea'].to_numpy()\n        data_y = data_batch['SalePrice'].to_numpy()\n        # update our slope and intercept based on the current values\n        m = updated_m(data_x,data_y,m,b,learning_rate_m)\n        b = updated_b(data_x,data_y,m,b,learning_rate_b)\n\n        # calculate the loss value\n        loss_value = np.mean(loss(data_x,data_y,m,b))\n\n        # keep a history of our loss values\n        loss_history.append(loss_value)\n    #loss_last_epoch = np.sum(loss_history[-num_batches:]*batch_size)/len(data)\n    return m, b, np.mean(loss(x,y,m,b))",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#minibatch-training",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#minibatch-training",
    "title": "Artificial intelligence in a nutshell",
    "section": "Minibatch training",
    "text": "Minibatch training\n\nlogger.info('previously calculated: y_i = %.2f * x + %.2f    loss: %f\\n=======================================' % (m_calc,b_calc,loss_value))\n\n\nfor bs in 64, 128, 256, 512:\n    m, b, l = train(bs, epochs=30)\n    logger.info(f\"batch size: {bs}, m={m:.4f}, b={b:.4f}, loss={l:.4f}\")\n\n[07/27/25 09:53:48] INFO     previously calculated: y_i = 87.69 * x + 34754.08    loss:              713484243.py:1\n                             1478200827.641291                                                                     \n                             =======================================                                               \n\n\n\n                    INFO     batch size: 64, m=86.8316, b=29771.1899, loss=1516286255.8953           713484243.py:6\n\n\n\n                    INFO     batch size: 128, m=89.9071, b=31946.8053, loss=1479234588.2271          713484243.py:6\n\n\n\n                    INFO     batch size: 256, m=91.0041, b=32724.7860, loss=1487768373.3579          713484243.py:6\n\n\n\n                    INFO     batch size: 512, m=88.1903, b=32893.5833, loss=1479181479.2735          713484243.py:6\n\n\n\nWe see that eventually, we all get similar results with the minibatch training. Of course, here, we still keep the same learning rate. A gene",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#learning-rate",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#learning-rate",
    "title": "Artificial intelligence in a nutshell",
    "section": "Learning rate",
    "text": "Learning rate\n\nfor i in 1, 2, 4, 8:\n    bs, lrm, lrb = np.array([64, 1e-7, 1e-1])*i\n    bs = int(bs)\n    m, b, l = train(int(bs), epochs=30, learning_rate_m = lrm, learning_rate_b = lrb)\n    logger.info(f\"batch size: {bs}, m={m:.4f}, b={b:.4f}, loss={l:.4f}\")\n\n                    INFO     batch size: 64, m=92.8489, b=28598.6718, loss=1486330744.6748          2109497066.py:5\n\n\n\n[07/27/25 09:53:49] INFO     batch size: 128, m=85.2836, b=36297.1378, loss=1482764241.4089         2109497066.py:5\n\n\n\n                    INFO     batch size: 256, m=86.3627, b=35005.9760, loss=1480869881.6530         2109497066.py:5\n\n\n\n                    INFO     batch size: 512, m=97074.6094, b=-18031524.7613,                       2109497066.py:5\n                             loss=16623091729030896.0000                                                           \n\n\n\nWe can see that, if we increase the batch size and the learning rate proportionally, at certain point, it does not converge for example for the case batch size = 512. To increase the learning rate proportional to the batch size is a general practice. However, if the learning rate is too large, it will continue to move around without finding a local minimum. One trick, people can do is to start with a smaller learning rate in the first few steps / epochs, and once the optimization becomes stable, increase the learning rate proportional to the batch size.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/4-nersc/index.html",
    "href": "00-intro-AI-HPC/4-nersc/index.html",
    "title": "Overview of the Argonne Leadership Computing Facility (ALCF)",
    "section": "",
    "text": "CitationBibTeX citation:@online{foreman2025,\n  author = {Foreman, Sam},\n  title = {Overview of the {Argonne} {Leadership} {Computing} {Facility}\n    {(ALCF)}},\n  date = {2025-07-15},\n  url = {https://saforem2.github.io/hpc-bootcamp-2025/00-intro-AI-HPC/4-nersc/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2025. ‚ÄúOverview of the Argonne Leadership Computing\nFacility (ALCF).‚Äù July 15, 2025. https://saforem2.github.io/hpc-bootcamp-2025/00-intro-AI-HPC/4-nersc/.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[4] NERSC"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/2-jupyter-notebooks/index.html#logging-in",
    "href": "00-intro-AI-HPC/2-jupyter-notebooks/index.html#logging-in",
    "title": "Jupyter Notebooks on ThetaGPU",
    "section": "Logging in",
    "text": "Logging in\nLogin via: https://jupyter.alcf.anl.gov/\nSelect ‚ÄúLogin Polaris‚Äù and use your ALCF credentials and Multi-Factor Authentication.\nNote: some of the images below show ‚ÄúThetaGPU‚Äù being used, however, similar instructions apply to ‚ÄúPolaris‚Äù or ‚ÄúSophia‚Äù.\n\n\n\nLogin GIF",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[2] Jupyter Notebooks"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/2-jupyter-notebooks/index.html#server-settings-start-up",
    "href": "00-intro-AI-HPC/2-jupyter-notebooks/index.html#server-settings-start-up",
    "title": "Jupyter Notebooks on ThetaGPU",
    "section": "Server settings & start up",
    "text": "Server settings & start up\n\n\n\nserver options\n\n\nYou‚Äôll want to set your server options to the following: * job profile: ThetaGPU Compute Node * Sophia Queue: by-gpu * Project List: ALCFAITP * Runtime: can be 5 to 60 minutes\nWhen you click ‚Äústart‚Äù you are submitting a job to the batch queue and waiting for the job to begin.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[2] Jupyter Notebooks"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/2-jupyter-notebooks/index.html#server-shutdown-logout",
    "href": "00-intro-AI-HPC/2-jupyter-notebooks/index.html#server-shutdown-logout",
    "title": "Jupyter Notebooks on ThetaGPU",
    "section": "Server shutdown & logout",
    "text": "Server shutdown & logout\nALCF‚Äôs guiding policy is:\n\n‚ÄúWe ask that all users follow good etiquette and be excellent to one another.‚Äù\n\nIf you simply close your browser window, or logout without shutting down the jupyter server, your job will continue to occupy the worker node. Be considerate and shutdown your job when you finish.\n\n\n\nShutdown GIF",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[2] Jupyter Notebooks"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/0-compute-systems/index.html",
    "href": "00-intro-AI-HPC/0-compute-systems/index.html",
    "title": "What is a Supercomputer?",
    "section": "",
    "text": "Argonne hosts DOE supercomputers for use by research scientists in need of large computational resources. Supercomputers are composed of many computing nodes (1 node = 1 physical computer) that are connected by a high-speed communications network so that groups of nodes can share information quickly, effectively operating together as a larger computer.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[0] Compute Systems"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/0-compute-systems/index.html#a-compute-node",
    "href": "00-intro-AI-HPC/0-compute-systems/index.html#a-compute-node",
    "title": "What is a Supercomputer?",
    "section": "A Compute Node",
    "text": "A Compute Node\nIf you look inside your Desktop or Laptop you‚Äôll find these parts:\n\n\n\nparts\n\n\nA computing node of a supercomputer is very similar, each has simliar parts, but it is designed as a single unit that can be inserted and removed from large closet-sized racks with many others:\n\n\n\nblade\n\n\nIn large supercomputers multiple computer processors (CPUs) and/or graphics processors (GPUs) are combined into a single node. It has a CPU on which the local operating system runs. It has local memory for running software. It may have GPUs for doing intensive calculations. Each node has a high-speed network connection that allows it to communicate with other nodes and to a large shared filesystem.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[0] Compute Systems"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/0-compute-systems/index.html#clusterhpc-computing-hardware-setup",
    "href": "00-intro-AI-HPC/0-compute-systems/index.html#clusterhpc-computing-hardware-setup",
    "title": "What is a Supercomputer?",
    "section": "Cluster/HPC Computing Hardware Setup",
    "text": "Cluster/HPC Computing Hardware Setup\n\n\n\nHardware\n\n\nLarge computer systems typically have worker nodes and login nodes. login nodes are the nodes on which every user arrives when they login to the system. login nodes should not be used for computation, but for compiling code, writing/editing code, and launching jobs on the system. A job is the application that will be launched on the worker nodes of the supercomputer.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[0] Compute Systems"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/0-compute-systems/index.html#supercomputers-are-big",
    "href": "00-intro-AI-HPC/0-compute-systems/index.html#supercomputers-are-big",
    "title": "What is a Supercomputer?",
    "section": "Supercomputers are Big!",
    "text": "Supercomputers are Big!\nThese supercomputers occupy a lot of space in the ACLF data center. Here is our staff at the time (2019) in front of Mira, an IBM supercomputer, that debuted as the third fastest supercomputer in the world in 2012:\n\n\n\n\n\n\nFigure¬†1: ALCF Staff",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[0] Compute Systems"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/0-compute-systems/index.html#alcf-computing-system-overview",
    "href": "00-intro-AI-HPC/0-compute-systems/index.html#alcf-computing-system-overview",
    "title": "What is a Supercomputer?",
    "section": "ALCF Computing System Overview",
    "text": "ALCF Computing System Overview\n\nAurora\nAt this time (October 2024), Aurora is being put through its paces with Argonne and Intel working together to prepare the system for scientific researchers. It is amongst the most power supercomputers in thew world, currently sitting at #2 on the Top500.\nHere you can see one of the many rows of Aurora nodes with their Red & Blue water cooling conduits visible.\n\n\n\n\n\n\nFigure¬†2: Aurora\n\n\n\nIn this photo you see a close up of the 16 nodes installed side-by-side:\n\n\n\n\n\n\nFigure¬†3: Aurora\n\n\n\n\n\nSophia\nInside Sophia, you‚Äôll see repetition, though NVidia placed these fancy plates over the hardware so you only see their logo.\nHowever, each plate covers 1 computer node.\n\n\n\nSophia Racks\nSophia Inside\n\n\n\n\n\n\n\n\n\nSophia is an NVIDIA DGX A100-based system. The DGX A100 comprises eight NVIDIA A100 GPUs that provide a total of 320 gigabytes of memory for training AI datasets, as well as high-speed NVIDIA Mellanox ConnectX-6 network interfaces.\nSophia Machine Specs\n\nSpeed: 3.9 petaflops\nEach Node has:\n\n8 NVIDIA (A100) GPUs each with 40GB onboard memory\n2 AMD EPYC (7742) CPUs\n1 TB DDR4 Memory\n\n24 Total Nodes installed in 7 Racks\n\n\n\nPolaris\n\n\n\nPolaris\n\n\nThe inside of Polaris again shows the nodes stacked up in a closet.\n\n\n\nPolaris-rack\n\n\nPolaris is an NVIDIA A100-based system.\nPolaris Machine Specs * Speed: 44 petaflops * Each Node has: * 4 NVIDIA (A100) GPUs * 1 AMD EPYC (Milan) CPUs * ~560 Total Nodes",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[0] Compute Systems"
    ]
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Intro to HPC Bootcamp 2025",
    "section": "Contents",
    "text": "Contents\n\nüè° Intro to {AI, HPC} for Science\n\nüìÇ 00-intro-AI-hpc/\n\nüìÑ 0-compute-systems\nüìÑ 1-shared-resources\nüìó 2-jupyter-notebooks\nüìÑ 3-homework\nüìÑ 4-nersc\nüìó 5-mcmc-example\nüìó 6-linear-regression \n\nüìÇ 01-neural-networks/\n\nüìÑ 0-intro\nüìó 1-mnist\nüìÑ 2-advanced\nüìó 3-conv-nets\nüìó 4-representation-learning\n\nüìÇ 02-llms/\n\nüìÑ 00-intro-to-llms\nüìó 01-hands-on-llms\nüìÑ 02-prompt-engineering \nüìó 06-parallel-training\nüìó 07-shakespeare-example\nüìó 08-shakespeare-example-colab\n\nüìÇ 03-ai-for-science/\n\n\n#| output_collapsed: true\nimport ezpz\n\nlogger = ezpz.get_logger(\"root\")\nlogger.info(\"Welcome to the Intro to HPC Bootcamp 2025!\")\nlogger.info(f\"Current time is: {ezpz.get_timestamp()}\")"
  },
  {
    "objectID": "00-intro-AI-HPC/1-shared-resources/index.html",
    "href": "00-intro-AI-HPC/1-shared-resources/index.html",
    "title": "Shared Resources",
    "section": "",
    "text": "Supercomputers contain many computer nodes and not every application will use ALL of them. Therefore, we use programs called schedulers that allow users to schedule a job based on how many nodes they need for a specified time.\nA job is defined by a user and requires these parameters to be defined:",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[1] Shared Resources"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/1-shared-resources/index.html#polaris-activity-webpage",
    "href": "00-intro-AI-HPC/1-shared-resources/index.html#polaris-activity-webpage",
    "title": "Shared Resources",
    "section": "Polaris Activity Webpage",
    "text": "Polaris Activity Webpage\n\nWe have a page that shows all the current activity on Polaris. The top of the page shows a graphical representation of all nodes. Colors indicate a job running on that node. Below the graphic there is a table that lists running and queued jobs.\n\nrunning refers to jobs running right now on comptuer nodes. If you hover on a colored node in the graphic, it will highlight all the nodes used by the same job AND the job in the running table below.\nqueued jobs are waiting for an opening in which to run.\n\nüìä Live Status of Jobs on Polaris\n\n\n\npolaris-activity",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[1] Shared Resources"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/3-homework/index.html",
    "href": "00-intro-AI-HPC/3-homework/index.html",
    "title": "GitHub Introduction: Homework Submissions",
    "section": "",
    "text": "You will submit weekly homeworks via this Homework Form The form expects you to submit a URL to a file on github.com. You will need an account on GitHub to do this.\nYou will also need to setup SSH Keys on github and in your ALCF account. * The SSH Key setup is described here * Setup git on Polaris (or target machine):\nNavigate to the main page of this repository and follow the steps below. If you need help with logging into ALCF, please have a look XXX",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[3] Homework"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/3-homework/index.html#fork-the-repo-on-github",
    "href": "00-intro-AI-HPC/3-homework/index.html#fork-the-repo-on-github",
    "title": "GitHub Introduction: Homework Submissions",
    "section": "Fork the repo on GitHub",
    "text": "Fork the repo on GitHub\nThis repository is controled by the ALCF account space, but you can make a copy in to your own github account space by forking the repository as show here. Simply go to the main repository page and click the fork button on the top right.\n\n\n\nfork repo",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[3] Homework"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/3-homework/index.html#clone-the-repo-in-your-home-directory",
    "href": "00-intro-AI-HPC/3-homework/index.html#clone-the-repo-in-your-home-directory",
    "title": "GitHub Introduction: Homework Submissions",
    "section": "Clone the repo in your Home directory",
    "text": "Clone the repo in your Home directory\nAfter you have forked the repo, login to Polaris, and clone the repository in to your home directory.\n\n\n\nclone repo\n\n\nThe command looks like git clone git@github.com:&lt;your-github-account&gt;/ai-science-training-series.git and it will clone the repo into a directory named ai-science-training-series.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[3] Homework"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/3-homework/index.html#do-your-homework",
    "href": "00-intro-AI-HPC/3-homework/index.html#do-your-homework",
    "title": "GitHub Introduction: Homework Submissions",
    "section": "Do your homework",
    "text": "Do your homework\nSome homework can be done via Jupyter notebooks, and others will be source code.\nWhen you sign in to our Jupyter server, you see the contents of your home directory. There you can open your repository and create a new file with an appropriate name, like YEAR-MONTH-DAY_Name_SessionNumber.\n\n\n\ncreate homework",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[3] Homework"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/3-homework/index.html#upload-your-homework-to-your-github-repo",
    "href": "00-intro-AI-HPC/3-homework/index.html#upload-your-homework-to-your-github-repo",
    "title": "GitHub Introduction: Homework Submissions",
    "section": "Upload your homework to your github repo",
    "text": "Upload your homework to your github repo\nNow you need to: - add your new homework file to the list of tracked files in your repository - commit your file to the repository - push your commit to the web server at github\n\n\n\ncommit homework",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[3] Homework"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/3-homework/index.html#sync-your-fork",
    "href": "00-intro-AI-HPC/3-homework/index.html#sync-your-fork",
    "title": "GitHub Introduction: Homework Submissions",
    "section": "Sync Your Fork",
    "text": "Sync Your Fork\nWe will periodically update the content in the ‚Äúupstream‚Äù repository at: https://github.com/argonne-lcf/ai-science-training-series\nYou‚Äôll want to execute these commmands each week, just prior to the start of class, to update your personal ‚Äúfork‚Äù with our latest updates.\nTo do this run these commands inside the directory ai-science-training-series where you cloned your personal fork:\nYou only need to do this once after you first clone your personal fork:\ngit remote add upstream git@github.com:argonne-lcf/ai-science-training-series.git\nThen, every week, prior to class run this:\ngit fetch upstream main\ngit merge upstream/main\ngit push\nThis will keep your fork up to date with the ACLF repository.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[3] Homework"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/5-mcmc-example/index.html#example-estimate-pi",
    "href": "00-intro-AI-HPC/5-mcmc-example/index.html#example-estimate-pi",
    "title": "Computing \\pi in Parallel with Markov Chain Monte Carlo (MCMC) and MPI",
    "section": "Example: Estimate \\pi",
    "text": "Example: Estimate \\pi\nWe can calculate the value of \\pi using a MPI parallelized version of the Monte Carlo method. The basic idea is to estimate \\pi by randomly sampling points within a square and determining how many fall inside a quarter circle inscribed within that square.\n\n\n\n\\pi\n\n\nThe ratio between the area of the circle and the square is\n\\frac{N_\\text{in}}{N_\\text{total}} = \\frac{\\pi r^2}{4r^2} = \\frac{\\pi}{4}\nTherefore, we can calculate \\pi using \\pi = \\frac{4N_\\text{in}}{N_\\text{total}}\n\nimport ambivalent\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use(ambivalent.STYLES['ambivalent'])\nsns.set_context(\"notebook\")\nplt.rcParams[\"figure.figsize\"] = [6.4, 4.8]\n\nfrom IPython.display import display, clear_output\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport time\n\nimport ezpz\nlogger = ezpz.get_logger('mcmc')\n\n\nfig, ax = plt.subplots()\n#ax = fig.add_subplot(111)\ncircle = plt.Circle(( 0. , 0. ), 0.5 )\nplt.xlim(-0.5, 0.5)\nplt.ylim(-0.5, 0.5)\nax.add_patch(circle)\nax.set_aspect('equal')\nN = 500\nNin = 0\nt0 = time.time()\nfor i in range(1, N+1):\n    x = random.uniform(-0.5, 0.5)\n    y = random.uniform(-0.5, 0.5)\n    if (np.sqrt(x*x + y*y) &lt; 0.5):\n        Nin += 1\n        plt.plot([x], [y], 'o', color='r', markersize=3)\n    else:\n        plt.plot([x], [y], 'o', color='b', markersize=3)\n    display(fig)\n    plt.xlabel(\"$\\pi$ = %3.4f \\n N_in / N_total = %5d/%5d\" %(Nin*4.0/i, Nin, i))\n    clear_output(wait=True)\n\nres = np.array(Nin, dtype='d')\nt1 = time.time()\nlogger.info(f\"Pi = {res/float(N/4.0)}\")\nlogger.info(\"Time: %s\" %(t1 - t0))\n\n[07/27/25 09:52:01] INFO     Pi = 3.2                                                               500593173.py:44\n\n\n\n                    INFO     Time: 26.57829713821411                                                500593173.py:45\n\n\n\n\n\n\n\n\n\n\n\nMPI example\n\n\n\nNodes\nPyTorch-2.5\nPyTorch-2.7\nPyTorch-2.8\n\n\n\n\nN1xR12\n17.39\n31.01\n33.09\n\n\nN2xR12\n3.81\n32.71\n33.26\n\n\n\n\nfrom mpi4py import MPI\nimport numpy as np\nimport random\nimport time\ncomm = MPI.COMM_WORLD\n\nN = 5000000\nNin = 0\nt0 = time.time()\nfor i in range(comm.rank, N, comm.size):\n    x = random.uniform(-0.5, 0.5)\n    y = random.uniform(-0.5, 0.5)\n    if (np.sqrt(x*x + y*y) &lt; 0.5):\n        Nin += 1\nres = np.array(Nin, dtype='d')\nres_tot = np.array(Nin, dtype='d')\ncomm.Allreduce(res, res_tot, op=MPI.SUM)\nt1 = time.time()\nif comm.rank==0:\n    logger.info(res_tot/float(N/4.0))\n    logger.info(\"Time: %s\" %(t1 - t0))\n\n[07/27/25 09:52:04] INFO     3.141568                                                                94852726.py:20\n\n\n\n                    INFO     Time: 2.999566078186035                                                 94852726.py:21\n\n\n\n\n\nRunning \\pi example on Google Colab\n\nGo to https://colab.research.google.com/, sign in or sign up\n‚ÄúFile‚Äù-&gt; ‚Äúopen notebook‚Äù\nChoose 01_intro_AI_on_Supercomputer/00_mpi.ipynb from the list \n\n\n! wget https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/01_intro_AI_on_Supercomputer/mpi_pi.py\n! pip install mpi4py\n\n\n! mpirun -np 1 --allow-run-as-root python mpi_pi.py\n\nNumber of processes: 1\nPi = 3.1408328\nTime: 2.876166\n\n\n\n! mpirun -np 2 --allow-run-as-root --oversubscribe python mpi_pi.py\n\nNumber of processes: 2\nPi = 3.1409008\nTime: 1.426253\n\n\n\n! mpirun -np 4 --allow-run-as-root --oversubscribe python mpi_pi.py\n\nNumber of processes: 4\nPi = 3.141604\nTime: 0.754426\n\n\n\n\nRunning \\pi on Polaris\nssh &lt;username&gt;@polaris.alcf.anl.gov\nqsub -A ALCFAITP -l select=1 -q ALCFAITP -l walltime=0:30:00 -l filesystems=home:eagle\n# choose debug queue outside of the class\n# qsub -A ALCFAITP -l select=1 -q debug -l walltime=0:30:00 -l filesystems=home:eagle\n\nmodule load conda/2023-10-04\nconda activate /soft/datascience/ALCFAITP/2023-10-04\ngit clone git@github.com:argonne-lcf/ai-science-training-series.git\ncd ai-science-training-series/01_intro_AI_on_Supercomputer/\nmpirun -np 1 python mpi_pi.py   # 3.141988,   8.029037714004517  s\nmpirun -np 2 python mpi_pi.py   # 3.1415096   4.212774038314819  s\nmpirun -np 4 python mpi_pi.py   # 3.1425632   2.093632459640503  s\nmpirun -np 8 python mpi_pi.py   # 3.1411632   1.0610620975494385 s",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[5] MCMC Example"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/5-mcmc-example/index.html#parallel-computing-in-ai",
    "href": "00-intro-AI-HPC/5-mcmc-example/index.html#parallel-computing-in-ai",
    "title": "Computing \\pi in Parallel with Markov Chain Monte Carlo (MCMC) and MPI",
    "section": "Parallel computing in AI",
    "text": "Parallel computing in AI\nThe parallel computing in AI is usually called distributed training.\nDistributed training is the process of training I models across multiple GPUs or other accelerators, with the goal of speeding up the training process and enabling the training of larger models on larger datasets.\nThere are two ways of parallelization in distributed training.\n\nData parallelism:\n\nEach worker (GPU) has a complete set of model\ndifferent workers work on different subsets of data.\n\nModel parallelism\n\nThe model is splitted into different parts and stored on different workers\nDifferent workers work on computation involved in different parts of the model\n\n\n\n\n\n\n\n\nFigure¬†1: PI\n\n\n\n\n\n\n\n\n\nFigure¬†2: 3D LLM",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[5] MCMC Example"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/7-statistical-learning/index.html",
    "href": "00-intro-AI-HPC/7-statistical-learning/index.html",
    "title": "Statistical Learning",
    "section": "",
    "text": "CitationBibTeX citation:@online{foreman2025,\n  author = {Foreman, Sam},\n  title = {Statistical {Learning}},\n  date = {2025-07-25},\n  url = {https://saforem2.github.io/hpc-bootcamp-2025/00-intro-AI-HPC/7-statistical-learning/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2025. ‚ÄúStatistical Learning.‚Äù July 25, 2025.\nhttps://saforem2.github.io/hpc-bootcamp-2025/00-intro-AI-HPC/7-statistical-learning/.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[7] Statistical Learning"
    ]
  },
  {
    "objectID": "01-neural-networks/0-intro/index.html",
    "href": "01-neural-networks/0-intro/index.html",
    "title": "Introduction to Neural Networks",
    "section": "",
    "text": "This tutorial covers the basics of neural networks (aka ‚Äúdeep learning‚Äù), which is a technique within machine learning that tends to outperform other techniques when dealing with a large amount of data.\nThe MNIST dataset contains thousands of examples of handwritten numbers, with each digit labeled 0-9.\nWe‚Äôll start with the MNIST problem in this notebook:\nüìì Fitting MNIST with a multi-layer perceptron (MLP)\nNext week, we‚Äôll learn about other types of neural networks.",
    "crumbs": [
      "[01] Neural Networks",
      "[0] Intro to NNs"
    ]
  },
  {
    "objectID": "01-neural-networks/0-intro/index.html#references",
    "href": "01-neural-networks/0-intro/index.html#references",
    "title": "Introduction to Neural Networks",
    "section": "References:",
    "text": "References:\n\nHere are some recommendations for further reading:\n\ntensorflow.org tutorials\nkeras.io tutorials\nCS231n: Convolutional Neural Networks for Visual Recognition\nDeep Learning Specialization, Andrew Ng\nPyTorch Challenge, Udacity\nDeep Learning with Python\nKeras Blog\nHands-on ML book with notebooks.",
    "crumbs": [
      "[01] Neural Networks",
      "[0] Intro to NNs"
    ]
  },
  {
    "objectID": "01-neural-networks/2-advanced/index.html",
    "href": "01-neural-networks/2-advanced/index.html",
    "title": "Advanced Topics in Neural Networks",
    "section": "",
    "text": "CitationBibTeX citation:@online{foreman2025,\n  author = {Foreman, Sam},\n  title = {Advanced {Topics} in {Neural} {Networks}},\n  date = {2025-07-17},\n  url = {https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/2-advanced/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2025. ‚ÄúAdvanced Topics in Neural Networks.‚Äù\nJuly 17, 2025. https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/2-advanced/.",
    "crumbs": [
      "[01] Neural Networks",
      "[2] Advanced"
    ]
  },
  {
    "objectID": "01-neural-networks/index.html",
    "href": "01-neural-networks/index.html",
    "title": "Introduction to Neural Networks",
    "section": "",
    "text": "üìÇ 01-neural-networks/\n\nüìÑ 0-intro\nüìó 1-mnist\nüìó 2-advanced\nüìó 3-conv-nets\nüìó 4-representation-learning\n\n\n\n\n\n\nCitationBibTeX citation:@online{foreman2025,\n  author = {Foreman, Sam},\n  title = {Introduction to {Neural} {Networks}},\n  date = {2025-07-15},\n  url = {https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2025. ‚ÄúIntroduction to Neural Networks.‚Äù July\n15, 2025. https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/.",
    "crumbs": [
      "[01] Neural Networks"
    ]
  },
  {
    "objectID": "02-llms/00-intro-to-llms/index.html",
    "href": "02-llms/00-intro-to-llms/index.html",
    "title": "Introduction to Large Language Models",
    "section": "",
    "text": "Author: Archit Vasan , including materials on LLMs by Varuni Sastri and Carlo Graziani at Argonne, and discussion/editorial work by Taylor Childers, Bethany Lusch, and Venkat Vishwanath (Argonne)\nInspiration from the blog posts ‚ÄúThe Illustrated Transformer‚Äù and ‚ÄúThe Illustrated GPT2‚Äù by Jay Alammar, highly recommended reading.\nThis tutorial covers the some fundamental concepts necessary to to study of large language models (LLMs).",
    "crumbs": [
      "[02] Large Language Models",
      "[0] Intro to LLMs"
    ]
  },
  {
    "objectID": "02-llms/00-intro-to-llms/index.html#brief-overview",
    "href": "02-llms/00-intro-to-llms/index.html#brief-overview",
    "title": "Introduction to Large Language Models",
    "section": "Brief overview",
    "text": "Brief overview\n\nScientific applications for language models\nGeneral overview of Transformers\nTokenization\nModel Architecture\nPipeline using HuggingFace\nModel loading",
    "crumbs": [
      "[02] Large Language Models",
      "[0] Intro to LLMs"
    ]
  },
  {
    "objectID": "02-llms/00-intro-to-llms/index.html#sophia-setup",
    "href": "02-llms/00-intro-to-llms/index.html#sophia-setup",
    "title": "Introduction to Large Language Models",
    "section": "Sophia Setup",
    "text": "Sophia Setup\n\nIf you are using ALCF, first log in. From a terminal run the following command:\nssh username@sophia.alcf.anl.gov\nAlthough we already cloned the repo before, you‚Äôll want the updated version. \nNow that we have the updated notebooks, we can open them. If you are using ALCF JupyterHub or Google Colab, you can be reminded of the steps here\nReminder: Change the notebook‚Äôs kernel to datascience/conda-2024-08-08 (you may need to change kernel each time you open a notebook for the first time):\n\nselect Kernel in the menu bar\nselect Change kernel‚Ä¶\nselect datascience/conda-2024-08-08 from the drop-down menu",
    "crumbs": [
      "[02] Large Language Models",
      "[0] Intro to LLMs"
    ]
  },
  {
    "objectID": "02-llms/00-intro-to-llms/index.html#google-colab-setup",
    "href": "02-llms/00-intro-to-llms/index.html#google-colab-setup",
    "title": "Introduction to Large Language Models",
    "section": "Google colab setup",
    "text": "Google colab setup\nIn case you have trouble accessing Sophia, all notebook material can be run in google colab.\nJust:\n\nGo to this link: Colab\nClick on File/Open notebook\nNagivate to the GitHub tab and find argonne-lcf/ai-science-training-series\nClick on 04_intro_to_llms/IntroLLMs.ipynb",
    "crumbs": [
      "[02] Large Language Models",
      "[0] Intro to LLMs"
    ]
  },
  {
    "objectID": "02-llms/00-intro-to-llms/index.html#references",
    "href": "02-llms/00-intro-to-llms/index.html#references",
    "title": "Introduction to Large Language Models",
    "section": "References",
    "text": "References\nI strongly recommend reading ‚ÄúThe Illustrated Transformer‚Äù by Jay AlammarAlammar also has a useful post dedicated more generally to Sequence-to-Sequence modeling ‚ÄúVisualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention), which illustrates the attention mechanism in the context of a more generic language translation model.",
    "crumbs": [
      "[02] Large Language Models",
      "[0] Intro to LLMs"
    ]
  },
  {
    "objectID": "02-llms/00-intro-to-llms/index.html#homework-solutions",
    "href": "02-llms/00-intro-to-llms/index.html#homework-solutions",
    "title": "Introduction to Large Language Models",
    "section": "Homework solutions",
    "text": "Homework solutions\nSolutions to homework problems are posted in IntroLLMHWSols.ipynb To see BertViz attention mechanisms, simply open the notebook in google colab.",
    "crumbs": [
      "[02] Large Language Models",
      "[0] Intro to LLMs"
    ]
  },
  {
    "objectID": "02-llms/06-parallel-training/index.html#hands-on",
    "href": "02-llms/06-parallel-training/index.html#hands-on",
    "title": "Intro to HPC Bootcamp 2025",
    "section": "üëã Hands On",
    "text": "üëã Hands On\n\nSubmit interactive job:\nqsub -A ALCFAITP -q by-node -l select=1 -l walltime=01:00:00,filesystems=eagle:home -I\nOn Sophia:\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\nClone repos:\n\nsaforem2/wordplay:\ngit clone https://github.com/saforem2/wordplay\ncd wordplay\nsaforem2/ezpz:\ngit clone https://github.com/saforem2/ezpz deps/ezpz\n\nSetup python:\nexport PBS_O_WORKDIR=$(pwd) && source deps/ezpz/src/ezpz/bin/utils.sh\nezpz_setup_python\nezpz_setup_job\nInstall {ezpz, wordplay}:\npython3 -m pip install -e deps/ezpz --require-virtualenv\npython3 -m pip install -e . --require-virtualenv\nSetup (or disable) wandb:\n# to setup:\nwandb login\n# to disable:\nexport WANDB_DISABLED=1\nTest Distributed Setup:\nmpirun -n \"${NGPUS}\" python3 -m ezpz.test_dist\nSee: ezpz/test_dist.py\nPrepare Data:\npython3 data/shakespeare_char/prepare.py\nLaunch Training:\nmpirun -n \"${NGPUS}\" python3 -m wordplay \\\n    train.backend=DDP \\\n    train.eval_interval=100 \\\n    data=shakespeare \\\n    train.dtype=bf16 \\\n    model.batch_size=64 \\\n    model.block_size=1024 \\\n    train.max_iters=1000 \\\n    train.log_interval=10 \\\n    train.compile=false",
    "crumbs": [
      "[02] Large Language Models",
      "[6] Parallel Training"
    ]
  },
  {
    "objectID": "02-llms/06-parallel-training/index.html#homework",
    "href": "02-llms/06-parallel-training/index.html#homework",
    "title": "Intro to HPC Bootcamp 2025",
    "section": "üéí Homework",
    "text": "üéí Homework\nSubmit proof that you were able to successfully follow the above instructions and launch a distributed data parallel training run.\nWhere proof can be any of:\n\nThe contents printed out to your terminal during the run\nA path to a logfile containing the output from a run on the ALCF filesystems\nA screenshot of:\n\nthe text printed out from the run\na graph from the W&B Run\nanything that shows that you clearly were able to run the example\n\nurl to a W&B Run or W&B Report\netc.",
    "crumbs": [
      "[02] Large Language Models",
      "[6] Parallel Training"
    ]
  }
]