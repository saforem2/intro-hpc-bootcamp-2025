{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intro to HPC Bootcamp 2025\n",
        "\n",
        "[Sam Foreman](https://samforeman.me)\n",
        "(\\[[ALCF](https://alcf.anl.gov/about/people/sam-foreman)\\](<https://alcf.anl.gov/about/people/sam-foreman>))  \n",
        "2025-07-22\n",
        "\n",
        "# ðŸš€ Parallel Training Methods for AI\n",
        "\n",
        "[Sam Foreman](https://samforeman.me)  \n",
        "[Intro to AI-driven Science on\n",
        "Supercomputers](https://www.alcf.anl.gov/alcf-ai-science-training-series)  \n",
        "*2024-11-05*\n",
        "\n",
        "-   Slides: <https://samforeman.me/talks/ai-for-science-2024/slides>\n",
        "    -   HTML version: <https://samforeman.me/talks/ai-for-science-2024>\n",
        "\n",
        "## ðŸ‘‹ Hands On\n",
        "\n",
        "1.  Submit interactive job:\n",
        "\n",
        "    ``` bash\n",
        "    qsub -A ALCFAITP -q by-node -l select=1 -l walltime=01:00:00,filesystems=eagle:home -I\n",
        "    ```\n",
        "\n",
        "2.  On Sophia:\n",
        "\n",
        "    ``` bash\n",
        "    export HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\n",
        "    export HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\n",
        "    export http_proxy=\"http://proxy.alcf.anl.gov:3128\"\n",
        "    export https_proxy=\"http://proxy.alcf.anl.gov:3128\"\n",
        "    export ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\n",
        "    ```\n",
        "\n",
        "3.  Clone repos:\n",
        "\n",
        "    1.  [`saforem2/wordplay`](https://github.com/saforem2/wordplay):\n",
        "\n",
        "        ``` bash\n",
        "        git clone https://github.com/saforem2/wordplay\n",
        "        cd wordplay\n",
        "        ```\n",
        "\n",
        "    2.  [`saforem2/ezpz`](https://github.com/saforem2/ezpz):\n",
        "\n",
        "        ``` bash\n",
        "        git clone https://github.com/saforem2/ezpz deps/ezpz\n",
        "        ```\n",
        "\n",
        "4.  Setup python:\n",
        "\n",
        "    ``` bash\n",
        "    export PBS_O_WORKDIR=$(pwd) && source deps/ezpz/src/ezpz/bin/utils.sh\n",
        "    ezpz_setup_python\n",
        "    ezpz_setup_job\n",
        "    ```\n",
        "\n",
        "5.  Install `{ezpz, wordplay}`:\n",
        "\n",
        "    ``` bash\n",
        "    python3 -m pip install -e deps/ezpz --require-virtualenv\n",
        "    python3 -m pip install -e . --require-virtualenv\n",
        "    ```\n",
        "\n",
        "6.  Setup (or disable) [`wandb`](https://wandb.ai):\n",
        "\n",
        "    ``` bash\n",
        "    # to setup:\n",
        "    wandb login\n",
        "    # to disable:\n",
        "    export WANDB_DISABLED=1\n",
        "    ```\n",
        "\n",
        "7.  Test Distributed Setup:\n",
        "\n",
        "    ``` bash\n",
        "    mpirun -n \"${NGPUS}\" python3 -m ezpz.test_dist\n",
        "    ```\n",
        "\n",
        "    See:\n",
        "    [`ezpz/test_dist.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/test_dist.py)\n",
        "\n",
        "8.  Prepare Data:\n",
        "\n",
        "    ``` bash\n",
        "    python3 data/shakespeare_char/prepare.py\n",
        "    ```\n",
        "\n",
        "9.  Launch Training:\n",
        "\n",
        "    ``` bash\n",
        "    mpirun -n \"${NGPUS}\" python3 -m wordplay \\\n",
        "        train.backend=DDP \\\n",
        "        train.eval_interval=100 \\\n",
        "        data=shakespeare \\\n",
        "        train.dtype=bf16 \\\n",
        "        model.batch_size=64 \\\n",
        "        model.block_size=1024 \\\n",
        "        train.max_iters=1000 \\\n",
        "        train.log_interval=10 \\\n",
        "        train.compile=false\n",
        "    ```\n",
        "\n",
        "## ðŸŽ’ Homework\n",
        "\n",
        "Submit *proof* that you were able to successfully follow the above\n",
        "instructions and launch a distributed data parallel training run.\n",
        "\n",
        "Where *proof* can be any of:\n",
        "\n",
        "-   The contents printed out to your terminal during the run\n",
        "-   A path to a logfile containing the output from a run on the ALCF\n",
        "    filesystems\n",
        "-   A screenshot of:\n",
        "    -   the text printed out from the run\n",
        "    -   a graph from the W&B Run\n",
        "    -   anything that shows that you clearly were able to run the\n",
        "        example\n",
        "-   url to a W&B Run or [W&B\n",
        "    Report](https://api.wandb.ai/links/aurora_gpt/7du35js1)\n",
        "-   etc."
      ],
      "id": "494fbefa-4b38-4fec-aafd-f3e90cc544aa"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}