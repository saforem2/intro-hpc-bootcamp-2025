{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Large Language Models (LLMs)\n",
        "\n",
        "[Sam Foreman](https://samforeman.me)\n",
        "(\\[[ALCF](https://alcf.anl.gov/about/people/sam-foreman)\\](<https://alcf.anl.gov/about/people/sam-foreman>))  \n",
        "2025-07-15\n",
        "\n",
        "> **Authors**\n",
        ">\n",
        "> Content modified from original content written by Archit Vasan,\n",
        "> including materials on LLMs by: Varuni Sastri and Carlo Graziani at\n",
        "> Argonne, and discussion/editorial work by Taylor Childers, Bethany\n",
        "> Lusch, and Venkat Vishwanath (Argonne)\n",
        "\n",
        "## Contents\n",
        "\n",
        "-   üìÇ [02-llms/](index.qmd)\n",
        "    -   [00-intro-to-llms](00-intro-to-llms/index.qmd)\n",
        "    -   [01-hands-on-llms](01-hands-on-llms/index.ipynb)\n",
        "    -   [02-prompt-engineering](02-prompt-engineering/index.qmd)\n",
        "    -   [05-advanced-llms](05-advanced-llms/index.qmd)\n",
        "    -   [06-parallel-training](06-parallel-training/index.qmd)\n",
        "    -   [07-shakespeare-example](07-shakespeare-example/index.qmd)\n",
        "    -   [08-shakespeare-example-colab](08-shakespeare-example-colab/index.ipynb)\n",
        "    -   [09-rag-tutorial](09-rag-tutorial/index.qmd)\n",
        "    -   [10-evaluating-llms](10-evaluating-llms/index.qmd)\n",
        "\n",
        "## Overview\n",
        "\n",
        "Inspiration from the blog posts ‚ÄúThe Illustrated Transformer‚Äù and ‚ÄúThe\n",
        "Illustrated GPT2‚Äù by Jay Alammar, highly recommended reading.\n",
        "\n",
        "This tutorial covers the some fundamental concepts necessary to to study\n",
        "of large language models (LLMs).\n",
        "\n",
        "### Topics\n",
        "\n",
        "-   Scientific applications for language models\n",
        "-   General overview of Transformers\n",
        "-   Tokenization\n",
        "-   Model Architecture\n",
        "-   Pipeline using HuggingFace\n",
        "-   Model loading\n",
        "\n",
        "## Natural Language Processing (NLP)\n",
        "\n",
        "Large Language Models (LLMs) are a subset of Natural Language Processing\n",
        "(NLP) techniques that focus on understanding and generating human\n",
        "language. NLP is a field of linguistics / artificial intelligence that\n",
        "enables computers to interpret, understand, and respond to human\n",
        "language in a way that is both meaningful and useful.\n",
        "\n",
        "The following is a list of common NLP tasks, with some examples:\n",
        "\n",
        "-   **Classifying whole sentences**: Getting the sentiment of a review,\n",
        "    detecting if an email is spam, determining if a sentence is\n",
        "    gramatically correct or whether two sentences are logically related\n",
        "    or not.\n",
        "-   **Classifying each word in a sentence**: Identifying the grammatical\n",
        "    components of a sentence (noun, verb, adjectvie, ‚Ä¶), or the named\n",
        "    entities (person, location, organization, ‚Ä¶).\n",
        "-   **Generating Text**: Completing a prompt with auto-generated text,\n",
        "    filling in the blanks in a text with masked words\n",
        "-   **Extracting an answer from a text**: Given a question and a\n",
        "    context, extracting the answer to the question based on the\n",
        "    information provided in the context.\n",
        "-   **Generating a new sentence from an input text**: Translating a text\n",
        "    into another language, summarizing a text\n",
        "\n",
        "## Large Language Models (LLMs)\n",
        "\n",
        "> A large lanuage model (LLM) is an AI model trained on massive amounts\n",
        "> of text data that can understand and generate human-like text,\n",
        "> recognize patterns in language, and perform a wide variety of language\n",
        "> tasks without task-specific training.  \n",
        "> They represent a significant advancement in the field of natural\n",
        "> language processing (NLP) (Face 2022).\n",
        "\n",
        "> **üöß Warning**\n",
        ">\n",
        "> While LLMs are are able to generate (what appears to be) human-like\n",
        "> text, they are not sentient, and do not have an understanding of the\n",
        "> world in the way that humans do. They are trained to predict the next\n",
        "> word in a sentence based on the context of the words that come before\n",
        "> it, and can generate text that is coherent and relevant to the input\n",
        "> they receive. However, they do not have a true understanding of the\n",
        "> meaning of the words they generate, and can sometimes produce text\n",
        "> that is nonsensical or irrelevant to the input.\n",
        "\n",
        "Even with the advances in LLMs, many fundamental challenges remain.\n",
        "These include understanding ambiguity, cultural context, sarcasm and\n",
        "humor. LLMs address these challenges through massive training on diverse\n",
        "datasets, but still often fall short of human-level understanding in\n",
        "many complex scenarios.\n",
        "\n",
        "## References\n",
        "\n",
        "I strongly recommend reading:\n",
        "\n",
        "-   [‚ÄúThe Illustrated\n",
        "    Transformer‚Äù](https://jalammar.github.io/illustrated-transformer/)\n",
        "    by Jay AlammarAlammar also has a useful post dedicated more\n",
        "    generally to Sequence-to-Sequence modeling\n",
        "-   [LLM Course by ü§ó\n",
        "    HuggingFace](https://huggingface.co/learn/llm-course/chapter1/1)\n",
        "-   [‚ÄúVisualizing A Neural Machine Translation Model (Mechanics of\n",
        "    Seq2seq Models With\n",
        "    Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/),\n",
        "    which illustrates the attention mechanism in the context of a more\n",
        "    generic language translation model.\n",
        "-   [GPT in 60 Lines of\n",
        "    NumPy](https://jaykmody.com/blog/gpt-from-scratch/)\n",
        "\n",
        "Face, Hugging. 2022. ‚ÄúThe Hugging Face Course, 2022.‚Äù\n",
        "<https://huggingface.co/course>."
      ],
      "id": "11997cf9-6b36-49f3-aaa5-7184fa787730"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}