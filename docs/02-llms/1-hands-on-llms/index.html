<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.17">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sam Foreman">
<meta name="dcterms.date" content="2025-07-23">
<meta name="description" content="An introduction to Large Language Models (LLMs), their architecture, and how they can be applied to scientific applications.">

<title>Hands On: Introduction to Large Language Models (LLMs) – Intro to HPC Bootcamp 2025</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../02-llms/4-evaluating-llms/index.html" rel="next">
<link href="../../02-llms/0-intro-to-llms/index.html" rel="prev">
<link href="../../assets/favicon-sf.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-64a9477eeb01bb4c41113dc2462ec3b8.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-047ee2ca16eaebc148183a87b705dca3.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-443033a9f5e99878e7a0f602c75fe65b.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-e16b86569eb0ee0a29795e9a01f276ff.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js" integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;family=IBM+Plex+Sans+Condensed:ital,wght@0,400;0,500;0,600;0,700&amp;family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans&amp;family=IBM+Plex+Sans+Condensed&amp;family=IBM+Plex+Mono&amp;display=swap" rel="stylesheet">
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-TC329HJ');</script>
<!-- End Google Tag Manager -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link href="https://iosevka-webfonts.github.io/iosevka/iosevka.css" rel="stylesheet">

  <script defer="" src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Hands On: Introduction to Large Language Models (LLMs) – Intro to HPC Bootcamp 2025">
<meta property="og:description" content="An introduction to Large Language Models (LLMs), their architecture, and how they can be applied to scientific applications.">
<meta property="og:image" content="https://saforem2.github.io/hpc-bootcamp-2025/02-llms/1-hands-on-llms/assets/thumbnail.png">
<meta property="og:site_name" content="Intro to HPC Bootcamp 2025">
<meta name="twitter:title" content="Hands On: Introduction to Large Language Models (LLMs) – Intro to HPC Bootcamp 2025">
<meta name="twitter:description" content="An introduction to Large Language Models (LLMs), their architecture, and how they can be applied to scientific applications.">
<meta name="twitter:image" content="https://saforem2.github.io/hpc-bootcamp-2025/02-llms/1-hands-on-llms/assets/thumbnail.png">
<meta name="twitter:site" content="saforem2">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="Hands On: Introduction to Large Language Models (LLMs)">
<meta name="citation_author" content="Sam Foreman">
<meta name="citation_publication_date" content="2025-07-23">
<meta name="citation_cover_date" content="2025-07-23">
<meta name="citation_year" content="2025">
<meta name="citation_online_date" content="2025-07-23">
<meta name="citation_fulltext_html_url" content="https://saforem2.github.io/hpc-bootcamp-2025/02-llms/1-hands-on-llms/">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=HiPerRAG: High-performance retrieval augmented generation for scientific insights;,citation_author=Ozan Gokdemir;,citation_author=Carlo Siebenschuh;,citation_author=Alexander Brace;,citation_author=Azton Wells;,citation_author=Brian Hsu;,citation_author=Kyle Hippe;,citation_author=Priyanka V. Setty;,citation_author=Aswathy Ajith;,citation_author=J. Gregory Pauloski;,citation_author=Varuni Sastry;,citation_author=Sam Foreman;,citation_author=Huihuo Zheng;,citation_author=Heng Ma;,citation_author=Bharat Kale;,citation_author=Nicholas Chia;,citation_author=Thomas Gibbs;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Francis J. Alexander;,citation_author=Anima Anandkumar;,citation_author=Ian Foster;,citation_author=Rick Stevens;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://arxiv.org/abs/2505.04846;">
<meta name="citation_reference" content="citation_title=MOFA: Discovering materials for carbon capture with a GenAI- and simulation-based workflow;,citation_author=Xiaoli Yan;,citation_author=Nathaniel Hudson;,citation_author=Hyun Park;,citation_author=Daniel Grzenda;,citation_author=J. Gregory Pauloski;,citation_author=Marcus Schwarting;,citation_author=Haochen Pan;,citation_author=Hassan Harb;,citation_author=Samuel Foreman;,citation_author=Chris Knight;,citation_author=Tom Gibbs;,citation_author=Kyle Chard;,citation_author=Santanu Chaudhuri;,citation_author=Emad Tajkhorshid;,citation_author=Ian Foster;,citation_author=Mohamad Moosavi;,citation_author=Logan Ward;,citation_author=E. A. Huerta;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://arxiv.org/abs/2501.10651;">
<meta name="citation_reference" content="citation_title=MProt-DPO: Breaking the ExaFLOPS barrier for multimodal protein design workflows with direct preference optimization;,citation_abstract=We present a scalable, end-to-end workflow for protein design. By augmenting protein sequences with natural language descriptions of their biochemical properties, we train generative models that can be preferentially aligned with protein fitness landscapes. Through complex experimental- and simulation-based observations, we integrate these measures as preferred parameters for generating new protein variants and demonstrate our workflow on five diverse supercomputers. We achieve &amp;amp;amp;gt;1 ExaFLOPS sustained performance in mixed precision on each supercomputer and a maximum sustained performance of 4.11 ExaFLOPS and peak performance of 5.57 ExaFLOPS. We establish the scientific performance of our model on two tasks: (1) across a predetermined benchmark dataset of deep mutational scanning experiments to optimize the fitness-determining mutations in the yeast protein HIS7, and (2) in optimizing the design of the enzyme malate dehydrogenase to achieve lower activation barriers (and therefore increased catalytic rates) using simulation data. Our implementation thus sets high watermarks for multimodal protein design workflows.;,citation_author=Gautham Dharuman;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Sam Foreman;,citation_author=Väinö Hatanpää;,citation_author=Varuni K. Sastry;,citation_author=Huihuo Zheng;,citation_author=Logan Ward;,citation_author=Servesh Muralidharan;,citation_author=Archit Vasan;,citation_author=Bharat Kale;,citation_author=Carla M. Mann;,citation_author=Heng Ma;,citation_author=Yun-Hsuan Cheng;,citation_author=Yuliana Zamora;,citation_author=Shengchao Liu;,citation_author=Chaowei Xiao;,citation_author=Murali Emani;,citation_author=Tom Gibbs;,citation_author=Mahidhar Tatineni;,citation_author=Deepak Canchi;,citation_author=Jerome Mitchell;,citation_author=Koichi Yamada;,citation_author=Maria Garzaran;,citation_author=Michael E. Papka;,citation_author=Ian Foster;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://doi.org/10.1109/SC41406.2024.00013;,citation_doi=10.1109/SC41406.2024.00013;,citation_isbn=9798350352917;,citation_conference_title=Proceedings of the international conference for high performance computing, networking, storage, and analysis;,citation_conference=IEEE Press;,citation_series_title=SC ’24;">
<meta name="citation_reference" content="citation_title=Quality measures for dynamic graph generative models;,citation_author=Ryien Hosseini;,citation_author=Filippo Simini;,citation_author=Venkatram Vishwanath;,citation_author=Rebecca Willett;,citation_author=Henry Hoffmann;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://openreview.net/forum?id=8bjspmAMBk;,citation_conference_title=The thirteenth international conference on learning representations;">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=RG-inspired machine learning for lattice field theory;,citation_author=Sam Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_volume=175;,citation_conference_title=EPJ web of conferences;,citation_conference=EDP Sciences;">
<meta name="citation_reference" content="citation_title=Large energy density in three-plate nanocapacitors due to coulomb blockade;,citation_author=A Hubler;,citation_author=S Foreman;,citation_author=J Liu;,citation_author=L Wortsmann;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=10;,citation_volume=123;,citation_journal_title=Journal of Applied Physics;,citation_publisher=AIP Publishing;">
<meta name="citation_reference" content="citation_title=Examples of renormalization group transformations for image sets;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=5;,citation_volume=98;,citation_journal_title=Physical Review E;,citation_publisher=American Physical Society;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the Ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.22323/1.334.0245;,citation_volume=LATTICE2018;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=Lattice 2018;">
<meta name="citation_reference" content="citation_title=Learning better physics: A machine learning approach to lattice gauge theory;,citation_author=Samuel Alfred Foreman;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_dissertation_institution=University of Iowa;">
<meta name="citation_reference" content="citation_title=Machine learning and neural networks for field theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=HMC with normalizing flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_journal_title=arXiv preprint arXiv:2112.01586;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy storage in quantum resonators;,citation_author=Jiaqi Liu;,citation_author=Alfred W Hubler;,citation_author=Samuel Alfred Foreman;,citation_author=Katharina Ott;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Applications of machine learning to lattice quantum field theory;,citation_author=Denis Boyda;,citation_author=Salvatore Calı̀;,citation_author=Sam Foreman;,citation_author=Lena Funcke;,citation_author=Daniel C Hackett;,citation_author=Yin Lin;,citation_author=Gert Aarts;,citation_author=Andrei Alexandru;,citation_author=Xiao-Yong Jin;,citation_author=Biagio Lucini;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_journal_title=arXiv preprint arXiv:2202.05838;">
<meta name="citation_reference" content="citation_title=Lattice QCD and particle physics;,citation_author=Andreas S Kronfeld;,citation_author=Tanmoy Bhattacharya;,citation_author=Thomas Blum;,citation_author=Norman H Christ;,citation_author=Carleton DeTar;,citation_author=William Detmold;,citation_author=Robert Edwards;,citation_author=Anna Hasenfratz;,citation_author=Huey-Wen Lin;,citation_author=Swagato Mukherjee;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2207.07641;,citation_journal_title=arXiv preprint arXiv:2207.07641;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=6;,citation_volume=37;,citation_journal_title=The International Journal of High Performance Computing Applications;,citation_publisher=SAGE Publications Sage UK: London, England;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=The international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=A comprehensive performance study of large language models on novel AI accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04607;,citation_journal_title=arXiv preprint arXiv:2310.04607;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04610;,citation_journal_title=arXiv preprint arXiv:2310.04610;">
<meta name="citation_reference" content="citation_title=Protein generation via genome-scale language models with bio-physical scoring;,citation_author=Gautham Dharuman;,citation_author=Logan Ward;,citation_author=Heng Ma;,citation_author=Priyanka V Setty;,citation_author=Ozan Gokdemir;,citation_author=Sam Foreman;,citation_author=Murali Emani;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Kristopher Keipert;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=Proceedings of the SC’23 workshops of the international conference on high performance computing, network, storage, and analysis;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2312.08936;,citation_journal_title=arXiv preprint arXiv:2312.08936;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 computational frontier CompF03 topical group report: Machine learning;,citation_author=Phiala Shanahan;,citation_author=Kazuhiro Terao;,citation_author=Daniel Whiteson;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;,citation_journal_title=arXiv preprint arXiv:2209.07559;">
<meta name="citation_reference" content="citation_title=Thorough characterization and analysis of large transformer model training at-scale;,citation_author=Scott Cheng;,citation_author=Jun-Liang Lin;,citation_author=Murali Emani;,citation_author=Siddhisanket Raskar;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Venkatram Vishwanath;,citation_author=Mahmut Taylan Kandemir;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=8;,citation_journal_title=Proceedings of the ACM on Measurement and Analysis of Computing Systems;,citation_publisher=ACM New York, NY, USA;">
<meta name="citation_reference" content="citation_title=Communities through energy justice projects;,citation_author=Mary Ann Leung;,citation_author=Katharine Cahill;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois Curfman McInnes;,citation_author=Suzanne Parete-Koon;,citation_author=Subil Abraham;,citation_author=Lacy Beach Barrier;,citation_author=Gladys Chen;,citation_author=Lizanne DeStefano;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science;">
<meta name="citation_reference" content="citation_title=Applications of a foundation model approach for weather and climate;,citation_author=Troy Arcomano;,citation_author=Alexander Wikner;,citation_author=Romit Maulik;,citation_author=Veerabhadra Rao Kotamarthi;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_volume=2023;,citation_conference_title=AGU fall meeting abstracts;">
<meta name="citation_reference" content="citation_title=Toward a holistic performance evaluation of large language models across diverse ai accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_author=Sanjif Shanmugavelu;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 IEEE international parallel and distributed processing symposium workshops (IPDPSW);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Intro to HPC bootcamp: Engaging new communities through energy justice projects;,citation_author=Suzanne Parete-Koon;,citation_author=Michael Sandoval;,citation_author=Kellen Leland;,citation_author=Subil Abraham;,citation_author=Mary Ann Leung;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois McInnes;,citation_author=Sreeranjani Ramprakash;,citation_author=Lacy Beach Barrier;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science Education;,citation_publisher=Oak Ridge National Laboratory (ORNL), Oak Ridge, TN (United States);">
<meta name="citation_reference" content="citation_title=MProt-DPO: Breaking the ExaFLOPS barrier for multimodal protein design workflows with direct preference optimization;,citation_author=Gautham Dharuman;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Sam Foreman;,citation_author=Väinä Hatanpää;,citation_author=Varuni K Sastry;,citation_author=Huihuo Zheng;,citation_author=Logan Ward;,citation_author=Servesh Muralidharan;,citation_author=Archit Vasan;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 SC24: International conference for high performance computing, networking, storage and analysis SC;,citation_conference=IEEE Computer Society;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=Connor Holmes;,citation_author=Martin Cai;,citation_author=Adam Ghanem;,citation_author=Zhongzhu Zhou;,citation_author=Yuxiong He;,citation_author=Pete Luferenko;,citation_author=Divya Kumar;,citation_author=Jonathan Weyn;,citation_author=Ruixiong Zhang;,citation_author=Sylwester Klocek;,citation_author=Volodymyr Vragov;,citation_author=Mohammed AlQuraishi;,citation_author=Gustaf Ahdritz;,citation_author=Christina Floristean;,citation_author=Cristina Negri;,citation_author=Rao Kotamarthi;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_author=Sam Foreman;,citation_author=Kyle Hippe;,citation_author=Troy Arcomano;,citation_author=Romit Maulik;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot;,citation_author=Murali Emani;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Prasanna Balaprakash;,citation_author=Gina Tourassi;,citation_author=John Gounley;,citation_author=Heidi Hanson;,citation_author=Thomas E Potok;,citation_author=Massimiliano Lupo Pasini;,citation_author=Kate Evans;,citation_author=Dan Lu;,citation_author=Dalton Lunga;,citation_author=Junqi Yin;,citation_author=Sajal Dash;,citation_author=Feiyi Wang;,citation_author=Mallikarjun Shankar;,citation_author=Isaac Lyngaas;,citation_author=Xiao Wang;,citation_author=Guojing Cong;,citation_author=Pei Zhang;,citation_author=Ming Fan;,citation_author=Siyan Liu;,citation_author=Adolfy Hoisie;,citation_author=Shinjae Yoo;,citation_author=Yihui Ren;,citation_author=William Tang;,citation_author=Kyle Felker;,citation_author=Alexey Svyatkovskiy;,citation_author=Hang Liu;,citation_author=Ashwin Aji;,citation_author=Angela Dalton;,citation_author=Michael Schulte;,citation_author=Karl Schulz;,citation_author=Yuntian Deng;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Anima Anandkumar;,citation_author=Rick Stevens;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04610;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=The climate risk &amp;amp;amp; resilience portal (ClimRR) metadata and data dictionary;,citation_author=C. Burdi;,citation_author=Wall. T Branham;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://dub.sh/ClimRR-Metadata;">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2105.03418;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy Justice Analysis of Climate Data with ClimRR;,citation_author=Sam Foreman;,citation_publication_date=2023-08-07;,citation_cover_date=2023-08-07;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/climate-analysis;,citation_language=en;">
<meta name="citation_reference" content="citation_author=Sam Foreman;,citation_publication_date=2023-08-19;,citation_cover_date=2023-08-19;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/l2hmc-qcd;,citation_language=en;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James Osborn;,citation_publication_date=00;,citation_cover_date=00;,citation_year=0;,citation_conference_title=40th international symposium on lattice field theory (lattice 2023) (batavia, IL, united states, 07/31/2023 - 08/04/2023);">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=S. Foreman;,citation_author=X. Jin;,citation_author=J. Osborn;,citation_publication_date=2022-07;,citation_cover_date=2022-07;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_conference_title=The 38th international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Mastering language models;,citation_author=Samuel Montgomery;,citation_publication_date=2023-10;,citation_cover_date=2023-10;,citation_year=2023;,citation_fulltext_html_url=https://towardsdatascience.com/mastering-language-models-32e1d891511a
           ;,citation_journal_title=Medium;,citation_publisher=Towards Data Science;">
<meta name="citation_reference" content="citation_title=Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond;,citation_author=Jingfeng Yang;,citation_author=Hongye Jin;,citation_author=Ruixiang Tang;,citation_author=Xiaotian Han;,citation_author=Qizhang Feng;,citation_author=Haoming Jiang;,citation_author=Bing Yin;,citation_author=Xia Hu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2304.13712;">
<meta name="citation_reference" content="citation_title=Training tips for the transformer model;,citation_author=Martin Popel;,citation_author=Ondřej Bojar;,citation_publication_date=2018-04;,citation_cover_date=2018-04;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.2478%2Fpralin-2018-0002;,citation_issue=1;,citation_doi=10.2478/pralin-2018-0002;,citation_volume=110;,citation_journal_title=The Prague Bulletin of Mathematical Linguistics;,citation_publisher=Charles University in Prague, Karolinum Press;">
<meta name="citation_reference" content="citation_title=Attention is all you need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N. Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03762;">
<meta name="citation_reference" content="citation_title=Tree of thoughts: Deliberate problem solving with large language models;,citation_author=Shunyu Yao;,citation_author=Dian Yu;,citation_author=Jeffrey Zhao;,citation_author=Izhak Shafran;,citation_author=Thomas L. Griffiths;,citation_author=Yuan Cao;,citation_author=Karthik Narasimhan;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2305.10601;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_abstract=We seek to transform how new and emergent variants of pandemiccausing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.Competing Interest StatementThe authors have declared no competing interest.;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot-Sasson;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.biorxiv.org/content/early/2022/11/23/2022.10.10.511571;,citation_doi=10.1101/2022.10.10.511571;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
</head>

<body class="nav-sidebar floating quarto-dark"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = true;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../02-llms/index.html">[02] Large Language Models</a></li><li class="breadcrumb-item"><a href="../../02-llms/1-hands-on-llms/index.html">[1] Hands-on LLMs</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Intro to HPC Bootcamp 2025</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="../../index.html" title="Home" class="quarto-navigation-tool px-1" aria-label="Home"><i class="bi bi-home"></i></a>
    <a href="https://github.com/saforem2/intro-hpc-bootcamp-2025" title="GitHub" class="quarto-navigation-tool px-1" aria-label="GitHub"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../00-intro-AI-HPC/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[00] Intro to AI and HPC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/0-compute-systems/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] Compute Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/1-shared-resources/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[1] Shared Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/2-jupyter-notebooks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[2] Jupyter Notebooks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/3-homework/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[3] Homework</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/4-nersc/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[4] NERSC</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/5-mcmc-example/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[5] MCMC Example</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/6-linear-regression/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[6] Linear Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../01-neural-networks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[01] Neural Networks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/0-intro/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] Intro to NNs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/1-mnist/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[1] MNIST Example</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/2-advanced/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[2] Advanced</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/3-conv-nets/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[3] Conv. Nets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/4-representation-learning/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[4] Representation Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../02-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[02] Large Language Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/0-intro-to-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] Intro to LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/1-hands-on-llms/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">[1] Hands-on LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/4-evaluating-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[4] Evaluating LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/5-advanced-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[5] Advanced LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/6-parallel-training/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[6] Parallel Training</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/7-shakespeare-example/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[7] Shakespeare Example</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/8-shakespeare-example-colab/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[8] Shakespeare Example (Colab)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../03-ai-for-science/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[03] AI for Science</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../03-ai-for-science/0-genslm/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] GenSLM</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#outline" id="toc-outline" class="nav-link active" data-scroll-target="#outline">Outline</a></li>
  <li><a href="#modeling-sequential-data" id="toc-modeling-sequential-data" class="nav-link" data-scroll-target="#modeling-sequential-data">Modeling Sequential Data</a></li>
  <li><a href="#scientific-sequential-data-modeling-examples" id="toc-scientific-sequential-data-modeling-examples" class="nav-link" data-scroll-target="#scientific-sequential-data-modeling-examples">Scientific sequential data modeling examples</a>
  <ul class="collapse">
  <li><a href="#nucleic-acid-sequences-genomic-data" id="toc-nucleic-acid-sequences-genomic-data" class="nav-link" data-scroll-target="#nucleic-acid-sequences-genomic-data">Nucleic acid sequences + genomic data</a></li>
  <li><a href="#protein-sequences" id="toc-protein-sequences" class="nav-link" data-scroll-target="#protein-sequences">Protein sequences</a></li>
  <li><a href="#other-applications" id="toc-other-applications" class="nav-link" data-scroll-target="#other-applications">Other applications</a></li>
  </ul></li>
  <li><a href="#overview-of-language-models" id="toc-overview-of-language-models" class="nav-link" data-scroll-target="#overview-of-language-models">Overview of Language models</a>
  <ul class="collapse">
  <li><a href="#transformers" id="toc-transformers" class="nav-link" data-scroll-target="#transformers">Transformers</a></li>
  </ul></li>
  <li><a href="#coding-example-of-llms-in-action" id="toc-coding-example-of-llms-in-action" class="nav-link" data-scroll-target="#coding-example-of-llms-in-action">Coding example of LLMs in action!</a></li>
  <li><a href="#whats-going-on-under-the-hood" id="toc-whats-going-on-under-the-hood" class="nav-link" data-scroll-target="#whats-going-on-under-the-hood">What’s going on under the hood?</a></li>
  <li><a href="#tokenization-and-embedding-of-sequential-data" id="toc-tokenization-and-embedding-of-sequential-data" class="nav-link" data-scroll-target="#tokenization-and-embedding-of-sequential-data">Tokenization and embedding of sequential data</a>
  <ul class="collapse">
  <li><a href="#example-of-tokenization" id="toc-example-of-tokenization" class="nav-link" data-scroll-target="#example-of-tokenization">Example of tokenization</a></li>
  <li><a href="#token-embedding" id="toc-token-embedding" class="nav-link" data-scroll-target="#token-embedding">Token embedding:</a></li>
  </ul></li>
  <li><a href="#transformer-model-architecture" id="toc-transformer-model-architecture" class="nav-link" data-scroll-target="#transformer-model-architecture">Transformer Model Architecture</a>
  <ul class="collapse">
  <li><a href="#attention-mechanisms" id="toc-attention-mechanisms" class="nav-link" data-scroll-target="#attention-mechanisms">Attention mechanisms</a></li>
  </ul></li>
  <li><a href="#pipeline-using-huggingface" id="toc-pipeline-using-huggingface" class="nav-link" data-scroll-target="#pipeline-using-huggingface">Pipeline using HuggingFace</a>
  <ul class="collapse">
  <li><a href="#setting-up-a-prompt" id="toc-setting-up-a-prompt" class="nav-link" data-scroll-target="#setting-up-a-prompt">1. Setting up a prompt</a></li>
  <li><a href="#loading-pretrained-models" id="toc-loading-pretrained-models" class="nav-link" data-scroll-target="#loading-pretrained-models">2. Loading Pretrained Models</a></li>
  <li><a href="#loading-in-the-tokenizer-and-tokenizing-input-text" id="toc-loading-in-the-tokenizer-and-tokenizing-input-text" class="nav-link" data-scroll-target="#loading-in-the-tokenizer-and-tokenizing-input-text">3. Loading in the tokenizer and tokenizing input text</a></li>
  <li><a href="#performing-inference-and-interpreting" id="toc-performing-inference-and-interpreting" class="nav-link" data-scroll-target="#performing-inference-and-interpreting">4. Performing inference and interpreting</a></li>
  <li><a href="#saving-and-loading-models" id="toc-saving-and-loading-models" class="nav-link" data-scroll-target="#saving-and-loading-models">Saving and loading models</a></li>
  </ul></li>
  <li><a href="#model-hub" id="toc-model-hub" class="nav-link" data-scroll-target="#model-hub">Model Hub</a></li>
  <li><a href="#recommended-reading" id="toc-recommended-reading" class="nav-link" data-scroll-target="#recommended-reading">Recommended reading</a></li>
  <li><a href="#homework" id="toc-homework" class="nav-link" data-scroll-target="#homework">Homework</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/blob/main/02-llms/1-hands-on-llms/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/edit/main/02-llms/1-hands-on-llms/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.md"><i class="bi bi-file-code"></i>Github (GFM)</a></li><li><a href="index.ipynb" download="index.ipynb"><i class="bi bi-journal-code"></i>Jupyter</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TC329HJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../02-llms/index.html">[02] Large Language Models</a></li><li class="breadcrumb-item"><a href="../../02-llms/1-hands-on-llms/index.html">[1] Hands-on LLMs</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Hands On: Introduction to Large Language Models (LLMs)</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>

<div>
  <div class="description">
    An introduction to Large Language Models (LLMs), their architecture, and how they can be applied to scientific applications.
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://samforeman.me">Sam Foreman</a> <a href="mailto:foremans@anl.gov" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://alcf.anl.gov/about/people/sam-foreman">
            </a><a href="https://alcf.anl.gov/about/people/sam-foreman">ALCF</a>
            
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 23, 2025</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">July 23, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<p><a href="https://colab.research.google.com/github/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/IntroLLMs.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></p>
<div class="callout callout-style-simple callout-note callout-titled" title="Authors">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Authors
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Content in this notebook is modified from content originally written by:</p>
<ul>
<li>Archit Vasan, Huihuo Zheng, Marieme Ngom, Bethany Lusch, Taylor Childers, Venkat Vishwanath</li>
</ul>
<p>Inspiration from the blog posts “The Illustrated Transformer” and “The Illustrated GPT2” by Jay Alammar, highly recommended reading.</p>
</div>
</div>
</div>
<p>Although the name “language models” is derived from Natural Language Processing, the models used in these approaches can be applied to diverse scientific applications as illustrated below.</p>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<p>During this session I will cover:</p>
<ol type="1">
<li>Scientific applications for language models</li>
<li>General overview of Transformers</li>
<li>Tokenization</li>
<li>Model Architecture</li>
<li>Pipeline using HuggingFace</li>
<li>Model loading</li>
</ol>
</section>
<section id="modeling-sequential-data" class="level2">
<h2 class="anchored" data-anchor-id="modeling-sequential-data">Modeling Sequential Data</h2>
<p>Sequences are variable-length lists with data in subsequent iterations that depends on previous iterations (or tokens).</p>
<p>Mathematically:</p>
<p>A sequence is a list of tokens:</p>
<p><span class="math display">T = [t_1, t_2, t_3,...,t_N]</span></p>
<p>where each token within the list depends on the others with a particular probability:</p>
<p><span class="math display">P(t_N | t_{N-1}, ..., t_3, t_2, t_1)</span></p>
<p>The purpose of sequential modeling is to learn these probabilities for possible tokens in a distribution to perform various tasks including:</p>
<ul>
<li>Sequence generation based on a prompt</li>
<li>Language translation (e.g.&nbsp;English –&gt; French)</li>
<li>Property prediction (predicting a property based on an entire sequence)</li>
<li>Identifying mistakes or missing elements in sequential data</li>
</ul>
</section>
<section id="scientific-sequential-data-modeling-examples" class="level2">
<h2 class="anchored" data-anchor-id="scientific-sequential-data-modeling-examples">Scientific sequential data modeling examples</h2>
<section id="nucleic-acid-sequences-genomic-data" class="level3">
<h3 class="anchored" data-anchor-id="nucleic-acid-sequences-genomic-data">Nucleic acid sequences + genomic data</h3>
<div id="fig-RNA-codons" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-RNA-codons-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="https://github.com/architvasan/ai_science_local/blob/main/images/RNA-codons.svg.png?raw=1" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: RNA Codons"><img src="https://github.com/architvasan/ai_science_local/blob/main/images/RNA-codons.svg.png?raw=1" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-RNA-codons-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: RNA Codons
</figcaption>
</figure>
</div>
<p>Nucleic acid sequences can be used to predict translation of proteins, mutations, and gene expression levels.</p>
<p>Here is an image of GenSLM. This is a language model developed by Argonne researchers that can model genomic information in a single model. It was shown to model the evolution of SARS-COV2 without expensive experiments.</p>
<div id="fig-genslm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genslm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="https://github.com/architvasan/ai_science_local/blob/main/images/genslm.png?raw=1" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Genomic Scale Language Models (GenSLM) Zvyagin et. al 2022. BioRXiv"><img src="https://github.com/architvasan/ai_science_local/blob/main/images/genslm.png?raw=1" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genslm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Genomic Scale Language Models (GenSLM) <a href="https://www.biorxiv.org/content/10.1101/2022.10.10.511571v1">Zvyagin et. al 2022. BioRXiv</a>
</figcaption>
</figure>
</div>
</section>
<section id="protein-sequences" class="level3">
<h3 class="anchored" data-anchor-id="protein-sequences">Protein sequences</h3>
<p>Protein sequences can be used to predict folding structure, protein-protein interactions, chemical/binding properties, protein function and many more properties.</p>
<div id="fig-protein-structure" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-protein-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="https://github.com/architvasan/ai_science_local/blob/main/images/Protein-Structure-06.png?raw=1" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Protein Structure"><img src="https://github.com/architvasan/ai_science_local/blob/main/images/Protein-Structure-06.png?raw=1" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-protein-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Protein Structure
</figcaption>
</figure>
</div>
<div id="fig-esmfold" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-esmfold-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="https://github.com/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/images/ESMFold.png?raw=1" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: ESMFold Lin et. al.&nbsp;2023. Science"><img src="https://github.com/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/images/ESMFold.png?raw=1" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-esmfold-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: ESMFold <a href="https://www.science.org/doi/10.1126/science.ade2574">Lin et. al.&nbsp;2023. Science</a>
</figcaption>
</figure>
</div>
</section>
<section id="other-applications" class="level3">
<h3 class="anchored" data-anchor-id="other-applications">Other applications</h3>
<ul>
<li>Biomedical text</li>
<li>SMILES strings</li>
<li>Weather predictions</li>
<li>Interfacing with simulations such as molecular dynamics simulation</li>
</ul>
</section>
</section>
<section id="overview-of-language-models" class="level2">
<h2 class="anchored" data-anchor-id="overview-of-language-models">Overview of Language models</h2>
<p>We will now briefly talk about the progression of language models.</p>
<section id="transformers" class="level3">
<h3 class="anchored" data-anchor-id="transformers">Transformers</h3>
<p>The most common LMs base their design on the Transformer architecture that was introduced in 2017 in the “Attention is all you need” paper.</p>
<div id="fig-attention-is-all-you-need" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-is-all-you-need-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="https://github.com/architvasan/ai_science_local/blob/main/images/attention_is_all_you_need.png?raw=1" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Attention is all you need Vaswani 2017. Advances in Neural Information Processing Systems"><img src="https://github.com/architvasan/ai_science_local/blob/main/images/attention_is_all_you_need.png?raw=1" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-is-all-you-need-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Attention is all you need <a href="https://arxiv.org/pdf/1706.03762">Vaswani 2017. Advances in Neural Information Processing Systems</a>
</figcaption>
</figure>
</div>
<p>Since then a multitude of LLM architectures have been designed.</p>
<div id="fig-ch1-transformers" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ch1-transformers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="https://github.com/architvasan/ai_science_local/blob/main/images/en_chapter1_transformers_chrono.svg?raw=1" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Transformers, chronologically"><img src="https://github.com/architvasan/ai_science_local/blob/main/images/en_chapter1_transformers_chrono.svg?raw=1" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ch1-transformers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Transformers, chronologically
</figcaption>
</figure>
</div>
<p><a href="https://huggingface.co/learn/nlp-course/chapter1/4">HuggingFace NLP Course</a></p>
</section>
</section>
<section id="coding-example-of-llms-in-action" class="level2">
<h2 class="anchored" data-anchor-id="coding-example-of-llms-in-action">Coding example of LLMs in action!</h2>
<p>Let’s look at an example of running inference with a LLM as a block box to generate text given a prompt and we will also initiate a training loop for an LLM</p>
<p>Here, we will use the <code>transformers</code> library which is as part of HuggingFace, a repository of different models, tokenizers and information on how to apply these models</p>
<div class="callout callout-style-simple callout-warning callout-titled" title="🦜 Stochastic Parrots">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>🦜 Stochastic Parrots
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p><strong>Warning</strong>: <em>Large Language Models are only as good as their training data</em>.</p>
<p>They have no ethics, judgement, or editing ability.</p>
<p>We will be using some pretrained models from Hugging Face which used wide samples of internet hosted text.</p>
<p>The datasets have not been strictly filtered to restrict all malign content so the generated text may be surprisingly dark or questionable.</p>
<p>They do not reflect our core values and are only used for demonstration purposes.</p>
</div>
</div>
</div>
<div id="1284b682" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:35}}" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co">'''</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="co">Uncomment below section if running on sophia jupyter notebook</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co">'''</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co"># import os</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co"># os.environ["HTTP_PROXY"]="proxy.alcf.anl.gov:3128"</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co"># os.environ["HTTPS_PROXY"]="proxy.alcf.anl.gov:3128"</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co"># os.environ["http_proxy"]="proxy.alcf.anl.gov:3128"</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co"># os.environ["https_proxy"]="proxy.alcf.anl.gov:3128"</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co"># os.environ["ftp_proxy"]="proxy.alcf.anl.gov:3128"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>'\nUncomment below section if running on sophia jupyter notebook\n'</code></pre>
</div>
</div>
<div id="a355e8ba" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="op">!</span>pip install transformers</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="op">!</span>pip install pandas</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="op">!</span>pip install torch</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="e942b6b1" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> ambivalent</span>
<span id="cb4-2"><a href="#cb4-2"></a></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb4-5"><a href="#cb4-5"></a></span>
<span id="cb4-6"><a href="#cb4-6"></a>plt.style.use(ambivalent.STYLES[<span class="st">'ambivalent'</span>])</span>
<span id="cb4-7"><a href="#cb4-7"></a>sns.set_context(<span class="st">"notebook"</span>)</span>
<span id="cb4-8"><a href="#cb4-8"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> [<span class="fl">6.4</span>, <span class="fl">4.8</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="6d539b73" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer,AutoModelForCausalLM, AutoConfig</span>
<span id="cb5-2"><a href="#cb5-2"></a>input_text <span class="op">=</span> <span class="st">"My dog really wanted to eat icecream because"</span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb5-4"><a href="#cb5-4"></a>generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>, model<span class="op">=</span><span class="st">"gpt2"</span>)</span>
<span id="cb5-5"><a href="#cb5-5"></a>pipe <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>, model<span class="op">=</span><span class="st">"gpt2"</span>)</span>
<span id="cb5-6"><a href="#cb5-6"></a>generator(input_text, max_length<span class="op">=</span><span class="dv">20</span>, num_return_sequences<span class="op">=</span><span class="dv">5</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Device set to use mps:0
Device set to use mps:0
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>[{'generated_text': "My dog really wanted to eat icecream because I had a big mouth for it, the ice cream tasted really good. She loved it so much she went to my mom's place and let me make it with her. I loved it and I think my mom was really into it. I'm going to keep putting this little thing up in the freezer for a while.\n\nI'm going to put this little thing up in the freezer for a while. It's going to be a lot of fun.\n\nI'm going to put this little thing up in the freezer for a while. It's going to be a lot of fun.\n\nMy girlfriend is going to be excited about this little thing because her boyfriend wants to get into it. I'm going to put it up in the freezer for a while. It's going to be a lot of fun.\n\nMy girlfriend is going to be excited about this little thing because her boyfriend wants to get into it. I'm going to put it up in the freezer for a while. It's going to be a lot of fun.\n\nI love it. I love it like a little sister.\n\nI love it. I love it like a little sister.\n\nI love it. I love it like a little sister.\n"},
 {'generated_text': 'My dog really wanted to eat icecream because she was starving and her friends were trying to find food for her. She said she was always hungry, but she was able to eat ice cream with her."\n\nThe mother of a nine-year-old girl who went missing last year said she had been out of the house for 30 to 40 minutes before she felt hungry and had been unable to find any food, according to the Toronto Star.\n\nWhen she arrived at her family\'s home near the University of Toronto campus Monday morning, she was greeted by three strangers who identified themselves as the parents, the Star reported. Their names were not released.\n\nThe trio told police they were going to buy ice cream at a local grocery store.\n\n"They went by the name \'Kitty,\' and it was really good, that was their first stop," Jennifer Tompkins, the mother of a missing six-year-old girl, told the Star.\n\nThe man, who is not being identified, didn\'t respond to a phone message from the Star.\n\nThe man, who is not being identified, didn\'t respond to a phone message from the Star. (CBC)\n\nTompkins said she was out in the woods with her baby when the man entered, got into'},
 {'generated_text': 'My dog really wanted to eat icecream because he felt like it was just too bland. I used the same recipe for all of my pancakes, but my dog had the same problem. He started to think that, "Oh, I\'m going to be a weirdo and eat ice cream."\n\nThe next day, I brought him home and took him to the veterinarian. He was found to have a full set of pups. I said, "It\'s really bad, but we have to get you a dog that can handle ice-cream." He said, "I\'m going to give you a dog that can handle ice cream." I said, "I\'m not going to give you a dog that can handle ice cream." He said, "You can give me a dog that can handle ice cream."\n\nI went to the vet and said, "We\'re going to give you a dog that can handle ice cream and we\'re going to give you a dog that can handle ice cream and we\'re going to give you a dog that can handle ice cream." He said, "I\'m going to give you a dog that can handle ice cream." I said, "I\'m giving you a dog that can handle ice cream."\n\nThe next day, I brought him home and took him'},
 {'generated_text': 'My dog really wanted to eat icecream because she was so excited to get it. It was so amazing to see her so excited about it."\n\nThe man was taken to a hospital, where he was put on life support.\n\nAnd the man was taken to a hospital in the city centre, where the dog could not be found.\n\nThe man was taken to hospital but discharged at 10pm.'},
 {'generated_text': "My dog really wanted to eat icecream because I knew it would be delicious. I was so happy when she came to me and gave me a chocolate ice cream cone. She loved it and I was so happy she was giving back to me.\n\nShe was such a wonderful dog. She was so affectionate and I can't thank her enough for that. She loves being around and loves to have her food. It was so much fun to have her.\n\nShe loved going out during the day and she is so cute and does so many things. I have been so happy for her, she loves to play with her and we just need to have fun with it.\n\nMy wife and I just got back from a long stay abroad and we went to Canada and got to go to the Bahamas. We got there at 12:30 that morning and we had a great time and we were so happy to see her as we were driving her home.\n\nWe both had very good time. We were both very nice to her and she was so friendly. We just had to get our hair done and she was very happy to have us as our special guest. She was so happy we had a good time.\n\nAfter a few days of driving, we got back home and headed for my dad"}]</code></pre>
</div>
</div>
</section>
<section id="whats-going-on-under-the-hood" class="level2">
<h2 class="anchored" data-anchor-id="whats-going-on-under-the-hood">What’s going on under the hood?</h2>
<p>There are two components that are “black-boxes” here:</p>
<ol type="1">
<li>The method for tokenization</li>
<li>The model that generates novel text.</li>
</ol>
</section>
<section id="tokenization-and-embedding-of-sequential-data" class="level2">
<h2 class="anchored" data-anchor-id="tokenization-and-embedding-of-sequential-data">Tokenization and embedding of sequential data</h2>
<p>Humans can inherently understand language data because they previously learned phonetic sounds.</p>
<p>Machines don’t have phonetic knowledge so they need to be told how to break text into standard units to process it.</p>
<p>They use a system called “tokenization”, where sequences of text are broken into smaller parts, or “tokens”, and then fed as input.</p>
<div>
<p><img src="https://github.com/architvasan/ai_science_local/blob/main/images/text-processing---machines-vs-humans.png?raw=1" width="400"></p>
</div>
<p>Tokenization is a data preprocessing step which transforms the raw text data into a format suitable for machine learning models. Tokenizers break down raw text into smaller units called tokens. These tokens are what is fed into the language models. Based on the type and configuration of the tokenizer, these tokens can be words, subwords, or characters.</p>
<p>Types of tokenizers:</p>
<ol type="1">
<li>Character Tokenizers: Split text into individual characters.</li>
<li>Word Tokenizers: Split text into words based on whitespace or punctuation.</li>
<li>Subword Tokenizers: Split text into subword units, such as morphemes or character n-grams. Common subword tokenization algorithms include:</li>
<li>Byte-Pair Encoding (BPE),</li>
<li>SentencePiece,</li>
<li>WordPiece.</li>
</ol>
<div>
<p><img src="https://github.com/architvasan/ai_science_local/blob/main/images/tokenization_image.webp?raw=1" width="400"></p>
</div>
<p><a href="https://nlpiation.medium.com/how-to-use-huggingfaces-transformers-pre-trained-tokenizers-e029e8d6d1fa">nlpiation</a></p>
<section id="example-of-tokenization" class="level3">
<h3 class="anchored" data-anchor-id="example-of-tokenization">Example of tokenization</h3>
<p>Let’s look at an example of tokenization using byte-pair encoding.</p>
<div id="5625b6e4" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb8-2"><a href="#cb8-2"></a></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="co"># A utility function to tokenize a sequence and print out some information about it.</span></span>
<span id="cb8-4"><a href="#cb8-4"></a></span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="kw">def</span> tokenization_summary(tokenizer, sequence):</span>
<span id="cb8-6"><a href="#cb8-6"></a></span>
<span id="cb8-7"><a href="#cb8-7"></a>    <span class="co"># get the vocabulary</span></span>
<span id="cb8-8"><a href="#cb8-8"></a>    vocab <span class="op">=</span> tokenizer.vocab</span>
<span id="cb8-9"><a href="#cb8-9"></a>    <span class="co"># Number of entries to print</span></span>
<span id="cb8-10"><a href="#cb8-10"></a>    n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb8-11"><a href="#cb8-11"></a></span>
<span id="cb8-12"><a href="#cb8-12"></a>    <span class="co"># Print subset of the vocabulary</span></span>
<span id="cb8-13"><a href="#cb8-13"></a>    <span class="bu">print</span>(<span class="st">"Subset of tokenizer.vocab:"</span>)</span>
<span id="cb8-14"><a href="#cb8-14"></a>    <span class="cf">for</span> i, (token, index) <span class="kw">in</span> <span class="bu">enumerate</span>(tokenizer.vocab.items()):</span>
<span id="cb8-15"><a href="#cb8-15"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>token<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>index<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-16"><a href="#cb8-16"></a>        <span class="cf">if</span> i <span class="op">&gt;=</span> n <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb8-17"><a href="#cb8-17"></a>            <span class="cf">break</span></span>
<span id="cb8-18"><a href="#cb8-18"></a></span>
<span id="cb8-19"><a href="#cb8-19"></a>    <span class="bu">print</span>(<span class="st">"Vocab size of the tokenizer = "</span>, <span class="bu">len</span>(vocab))</span>
<span id="cb8-20"><a href="#cb8-20"></a>    <span class="bu">print</span>(<span class="st">"------------------------------------------"</span>)</span>
<span id="cb8-21"><a href="#cb8-21"></a></span>
<span id="cb8-22"><a href="#cb8-22"></a>    <span class="co"># .tokenize chunks the existing sequence into different tokens based on the rules and vocab of the tokenizer.</span></span>
<span id="cb8-23"><a href="#cb8-23"></a>    tokens <span class="op">=</span> tokenizer.tokenize(sequence)</span>
<span id="cb8-24"><a href="#cb8-24"></a>    <span class="bu">print</span>(<span class="st">"Tokens : "</span>, tokens)</span>
<span id="cb8-25"><a href="#cb8-25"></a>    <span class="bu">print</span>(<span class="st">"------------------------------------------"</span>)</span>
<span id="cb8-26"><a href="#cb8-26"></a></span>
<span id="cb8-27"><a href="#cb8-27"></a>    <span class="co"># .convert_tokens_to_ids or .encode or .tokenize converts the tokens to their corresponding numerical representation.</span></span>
<span id="cb8-28"><a href="#cb8-28"></a>    <span class="co">#  .convert_tokens_to_ids has a 1-1 mapping between tokens and numerical representation</span></span>
<span id="cb8-29"><a href="#cb8-29"></a>    <span class="co"># ids = tokenizer.convert_tokens_to_ids(tokens)</span></span>
<span id="cb8-30"><a href="#cb8-30"></a>    <span class="co"># print("encoded Ids: ", ids)</span></span>
<span id="cb8-31"><a href="#cb8-31"></a></span>
<span id="cb8-32"><a href="#cb8-32"></a>    <span class="co"># .encode also adds additional information like Start of sequence tokens and End of sequene</span></span>
<span id="cb8-33"><a href="#cb8-33"></a>    <span class="bu">print</span>(<span class="st">"tokenized sequence : "</span>, tokenizer.encode(sequence))</span>
<span id="cb8-34"><a href="#cb8-34"></a></span>
<span id="cb8-35"><a href="#cb8-35"></a>    <span class="co"># .tokenizer has additional information about attention_mask.</span></span>
<span id="cb8-36"><a href="#cb8-36"></a>    <span class="co"># encode = tokenizer(sequence)</span></span>
<span id="cb8-37"><a href="#cb8-37"></a>    <span class="co"># print("Encode sequence : ", encode)</span></span>
<span id="cb8-38"><a href="#cb8-38"></a>    <span class="co"># print("------------------------------------------")</span></span>
<span id="cb8-39"><a href="#cb8-39"></a></span>
<span id="cb8-40"><a href="#cb8-40"></a>    <span class="co"># .decode decodes the ids to raw text</span></span>
<span id="cb8-41"><a href="#cb8-41"></a>    ids <span class="op">=</span> tokenizer.convert_tokens_to_ids(tokens)</span>
<span id="cb8-42"><a href="#cb8-42"></a>    decode <span class="op">=</span> tokenizer.decode(ids)</span>
<span id="cb8-43"><a href="#cb8-43"></a>    <span class="bu">print</span>(<span class="st">"Decode sequence : "</span>, decode)</span>
<span id="cb8-44"><a href="#cb8-44"></a></span>
<span id="cb8-45"><a href="#cb8-45"></a></span>
<span id="cb8-46"><a href="#cb8-46"></a>tokenizer_1  <span class="op">=</span>  AutoTokenizer.from_pretrained(<span class="st">"gpt2"</span>) <span class="co"># GPT-2 uses "Byte-Pair Encoding (BPE)"</span></span>
<span id="cb8-47"><a href="#cb8-47"></a></span>
<span id="cb8-48"><a href="#cb8-48"></a>sequence <span class="op">=</span> <span class="st">"Counselor, please adjust your Zoom filter to appear as a human, rather than as a cat"</span></span>
<span id="cb8-49"><a href="#cb8-49"></a></span>
<span id="cb8-50"><a href="#cb8-50"></a>tokenization_summary(tokenizer_1, sequence)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Subset of tokenizer.vocab:
ithe: 31470
locked: 24162
ĠElemental: 21340
ny: 3281
Ġpair: 5166
Ġreasonably: 13025
echo: 30328
IDENT: 25256
Ġinterpret: 6179
atson: 13506
Vocab size of the tokenizer =  50257
------------------------------------------
Tokens :  ['Coun', 'sel', 'or', ',', 'Ġplease', 'Ġadjust', 'Ġyour', 'ĠZoom', 'Ġfilter', 'Ġto', 'Ġappear', 'Ġas', 'Ġa', 'Ġhuman', ',', 'Ġrather', 'Ġthan', 'Ġas', 'Ġa', 'Ġcat']
------------------------------------------
tokenized sequence :  [31053, 741, 273, 11, 3387, 4532, 534, 40305, 8106, 284, 1656, 355, 257, 1692, 11, 2138, 621, 355, 257, 3797]
Decode sequence :  Counselor, please adjust your Zoom filter to appear as a human, rather than as a cat</code></pre>
</div>
</div>
</section>
<section id="token-embedding" class="level3">
<h3 class="anchored" data-anchor-id="token-embedding">Token embedding:</h3>
<p>Words are turned into vectors based on their location within a vocabulary.</p>
<p>The strategy of choice for learning language structure from tokenized text is to find a clever way to map each token into a moderate-dimension vector space, adjusting the mapping so that</p>
<p>Similar, or associated tokens take up residence nearby each other, and different regions of the space correspond to different position in the sequence. Such a mapping from token ID to a point in a vector space is called a token embedding. The dimension of the vector space is often high (e.g.&nbsp;1024-dimensional), but much smaller than the vocabulary size (30,000–500,000).</p>
<p>Various approaches have been attempted for generating such embeddings, including static algorithms that operate on a corpus of tokenized data as preprocessors for NLP tasks. Transformers, however, adjust their embeddings during training.</p>
</section>
</section>
<section id="transformer-model-architecture" class="level2">
<h2 class="anchored" data-anchor-id="transformer-model-architecture">Transformer Model Architecture</h2>
<p>Now let’s look at the base elements that make up a Transformer by dissecting the popular GPT2 model</p>
<div id="2f216fec" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="im">from</span> transformers <span class="im">import</span> GPT2Tokenizer, GPT2LMHeadModel</span>
<span id="cb10-2"><a href="#cb10-2"></a>model <span class="op">=</span> GPT2LMHeadModel.from_pretrained(<span class="st">'gpt2'</span>)</span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="bu">print</span>(model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D(nf=2304, nx=768)
          (c_proj): Conv1D(nf=768, nx=768)
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D(nf=3072, nx=768)
          (c_proj): Conv1D(nf=768, nx=3072)
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)</code></pre>
</div>
</div>
<p>GPT2 is an example of a Transformer Decoder which is used to generate novel text.</p>
<p>Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models. The pretraining of decoder models usually revolves around predicting the next word in the sentence.</p>
<p>These models are best suited for tasks involving text generation.</p>
<p>The architecture of GPT-2 is inspired by the paper: “Generating Wikipedia by Summarizing Long Sequences” which is another arrangement of the transformer block that can do language modeling. This model threw away the encoder and thus is known as the “Transformer-Decoder”.</p>
<div>
<p><img src="https://github.com/architvasan/ai_science_local/blob/main/images/transformer-decoder-intro.png?raw=1" width="500"></p>
</div>
<p><a href="https://jalammar.github.io/illustrated-gpt2/">Illustrated GPT2</a></p>
<p>Key components of the transformer architecture include:</p>
<ul>
<li><p>Input Embeddings: Word embedding or word vectors help us represent words or text as a numeric vector where words with similar meanings have the similar representation.</p></li>
<li><p>Positional Encoding: Injects information about the position of words in a sequence, helping the model understand word order.</p></li>
<li><p>Self-Attention Mechanism: Allows the model to weigh the importance of different words in a sentence, enabling it to effectively capture contextual information.</p></li>
<li><p>Feedforward Neural Networks: Process information from self-attention layers to generate output for each word/token.</p></li>
<li><p>Layer Normalization and Residual Connections: Aid in stabilizing training and mitigating the vanishing gradient problem.</p></li>
<li><p>Transformer Blocks: Comprised of multiple layers of self-attention and feedforward neural networks, stacked together to form the model.</p></li>
</ul>
<section id="attention-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="attention-mechanisms">Attention mechanisms</h3>
<p>Since attention mechanisms are arguably the most powerful component of the Transformer, let’s discuss this in a little more detail.</p>
<p>Suppose the following sentence is an input sentence we want to translate using an LLM:</p>
<p><code>”The animal didn't cross the street because it was too tired”</code></p>
<p>To understand a full sentence, the model needs to understand what each word means in relation to other words.</p>
<p>For example, when we read the sentence: <code>”The animal didn't cross the street because it was too tired”</code> we know intuitively that the word <code>"it"</code> refers to <code>"animal"</code>, the state for <code>"it"</code> is <code>"tired"</code>, and the associated action is <code>"didn't cross"</code>.</p>
<p>However, the model needs a way to learn all of this information in a simple yet generalizable way. What makes Transformers particularly powerful compared to earlier sequential architectures is how it encodes context with the <strong>self-attention mechanism</strong>.</p>
<p>As the model processes each word in the input sequence, attention looks at other positions in the input sequence for clues to a better understanding for this word.</p>
<div>
<p><img src="https://github.com/architvasan/ai_science_local/blob/main/images/transformer_self-attention_visualization.png?raw=1" width="400"></p>
</div>
<p><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p>
<section id="multi-head-attention" class="level4">
<h4 class="anchored" data-anchor-id="multi-head-attention">Multi-head attention</h4>
<p>In practice, multiple attention heads are used simultaneously.</p>
<p>This: * Expands the model’s ability to focus on different positions. * Prevents the attention to be dominated by the word itself.</p>
</section>
<section id="lets-see-multi-head-attention-mechanisms-in-action" class="level4">
<h4 class="anchored" data-anchor-id="lets-see-multi-head-attention-mechanisms-in-action">Let’s see multi-head attention mechanisms in action!</h4>
<p>We are going to use the powerful visualization tool bertviz, which allows an interactive experience of the attention mechanisms. Normally these mechanisms are abstracted away but this will allow us to inspect our model in more detail.</p>
<div id="8fd552c8" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="op">!</span>pip install bertviz</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Let’s load in the model, GPT2 and look at the attention mechanisms.</p>
<p><strong>Hint… click on the different blocks in the visualization to see the attention</strong></p>
<div id="cf3312d4" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:907,&quot;referenced_widgets&quot;:[&quot;ad748b4c592645cb90d427c9012d7368&quot;,&quot;a34cdc4592cf46ad9ccf69caa52363bd&quot;,&quot;c1eac1886ea84c87bed161c960569d3d&quot;,&quot;ca50498e237843709d8c44e7ea0e94b5&quot;,&quot;8f4387f0bb3e402ab1ab4d443c693f96&quot;,&quot;47316e9804804c8d9c9b031fa3d86052&quot;,&quot;6cc5bd956c694bc5b63c0c7bba2a29f8&quot;,&quot;93728bade1714bcabbe90f1b9b95b7c2&quot;,&quot;83cea646339b4e74a9582827afdf5753&quot;,&quot;c765d253aa974bc59654f0398afb964d&quot;,&quot;3d71478078bc4743a580dc6200cccbd0&quot;,&quot;099e1574f4a14c308be2364d0bba2a76&quot;,&quot;6f822f4ae7734ce09736d50bf17ad99e&quot;,&quot;93fbd0ce38df488fa725d7c584344e6b&quot;,&quot;ee56c1c4c2be4086883a95e15adc655d&quot;,&quot;846bb66c196540febf6a7bc32b60dc3a&quot;,&quot;37dcd77ea3154208be44bfa35a88fa90&quot;,&quot;fdd9ba7947794f4aa0e2782e3b379f61&quot;,&quot;64fa9056ffef4ab3a4e2db085a48cbfb&quot;,&quot;acee154ece2e4a939f42236ec38de871&quot;,&quot;46acb20f74174029962ecb8142c576fb&quot;,&quot;b58662cfa5fb4406a171135e01d25368&quot;,&quot;c770f4a4cad0435ebedbb932c1431f52&quot;,&quot;14dab4fee3d34783bf838cfbd4bdbcaa&quot;,&quot;d00cd656bc4f4f0d9cf52b942fceb155&quot;,&quot;d5fbc43ad5204bb88b11ddceaedc5d6d&quot;,&quot;8d825e3d51524720ada2f1ecd9a9ffd5&quot;,&quot;9a17934807c140eeb8c0a5cf80c4cb50&quot;,&quot;7d2254be42234d24b044f6c9c0d49119&quot;,&quot;9eddddc3c9034e6da5bf612da55f5714&quot;,&quot;9548afdd0d154788ba3edb96c326c85f&quot;,&quot;2213cf9588c545f2b200186766ac425d&quot;,&quot;d56ccfc975d949b7a6cbe7b9a89a1101&quot;,&quot;ff3907fe32a14ae681a6a01933e6eaf5&quot;,&quot;f08eac6bbad843ca95157d035c1ef043&quot;,&quot;7e8af1c44d984d99a2b27e80c05cec75&quot;,&quot;0a381955a75148d5a59815efb2b352bd&quot;,&quot;7ac42549074746b48926a9cffe0a542c&quot;,&quot;55b65d9fa1e44149bfc763a8c99e7c2a&quot;,&quot;cd7ca2bbf9b04de4842b316ea7a5535f&quot;,&quot;0c0b4f2c4ef14413bc24340afc040431&quot;,&quot;6442fa46c934428998cf8e3d5835e8bc&quot;,&quot;8da520e3f829425b83e9b92149f792ad&quot;,&quot;78c5836dbb6b4538abcc3cd348c5b958&quot;,&quot;3f9fa0a2a0614bae87f7c698d043a3b5&quot;,&quot;6c5fb86539584d6dad2af92729b9a1ef&quot;,&quot;1eb460d0dad44857a14646782366c7c1&quot;,&quot;67c1873c51d34503833366af483744a3&quot;,&quot;713fef79f5d441b18f372c64d9569551&quot;,&quot;4b5d59490f264d03a9ac52897ae94d33&quot;,&quot;42519c4cca864ad78595214e86079c8f&quot;,&quot;bd3444ba231347378ae1a7ffa4181648&quot;,&quot;d701e46a5472481ea942b613e3506f03&quot;,&quot;e2effa7f78034f5c98a2c7f2d12b7ec3&quot;,&quot;8ee1c3f4f84f49b9aa78590d20e03005&quot;,&quot;00d232fe28f74a1ea9fad0510586ff9c&quot;,&quot;e2e8c12232854688802988193dd6bdc8&quot;,&quot;3129da774c2c4298afbf3d766ca7f428&quot;,&quot;0d36758d65ed404e8181bdc4e2a9d26b&quot;,&quot;3e75401bc3bf42da8f9e1a377401b69d&quot;,&quot;fbd25c280a554d50ace7384d5939e335&quot;,&quot;3371add482984a3bab5c31379484af71&quot;,&quot;001363aab0a143a683a07e48cbbf5594&quot;,&quot;c87375d637f74dc4ab4bab00852c758c&quot;,&quot;b5f27258ef4a4b9e8b7d13387b14d9a1&quot;,&quot;d201d648b932443ca9141aa9129e5661&quot;,&quot;0a2f04862b3a4c36becf5949c67a14a5&quot;,&quot;8f57a09202cf4cddb2ba697ecfc78daf&quot;,&quot;a9fae2f4af744d33bddeeb48420cda3e&quot;,&quot;2994daa8857945509c97e3f2cdb4ee97&quot;,&quot;593dfccfa5ca4e56a32e9cead70714d2&quot;,&quot;5f3d428e20334a3da4c82d0fc2e3eb3e&quot;,&quot;5e028ae3b1824c74a0d610129dcc8246&quot;,&quot;28ba4c7a70294e589cb2ced5766072e4&quot;,&quot;d3a9c7752afb49a5bb1ea70bdb8744fe&quot;,&quot;09eed5891aa647da9f7c9beceac50622&quot;,&quot;4868e95bf5ec4cecab53e5e65862aea6&quot;]}}" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModel, utils, AutoModelForCausalLM</span>
<span id="cb13-2"><a href="#cb13-2"></a></span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="im">from</span> bertviz <span class="im">import</span> model_view</span>
<span id="cb13-4"><a href="#cb13-4"></a>utils.logging.set_verbosity_error()  <span class="co"># Suppress standard warnings</span></span>
<span id="cb13-5"><a href="#cb13-5"></a></span>
<span id="cb13-6"><a href="#cb13-6"></a>model_name <span class="op">=</span> <span class="st">'openai-community/gpt2'</span></span>
<span id="cb13-7"><a href="#cb13-7"></a>input_text <span class="op">=</span> <span class="st">"No, I am your father"</span></span>
<span id="cb13-8"><a href="#cb13-8"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_name, output_attentions<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-9"><a href="#cb13-9"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb13-10"><a href="#cb13-10"></a>inputs <span class="op">=</span> tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)  <span class="co"># Tokenize input text</span></span>
<span id="cb13-11"><a href="#cb13-11"></a>outputs <span class="op">=</span> model(inputs)  <span class="co"># Run model</span></span>
<span id="cb13-12"><a href="#cb13-12"></a>attention <span class="op">=</span> outputs[<span class="op">-</span><span class="dv">1</span>]  <span class="co"># Retrieve attention from model outputs</span></span>
<span id="cb13-13"><a href="#cb13-13"></a>tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(inputs[<span class="dv">0</span>])  <span class="co"># Convert input ids to token strings</span></span>
<span id="cb13-14"><a href="#cb13-14"></a>model_view(attention, tokens)  <span class="co"># Display model view</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
</div>
<div class="cell-output cell-output-display">
      
        <div id="bertviz-35ff1a37e60b46acbd7953cb14920dca" style="font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;">
            <span style="user-select:none">
                
            </span>
            <div id="vis"></div>
        </div>
    
</div>
<div class="cell-output cell-output-display">
<script type="application/javascript">
/**
 * @fileoverview Transformer Visualization D3 javascript code.
 *
 * Based on: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/visualization/attention.js
 *
 * Change log:
 *
 * 02/01/19  Jesse Vig   Initial implementation
 * 12/31/20  Jesse Vig   Support multiple visualizations in single notebook.
 * 01/19/21  Jesse Vig   Support light/dark modes
 * 02/06/21  Jesse Vig   Move require config from separate jupyter notebook step
 * 05/03/21  Jesse Vig   Adjust visualization height dynamically
 * 03/23/22  Daniel SC   Update requirement URLs for d3 and jQuery (source of bug not allowing end result to be displayed on browsers)
 **/

require.config({
  paths: {
      d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',
    jquery: 'https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min',
  }
});

requirejs(['jquery', 'd3'], function($, d3) {

        const params = {"attention": [{"name": null, "attn": [[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.961219847202301, 0.038780126720666885, 0.0, 0.0, 0.0, 0.0], [0.7466979622840881, 0.11987314373254776, 0.1334289163351059, 0.0, 0.0, 0.0], [0.5885028839111328, 0.13792073726654053, 0.21213741600513458, 0.06143900379538536, 0.0, 0.0], [0.6570857167243958, 0.08996305614709854, 0.12751275300979614, 0.08361562341451645, 0.041822828352451324, 0.0], [0.27288734912872314, 0.11203354597091675, 0.16639846563339233, 0.08467115461826324, 0.16952739655971527, 0.19448213279247284]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010616553947329521, 0.9893833994865417, 0.0, 0.0, 0.0, 0.0], [0.0024677517358213663, 0.008448000065982342, 0.9890843033790588, 0.0, 0.0, 0.0], [0.00012328448065090925, 0.001873313682153821, 0.013126963749527931, 0.9848763942718506, 0.0, 0.0], [0.0010669559706002474, 0.001136627048254013, 0.003034998197108507, 0.0015735088381916285, 0.9931879043579102, 0.0], [0.00019792020611930639, 0.0010528132552281022, 0.0015437394613400102, 0.0009642784134484828, 3.492449832265265e-05, 0.9962062835693359]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4757845103740692, 0.5242154598236084, 0.0, 0.0, 0.0, 0.0], [0.5906044840812683, 0.2486610859632492, 0.1607344001531601, 0.0, 0.0, 0.0], [0.552929162979126, 0.18856704235076904, 0.14457565546035767, 0.11392819881439209, 0.0, 0.0], [0.45094069838523865, 0.16486795246601105, 0.17318040132522583, 0.11748011410236359, 0.09353082627058029, 0.0], [0.42572465538978577, 0.17328651249408722, 0.15651951730251312, 0.07022642344236374, 0.0808701142668724, 0.09337279945611954]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6133624315261841, 0.3866375684738159, 0.0, 0.0, 0.0, 0.0], [0.06098507344722748, 0.03253458812832832, 0.9064804315567017, 0.0, 0.0, 0.0], [0.006717091891914606, 0.0004012881254311651, 0.7572961449623108, 0.23558545112609863, 0.0, 0.0], [0.03722784295678139, 0.0029488641303032637, 0.10081139206886292, 0.041422780603170395, 0.8175891637802124, 0.0], [0.04989779368042946, 0.0003075827844440937, 0.002419829135760665, 0.0034334962256252766, 0.0006823891890235245, 0.9432588815689087]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9489555954933167, 0.05104440078139305, 0.0, 0.0, 0.0, 0.0], [0.6821408271789551, 0.1395242065191269, 0.17833493649959564, 0.0, 0.0, 0.0], [0.20366333425045013, 0.05641490966081619, 0.06399299949407578, 0.6759287118911743, 0.0, 0.0], [0.34195491671562195, 0.06725442409515381, 0.07926180213689804, 0.17836210131645203, 0.33316683769226074, 0.0], [0.09464015066623688, 0.007428212556988001, 0.006983965635299683, 0.007184378802776337, 0.018724262714385986, 0.8650389313697815]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33834579586982727, 0.6616541743278503, 0.0, 0.0, 0.0, 0.0], [0.0785597637295723, 0.006165453232824802, 0.9152747988700867, 0.0, 0.0, 0.0], [0.01677590422332287, 0.00040376908145844936, 0.0033404543064534664, 0.9794798493385315, 0.0, 0.0], [0.027600567787885666, 0.0004441539931576699, 0.0006541709881275892, 0.00022661943512503058, 0.9710745215415955, 0.0], [0.01024820376187563, 3.701564128277823e-05, 0.00016064087685663253, 2.7341924578649923e-05, 1.0187323823629413e-05, 0.98951655626297]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.982503354549408, 0.017496641725301743, 0.0, 0.0, 0.0, 0.0], [0.8874198794364929, 0.05467931181192398, 0.05790078267455101, 0.0, 0.0, 0.0], [0.6849911212921143, 0.12280683219432831, 0.04972023144364357, 0.14248180389404297, 0.0, 0.0], [0.6015856266021729, 0.09881891310214996, 0.07070109993219376, 0.16652531921863556, 0.06236904859542847, 0.0], [0.3232504427433014, 0.12567415833473206, 0.04432179406285286, 0.0707697868347168, 0.06606657058000565, 0.3699173331260681]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9191647171974182, 0.0808352455496788, 0.0, 0.0, 0.0, 0.0], [0.45986422896385193, 0.3970310389995575, 0.1431046575307846, 0.0, 0.0, 0.0], [0.30038726329803467, 0.22181735932826996, 0.3816150426864624, 0.09618031233549118, 0.0, 0.0], [0.18963950872421265, 0.13763730227947235, 0.20173467695713043, 0.2363215535879135, 0.23466698825359344, 0.0], [0.15410441160202026, 0.09489507973194122, 0.11902560293674469, 0.10277969390153885, 0.431721955537796, 0.09747327119112015]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3650002181529999, 0.6349997520446777, 0.0, 0.0, 0.0, 0.0], [0.24595215916633606, 0.5519200563430786, 0.20212781429290771, 0.0, 0.0, 0.0], [0.27213600277900696, 0.4073861241340637, 0.25186216831207275, 0.06861580163240433, 0.0, 0.0], [0.10242559760808945, 0.16683609783649445, 0.5248048901557922, 0.05445466190576553, 0.15147875249385834, 0.0], [0.2502950131893158, 0.22198116779327393, 0.1889996975660324, 0.10677123814821243, 0.13032680749893188, 0.10162603110074997]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6990504264831543, 0.3009495735168457, 0.0, 0.0, 0.0, 0.0], [0.5107941031455994, 0.294864296913147, 0.19434157013893127, 0.0, 0.0, 0.0], [0.460470587015152, 0.2805192172527313, 0.19174805283546448, 0.06726215034723282, 0.0, 0.0], [0.37648412585258484, 0.21120671927928925, 0.20214536786079407, 0.1020701676607132, 0.10809355974197388, 0.0], [0.30138444900512695, 0.20456187427043915, 0.1825033575296402, 0.11019378900527954, 0.16291272640228271, 0.03844381496310234]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7131580114364624, 0.2868419587612152, 0.0, 0.0, 0.0, 0.0], [0.405879944562912, 0.18063302338123322, 0.4134870171546936, 0.0, 0.0, 0.0], [0.265546053647995, 0.1698586493730545, 0.33585938811302185, 0.22873595356941223, 0.0, 0.0], [0.31385400891304016, 0.18316704034805298, 0.14928361773490906, 0.05377671867609024, 0.29991868138313293, 0.0], [0.20466554164886475, 0.18731124699115753, 0.15959152579307556, 0.06381776928901672, 0.036423031240701675, 0.34819096326828003]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6586242914199829, 0.3413756787776947, 0.0, 0.0, 0.0, 0.0], [0.5917775630950928, 0.31600356101989746, 0.0922188088297844, 0.0, 0.0, 0.0], [0.5477150678634644, 0.23586955666542053, 0.06145602837204933, 0.1549593061208725, 0.0, 0.0], [0.4587060511112213, 0.22439995408058167, 0.07887425273656845, 0.0992034450173378, 0.13881632685661316, 0.0], [0.32743722200393677, 0.19600819051265717, 0.06805713474750519, 0.0892510786652565, 0.11618088185787201, 0.20306551456451416]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9961552023887634, 0.0038448444101959467, 0.0, 0.0, 0.0, 0.0], [0.8594855070114136, 0.06906120479106903, 0.07145323604345322, 0.0, 0.0, 0.0], [0.3800051808357239, 0.0412757508456707, 0.5496610999107361, 0.029057921841740608, 0.0, 0.0], [0.2144520878791809, 0.05088745057582855, 0.4317440092563629, 0.25869324803352356, 0.044223327189683914, 0.0], [0.11175242811441422, 0.017593061551451683, 0.027507437393069267, 0.040867775678634644, 0.7754672169685364, 0.026812128722667694]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9285966753959656, 0.07140329480171204, 0.0, 0.0, 0.0, 0.0], [0.6077283620834351, 0.3121430575847626, 0.08012863993644714, 0.0, 0.0, 0.0], [0.4942907392978668, 0.285037100315094, 0.11849302053451538, 0.10217911750078201, 0.0, 0.0], [0.41838762164115906, 0.23117919266223907, 0.0834062322974205, 0.11365960538387299, 0.15336737036705017, 0.0], [0.42215877771377563, 0.12917152047157288, 0.08740924298763275, 0.10163769125938416, 0.21230241656303406, 0.0473204180598259]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9786475896835327, 0.021352410316467285, 0.0, 0.0, 0.0, 0.0], [0.7749119400978088, 0.06510379910469055, 0.1599842607975006, 0.0, 0.0, 0.0], [0.6484918594360352, 0.07483147084712982, 0.1475161612033844, 0.12916044890880585, 0.0, 0.0], [0.5224637389183044, 0.06921815127134323, 0.13823407888412476, 0.11106600612401962, 0.15901802480220795, 0.0], [0.396451473236084, 0.07325827330350876, 0.12938156723976135, 0.10642445087432861, 0.14864006638526917, 0.14584414660930634]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5525909662246704, 0.447409063577652, 0.0, 0.0, 0.0, 0.0], [0.5585010051727295, 0.21762587130069733, 0.22387316823005676, 0.0, 0.0, 0.0], [0.5143133401870728, 0.15964657068252563, 0.1549195498228073, 0.17112062871456146, 0.0, 0.0], [0.5039961338043213, 0.11401885002851486, 0.11974025517702103, 0.12552587687969208, 0.13671891391277313, 0.0], [0.5061840415000916, 0.08567393571138382, 0.0890302062034607, 0.09759823232889175, 0.10275731235742569, 0.1187562569975853]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9242545962333679, 0.07574540376663208, 0.0, 0.0, 0.0, 0.0], [0.8257423043251038, 0.07932543754577637, 0.09493227303028107, 0.0, 0.0, 0.0], [0.7306382656097412, 0.08571825176477432, 0.08043920248746872, 0.10320423543453217, 0.0, 0.0], [0.6383239030838013, 0.07886391133069992, 0.07815028727054596, 0.08758103102445602, 0.11708080023527145, 0.0], [0.5552154183387756, 0.07409127801656723, 0.06834893673658371, 0.07778607308864594, 0.09999319911003113, 0.12456496804952621]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8578912019729614, 0.14210878312587738, 0.0, 0.0, 0.0, 0.0], [0.6423032879829407, 0.16629046201705933, 0.19140629470348358, 0.0, 0.0, 0.0], [0.5530969500541687, 0.10609286278486252, 0.07821279019117355, 0.26259738206863403, 0.0, 0.0], [0.40121644735336304, 0.12223614007234573, 0.19347335398197174, 0.14164642989635468, 0.1414276361465454, 0.0], [0.40212586522102356, 0.18450748920440674, 0.07516804337501526, 0.058490585535764694, 0.1444634348154068, 0.13524460792541504]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791560173034668, 0.020844005048274994, 0.0, 0.0, 0.0, 0.0], [0.8829841613769531, 0.062332428991794586, 0.05468343570828438, 0.0, 0.0, 0.0], [0.8105451464653015, 0.08617085963487625, 0.07321798801422119, 0.03006598725914955, 0.0, 0.0], [0.6819810271263123, 0.049908194690942764, 0.08296558260917664, 0.08369531482458115, 0.10144990682601929, 0.0], [0.4056691825389862, 0.07337658852338791, 0.08601400256156921, 0.06170940399169922, 0.1322644203901291, 0.24096642434597015]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9670190811157227, 0.03298091143369675, 0.0, 0.0, 0.0, 0.0], [0.8449065089225769, 0.08514493703842163, 0.06994852423667908, 0.0, 0.0, 0.0], [0.7123570442199707, 0.07896050810813904, 0.05541075021028519, 0.15327171981334686, 0.0, 0.0], [0.6402614116668701, 0.07397546619176865, 0.04439306631684303, 0.1432214379310608, 0.09814866632223129, 0.0], [0.5073904991149902, 0.07523056864738464, 0.07754648476839066, 0.11362502723932266, 0.13947944343090057, 0.08672793954610825]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.848756730556488, 0.15124329924583435, 0.0, 0.0, 0.0, 0.0], [0.8415652513504028, 0.12107212841510773, 0.037362683564424515, 0.0, 0.0, 0.0], [0.7505511045455933, 0.11348968744277954, 0.06179966405034065, 0.07415955513715744, 0.0, 0.0], [0.6614716053009033, 0.10242649912834167, 0.05293430760502815, 0.07529721409082413, 0.10787041485309601, 0.0], [0.6014202237129211, 0.11340387165546417, 0.05631927400827408, 0.07096721231937408, 0.1090628057718277, 0.04882660508155823]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9445481896400452, 0.05545181781053543, 0.0, 0.0, 0.0, 0.0], [0.8874567747116089, 0.054742202162742615, 0.05780098959803581, 0.0, 0.0, 0.0], [0.8281881213188171, 0.06895041465759277, 0.059034936130046844, 0.043826598674058914, 0.0, 0.0], [0.6429886221885681, 0.06747572124004364, 0.11629719287157059, 0.05417964980006218, 0.11905882507562637, 0.0], [0.7367817163467407, 0.056119244545698166, 0.0685729905962944, 0.0342196486890316, 0.07875392585992813, 0.025552459061145782]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002913392090704292, 0.9997085928916931, 0.0, 0.0, 0.0, 0.0], [0.0007981215021573007, 0.5288335084915161, 0.47036832571029663, 0.0, 0.0, 0.0], [0.0007648495957255363, 0.34519800543785095, 0.3085266947746277, 0.3455105125904083, 0.0, 0.0], [0.0010283137671649456, 0.24135896563529968, 0.23320136964321136, 0.2555716633796692, 0.2688397169113159, 0.0], [0.0009746801806613803, 0.17789681255817413, 0.16743150353431702, 0.18587623536586761, 0.18734429776668549, 0.2804764211177826]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8244926333427429, 0.17550736665725708, 0.0, 0.0, 0.0, 0.0], [0.12386886030435562, 0.04450000822544098, 0.8316311240196228, 0.0, 0.0, 0.0], [0.07924386113882065, 0.012965965084731579, 0.001527726068161428, 0.9062625169754028, 0.0, 0.0], [0.08806377649307251, 0.021341020241379738, 0.0028886187355965376, 0.0028453918639570475, 0.8848611116409302, 0.0], [0.09983241558074951, 0.03363398462533951, 0.005499999970197678, 0.002433058572933078, 0.001508240820840001, 0.8570923805236816]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9646893739700317, 0.03531067073345184, 0.0, 0.0, 0.0, 0.0], [0.752915620803833, 0.0873347818851471, 0.1597496122121811, 0.0, 0.0, 0.0], [0.42022907733917236, 0.09195100516080856, 0.23549817502498627, 0.252321720123291, 0.0, 0.0], [0.3084893524646759, 0.05908145010471344, 0.38391321897506714, 0.15659162402153015, 0.09192444384098053, 0.0], [0.44790348410606384, 0.04329321160912514, 0.07969199120998383, 0.11081957072019577, 0.2212458997964859, 0.09704583883285522]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.991096019744873, 0.00890398770570755, 0.0, 0.0, 0.0, 0.0], [0.9697675704956055, 0.026084454730153084, 0.0041479249484837055, 0.0, 0.0, 0.0], [0.9082900285720825, 0.033206015825271606, 0.009421179071068764, 0.04908282309770584, 0.0, 0.0], [0.8949136137962341, 0.055445361882448196, 0.005577626638114452, 0.03150685876607895, 0.012556513771414757, 0.0], [0.8497741222381592, 0.028890041634440422, 0.0036647941451519728, 0.0375199019908905, 0.03842775151133537, 0.041723500937223434]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9984525442123413, 0.0015474447282031178, 0.0, 0.0, 0.0, 0.0], [0.48947733640670776, 0.4812204837799072, 0.029302237555384636, 0.0, 0.0, 0.0], [0.1177220344543457, 0.131212055683136, 0.670230507850647, 0.08083536475896835, 0.0, 0.0], [0.13043680787086487, 0.040686700493097305, 0.2652038633823395, 0.41143620014190674, 0.15223641693592072, 0.0], [0.1266186386346817, 0.032751280814409256, 0.035678680986166, 0.06039194017648697, 0.6021823287010193, 0.14237718284130096]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9805176854133606, 0.019482361152768135, 0.0, 0.0, 0.0, 0.0], [0.7948845028877258, 0.12061940878629684, 0.08449607342481613, 0.0, 0.0, 0.0], [0.5612362623214722, 0.15743127465248108, 0.20339690148830414, 0.07793557643890381, 0.0, 0.0], [0.42583802342414856, 0.107420414686203, 0.15123680233955383, 0.08755015581846237, 0.2279546856880188, 0.0], [0.247526153922081, 0.024188315495848656, 0.030395271256566048, 0.0858696773648262, 0.5714337229728699, 0.04058678075671196]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887767434120178, 0.011223193258047104, 0.0, 0.0, 0.0, 0.0], [0.7572689056396484, 0.22317391633987427, 0.01955711469054222, 0.0, 0.0, 0.0], [0.534188985824585, 0.22107531130313873, 0.17621807754039764, 0.06851760298013687, 0.0, 0.0], [0.1709531843662262, 0.08229412138462067, 0.5760221481323242, 0.11097586899995804, 0.05975470319390297, 0.0], [0.24871155619621277, 0.0888080894947052, 0.0898018553853035, 0.09729322791099548, 0.4413086771965027, 0.03407656028866768]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8422133326530457, 0.15778674185276031, 0.0, 0.0, 0.0, 0.0], [0.4684129059314728, 0.46105366945266724, 0.07053348422050476, 0.0, 0.0, 0.0], [0.25881505012512207, 0.4635883867740631, 0.18503470718860626, 0.09256187081336975, 0.0, 0.0], [0.1839958131313324, 0.2915419340133667, 0.17031101882457733, 0.27172985672950745, 0.08242136985063553, 0.0], [0.16469866037368774, 0.24726979434490204, 0.08770564198493958, 0.2257501780986786, 0.17745359241962433, 0.0971221849322319]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9919946193695068, 0.008005390875041485, 0.0, 0.0, 0.0, 0.0], [0.9068724513053894, 0.04406515136361122, 0.04906249791383743, 0.0, 0.0, 0.0], [0.8582221865653992, 0.055348340421915054, 0.04041949659585953, 0.04600999131798744, 0.0, 0.0], [0.7855252027511597, 0.04124243184924126, 0.08369303494691849, 0.04887612164020538, 0.04066324606537819, 0.0], [0.7856316566467285, 0.050146445631980896, 0.04751276969909668, 0.027365924790501595, 0.0561474971473217, 0.03319568186998367]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9041039347648621, 0.09589607268571854, 0.0, 0.0, 0.0, 0.0], [0.5862313508987427, 0.07199801504611969, 0.34177055954933167, 0.0, 0.0, 0.0], [0.38789603114128113, 0.04660792276263237, 0.20279011130332947, 0.36270591616630554, 0.0, 0.0], [0.266524076461792, 0.024533001706004143, 0.12211966514587402, 0.20041236281394958, 0.3864108622074127, 0.0], [0.2335742861032486, 0.02053731307387352, 0.09610352665185928, 0.13062262535095215, 0.22990448772907257, 0.2892577648162842]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639912843704224, 0.03600866720080376, 0.0, 0.0, 0.0, 0.0], [0.7075554132461548, 0.25427746772766113, 0.03816713020205498, 0.0, 0.0, 0.0], [0.2566538155078888, 0.20589275658130646, 0.01665661484003067, 0.5207968354225159, 0.0, 0.0], [0.10379398614168167, 0.046390973031520844, 0.008698644116520882, 0.7866848707199097, 0.054431553930044174, 0.0], [0.22143355011940002, 0.033797506242990494, 0.02902393788099289, 0.5412923097610474, 0.15286099910736084, 0.021591614931821823]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9891703724861145, 0.010829598642885685, 0.0, 0.0, 0.0, 0.0], [0.7913159728050232, 0.1230960264801979, 0.0855880156159401, 0.0, 0.0, 0.0], [0.29546085000038147, 0.15808266401290894, 0.4217235743999481, 0.12473298609256744, 0.0, 0.0], [0.23441052436828613, 0.09886527806520462, 0.33160147070884705, 0.1971394270658493, 0.1379832923412323, 0.0], [0.19728443026542664, 0.057418256998062134, 0.06909003108739853, 0.16469796001911163, 0.2797275185585022, 0.2317817509174347]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9359127283096313, 0.06408724933862686, 0.0, 0.0, 0.0, 0.0], [0.7888627052307129, 0.086734838783741, 0.1244024783372879, 0.0, 0.0, 0.0], [0.6535118818283081, 0.07573550194501877, 0.09732561558485031, 0.17342697083950043, 0.0, 0.0], [0.5222765803337097, 0.058278873562812805, 0.09920476377010345, 0.17020854353904724, 0.15003123879432678, 0.0], [0.4108838140964508, 0.04730610176920891, 0.07265669107437134, 0.10560758411884308, 0.10550003498792648, 0.2580457329750061]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9683833122253418, 0.03161666914820671, 0.0, 0.0, 0.0, 0.0], [0.8965396881103516, 0.03887055069208145, 0.06458982080221176, 0.0, 0.0, 0.0], [0.8264954090118408, 0.03213459998369217, 0.051967158913612366, 0.08940287679433823, 0.0, 0.0], [0.771817147731781, 0.030402876436710358, 0.04582748934626579, 0.07118481397628784, 0.08076764643192291, 0.0], [0.7292332053184509, 0.021699808537960052, 0.033074751496315, 0.047200970351696014, 0.06474553793668747, 0.10404572635889053]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9979567527770996, 0.0020432781893759966, 0.0, 0.0, 0.0, 0.0], [0.9552940726280212, 0.00802531000226736, 0.03668056055903435, 0.0, 0.0, 0.0], [0.925471305847168, 0.0027555758133530617, 0.00206298497505486, 0.0697101429104805, 0.0, 0.0], [0.8660571575164795, 0.0038883730303496122, 0.000678599055390805, 0.0006981441983953118, 0.12867768108844757, 0.0], [0.8455925583839417, 0.0037804089952260256, 0.00025342273875139654, 6.027055496815592e-05, 0.0001182075502583757, 0.1501951515674591]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9262459874153137, 0.07375400513410568, 0.0, 0.0, 0.0, 0.0], [0.7717162370681763, 0.16241960227489471, 0.06586415320634842, 0.0, 0.0, 0.0], [0.8167640566825867, 0.0780717059969902, 0.06324010342359543, 0.041924092918634415, 0.0, 0.0], [0.6867192983627319, 0.0775517150759697, 0.10056842118501663, 0.059550508856773376, 0.07561004161834717, 0.0], [0.642117440700531, 0.11014864593744278, 0.0768813043832779, 0.054033227264881134, 0.10333622992038727, 0.01348315179347992]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9395954012870789, 0.06040458753705025, 0.0, 0.0, 0.0, 0.0], [0.23004719614982605, 0.6617377400398254, 0.10821503400802612, 0.0, 0.0, 0.0], [0.26702091097831726, 0.3607969880104065, 0.3249621093273163, 0.04721994698047638, 0.0, 0.0], [0.595203697681427, 0.12269200384616852, 0.06301991641521454, 0.08916746824979782, 0.1299169361591339, 0.0], [0.10284474492073059, 0.029379907995462418, 0.013739032670855522, 0.04586051404476166, 0.769851803779602, 0.03832397609949112]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.904097855091095, 0.09590208530426025, 0.0, 0.0, 0.0, 0.0], [0.3572397530078888, 0.6274594664573669, 0.015300781466066837, 0.0, 0.0, 0.0], [0.5918020606040955, 0.27640295028686523, 0.10475941002368927, 0.02703559212386608, 0.0, 0.0], [0.7254431843757629, 0.049830954521894455, 0.01498264167457819, 0.17781217396259308, 0.03193102404475212, 0.0], [0.7612760663032532, 0.06158866360783577, 0.005942120216786861, 0.016426634043455124, 0.126780703663826, 0.027985917404294014]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9947587847709656, 0.005241225007921457, 0.0, 0.0, 0.0, 0.0], [0.963241696357727, 0.017816387116909027, 0.018941964954137802, 0.0, 0.0, 0.0], [0.9671077132225037, 0.00850958563387394, 0.008562212809920311, 0.015820462256669998, 0.0, 0.0], [0.9340999722480774, 0.01195237971842289, 0.0201801136136055, 0.026750775054097176, 0.007016872521489859, 0.0], [0.9587239027023315, 0.004657103214412928, 0.0033267680555582047, 0.006545295938849449, 0.010182415135204792, 0.0165645033121109]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9769991040229797, 0.023000944405794144, 0.0, 0.0, 0.0, 0.0], [0.7917606830596924, 0.17533230781555176, 0.03290707990527153, 0.0, 0.0, 0.0], [0.7949199080467224, 0.1053181067109108, 0.040218379348516464, 0.059543587267398834, 0.0, 0.0], [0.7097731232643127, 0.10552491247653961, 0.0659753829240799, 0.057655807584524155, 0.061070751398801804, 0.0], [0.7506603002548218, 0.026514416560530663, 0.02157599851489067, 0.03429661691188812, 0.08494441956281662, 0.08200817555189133]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9837515950202942, 0.016248442232608795, 0.0, 0.0, 0.0, 0.0], [0.5615504384040833, 0.08956819772720337, 0.3488813042640686, 0.0, 0.0, 0.0], [0.3292921185493469, 0.02411479502916336, 0.5428044199943542, 0.10378871113061905, 0.0, 0.0], [0.3433031141757965, 0.013086349703371525, 0.5121973156929016, 0.11146238446235657, 0.019950827583670616, 0.0], [0.47928208112716675, 0.017333462834358215, 0.11805327236652374, 0.061302702873945236, 0.20071855187416077, 0.12330997735261917]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9908847212791443, 0.009115338325500488, 0.0, 0.0, 0.0, 0.0], [0.5282697081565857, 0.329226553440094, 0.14250372350215912, 0.0, 0.0, 0.0], [0.4878896176815033, 0.23368458449840546, 0.17577968537807465, 0.10264609754085541, 0.0, 0.0], [0.314449667930603, 0.180649995803833, 0.16871333122253418, 0.09506484866142273, 0.24112209677696228, 0.0], [0.516875147819519, 0.03589686378836632, 0.02618817426264286, 0.04039689525961876, 0.18791928887367249, 0.19272355735301971]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8750306963920593, 0.12496932595968246, 0.0, 0.0, 0.0, 0.0], [0.45506006479263306, 0.49004384875297546, 0.054896049201488495, 0.0, 0.0, 0.0], [0.29337164759635925, 0.5449909567832947, 0.09444276243448257, 0.0671946257352829, 0.0, 0.0], [0.4897095561027527, 0.27209898829460144, 0.068619504570961, 0.14694790542125702, 0.022624004632234573, 0.0], [0.4729057848453522, 0.08103073388338089, 0.016052119433879852, 0.3067234754562378, 0.10120765119791031, 0.022080309689044952]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9630221724510193, 0.03697776421904564, 0.0, 0.0, 0.0, 0.0], [0.7557196021080017, 0.1643640547990799, 0.07991636544466019, 0.0, 0.0, 0.0], [0.694770872592926, 0.08409861475229263, 0.06382570415735245, 0.1573048233985901, 0.0, 0.0], [0.5821168422698975, 0.03297801688313484, 0.07936544716358185, 0.19441282749176025, 0.11112681776285172, 0.0], [0.597453236579895, 0.042611002922058105, 0.06919699907302856, 0.14563480019569397, 0.12481795251369476, 0.020285995677113533]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9957822561264038, 0.004217796493321657, 0.0, 0.0, 0.0, 0.0], [0.931283712387085, 0.010560186579823494, 0.05815601721405983, 0.0, 0.0, 0.0], [0.8435329794883728, 0.015694962814450264, 0.04575089365243912, 0.09502116590738297, 0.0, 0.0], [0.7724104523658752, 0.011981171555817127, 0.03504578396677971, 0.038767702877521515, 0.14179477095603943, 0.0], [0.7642908096313477, 0.009868745692074299, 0.008122707717120647, 0.013314425013959408, 0.04824399575591087, 0.1561594009399414]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9701175093650818, 0.029882533475756645, 0.0, 0.0, 0.0, 0.0], [0.6564013957977295, 0.2250615507364273, 0.11853699386119843, 0.0, 0.0, 0.0], [0.6958046555519104, 0.1470194011926651, 0.07146008312702179, 0.0857158824801445, 0.0, 0.0], [0.6353287100791931, 0.13460613787174225, 0.03099406138062477, 0.05691603198647499, 0.14215515553951263, 0.0], [0.6779382824897766, 0.05365435779094696, 0.01800640858709812, 0.0628453940153122, 0.11038312315940857, 0.07717251032590866]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9822334051132202, 0.017766587436199188, 0.0, 0.0, 0.0, 0.0], [0.9037660956382751, 0.06541557610034943, 0.03081827238202095, 0.0, 0.0, 0.0], [0.8119180202484131, 0.03679049760103226, 0.06056082993745804, 0.09073063731193542, 0.0, 0.0], [0.40546438097953796, 0.10383863002061844, 0.10211163759231567, 0.3543432950973511, 0.034242067486047745, 0.0], [0.2282433658838272, 0.01727871783077717, 0.05055440962314606, 0.6015747785568237, 0.09411729127168655, 0.008231495507061481]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873148202896118, 0.012685106135904789, 0.0, 0.0, 0.0, 0.0], [0.3544553816318512, 0.5317620635032654, 0.11378253996372223, 0.0, 0.0, 0.0], [0.07823341339826584, 0.7221351861953735, 0.10936630517244339, 0.09026506543159485, 0.0, 0.0], [0.2196795493364334, 0.4048422873020172, 0.12358050793409348, 0.2001895010471344, 0.0517081692814827, 0.0], [0.3608974516391754, 0.10458994656801224, 0.06983785331249237, 0.2976486384868622, 0.1386992484331131, 0.028326794505119324]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9732162356376648, 0.02678375132381916, 0.0, 0.0, 0.0, 0.0], [0.9167549014091492, 0.06145293265581131, 0.021792078390717506, 0.0, 0.0, 0.0], [0.8543080687522888, 0.08049597591161728, 0.030335018411278725, 0.03486090898513794, 0.0, 0.0], [0.8919216394424438, 0.04280775412917137, 0.022045090794563293, 0.023470621556043625, 0.01975489780306816, 0.0], [0.8116760849952698, 0.0341353714466095, 0.03567682206630707, 0.04748590663075447, 0.02539718709886074, 0.045628681778907776]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502761363983154, 0.049723926931619644, 0.0, 0.0, 0.0, 0.0], [0.7637450695037842, 0.20073653757572174, 0.035518333315849304, 0.0, 0.0, 0.0], [0.6279101967811584, 0.0376814566552639, 0.19945232570171356, 0.13495612144470215, 0.0, 0.0], [0.6397072672843933, 0.027007289230823517, 0.09081897139549255, 0.20653867721557617, 0.035927820950746536, 0.0], [0.4559415578842163, 0.021641148254275322, 0.12939520180225372, 0.21801024675369263, 0.10379895567893982, 0.07121287286281586]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.949840784072876, 0.05015924200415611, 0.0, 0.0, 0.0, 0.0], [0.8688720464706421, 0.08722209185361862, 0.04390587657690048, 0.0, 0.0, 0.0], [0.6937950253486633, 0.06359221041202545, 0.09179061651229858, 0.15082214772701263, 0.0, 0.0], [0.726659893989563, 0.04389888793230057, 0.04683985933661461, 0.09851793199777603, 0.08408348262310028, 0.0], [0.7848988175392151, 0.037147946655750275, 0.012907883152365685, 0.010539398528635502, 0.12079229950904846, 0.03371357545256615]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9891054034233093, 0.01089461799710989, 0.0, 0.0, 0.0, 0.0], [0.8929510116577148, 0.087001271545887, 0.020047681406140327, 0.0, 0.0, 0.0], [0.7891110777854919, 0.09797301143407822, 0.0863330066204071, 0.026582954451441765, 0.0, 0.0], [0.8850634098052979, 0.03645016998052597, 0.05395463854074478, 0.012377261184155941, 0.012154542841017246, 0.0], [0.6861327290534973, 0.05720385164022446, 0.011636361479759216, 0.021660521626472473, 0.17488014698028564, 0.048486415296792984]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9396193623542786, 0.060380659997463226, 0.0, 0.0, 0.0, 0.0], [0.7851788997650146, 0.19751502573490143, 0.017306093126535416, 0.0, 0.0, 0.0], [0.7660511136054993, 0.15444667637348175, 0.03188285231590271, 0.04761938750743866, 0.0, 0.0], [0.7035228610038757, 0.05171423405408859, 0.07760986685752869, 0.15339063107967377, 0.013762437738478184, 0.0], [0.7121887803077698, 0.049942418932914734, 0.037725359201431274, 0.0864911898970604, 0.06541414558887482, 0.04823815077543259]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9740723967552185, 0.025927582755684853, 0.0, 0.0, 0.0, 0.0], [0.7925393581390381, 0.01171562448143959, 0.19574503600597382, 0.0, 0.0, 0.0], [0.5106777548789978, 0.007296778727322817, 0.03962007910013199, 0.4424054026603699, 0.0, 0.0], [0.5862482190132141, 0.012099675834178925, 0.024585217237472534, 0.06737817078828812, 0.3096887171268463, 0.0], [0.3019630014896393, 0.007723969407379627, 0.011518090032041073, 0.04694710671901703, 0.22146669030189514, 0.4103810787200928]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9744554758071899, 0.025544486939907074, 0.0, 0.0, 0.0, 0.0], [0.9769196510314941, 0.015048469416797161, 0.00803192239254713, 0.0, 0.0, 0.0], [0.9060620665550232, 0.025875359773635864, 0.025954803451895714, 0.04210779070854187, 0.0, 0.0], [0.9400083422660828, 0.005556613672524691, 0.005828283727169037, 0.031757839024066925, 0.01684902422130108, 0.0], [0.9105741381645203, 0.001975212013348937, 0.008646724745631218, 0.013360838405787945, 0.03543954715132713, 0.030003532767295837]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791666865348816, 0.020833317190408707, 0.0, 0.0, 0.0, 0.0], [0.8444864153862, 0.13507819175720215, 0.020435353741049767, 0.0, 0.0, 0.0], [0.79030841588974, 0.1455916166305542, 0.037530187517404556, 0.026569800451397896, 0.0, 0.0], [0.729892373085022, 0.05649634823203087, 0.032735567539930344, 0.10400468111038208, 0.07687091082334518, 0.0], [0.568419337272644, 0.043888114392757416, 0.026293445378541946, 0.08117109537124634, 0.24314820766448975, 0.03707980737090111]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9499869346618652, 0.05001309886574745, 0.0, 0.0, 0.0, 0.0], [0.9336172342300415, 0.058488547801971436, 0.007894248701632023, 0.0, 0.0, 0.0], [0.789783775806427, 0.11071790754795074, 0.0536017119884491, 0.04589656740427017, 0.0, 0.0], [0.8859301805496216, 0.057529836893081665, 0.013743293471634388, 0.003387755248695612, 0.03940898925065994, 0.0], [0.9337608814239502, 0.026470575481653214, 0.004523397423326969, 0.006190492305904627, 0.014132915996015072, 0.014921690337359905]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.8521365552359725e-09, 0.0, 0.0, 0.0, 0.0], [6.676049906673143e-06, 0.9999804496765137, 1.2841427633247804e-05, 0.0, 0.0, 0.0], [2.2194576843048708e-08, 2.668524290427854e-09, 0.9999971389770508, 2.8137685603724094e-06, 0.0, 0.0], [1.014530539578118e-06, 4.4639637764021245e-08, 0.00035354969440959394, 0.9993677735328674, 0.00027762516401708126, 0.0], [9.436316172894976e-10, 1.3820019781340953e-11, 5.017615212210558e-10, 2.96517832509835e-09, 0.9999971389770508, 2.864343969122274e-06]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9948632121086121, 0.005136759020388126, 0.0, 0.0, 0.0, 0.0], [0.9274215698242188, 0.018323808908462524, 0.05425465852022171, 0.0, 0.0, 0.0], [0.9678992033004761, 0.004143428988754749, 0.004314454738050699, 0.023642895743250847, 0.0, 0.0], [0.899907648563385, 0.0014671545941382647, 0.0002913349017035216, 0.0025850010570138693, 0.09574876725673676, 0.0], [0.9386117458343506, 0.00022248072491493076, 0.0006146605010144413, 0.001549550681374967, 0.030689243227243423, 0.02831234224140644]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9999959468841553, 4.042674390802858e-06, 0.0, 0.0, 0.0, 0.0], [0.9982761144638062, 3.2613336315989727e-06, 0.0017206775955855846, 0.0, 0.0, 0.0], [0.9998809099197388, 5.328825736228282e-08, 6.376306487254624e-07, 0.0001184718421427533, 0.0, 0.0], [0.9996154308319092, 3.47315932458514e-07, 3.892111877235038e-08, 4.468437566629291e-07, 0.0003836959949694574, 0.0], [0.9994840621948242, 1.6550142589721872e-08, 2.871566806561532e-08, 1.0638365210979828e-06, 0.00021266820840537548, 0.00030212124693207443]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9514135122299194, 0.04858648031949997, 0.0, 0.0, 0.0, 0.0], [0.574993908405304, 0.3902817368507385, 0.03472433611750603, 0.0, 0.0, 0.0], [0.7442327737808228, 0.1752404421567917, 0.07564744353294373, 0.004879289772361517, 0.0, 0.0], [0.5232067108154297, 0.09429281949996948, 0.11381927132606506, 0.19979366660118103, 0.06888758391141891, 0.0], [0.4747253656387329, 0.056365687400102615, 0.04530392214655876, 0.06967338174581528, 0.30980202555656433, 0.04412958398461342]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8734649419784546, 0.1265350729227066, 0.0, 0.0, 0.0, 0.0], [0.6097922921180725, 0.35417166352272034, 0.036035966128110886, 0.0, 0.0, 0.0], [0.45984140038490295, 0.38697922229766846, 0.09960115700960159, 0.05357823148369789, 0.0, 0.0], [0.572220504283905, 0.23636272549629211, 0.08344549685716629, 0.06921910494565964, 0.038752175867557526, 0.0], [0.514357328414917, 0.16723060607910156, 0.09019400179386139, 0.07654452323913574, 0.10578082501888275, 0.04589270055294037]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.981228768825531, 0.018771247938275337, 0.0, 0.0, 0.0, 0.0], [0.6142950057983398, 0.35039687156677246, 0.0353081114590168, 0.0, 0.0, 0.0], [0.5770686268806458, 0.3285844624042511, 0.05508274585008621, 0.03926417976617813, 0.0, 0.0], [0.17188261449337006, 0.011042488738894463, 0.054578930139541626, 0.7326573133468628, 0.029838675633072853, 0.0], [0.37830254435539246, 0.017070047557353973, 0.021754169836640358, 0.44096678495407104, 0.06093871220946312, 0.08096770942211151]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9923112988471985, 0.0076887100003659725, 0.0, 0.0, 0.0, 0.0], [0.9498788118362427, 0.01670972816646099, 0.03341144695878029, 0.0, 0.0, 0.0], [0.9961296319961548, 0.0008787262486293912, 0.0006868175696581602, 0.0023048429284244776, 0.0, 0.0], [0.9935756325721741, 0.0032634977251291275, 0.000999384792521596, 0.0002793250896502286, 0.0018820626428350806, 0.0], [0.9907532930374146, 0.00021344258857425302, 0.0004595253267325461, 0.0007905654492788017, 0.00442475313320756, 0.0033583587501198053]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9647741317749023, 0.03522586449980736, 0.0, 0.0, 0.0, 0.0], [0.8194136023521423, 0.13654311001300812, 0.04404330253601074, 0.0, 0.0, 0.0], [0.7584235668182373, 0.006878924090415239, 0.20653478801250458, 0.028162678703665733, 0.0, 0.0], [0.5298117995262146, 0.002678808057680726, 0.07857982069253922, 0.35983842611312866, 0.029091155156493187, 0.0], [0.754441499710083, 0.00036781918606720865, 0.0019713419023901224, 0.0032400439959019423, 0.19423416256904602, 0.045745160430669785]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9749132394790649, 0.02508677914738655, 0.0, 0.0, 0.0, 0.0], [0.9306473135948181, 0.05705645680427551, 0.012296212837100029, 0.0, 0.0, 0.0], [0.9305253624916077, 0.05277096852660179, 0.011119387112557888, 0.005584376864135265, 0.0, 0.0], [0.8863321542739868, 0.012924176640808582, 0.01772472821176052, 0.06150190532207489, 0.021517064422369003, 0.0], [0.7916851043701172, 0.015036048367619514, 0.031747814267873764, 0.03392194211483002, 0.03707972168922424, 0.09052935987710953]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9608504772186279, 0.03914956748485565, 0.0, 0.0, 0.0, 0.0], [0.9121270179748535, 0.0225764662027359, 0.06529650092124939, 0.0, 0.0, 0.0], [0.9364106059074402, 0.015584406442940235, 0.024545013904571533, 0.02345995232462883, 0.0, 0.0], [0.9454619288444519, 0.006762223318219185, 0.022026218473911285, 0.00913779903203249, 0.016611794009804726, 0.0], [0.8346170783042908, 0.0018816773081198335, 0.005609007552266121, 0.018873529508709908, 0.12449172884225845, 0.01452699489891529]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9964227080345154, 0.003577274037525058, 0.0, 0.0, 0.0, 0.0], [0.9713929891586304, 0.02445298247039318, 0.004154051188379526, 0.0, 0.0, 0.0], [0.9735793471336365, 0.019003307446837425, 0.0036644041538238525, 0.003752927528694272, 0.0, 0.0], [0.9586312174797058, 0.007116167340427637, 0.009218405932188034, 0.02272568643093109, 0.0023084785789251328, 0.0], [0.973607063293457, 0.008490562438964844, 0.003251239424571395, 0.003606440033763647, 0.0048774476163089275, 0.006167203187942505]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9759878516197205, 0.02401207946240902, 0.0, 0.0, 0.0, 0.0], [0.9460638165473938, 0.04211374744772911, 0.011822420172393322, 0.0, 0.0, 0.0], [0.8446815609931946, 0.042930979281663895, 0.052181802690029144, 0.06020564213395119, 0.0, 0.0], [0.9378370046615601, 0.03354860842227936, 0.008826478384435177, 0.0028792452067136765, 0.016908586025238037, 0.0], [0.8124936819076538, 0.0269674900919199, 0.05999195575714111, 0.03445734083652496, 0.011011873371899128, 0.05507773160934448]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9001209735870361, 0.09987906366586685, 0.0, 0.0, 0.0, 0.0], [0.6271924376487732, 0.07988706976175308, 0.29292047023773193, 0.0, 0.0, 0.0], [0.7624086141586304, 0.02734416350722313, 0.038679495453834534, 0.17156772315502167, 0.0, 0.0], [0.7995964288711548, 0.014336240477859974, 0.01437566988170147, 0.025438444688916206, 0.14625321328639984, 0.0], [0.7851970791816711, 0.04204041510820389, 0.0252536628395319, 0.029083900153636932, 0.02930637076497078, 0.08911861479282379]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954467415809631, 0.004553188104182482, 0.0, 0.0, 0.0, 0.0], [0.9356000423431396, 0.044767431914806366, 0.019632453098893166, 0.0, 0.0, 0.0], [0.5605552196502686, 0.09861954301595688, 0.29983270168304443, 0.04099256917834282, 0.0, 0.0], [0.5893716812133789, 0.11000912636518478, 0.08033595234155655, 0.16754071414470673, 0.052742499858140945, 0.0], [0.22306011617183685, 0.05680781602859497, 0.05467957630753517, 0.24734029173851013, 0.31112343072891235, 0.10698885470628738]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9301450252532959, 0.0698549896478653, 0.0, 0.0, 0.0, 0.0], [0.8936473727226257, 0.08535744249820709, 0.02099512703716755, 0.0, 0.0, 0.0], [0.8404536247253418, 0.10619217902421951, 0.023636844009160995, 0.029717376455664635, 0.0, 0.0], [0.8927386403083801, 0.024784628301858902, 0.008319017477333546, 0.05165451765060425, 0.022503133863210678, 0.0], [0.8646613359451294, 0.009503128007054329, 0.0024329759180545807, 0.04796736314892769, 0.04273195192217827, 0.032703179866075516]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9859625697135925, 0.014037422835826874, 0.0, 0.0, 0.0, 0.0], [0.9702038764953613, 0.01680702343583107, 0.012989108450710773, 0.0, 0.0, 0.0], [0.9524770379066467, 0.01606440730392933, 0.013456220738589764, 0.018002288416028023, 0.0, 0.0], [0.9332927465438843, 0.018971994519233704, 0.020146913826465607, 0.017023799940943718, 0.010564584285020828, 0.0], [0.9113592505455017, 0.012528584338724613, 0.02209623157978058, 0.017518600448966026, 0.018517879769206047, 0.017979370430111885]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9681767821311951, 0.031823206692934036, 0.0, 0.0, 0.0, 0.0], [0.909641683101654, 0.07916704565286636, 0.011191285215318203, 0.0, 0.0, 0.0], [0.8379938006401062, 0.13078218698501587, 0.012140980921685696, 0.0190830510109663, 0.0, 0.0], [0.9116524457931519, 0.054519571363925934, 0.009499364532530308, 0.0074658701196312904, 0.016862764954566956, 0.0], [0.8510292172431946, 0.07338184863328934, 0.008022483438253403, 0.00908315647393465, 0.04261001572012901, 0.015873229131102562]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9799023866653442, 0.02009764313697815, 0.0, 0.0, 0.0, 0.0], [0.9558743238449097, 0.029063154011964798, 0.015062526799738407, 0.0, 0.0, 0.0], [0.7943134903907776, 0.060740746557712555, 0.06907663494348526, 0.07586921006441116, 0.0, 0.0], [0.5494319796562195, 0.03154705464839935, 0.05482025071978569, 0.0578809417784214, 0.30631980299949646, 0.0], [0.6453986763954163, 0.010770875960588455, 0.017528092488646507, 0.021579833701252937, 0.249582439661026, 0.05514010041952133]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9506809115409851, 0.0493190623819828, 0.0, 0.0, 0.0, 0.0], [0.8553215265274048, 0.09256266057491302, 0.05211571976542473, 0.0, 0.0, 0.0], [0.8508530855178833, 0.04734566807746887, 0.04417697712779045, 0.05762428790330887, 0.0, 0.0], [0.769712507724762, 0.027885742485523224, 0.031017223373055458, 0.06842542439699173, 0.10295906662940979, 0.0], [0.7931907773017883, 0.04052164778113365, 0.029241811484098434, 0.04478137567639351, 0.048946961760520935, 0.04331738129258156]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9770310521125793, 0.022968975827097893, 0.0, 0.0, 0.0, 0.0], [0.9429819583892822, 0.017321424558758736, 0.039696723222732544, 0.0, 0.0, 0.0], [0.9144344925880432, 0.008583568967878819, 0.013035798445343971, 0.06394603103399277, 0.0, 0.0], [0.9222429990768433, 0.0036440384574234486, 0.0037402792368084192, 0.010410374030470848, 0.059962350875139236, 0.0], [0.9198879599571228, 0.0030822583939880133, 0.0034827394410967827, 0.004206802695989609, 0.021254297345876694, 0.048085931688547134]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9774582386016846, 0.022541804239153862, 0.0, 0.0, 0.0, 0.0], [0.8929325342178345, 0.07475457340478897, 0.03231294825673103, 0.0, 0.0, 0.0], [0.8423509001731873, 0.059802666306495667, 0.03740093484520912, 0.060445431619882584, 0.0, 0.0], [0.7674630284309387, 0.035363294184207916, 0.04215523228049278, 0.06658652424812317, 0.08843188732862473, 0.0], [0.6182613968849182, 0.01611054502427578, 0.020167633891105652, 0.038688939064741135, 0.23146985471248627, 0.07530158758163452]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9634859561920166, 0.036514006555080414, 0.0, 0.0, 0.0, 0.0], [0.4363924264907837, 0.5226381421089172, 0.0409693717956543, 0.0, 0.0, 0.0], [0.36086317896842957, 0.35129547119140625, 0.26551011204719543, 0.02233118936419487, 0.0, 0.0], [0.3942912817001343, 0.021704476326704025, 0.07794322818517685, 0.3716898262500763, 0.13437117636203766, 0.0], [0.6310707926750183, 0.01698385737836361, 0.025941932573914528, 0.08615948259830475, 0.21832098066806793, 0.021522969007492065]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9988250136375427, 0.0011750120902433991, 0.0, 0.0, 0.0, 0.0], [0.9944871068000793, 0.0004826366784982383, 0.00503023574128747, 0.0, 0.0, 0.0], [0.9981209635734558, 2.7051681172451936e-05, 0.00011307386739645153, 0.0017389492131769657, 0.0, 0.0], [0.9982240796089172, 6.836546526756138e-05, 0.00010199946700595319, 6.0283709899522364e-05, 0.0015453151427209377, 0.0], [0.9982888102531433, 1.0552126923357719e-06, 3.278084477642551e-05, 0.0001303898898186162, 0.0006605913513340056, 0.000886370544321835]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9936710596084595, 0.006328925024718046, 0.0, 0.0, 0.0, 0.0], [0.9727686047554016, 0.0018561407923698425, 0.02537521719932556, 0.0, 0.0, 0.0], [0.9724299907684326, 0.0019586130511015654, 0.011192455887794495, 0.01441890373826027, 0.0, 0.0], [0.9782041311264038, 0.0009589171386323869, 0.0018706527771428227, 0.006326586939394474, 0.012639714404940605, 0.0], [0.9592596888542175, 0.0024555076379328966, 0.0016124167013913393, 0.005019662901759148, 0.006687106564640999, 0.024965673685073853]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629000425338745, 0.03710000589489937, 0.0, 0.0, 0.0, 0.0], [0.368019700050354, 0.6152254939079285, 0.016754835844039917, 0.0, 0.0, 0.0], [0.3173525333404541, 0.6139997839927673, 0.05375159531831741, 0.014896123670041561, 0.0, 0.0], [0.48987340927124023, 0.2107142210006714, 0.04693015292286873, 0.20700448751449585, 0.04547775909304619, 0.0], [0.48774290084838867, 0.1769522726535797, 0.06915199756622314, 0.09849286824464798, 0.12091417610645294, 0.046745769679546356]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9794418811798096, 0.020558105781674385, 0.0, 0.0, 0.0, 0.0], [0.6677899956703186, 0.3103238642215729, 0.021886125206947327, 0.0, 0.0, 0.0], [0.7118754982948303, 0.11108537018299103, 0.14187413454055786, 0.035164978355169296, 0.0, 0.0], [0.4501444399356842, 0.04036037623882294, 0.04045833647251129, 0.38857150077819824, 0.08046536147594452, 0.0], [0.49346134066581726, 0.013696889393031597, 0.008126810193061829, 0.1307455599308014, 0.30861467123031616, 0.045354776084423065]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846054315567017, 0.015394587069749832, 0.0, 0.0, 0.0, 0.0], [0.9806739091873169, 0.007713802624493837, 0.011612347327172756, 0.0, 0.0, 0.0], [0.9326629042625427, 0.019578414037823677, 0.024103600531816483, 0.02365502528846264, 0.0, 0.0], [0.9422017931938171, 0.000953898299485445, 0.0010898022446781397, 0.003193383105099201, 0.05256117507815361, 0.0], [0.9352929592132568, 0.0010279379785060883, 0.004444452468305826, 0.0016371518140658736, 0.010591037571430206, 0.04700646921992302]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9985783100128174, 0.0014216642593964934, 0.0, 0.0, 0.0, 0.0], [0.9893348813056946, 0.001117888605222106, 0.009547184221446514, 0.0, 0.0, 0.0], [0.9979978203773499, 7.997058128239587e-05, 0.0001321886375080794, 0.001790043548680842, 0.0, 0.0], [0.9986976385116577, 4.1044077079277486e-05, 3.868332441925304e-06, 2.367625893384684e-05, 0.0012337120715528727, 0.0], [0.9971563816070557, 1.852221612352878e-05, 1.8826470977728604e-06, 2.790033795463387e-05, 0.0006533504347316921, 0.0021419881377369165]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9768233299255371, 0.02317671850323677, 0.0, 0.0, 0.0, 0.0], [0.9194679856300354, 0.05088174715638161, 0.029650233685970306, 0.0, 0.0, 0.0], [0.8474555015563965, 0.06100161001086235, 0.0437236987054348, 0.04781917482614517, 0.0, 0.0], [0.801162600517273, 0.04186687245965004, 0.043757982552051544, 0.04189474880695343, 0.07131775468587875, 0.0], [0.8031873106956482, 0.0245048888027668, 0.017323536798357964, 0.04744400829076767, 0.06109917163848877, 0.046441033482551575]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9829427003860474, 0.017057321965694427, 0.0, 0.0, 0.0, 0.0], [0.886372983455658, 0.09492714703083038, 0.01869990862905979, 0.0, 0.0, 0.0], [0.9231082797050476, 0.03696348890662193, 0.03219844773411751, 0.007729684002697468, 0.0, 0.0], [0.9068528413772583, 0.016046611592173576, 0.014310491271317005, 0.04543787240982056, 0.01735224574804306, 0.0], [0.6555972695350647, 0.050910260528326035, 0.028384851291775703, 0.1256551593542099, 0.10546848177909851, 0.03398388996720314]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502319693565369, 0.04976798966526985, 0.0, 0.0, 0.0, 0.0], [0.8829870223999023, 0.10009586066007614, 0.016917143017053604, 0.0, 0.0, 0.0], [0.80574631690979, 0.14463508129119873, 0.03018917515873909, 0.019429394975304604, 0.0, 0.0], [0.8706235885620117, 0.03244053199887276, 0.026951542124152184, 0.04410284012556076, 0.02588149905204773, 0.0], [0.6883642673492432, 0.009681438095867634, 0.016449328511953354, 0.09871133416891098, 0.08971213549375534, 0.09708155691623688]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9792683720588684, 0.020731620490550995, 0.0, 0.0, 0.0, 0.0], [0.9523285627365112, 0.02593371830880642, 0.021737750619649887, 0.0, 0.0, 0.0], [0.9144353866577148, 0.01767120137810707, 0.02235855720937252, 0.04553478583693504, 0.0, 0.0], [0.9448292851448059, 0.006467591505497694, 0.006386075634509325, 0.03263094648718834, 0.009686185047030449, 0.0], [0.9347903728485107, 0.007862498983740807, 0.007788185030221939, 0.02143281325697899, 0.008491134271025658, 0.019634833559393883]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.983370304107666, 0.016629742458462715, 0.0, 0.0, 0.0, 0.0], [0.963111400604248, 0.009229922667145729, 0.027658650651574135, 0.0, 0.0, 0.0], [0.9706627130508423, 0.004149414133280516, 0.006813078187406063, 0.01837468333542347, 0.0, 0.0], [0.987951934337616, 0.002165879588574171, 0.0003490079543553293, 0.0015838086837902665, 0.00794939324259758, 0.0], [0.9457951188087463, 0.01458354014903307, 0.0003652939631137997, 0.0009569547255523503, 0.013621637597680092, 0.024677468463778496]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9878058433532715, 0.012194144539535046, 0.0, 0.0, 0.0, 0.0], [0.87103670835495, 0.09448167681694031, 0.03448163717985153, 0.0, 0.0, 0.0], [0.6309786438941956, 0.11090352386236191, 0.19230222702026367, 0.06581560522317886, 0.0, 0.0], [0.536048412322998, 0.04618947580456734, 0.1360531747341156, 0.26455560326576233, 0.017153318971395493, 0.0], [0.8287516832351685, 0.02373271808028221, 0.020080402493476868, 0.07245294004678726, 0.030431287363171577, 0.024550972506403923]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8995685577392578, 0.10043143481016159, 0.0, 0.0, 0.0, 0.0], [0.27034327387809753, 0.6504329442977905, 0.07922383397817612, 0.0, 0.0, 0.0], [0.20541630685329437, 0.5892508625984192, 0.18085940182209015, 0.02447350136935711, 0.0, 0.0], [0.5573862195014954, 0.177413210272789, 0.088068388402462, 0.09881831705570221, 0.07831386476755142, 0.0], [0.592290997505188, 0.08700627833604813, 0.05643303692340851, 0.056858859956264496, 0.12181548029184341, 0.08559539169073105]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.931638777256012, 0.06836120039224625, 0.0, 0.0, 0.0, 0.0], [0.9572949409484863, 0.026243247091770172, 0.016461802646517754, 0.0, 0.0, 0.0], [0.9880545735359192, 0.004273299127817154, 0.002954578958451748, 0.004717648029327393, 0.0, 0.0], [0.99403977394104, 0.0009413357474841177, 0.00047398044262081385, 0.00011646909115370363, 0.004428436513990164, 0.0], [0.9806036353111267, 2.546862197050359e-05, 0.00016239289834629744, 0.00014764144725631922, 0.0013442367780953646, 0.017716728150844574]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.993178129196167, 0.006821870803833008, 0.0, 0.0, 0.0, 0.0], [0.9756524562835693, 0.013184125535190105, 0.011163492687046528, 0.0, 0.0, 0.0], [0.9418964385986328, 0.004721764009445906, 0.0023818116169422865, 0.05099993944168091, 0.0, 0.0], [0.9905040860176086, 0.0022848148364573717, 6.198474875418469e-05, 0.0005984468152746558, 0.006550672464072704, 0.0], [0.9697660207748413, 0.0008878824883140624, 0.00023466601851396263, 0.0017040767706930637, 0.004128345753997564, 0.023278921842575073]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9716231822967529, 0.02837679162621498, 0.0, 0.0, 0.0, 0.0], [0.9223621487617493, 0.028907138854265213, 0.04873068258166313, 0.0, 0.0, 0.0], [0.8426313400268555, 0.023872144520282745, 0.047481436282396317, 0.08601505309343338, 0.0, 0.0], [0.85211181640625, 0.020744264125823975, 0.04494625702500343, 0.05765020102262497, 0.02454744279384613, 0.0], [0.8800727725028992, 0.022448452189564705, 0.018235687166452408, 0.019254818558692932, 0.015854254364967346, 0.04413411021232605]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9412726759910583, 0.058727364987134933, 0.0, 0.0, 0.0, 0.0], [0.9163140058517456, 0.05759183689951897, 0.026094170287251472, 0.0, 0.0, 0.0], [0.839242160320282, 0.0576905831694603, 0.013829091563820839, 0.08923812210559845, 0.0, 0.0], [0.8987159132957458, 0.013477819971740246, 0.0003456466947682202, 0.0032987503800541162, 0.0841619074344635, 0.0], [0.8701689839363098, 0.002700856886804104, 0.0014349964912980795, 0.005666173063218594, 0.08874330669641495, 0.03128569573163986]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9656726121902466, 0.03432736173272133, 0.0, 0.0, 0.0, 0.0], [0.9178615212440491, 0.06225801631808281, 0.019880563020706177, 0.0, 0.0, 0.0], [0.8233142495155334, 0.0628240779042244, 0.03670453652739525, 0.07715719193220139, 0.0, 0.0], [0.8501748442649841, 0.03816916421055794, 0.03196500241756439, 0.05160141363739967, 0.028089623898267746, 0.0], [0.6572404503822327, 0.05877385288476944, 0.04336012154817581, 0.09013212472200394, 0.08146592229604721, 0.06902746111154556]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9162060022354126, 0.083794005215168, 0.0, 0.0, 0.0, 0.0], [0.9451773762702942, 0.040992800146341324, 0.013829780742526054, 0.0, 0.0, 0.0], [0.8928354978561401, 0.053686901926994324, 0.017596911638975143, 0.035880785435438156, 0.0, 0.0], [0.8337052464485168, 0.04799613356590271, 0.03351316228508949, 0.04680856689810753, 0.037976887077093124, 0.0], [0.8167197704315186, 0.06337109208106995, 0.013286222703754902, 0.020469728857278824, 0.025292275473475456, 0.06086099520325661]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9525134563446045, 0.047486551105976105, 0.0, 0.0, 0.0, 0.0], [0.3019862174987793, 0.6520944237709045, 0.04591928794980049, 0.0, 0.0, 0.0], [0.2855839431285858, 0.5569514036178589, 0.14447414875030518, 0.012990470044314861, 0.0, 0.0], [0.8438048958778381, 0.03225119784474373, 0.03954283148050308, 0.06848110258579254, 0.015920016914606094, 0.0], [0.6664943695068359, 0.06095915660262108, 0.04064341261982918, 0.06804454326629639, 0.09186326712369919, 0.07199524343013763]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9682655334472656, 0.03173443675041199, 0.0, 0.0, 0.0, 0.0], [0.738522469997406, 0.22856806218624115, 0.03290953114628792, 0.0, 0.0, 0.0], [0.5946678519248962, 0.23033133149147034, 0.14867620170116425, 0.026324663311243057, 0.0, 0.0], [0.633925199508667, 0.058130376040935516, 0.09654313325881958, 0.14291979372501373, 0.06848157942295074, 0.0], [0.4037562310695648, 0.08945389837026596, 0.07635080814361572, 0.2558715045452118, 0.1433037519454956, 0.03126386180520058]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9869793653488159, 0.013020625337958336, 0.0, 0.0, 0.0, 0.0], [0.8631383776664734, 0.11056670546531677, 0.026294900104403496, 0.0, 0.0, 0.0], [0.9488076567649841, 0.028615130111575127, 0.0065356092527508736, 0.016041582450270653, 0.0, 0.0], [0.9672170877456665, 0.006604980677366257, 0.00045171534293331206, 0.0048444150015711784, 0.02088177390396595, 0.0], [0.9354620575904846, 0.02047809027135372, 0.0011700252071022987, 0.007056962698698044, 0.016318202018737793, 0.019514672458171844]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846674203872681, 0.015332573093473911, 0.0, 0.0, 0.0, 0.0], [0.9052745699882507, 0.08373618870973587, 0.010989260859787464, 0.0, 0.0, 0.0], [0.8145952224731445, 0.04283708333969116, 0.10568203777074814, 0.03688560053706169, 0.0, 0.0], [0.2351973056793213, 0.012018326669931412, 0.05280076339840889, 0.6516193747520447, 0.048364195972681046, 0.0], [0.31818506121635437, 0.01863221451640129, 0.039481643587350845, 0.3755551874637604, 0.20787331461906433, 0.040272586047649384]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9811733365058899, 0.01882672682404518, 0.0, 0.0, 0.0, 0.0], [0.861893892288208, 0.06479160487651825, 0.07331453263759613, 0.0, 0.0, 0.0], [0.7664538025856018, 0.07330423593521118, 0.10353542119264603, 0.05670652166008949, 0.0, 0.0], [0.8128498196601868, 0.0321548730134964, 0.05900559946894646, 0.05416519567370415, 0.04182438552379608, 0.0], [0.8687861561775208, 0.026987716555595398, 0.02046993374824524, 0.016297345981001854, 0.03218373283743858, 0.035275135189294815]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9264581799507141, 0.07354187220335007, 0.0, 0.0, 0.0, 0.0], [0.8403536081314087, 0.06373773515224457, 0.09590869396924973, 0.0, 0.0, 0.0], [0.7330989837646484, 0.06451132148504257, 0.10380109399557114, 0.09858868271112442, 0.0, 0.0], [0.9143612384796143, 0.008257808163762093, 0.0073203956708312035, 0.017966309562325478, 0.05209435895085335, 0.0], [0.8971917033195496, 0.008555500768125057, 0.007019444834440947, 0.014860525727272034, 0.0339975580573082, 0.03837524726986885]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9180349111557007, 0.08196508139371872, 0.0, 0.0, 0.0, 0.0], [0.8328663110733032, 0.1219903975725174, 0.04514330253005028, 0.0, 0.0, 0.0], [0.7994157671928406, 0.08744129538536072, 0.03605783358216286, 0.07708507031202316, 0.0, 0.0], [0.8809850811958313, 0.02074955217540264, 0.020554589107632637, 0.01712079532444477, 0.060589853674173355, 0.0], [0.7453036904335022, 0.04433392733335495, 0.022549254819750786, 0.03315271809697151, 0.033570535480976105, 0.1210898905992508]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9867060780525208, 0.013293919153511524, 0.0, 0.0, 0.0, 0.0], [0.982177734375, 0.012414089404046535, 0.0054081035777926445, 0.0, 0.0, 0.0], [0.96304851770401, 0.015290765091776848, 0.010345722548663616, 0.011315010488033295, 0.0, 0.0], [0.921357274055481, 0.01413238886743784, 0.017639128491282463, 0.016567641869187355, 0.030303535982966423, 0.0], [0.9373326301574707, 0.009064320474863052, 0.007548393681645393, 0.006576480343937874, 0.011827622540295124, 0.0276504959911108]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9951003193855286, 0.004899636376649141, 0.0, 0.0, 0.0, 0.0], [0.9476009607315063, 0.0414077453315258, 0.010991236194968224, 0.0, 0.0, 0.0], [0.9142175316810608, 0.023523783311247826, 0.039145033806562424, 0.023113638162612915, 0.0, 0.0], [0.953473687171936, 0.008932948112487793, 0.01527277659624815, 0.007908261381089687, 0.01441231183707714, 0.0], [0.9427104592323303, 0.008233043365180492, 0.00465096440166235, 0.004178092814981937, 0.0054634977132081985, 0.03476399928331375]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9543376564979553, 0.045662373304367065, 0.0, 0.0, 0.0, 0.0], [0.9696041941642761, 0.01954755000770092, 0.01084830705076456, 0.0, 0.0, 0.0], [0.9710449576377869, 0.012425432913005352, 0.00806890707463026, 0.008460727520287037, 0.0, 0.0], [0.9726192951202393, 0.0026976638473570347, 0.0004483149677980691, 0.0013814783887937665, 0.022853175178170204, 0.0], [0.9675466418266296, 0.00961342267692089, 0.0032030262518674135, 0.0042488230392336845, 0.0074422284960746765, 0.007945878431200981]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887008666992188, 0.011299073696136475, 0.0, 0.0, 0.0, 0.0], [0.9382636547088623, 0.042042046785354614, 0.019694261252880096, 0.0, 0.0, 0.0], [0.8351991176605225, 0.03487854823470116, 0.05134488269686699, 0.07857739180326462, 0.0, 0.0], [0.9042680263519287, 0.010541534051299095, 0.016426660120487213, 0.025921886786818504, 0.0428418405354023, 0.0], [0.891313910484314, 0.008912678807973862, 0.00501069612801075, 0.008175642229616642, 0.013514752499759197, 0.07307230681180954]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8693918585777283, 0.1306082010269165, 0.0, 0.0, 0.0, 0.0], [0.35080042481422424, 0.6063504219055176, 0.042849235236644745, 0.0, 0.0, 0.0], [0.3547583222389221, 0.3502003252506256, 0.24722403287887573, 0.04781735688447952, 0.0, 0.0], [0.35370761156082153, 0.03527713194489479, 0.09567099064588547, 0.449795126914978, 0.06554913520812988, 0.0], [0.41325968503952026, 0.09055472165346146, 0.052865613251924515, 0.1746797263622284, 0.1738494336605072, 0.09479081630706787]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629756212234497, 0.037024397403001785, 0.0, 0.0, 0.0, 0.0], [0.9756426811218262, 0.019658580422401428, 0.004698706325143576, 0.0, 0.0, 0.0], [0.9775736331939697, 0.013286329805850983, 0.002559035550802946, 0.006581075489521027, 0.0, 0.0], [0.9870142936706543, 0.007388250902295113, 0.0009579145698808134, 0.0018318254733458161, 0.0028077505994588137, 0.0], [0.9409247040748596, 0.016633691266179085, 0.0022979003842920065, 0.005890632979571819, 0.005512909963726997, 0.02874019555747509]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9628273248672485, 0.03717269375920296, 0.0, 0.0, 0.0, 0.0], [0.9582238793373108, 0.024641714990139008, 0.017134323716163635, 0.0, 0.0, 0.0], [0.93513023853302, 0.01533149741590023, 0.014810922555625439, 0.0347273163497448, 0.0, 0.0], [0.9225173592567444, 0.010528713464736938, 0.011010120622813702, 0.019440004602074623, 0.036503810435533524, 0.0], [0.8420169949531555, 0.043571874499320984, 0.007488242816179991, 0.01496151927858591, 0.023852799087762833, 0.06810850650072098]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9926387071609497, 0.007361325901001692, 0.0, 0.0, 0.0, 0.0], [0.9957393407821655, 0.003346974030137062, 0.0009136885637417436, 0.0, 0.0, 0.0], [0.9869900345802307, 0.001974782906472683, 0.0015245474642142653, 0.009510640986263752, 0.0, 0.0], [0.9933527708053589, 0.0010203201090916991, 0.0003433709207456559, 0.0010291049256920815, 0.00425435695797205, 0.0], [0.9749016761779785, 0.00043480066233314574, 0.00043065325007773936, 0.0012364318827167153, 0.0015347633743658662, 0.02146158739924431]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9897475242614746, 0.010252462700009346, 0.0, 0.0, 0.0, 0.0], [0.9790638089179993, 0.01650909334421158, 0.0044271135702729225, 0.0, 0.0, 0.0], [0.9521436095237732, 0.029432358220219612, 0.008943170309066772, 0.009480937384068966, 0.0, 0.0], [0.9395942091941833, 0.021510882303118706, 0.010278509929776192, 0.004555223509669304, 0.024061109870672226, 0.0], [0.9205077886581421, 0.016153603792190552, 0.01081855595111847, 0.016644351184368134, 0.014566346071660519, 0.021309340372681618]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9898501634597778, 0.010149819776415825, 0.0, 0.0, 0.0, 0.0], [0.9820910096168518, 0.006907487753778696, 0.01100151427090168, 0.0, 0.0, 0.0], [0.9684998393058777, 0.008987552486360073, 0.015342521481215954, 0.007170091383159161, 0.0, 0.0], [0.9274123311042786, 0.00948520191013813, 0.0220660287886858, 0.03222888335585594, 0.008807597681879997, 0.0], [0.9006659388542175, 0.021623671054840088, 0.013808254152536392, 0.009843875654041767, 0.00852135755121708, 0.045536916702985764]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954444169998169, 0.004555590450763702, 0.0, 0.0, 0.0, 0.0], [0.995254397392273, 0.0024602320045232773, 0.0022854781709611416, 0.0, 0.0, 0.0], [0.9862446188926697, 0.001516818767413497, 0.0040722922421991825, 0.0081663578748703, 0.0, 0.0], [0.9889963865280151, 0.0012260394869372249, 0.0007996344938874245, 0.0006774224457331002, 0.008300562389194965, 0.0], [0.9865204095840454, 0.0003942691837437451, 0.0009571714326739311, 0.0004954344476573169, 0.0009604907245375216, 0.010672236792743206]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9821295142173767, 0.017870459705591202, 0.0, 0.0, 0.0, 0.0], [0.7489436864852905, 0.2200269252061844, 0.03102932497859001, 0.0, 0.0, 0.0], [0.2854772210121155, 0.21125562489032745, 0.4787178933620453, 0.02454923279583454, 0.0, 0.0], [0.8056645393371582, 0.026974491775035858, 0.043028220534324646, 0.06993702799081802, 0.05439576134085655, 0.0], [0.33072200417518616, 0.022326502948999405, 0.01662714220583439, 0.08019431680440903, 0.4157475531101227, 0.1343824714422226]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9697749018669128, 0.030225154012441635, 0.0, 0.0, 0.0, 0.0], [0.9800567030906677, 0.015018866397440434, 0.004924519918859005, 0.0, 0.0, 0.0], [0.9237860441207886, 0.052764892578125, 0.0063024163246154785, 0.01714668609201908, 0.0, 0.0], [0.9451844096183777, 0.036180466413497925, 0.0019892053678631783, 0.00395871140062809, 0.012687263078987598, 0.0], [0.9633329510688782, 0.018662935122847557, 0.0030418343376368284, 0.007070871535688639, 0.00500940065830946, 0.0028820636216551065]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.987324595451355, 0.012675370089709759, 0.0, 0.0, 0.0, 0.0], [0.9904569983482361, 0.005541915539652109, 0.004001101944595575, 0.0, 0.0, 0.0], [0.9814971685409546, 0.004653455223888159, 0.003725287737324834, 0.0101240836083889, 0.0, 0.0], [0.9744365811347961, 0.004632247146219015, 0.0023799941409379244, 0.006518087349832058, 0.012033028528094292, 0.0], [0.9624497294425964, 0.0033743558451533318, 0.0013198574306443334, 0.0017275019781664014, 0.0029446815606206656, 0.028183799237012863]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9807674288749695, 0.01923258975148201, 0.0, 0.0, 0.0, 0.0], [0.9664245843887329, 0.015413926914334297, 0.01816139742732048, 0.0, 0.0, 0.0], [0.9632682204246521, 0.004538126289844513, 0.0029253901448100805, 0.029268160462379456, 0.0, 0.0], [0.9562349319458008, 0.0012223595986142755, 0.0005304060177877545, 0.00867142528295517, 0.033340804278850555, 0.0], [0.9657101035118103, 0.0009808256290853024, 0.0016686187591403723, 0.002634819597005844, 0.005866350140422583, 0.023139353841543198]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639713764190674, 0.03602859377861023, 0.0, 0.0, 0.0, 0.0], [0.9562800526618958, 0.033733122050762177, 0.009986846707761288, 0.0, 0.0, 0.0], [0.8539993166923523, 0.08073055744171143, 0.03334458917379379, 0.03192553296685219, 0.0, 0.0], [0.9547491073608398, 0.009605043567717075, 0.004146174993366003, 0.0020133228972554207, 0.029486412182450294, 0.0], [0.9331135749816895, 0.028699703514575958, 0.005477482918649912, 0.006368076894432306, 0.012613032944500446, 0.013728110119700432]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9392993450164795, 0.06070062145590782, 0.0, 0.0, 0.0, 0.0], [0.9298389554023743, 0.061895567923784256, 0.008265494368970394, 0.0, 0.0, 0.0], [0.8471819758415222, 0.0903506875038147, 0.017636122182011604, 0.04483122378587723, 0.0, 0.0], [0.885770320892334, 0.03918176144361496, 0.007867710664868355, 0.02276587300002575, 0.04441443085670471, 0.0], [0.8563281893730164, 0.10088986903429031, 0.006531456485390663, 0.008485931903123856, 0.007368435151875019, 0.020396223291754723]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8353266716003418, 0.1646733283996582, 0.0, 0.0, 0.0, 0.0], [0.616086483001709, 0.313764363527298, 0.07014919072389603, 0.0, 0.0, 0.0], [0.34316375851631165, 0.27584928274154663, 0.11966024339199066, 0.26132670044898987, 0.0, 0.0], [0.5908174514770508, 0.0502907820045948, 0.041665904223918915, 0.21994897723197937, 0.0972769483923912, 0.0], [0.8481414318084717, 0.06318086385726929, 0.014733674004673958, 0.05526731535792351, 0.009015018120408058, 0.009661629796028137]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9627320766448975, 0.037267882376909256, 0.0, 0.0, 0.0, 0.0], [0.7757525444030762, 0.17996230721473694, 0.044285181909799576, 0.0, 0.0, 0.0], [0.6317052245140076, 0.24380743503570557, 0.10925715416669846, 0.01523028127849102, 0.0, 0.0], [0.9539909958839417, 0.01818225346505642, 0.011601817794144154, 0.012299083173274994, 0.003925768658518791, 0.0], [0.4035692811012268, 0.1423754096031189, 0.05661242455244064, 0.19757378101348877, 0.09299232810735703, 0.10687679052352905]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9802618026733398, 0.01973816193640232, 0.0, 0.0, 0.0, 0.0], [0.98739093542099, 0.007800442166626453, 0.0048086862079799175, 0.0, 0.0, 0.0], [0.9283917546272278, 0.008301272988319397, 0.013305719941854477, 0.05000118911266327, 0.0, 0.0], [0.8981058597564697, 0.015591289848089218, 0.010177561081945896, 0.039986737072467804, 0.03613854944705963, 0.0], [0.9753499031066895, 0.00035433052107691765, 0.0005866037681698799, 0.0011877462966367602, 0.001075089443475008, 0.021446414291858673]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9295329451560974, 0.07046709209680557, 0.0, 0.0, 0.0, 0.0], [0.9361505508422852, 0.04116687551140785, 0.022682568058371544, 0.0, 0.0, 0.0], [0.8486817479133606, 0.05802813917398453, 0.0248562078922987, 0.06843385845422745, 0.0, 0.0], [0.8661177754402161, 0.022324735298752785, 0.010369136929512024, 0.02600197307765484, 0.07518628984689713, 0.0], [0.8074417114257812, 0.04438265413045883, 0.01849714107811451, 0.03357793390750885, 0.01856130361557007, 0.07753922790288925]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9680535197257996, 0.031946420669555664, 0.0, 0.0, 0.0, 0.0], [0.9693689942359924, 0.02568492479622364, 0.004946073051542044, 0.0, 0.0, 0.0], [0.9620568156242371, 0.022552471607923508, 0.005471337586641312, 0.009919446893036366, 0.0, 0.0], [0.9727528095245361, 0.010137113742530346, 0.0007573263137601316, 0.0028828817885369062, 0.01346979383379221, 0.0], [0.9624636769294739, 0.0031109012197703123, 0.0010007594246417284, 0.0019475924782454967, 0.00826623197644949, 0.02321087382733822]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8542509078979492, 0.1457490772008896, 0.0, 0.0, 0.0, 0.0], [0.9725967645645142, 0.014116342179477215, 0.013286887668073177, 0.0, 0.0, 0.0], [0.9257619976997375, 0.032572753727436066, 0.014612194150686264, 0.027053089812397957, 0.0, 0.0], [0.7923442721366882, 0.027304992079734802, 0.018806710839271545, 0.13854004442691803, 0.023003995418548584, 0.0], [0.6152051091194153, 0.026655351743102074, 0.029353095218539238, 0.05590879172086716, 0.11611329019069672, 0.1567644327878952]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9804654121398926, 0.019534548744559288, 0.0, 0.0, 0.0, 0.0], [0.9882452487945557, 0.007509480696171522, 0.004245338030159473, 0.0, 0.0, 0.0], [0.9584203958511353, 0.010963640175759792, 0.01045613270252943, 0.020159821957349777, 0.0, 0.0], [0.9604810476303101, 0.007182626519352198, 0.0030723433010280132, 0.006898912601172924, 0.022365078330039978, 0.0], [0.9668889045715332, 0.0032812952995300293, 0.00550055131316185, 0.004234095104038715, 0.005038043484091759, 0.01505721639841795]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9498194456100464, 0.05018055811524391, 0.0, 0.0, 0.0, 0.0], [0.9781363606452942, 0.01643003709614277, 0.00543356453999877, 0.0, 0.0, 0.0], [0.8618696928024292, 0.03609364852309227, 0.07555550336837769, 0.026481153443455696, 0.0, 0.0], [0.5449840426445007, 0.0154111972078681, 0.023516522720456123, 0.25743579864501953, 0.15865245461463928, 0.0], [0.9571872353553772, 0.003080392023548484, 0.001444687251932919, 0.006861586589366198, 0.014818808995187283, 0.01660732924938202]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6156567931175232, 0.3843432068824768, 0.0, 0.0, 0.0, 0.0], [0.36760643124580383, 0.4281637966632843, 0.20422977209091187, 0.0, 0.0, 0.0], [0.16471539437770844, 0.4136793613433838, 0.2509235441684723, 0.17068175971508026, 0.0, 0.0], [0.41844621300697327, 0.1524759978055954, 0.10305390506982803, 0.1107151210308075, 0.21530869603157043, 0.0], [0.19686952233314514, 0.2014622539281845, 0.1282723993062973, 0.09203261137008667, 0.09167557209730148, 0.2896876931190491]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9027363657951355, 0.09726360440254211, 0.0, 0.0, 0.0, 0.0], [0.973663330078125, 0.014004354365170002, 0.012332331389188766, 0.0, 0.0, 0.0], [0.8504455089569092, 0.05690591782331467, 0.03206094354391098, 0.06058768928050995, 0.0, 0.0], [0.766120433807373, 0.03530407324433327, 0.03433050960302353, 0.0967523530125618, 0.06749258935451508, 0.0], [0.8650373220443726, 0.020085325464606285, 0.01149807684123516, 0.018558409065008163, 0.018430322408676147, 0.06639060378074646]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9653082489967346, 0.03469177335500717, 0.0, 0.0, 0.0, 0.0], [0.9816323518753052, 0.014176106080412865, 0.004191514104604721, 0.0, 0.0, 0.0], [0.9275255799293518, 0.047372251749038696, 0.011528252623975277, 0.013573972508311272, 0.0, 0.0], [0.9293117523193359, 0.02583330124616623, 0.007227106485515833, 0.014300605282187462, 0.023327238857746124, 0.0], [0.8895061612129211, 0.04689629003405571, 0.004717099480330944, 0.006286595948040485, 0.0060901278629899025, 0.046503696590662]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8938028812408447, 0.10619710385799408, 0.0, 0.0, 0.0, 0.0], [0.8221707344055176, 0.06304486840963364, 0.11478440463542938, 0.0, 0.0, 0.0], [0.5047385692596436, 0.15375731885433197, 0.2277035266160965, 0.11380063742399216, 0.0, 0.0], [0.4082077145576477, 0.09066348522901535, 0.11696866899728775, 0.24553164839744568, 0.1386285275220871, 0.0], [0.7291039228439331, 0.06638886779546738, 0.023112820461392403, 0.03110307827591896, 0.05714328587055206, 0.09314810484647751]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9247531890869141, 0.07524681836366653, 0.0, 0.0, 0.0, 0.0], [0.8957375288009644, 0.0698956698179245, 0.03436674177646637, 0.0, 0.0, 0.0], [0.7924939393997192, 0.0960114598274231, 0.05509104207158089, 0.0564035065472126, 0.0, 0.0], [0.789150595664978, 0.07880311459302902, 0.038401514291763306, 0.05396990105509758, 0.03967490792274475, 0.0], [0.7807855606079102, 0.07993552833795547, 0.04253167659044266, 0.032342202961444855, 0.0178169384598732, 0.04658801108598709]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9480886459350586, 0.05191135033965111, 0.0, 0.0, 0.0, 0.0], [0.863694429397583, 0.04756231606006622, 0.0887431651353836, 0.0, 0.0, 0.0], [0.9341367483139038, 0.022224243730306625, 0.02262451872229576, 0.02101437933743, 0.0, 0.0], [0.9588143229484558, 0.008020943962037563, 0.00449007423594594, 0.005862299352884293, 0.022812390699982643, 0.0], [0.938591718673706, 0.021227749064564705, 0.004872452467679977, 0.010940167121589184, 0.009524567052721977, 0.014843398705124855]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9763734340667725, 0.023626619949936867, 0.0, 0.0, 0.0, 0.0], [0.9884802103042603, 0.005189398769289255, 0.0063303615897893906, 0.0, 0.0, 0.0], [0.9477092027664185, 0.017985178157687187, 0.0101566007360816, 0.02414904534816742, 0.0, 0.0], [0.9671926498413086, 0.006552821025252342, 0.003322785021737218, 0.005563333630561829, 0.017368415370583534, 0.0], [0.9584562182426453, 0.007502950727939606, 0.00513631384819746, 0.008071641437709332, 0.005997116211801767, 0.014835822395980358]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8840707540512085, 0.11592917889356613, 0.0, 0.0, 0.0, 0.0], [0.993125319480896, 0.005070820450782776, 0.0018038044217973948, 0.0, 0.0, 0.0], [0.9534158110618591, 0.023829098790884018, 0.007748990319669247, 0.01500610914081335, 0.0, 0.0], [0.9151289463043213, 0.010873105376958847, 0.013190964236855507, 0.011050431989133358, 0.04975655674934387, 0.0], [0.8769674301147461, 0.03385206311941147, 0.008486470207571983, 0.009969159960746765, 0.03468584269285202, 0.03603913262486458]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003709544544108212, 0.999629020690918, 0.0, 0.0, 0.0, 0.0], [6.525075150420889e-05, 0.37378257513046265, 0.6261522173881531, 0.0, 0.0, 0.0], [4.606056972988881e-05, 0.21050865948200226, 0.411596417427063, 0.37784892320632935, 0.0, 0.0], [4.753119355882518e-05, 0.11616958677768707, 0.23264272511005402, 0.3985331058502197, 0.2526070475578308, 0.0], [1.2476451729526161e-06, 0.14819666743278503, 0.15813177824020386, 0.30074360966682434, 0.1193905919790268, 0.27353614568710327]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.971555769443512, 0.028444228693842888, 0.0, 0.0, 0.0, 0.0], [0.9529062509536743, 0.03233089670538902, 0.014762827195227146, 0.0, 0.0, 0.0], [0.9343125820159912, 0.023513032123446465, 0.020498063415288925, 0.02167630009353161, 0.0, 0.0], [0.9529678225517273, 0.008551443926990032, 0.004359325394034386, 0.008064579218626022, 0.026056963950395584, 0.0], [0.9653592705726624, 0.008487657643854618, 0.0034992764703929424, 0.002721586264669895, 0.0032828878611326218, 0.016649404540657997]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8630780577659607, 0.1369219720363617, 0.0, 0.0, 0.0, 0.0], [0.7696154117584229, 0.08513359725475311, 0.14525099098682404, 0.0, 0.0, 0.0], [0.7133333683013916, 0.10170912742614746, 0.1193128377199173, 0.06564464420080185, 0.0, 0.0], [0.7186221480369568, 0.054442938417196274, 0.013868182897567749, 0.07808040082454681, 0.13498634099960327, 0.0], [0.7990152835845947, 0.05805584415793419, 0.009447015821933746, 0.01777045428752899, 0.0211385078728199, 0.0945729911327362]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9518988728523254, 0.04810105636715889, 0.0, 0.0, 0.0, 0.0], [0.8580657839775085, 0.029445689171552658, 0.11248862743377686, 0.0, 0.0, 0.0], [0.6577740907669067, 0.08513446152210236, 0.12613065540790558, 0.13096080720424652, 0.0, 0.0], [0.80873703956604, 0.03230154141783714, 0.018418151885271072, 0.06856143474578857, 0.07198182493448257, 0.0], [0.6683299541473389, 0.1328137218952179, 0.021880580112338066, 0.027877412736415863, 0.04923404008150101, 0.09986433386802673]]]], "left_text": ["No", ",", " I", " am", " your", " father"], "right_text": ["No", ",", " I", " am", " your", " father"]}], "default_filter": "0", "display_mode": "dark", "root_div_id": "bertviz-35ff1a37e60b46acbd7953cb14920dca", "include_layers": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "include_heads": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "total_heads": 12}; // HACK: {"attention": [{"name": null, "attn": [[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.961219847202301, 0.038780126720666885, 0.0, 0.0, 0.0, 0.0], [0.7466979622840881, 0.11987314373254776, 0.1334289163351059, 0.0, 0.0, 0.0], [0.5885028839111328, 0.13792073726654053, 0.21213741600513458, 0.06143900379538536, 0.0, 0.0], [0.6570857167243958, 0.08996305614709854, 0.12751275300979614, 0.08361562341451645, 0.041822828352451324, 0.0], [0.27288734912872314, 0.11203354597091675, 0.16639846563339233, 0.08467115461826324, 0.16952739655971527, 0.19448213279247284]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010616553947329521, 0.9893833994865417, 0.0, 0.0, 0.0, 0.0], [0.0024677517358213663, 0.008448000065982342, 0.9890843033790588, 0.0, 0.0, 0.0], [0.00012328448065090925, 0.001873313682153821, 0.013126963749527931, 0.9848763942718506, 0.0, 0.0], [0.0010669559706002474, 0.001136627048254013, 0.003034998197108507, 0.0015735088381916285, 0.9931879043579102, 0.0], [0.00019792020611930639, 0.0010528132552281022, 0.0015437394613400102, 0.0009642784134484828, 3.492449832265265e-05, 0.9962062835693359]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4757845103740692, 0.5242154598236084, 0.0, 0.0, 0.0, 0.0], [0.5906044840812683, 0.2486610859632492, 0.1607344001531601, 0.0, 0.0, 0.0], [0.552929162979126, 0.18856704235076904, 0.14457565546035767, 0.11392819881439209, 0.0, 0.0], [0.45094069838523865, 0.16486795246601105, 0.17318040132522583, 0.11748011410236359, 0.09353082627058029, 0.0], [0.42572465538978577, 0.17328651249408722, 0.15651951730251312, 0.07022642344236374, 0.0808701142668724, 0.09337279945611954]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6133624315261841, 0.3866375684738159, 0.0, 0.0, 0.0, 0.0], [0.06098507344722748, 0.03253458812832832, 0.9064804315567017, 0.0, 0.0, 0.0], [0.006717091891914606, 0.0004012881254311651, 0.7572961449623108, 0.23558545112609863, 0.0, 0.0], [0.03722784295678139, 0.0029488641303032637, 0.10081139206886292, 0.041422780603170395, 0.8175891637802124, 0.0], [0.04989779368042946, 0.0003075827844440937, 0.002419829135760665, 0.0034334962256252766, 0.0006823891890235245, 0.9432588815689087]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9489555954933167, 0.05104440078139305, 0.0, 0.0, 0.0, 0.0], [0.6821408271789551, 0.1395242065191269, 0.17833493649959564, 0.0, 0.0, 0.0], [0.20366333425045013, 0.05641490966081619, 0.06399299949407578, 0.6759287118911743, 0.0, 0.0], [0.34195491671562195, 0.06725442409515381, 0.07926180213689804, 0.17836210131645203, 0.33316683769226074, 0.0], [0.09464015066623688, 0.007428212556988001, 0.006983965635299683, 0.007184378802776337, 0.018724262714385986, 0.8650389313697815]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33834579586982727, 0.6616541743278503, 0.0, 0.0, 0.0, 0.0], [0.0785597637295723, 0.006165453232824802, 0.9152747988700867, 0.0, 0.0, 0.0], [0.01677590422332287, 0.00040376908145844936, 0.0033404543064534664, 0.9794798493385315, 0.0, 0.0], [0.027600567787885666, 0.0004441539931576699, 0.0006541709881275892, 0.00022661943512503058, 0.9710745215415955, 0.0], [0.01024820376187563, 3.701564128277823e-05, 0.00016064087685663253, 2.7341924578649923e-05, 1.0187323823629413e-05, 0.98951655626297]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.982503354549408, 0.017496641725301743, 0.0, 0.0, 0.0, 0.0], [0.8874198794364929, 0.05467931181192398, 0.05790078267455101, 0.0, 0.0, 0.0], [0.6849911212921143, 0.12280683219432831, 0.04972023144364357, 0.14248180389404297, 0.0, 0.0], [0.6015856266021729, 0.09881891310214996, 0.07070109993219376, 0.16652531921863556, 0.06236904859542847, 0.0], [0.3232504427433014, 0.12567415833473206, 0.04432179406285286, 0.0707697868347168, 0.06606657058000565, 0.3699173331260681]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9191647171974182, 0.0808352455496788, 0.0, 0.0, 0.0, 0.0], [0.45986422896385193, 0.3970310389995575, 0.1431046575307846, 0.0, 0.0, 0.0], [0.30038726329803467, 0.22181735932826996, 0.3816150426864624, 0.09618031233549118, 0.0, 0.0], [0.18963950872421265, 0.13763730227947235, 0.20173467695713043, 0.2363215535879135, 0.23466698825359344, 0.0], [0.15410441160202026, 0.09489507973194122, 0.11902560293674469, 0.10277969390153885, 0.431721955537796, 0.09747327119112015]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3650002181529999, 0.6349997520446777, 0.0, 0.0, 0.0, 0.0], [0.24595215916633606, 0.5519200563430786, 0.20212781429290771, 0.0, 0.0, 0.0], [0.27213600277900696, 0.4073861241340637, 0.25186216831207275, 0.06861580163240433, 0.0, 0.0], [0.10242559760808945, 0.16683609783649445, 0.5248048901557922, 0.05445466190576553, 0.15147875249385834, 0.0], [0.2502950131893158, 0.22198116779327393, 0.1889996975660324, 0.10677123814821243, 0.13032680749893188, 0.10162603110074997]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6990504264831543, 0.3009495735168457, 0.0, 0.0, 0.0, 0.0], [0.5107941031455994, 0.294864296913147, 0.19434157013893127, 0.0, 0.0, 0.0], [0.460470587015152, 0.2805192172527313, 0.19174805283546448, 0.06726215034723282, 0.0, 0.0], [0.37648412585258484, 0.21120671927928925, 0.20214536786079407, 0.1020701676607132, 0.10809355974197388, 0.0], [0.30138444900512695, 0.20456187427043915, 0.1825033575296402, 0.11019378900527954, 0.16291272640228271, 0.03844381496310234]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7131580114364624, 0.2868419587612152, 0.0, 0.0, 0.0, 0.0], [0.405879944562912, 0.18063302338123322, 0.4134870171546936, 0.0, 0.0, 0.0], [0.265546053647995, 0.1698586493730545, 0.33585938811302185, 0.22873595356941223, 0.0, 0.0], [0.31385400891304016, 0.18316704034805298, 0.14928361773490906, 0.05377671867609024, 0.29991868138313293, 0.0], [0.20466554164886475, 0.18731124699115753, 0.15959152579307556, 0.06381776928901672, 0.036423031240701675, 0.34819096326828003]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6586242914199829, 0.3413756787776947, 0.0, 0.0, 0.0, 0.0], [0.5917775630950928, 0.31600356101989746, 0.0922188088297844, 0.0, 0.0, 0.0], [0.5477150678634644, 0.23586955666542053, 0.06145602837204933, 0.1549593061208725, 0.0, 0.0], [0.4587060511112213, 0.22439995408058167, 0.07887425273656845, 0.0992034450173378, 0.13881632685661316, 0.0], [0.32743722200393677, 0.19600819051265717, 0.06805713474750519, 0.0892510786652565, 0.11618088185787201, 0.20306551456451416]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9961552023887634, 0.0038448444101959467, 0.0, 0.0, 0.0, 0.0], [0.8594855070114136, 0.06906120479106903, 0.07145323604345322, 0.0, 0.0, 0.0], [0.3800051808357239, 0.0412757508456707, 0.5496610999107361, 0.029057921841740608, 0.0, 0.0], [0.2144520878791809, 0.05088745057582855, 0.4317440092563629, 0.25869324803352356, 0.044223327189683914, 0.0], [0.11175242811441422, 0.017593061551451683, 0.027507437393069267, 0.040867775678634644, 0.7754672169685364, 0.026812128722667694]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9285966753959656, 0.07140329480171204, 0.0, 0.0, 0.0, 0.0], [0.6077283620834351, 0.3121430575847626, 0.08012863993644714, 0.0, 0.0, 0.0], [0.4942907392978668, 0.285037100315094, 0.11849302053451538, 0.10217911750078201, 0.0, 0.0], [0.41838762164115906, 0.23117919266223907, 0.0834062322974205, 0.11365960538387299, 0.15336737036705017, 0.0], [0.42215877771377563, 0.12917152047157288, 0.08740924298763275, 0.10163769125938416, 0.21230241656303406, 0.0473204180598259]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9786475896835327, 0.021352410316467285, 0.0, 0.0, 0.0, 0.0], [0.7749119400978088, 0.06510379910469055, 0.1599842607975006, 0.0, 0.0, 0.0], [0.6484918594360352, 0.07483147084712982, 0.1475161612033844, 0.12916044890880585, 0.0, 0.0], [0.5224637389183044, 0.06921815127134323, 0.13823407888412476, 0.11106600612401962, 0.15901802480220795, 0.0], [0.396451473236084, 0.07325827330350876, 0.12938156723976135, 0.10642445087432861, 0.14864006638526917, 0.14584414660930634]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5525909662246704, 0.447409063577652, 0.0, 0.0, 0.0, 0.0], [0.5585010051727295, 0.21762587130069733, 0.22387316823005676, 0.0, 0.0, 0.0], [0.5143133401870728, 0.15964657068252563, 0.1549195498228073, 0.17112062871456146, 0.0, 0.0], [0.5039961338043213, 0.11401885002851486, 0.11974025517702103, 0.12552587687969208, 0.13671891391277313, 0.0], [0.5061840415000916, 0.08567393571138382, 0.0890302062034607, 0.09759823232889175, 0.10275731235742569, 0.1187562569975853]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9242545962333679, 0.07574540376663208, 0.0, 0.0, 0.0, 0.0], [0.8257423043251038, 0.07932543754577637, 0.09493227303028107, 0.0, 0.0, 0.0], [0.7306382656097412, 0.08571825176477432, 0.08043920248746872, 0.10320423543453217, 0.0, 0.0], [0.6383239030838013, 0.07886391133069992, 0.07815028727054596, 0.08758103102445602, 0.11708080023527145, 0.0], [0.5552154183387756, 0.07409127801656723, 0.06834893673658371, 0.07778607308864594, 0.09999319911003113, 0.12456496804952621]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8578912019729614, 0.14210878312587738, 0.0, 0.0, 0.0, 0.0], [0.6423032879829407, 0.16629046201705933, 0.19140629470348358, 0.0, 0.0, 0.0], [0.5530969500541687, 0.10609286278486252, 0.07821279019117355, 0.26259738206863403, 0.0, 0.0], [0.40121644735336304, 0.12223614007234573, 0.19347335398197174, 0.14164642989635468, 0.1414276361465454, 0.0], [0.40212586522102356, 0.18450748920440674, 0.07516804337501526, 0.058490585535764694, 0.1444634348154068, 0.13524460792541504]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791560173034668, 0.020844005048274994, 0.0, 0.0, 0.0, 0.0], [0.8829841613769531, 0.062332428991794586, 0.05468343570828438, 0.0, 0.0, 0.0], [0.8105451464653015, 0.08617085963487625, 0.07321798801422119, 0.03006598725914955, 0.0, 0.0], [0.6819810271263123, 0.049908194690942764, 0.08296558260917664, 0.08369531482458115, 0.10144990682601929, 0.0], [0.4056691825389862, 0.07337658852338791, 0.08601400256156921, 0.06170940399169922, 0.1322644203901291, 0.24096642434597015]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9670190811157227, 0.03298091143369675, 0.0, 0.0, 0.0, 0.0], [0.8449065089225769, 0.08514493703842163, 0.06994852423667908, 0.0, 0.0, 0.0], [0.7123570442199707, 0.07896050810813904, 0.05541075021028519, 0.15327171981334686, 0.0, 0.0], [0.6402614116668701, 0.07397546619176865, 0.04439306631684303, 0.1432214379310608, 0.09814866632223129, 0.0], [0.5073904991149902, 0.07523056864738464, 0.07754648476839066, 0.11362502723932266, 0.13947944343090057, 0.08672793954610825]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.848756730556488, 0.15124329924583435, 0.0, 0.0, 0.0, 0.0], [0.8415652513504028, 0.12107212841510773, 0.037362683564424515, 0.0, 0.0, 0.0], [0.7505511045455933, 0.11348968744277954, 0.06179966405034065, 0.07415955513715744, 0.0, 0.0], [0.6614716053009033, 0.10242649912834167, 0.05293430760502815, 0.07529721409082413, 0.10787041485309601, 0.0], [0.6014202237129211, 0.11340387165546417, 0.05631927400827408, 0.07096721231937408, 0.1090628057718277, 0.04882660508155823]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9445481896400452, 0.05545181781053543, 0.0, 0.0, 0.0, 0.0], [0.8874567747116089, 0.054742202162742615, 0.05780098959803581, 0.0, 0.0, 0.0], [0.8281881213188171, 0.06895041465759277, 0.059034936130046844, 0.043826598674058914, 0.0, 0.0], [0.6429886221885681, 0.06747572124004364, 0.11629719287157059, 0.05417964980006218, 0.11905882507562637, 0.0], [0.7367817163467407, 0.056119244545698166, 0.0685729905962944, 0.0342196486890316, 0.07875392585992813, 0.025552459061145782]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002913392090704292, 0.9997085928916931, 0.0, 0.0, 0.0, 0.0], [0.0007981215021573007, 0.5288335084915161, 0.47036832571029663, 0.0, 0.0, 0.0], [0.0007648495957255363, 0.34519800543785095, 0.3085266947746277, 0.3455105125904083, 0.0, 0.0], [0.0010283137671649456, 0.24135896563529968, 0.23320136964321136, 0.2555716633796692, 0.2688397169113159, 0.0], [0.0009746801806613803, 0.17789681255817413, 0.16743150353431702, 0.18587623536586761, 0.18734429776668549, 0.2804764211177826]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8244926333427429, 0.17550736665725708, 0.0, 0.0, 0.0, 0.0], [0.12386886030435562, 0.04450000822544098, 0.8316311240196228, 0.0, 0.0, 0.0], [0.07924386113882065, 0.012965965084731579, 0.001527726068161428, 0.9062625169754028, 0.0, 0.0], [0.08806377649307251, 0.021341020241379738, 0.0028886187355965376, 0.0028453918639570475, 0.8848611116409302, 0.0], [0.09983241558074951, 0.03363398462533951, 0.005499999970197678, 0.002433058572933078, 0.001508240820840001, 0.8570923805236816]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9646893739700317, 0.03531067073345184, 0.0, 0.0, 0.0, 0.0], [0.752915620803833, 0.0873347818851471, 0.1597496122121811, 0.0, 0.0, 0.0], [0.42022907733917236, 0.09195100516080856, 0.23549817502498627, 0.252321720123291, 0.0, 0.0], [0.3084893524646759, 0.05908145010471344, 0.38391321897506714, 0.15659162402153015, 0.09192444384098053, 0.0], [0.44790348410606384, 0.04329321160912514, 0.07969199120998383, 0.11081957072019577, 0.2212458997964859, 0.09704583883285522]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.991096019744873, 0.00890398770570755, 0.0, 0.0, 0.0, 0.0], [0.9697675704956055, 0.026084454730153084, 0.0041479249484837055, 0.0, 0.0, 0.0], [0.9082900285720825, 0.033206015825271606, 0.009421179071068764, 0.04908282309770584, 0.0, 0.0], [0.8949136137962341, 0.055445361882448196, 0.005577626638114452, 0.03150685876607895, 0.012556513771414757, 0.0], [0.8497741222381592, 0.028890041634440422, 0.0036647941451519728, 0.0375199019908905, 0.03842775151133537, 0.041723500937223434]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9984525442123413, 0.0015474447282031178, 0.0, 0.0, 0.0, 0.0], [0.48947733640670776, 0.4812204837799072, 0.029302237555384636, 0.0, 0.0, 0.0], [0.1177220344543457, 0.131212055683136, 0.670230507850647, 0.08083536475896835, 0.0, 0.0], [0.13043680787086487, 0.040686700493097305, 0.2652038633823395, 0.41143620014190674, 0.15223641693592072, 0.0], [0.1266186386346817, 0.032751280814409256, 0.035678680986166, 0.06039194017648697, 0.6021823287010193, 0.14237718284130096]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9805176854133606, 0.019482361152768135, 0.0, 0.0, 0.0, 0.0], [0.7948845028877258, 0.12061940878629684, 0.08449607342481613, 0.0, 0.0, 0.0], [0.5612362623214722, 0.15743127465248108, 0.20339690148830414, 0.07793557643890381, 0.0, 0.0], [0.42583802342414856, 0.107420414686203, 0.15123680233955383, 0.08755015581846237, 0.2279546856880188, 0.0], [0.247526153922081, 0.024188315495848656, 0.030395271256566048, 0.0858696773648262, 0.5714337229728699, 0.04058678075671196]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887767434120178, 0.011223193258047104, 0.0, 0.0, 0.0, 0.0], [0.7572689056396484, 0.22317391633987427, 0.01955711469054222, 0.0, 0.0, 0.0], [0.534188985824585, 0.22107531130313873, 0.17621807754039764, 0.06851760298013687, 0.0, 0.0], [0.1709531843662262, 0.08229412138462067, 0.5760221481323242, 0.11097586899995804, 0.05975470319390297, 0.0], [0.24871155619621277, 0.0888080894947052, 0.0898018553853035, 0.09729322791099548, 0.4413086771965027, 0.03407656028866768]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8422133326530457, 0.15778674185276031, 0.0, 0.0, 0.0, 0.0], [0.4684129059314728, 0.46105366945266724, 0.07053348422050476, 0.0, 0.0, 0.0], [0.25881505012512207, 0.4635883867740631, 0.18503470718860626, 0.09256187081336975, 0.0, 0.0], [0.1839958131313324, 0.2915419340133667, 0.17031101882457733, 0.27172985672950745, 0.08242136985063553, 0.0], [0.16469866037368774, 0.24726979434490204, 0.08770564198493958, 0.2257501780986786, 0.17745359241962433, 0.0971221849322319]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9919946193695068, 0.008005390875041485, 0.0, 0.0, 0.0, 0.0], [0.9068724513053894, 0.04406515136361122, 0.04906249791383743, 0.0, 0.0, 0.0], [0.8582221865653992, 0.055348340421915054, 0.04041949659585953, 0.04600999131798744, 0.0, 0.0], [0.7855252027511597, 0.04124243184924126, 0.08369303494691849, 0.04887612164020538, 0.04066324606537819, 0.0], [0.7856316566467285, 0.050146445631980896, 0.04751276969909668, 0.027365924790501595, 0.0561474971473217, 0.03319568186998367]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9041039347648621, 0.09589607268571854, 0.0, 0.0, 0.0, 0.0], [0.5862313508987427, 0.07199801504611969, 0.34177055954933167, 0.0, 0.0, 0.0], [0.38789603114128113, 0.04660792276263237, 0.20279011130332947, 0.36270591616630554, 0.0, 0.0], [0.266524076461792, 0.024533001706004143, 0.12211966514587402, 0.20041236281394958, 0.3864108622074127, 0.0], [0.2335742861032486, 0.02053731307387352, 0.09610352665185928, 0.13062262535095215, 0.22990448772907257, 0.2892577648162842]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639912843704224, 0.03600866720080376, 0.0, 0.0, 0.0, 0.0], [0.7075554132461548, 0.25427746772766113, 0.03816713020205498, 0.0, 0.0, 0.0], [0.2566538155078888, 0.20589275658130646, 0.01665661484003067, 0.5207968354225159, 0.0, 0.0], [0.10379398614168167, 0.046390973031520844, 0.008698644116520882, 0.7866848707199097, 0.054431553930044174, 0.0], [0.22143355011940002, 0.033797506242990494, 0.02902393788099289, 0.5412923097610474, 0.15286099910736084, 0.021591614931821823]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9891703724861145, 0.010829598642885685, 0.0, 0.0, 0.0, 0.0], [0.7913159728050232, 0.1230960264801979, 0.0855880156159401, 0.0, 0.0, 0.0], [0.29546085000038147, 0.15808266401290894, 0.4217235743999481, 0.12473298609256744, 0.0, 0.0], [0.23441052436828613, 0.09886527806520462, 0.33160147070884705, 0.1971394270658493, 0.1379832923412323, 0.0], [0.19728443026542664, 0.057418256998062134, 0.06909003108739853, 0.16469796001911163, 0.2797275185585022, 0.2317817509174347]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9359127283096313, 0.06408724933862686, 0.0, 0.0, 0.0, 0.0], [0.7888627052307129, 0.086734838783741, 0.1244024783372879, 0.0, 0.0, 0.0], [0.6535118818283081, 0.07573550194501877, 0.09732561558485031, 0.17342697083950043, 0.0, 0.0], [0.5222765803337097, 0.058278873562812805, 0.09920476377010345, 0.17020854353904724, 0.15003123879432678, 0.0], [0.4108838140964508, 0.04730610176920891, 0.07265669107437134, 0.10560758411884308, 0.10550003498792648, 0.2580457329750061]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9683833122253418, 0.03161666914820671, 0.0, 0.0, 0.0, 0.0], [0.8965396881103516, 0.03887055069208145, 0.06458982080221176, 0.0, 0.0, 0.0], [0.8264954090118408, 0.03213459998369217, 0.051967158913612366, 0.08940287679433823, 0.0, 0.0], [0.771817147731781, 0.030402876436710358, 0.04582748934626579, 0.07118481397628784, 0.08076764643192291, 0.0], [0.7292332053184509, 0.021699808537960052, 0.033074751496315, 0.047200970351696014, 0.06474553793668747, 0.10404572635889053]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9979567527770996, 0.0020432781893759966, 0.0, 0.0, 0.0, 0.0], [0.9552940726280212, 0.00802531000226736, 0.03668056055903435, 0.0, 0.0, 0.0], [0.925471305847168, 0.0027555758133530617, 0.00206298497505486, 0.0697101429104805, 0.0, 0.0], [0.8660571575164795, 0.0038883730303496122, 0.000678599055390805, 0.0006981441983953118, 0.12867768108844757, 0.0], [0.8455925583839417, 0.0037804089952260256, 0.00025342273875139654, 6.027055496815592e-05, 0.0001182075502583757, 0.1501951515674591]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9262459874153137, 0.07375400513410568, 0.0, 0.0, 0.0, 0.0], [0.7717162370681763, 0.16241960227489471, 0.06586415320634842, 0.0, 0.0, 0.0], [0.8167640566825867, 0.0780717059969902, 0.06324010342359543, 0.041924092918634415, 0.0, 0.0], [0.6867192983627319, 0.0775517150759697, 0.10056842118501663, 0.059550508856773376, 0.07561004161834717, 0.0], [0.642117440700531, 0.11014864593744278, 0.0768813043832779, 0.054033227264881134, 0.10333622992038727, 0.01348315179347992]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9395954012870789, 0.06040458753705025, 0.0, 0.0, 0.0, 0.0], [0.23004719614982605, 0.6617377400398254, 0.10821503400802612, 0.0, 0.0, 0.0], [0.26702091097831726, 0.3607969880104065, 0.3249621093273163, 0.04721994698047638, 0.0, 0.0], [0.595203697681427, 0.12269200384616852, 0.06301991641521454, 0.08916746824979782, 0.1299169361591339, 0.0], [0.10284474492073059, 0.029379907995462418, 0.013739032670855522, 0.04586051404476166, 0.769851803779602, 0.03832397609949112]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.904097855091095, 0.09590208530426025, 0.0, 0.0, 0.0, 0.0], [0.3572397530078888, 0.6274594664573669, 0.015300781466066837, 0.0, 0.0, 0.0], [0.5918020606040955, 0.27640295028686523, 0.10475941002368927, 0.02703559212386608, 0.0, 0.0], [0.7254431843757629, 0.049830954521894455, 0.01498264167457819, 0.17781217396259308, 0.03193102404475212, 0.0], [0.7612760663032532, 0.06158866360783577, 0.005942120216786861, 0.016426634043455124, 0.126780703663826, 0.027985917404294014]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9947587847709656, 0.005241225007921457, 0.0, 0.0, 0.0, 0.0], [0.963241696357727, 0.017816387116909027, 0.018941964954137802, 0.0, 0.0, 0.0], [0.9671077132225037, 0.00850958563387394, 0.008562212809920311, 0.015820462256669998, 0.0, 0.0], [0.9340999722480774, 0.01195237971842289, 0.0201801136136055, 0.026750775054097176, 0.007016872521489859, 0.0], [0.9587239027023315, 0.004657103214412928, 0.0033267680555582047, 0.006545295938849449, 0.010182415135204792, 0.0165645033121109]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9769991040229797, 0.023000944405794144, 0.0, 0.0, 0.0, 0.0], [0.7917606830596924, 0.17533230781555176, 0.03290707990527153, 0.0, 0.0, 0.0], [0.7949199080467224, 0.1053181067109108, 0.040218379348516464, 0.059543587267398834, 0.0, 0.0], [0.7097731232643127, 0.10552491247653961, 0.0659753829240799, 0.057655807584524155, 0.061070751398801804, 0.0], [0.7506603002548218, 0.026514416560530663, 0.02157599851489067, 0.03429661691188812, 0.08494441956281662, 0.08200817555189133]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9837515950202942, 0.016248442232608795, 0.0, 0.0, 0.0, 0.0], [0.5615504384040833, 0.08956819772720337, 0.3488813042640686, 0.0, 0.0, 0.0], [0.3292921185493469, 0.02411479502916336, 0.5428044199943542, 0.10378871113061905, 0.0, 0.0], [0.3433031141757965, 0.013086349703371525, 0.5121973156929016, 0.11146238446235657, 0.019950827583670616, 0.0], [0.47928208112716675, 0.017333462834358215, 0.11805327236652374, 0.061302702873945236, 0.20071855187416077, 0.12330997735261917]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9908847212791443, 0.009115338325500488, 0.0, 0.0, 0.0, 0.0], [0.5282697081565857, 0.329226553440094, 0.14250372350215912, 0.0, 0.0, 0.0], [0.4878896176815033, 0.23368458449840546, 0.17577968537807465, 0.10264609754085541, 0.0, 0.0], [0.314449667930603, 0.180649995803833, 0.16871333122253418, 0.09506484866142273, 0.24112209677696228, 0.0], [0.516875147819519, 0.03589686378836632, 0.02618817426264286, 0.04039689525961876, 0.18791928887367249, 0.19272355735301971]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8750306963920593, 0.12496932595968246, 0.0, 0.0, 0.0, 0.0], [0.45506006479263306, 0.49004384875297546, 0.054896049201488495, 0.0, 0.0, 0.0], [0.29337164759635925, 0.5449909567832947, 0.09444276243448257, 0.0671946257352829, 0.0, 0.0], [0.4897095561027527, 0.27209898829460144, 0.068619504570961, 0.14694790542125702, 0.022624004632234573, 0.0], [0.4729057848453522, 0.08103073388338089, 0.016052119433879852, 0.3067234754562378, 0.10120765119791031, 0.022080309689044952]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9630221724510193, 0.03697776421904564, 0.0, 0.0, 0.0, 0.0], [0.7557196021080017, 0.1643640547990799, 0.07991636544466019, 0.0, 0.0, 0.0], [0.694770872592926, 0.08409861475229263, 0.06382570415735245, 0.1573048233985901, 0.0, 0.0], [0.5821168422698975, 0.03297801688313484, 0.07936544716358185, 0.19441282749176025, 0.11112681776285172, 0.0], [0.597453236579895, 0.042611002922058105, 0.06919699907302856, 0.14563480019569397, 0.12481795251369476, 0.020285995677113533]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9957822561264038, 0.004217796493321657, 0.0, 0.0, 0.0, 0.0], [0.931283712387085, 0.010560186579823494, 0.05815601721405983, 0.0, 0.0, 0.0], [0.8435329794883728, 0.015694962814450264, 0.04575089365243912, 0.09502116590738297, 0.0, 0.0], [0.7724104523658752, 0.011981171555817127, 0.03504578396677971, 0.038767702877521515, 0.14179477095603943, 0.0], [0.7642908096313477, 0.009868745692074299, 0.008122707717120647, 0.013314425013959408, 0.04824399575591087, 0.1561594009399414]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9701175093650818, 0.029882533475756645, 0.0, 0.0, 0.0, 0.0], [0.6564013957977295, 0.2250615507364273, 0.11853699386119843, 0.0, 0.0, 0.0], [0.6958046555519104, 0.1470194011926651, 0.07146008312702179, 0.0857158824801445, 0.0, 0.0], [0.6353287100791931, 0.13460613787174225, 0.03099406138062477, 0.05691603198647499, 0.14215515553951263, 0.0], [0.6779382824897766, 0.05365435779094696, 0.01800640858709812, 0.0628453940153122, 0.11038312315940857, 0.07717251032590866]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9822334051132202, 0.017766587436199188, 0.0, 0.0, 0.0, 0.0], [0.9037660956382751, 0.06541557610034943, 0.03081827238202095, 0.0, 0.0, 0.0], [0.8119180202484131, 0.03679049760103226, 0.06056082993745804, 0.09073063731193542, 0.0, 0.0], [0.40546438097953796, 0.10383863002061844, 0.10211163759231567, 0.3543432950973511, 0.034242067486047745, 0.0], [0.2282433658838272, 0.01727871783077717, 0.05055440962314606, 0.6015747785568237, 0.09411729127168655, 0.008231495507061481]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873148202896118, 0.012685106135904789, 0.0, 0.0, 0.0, 0.0], [0.3544553816318512, 0.5317620635032654, 0.11378253996372223, 0.0, 0.0, 0.0], [0.07823341339826584, 0.7221351861953735, 0.10936630517244339, 0.09026506543159485, 0.0, 0.0], [0.2196795493364334, 0.4048422873020172, 0.12358050793409348, 0.2001895010471344, 0.0517081692814827, 0.0], [0.3608974516391754, 0.10458994656801224, 0.06983785331249237, 0.2976486384868622, 0.1386992484331131, 0.028326794505119324]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9732162356376648, 0.02678375132381916, 0.0, 0.0, 0.0, 0.0], [0.9167549014091492, 0.06145293265581131, 0.021792078390717506, 0.0, 0.0, 0.0], [0.8543080687522888, 0.08049597591161728, 0.030335018411278725, 0.03486090898513794, 0.0, 0.0], [0.8919216394424438, 0.04280775412917137, 0.022045090794563293, 0.023470621556043625, 0.01975489780306816, 0.0], [0.8116760849952698, 0.0341353714466095, 0.03567682206630707, 0.04748590663075447, 0.02539718709886074, 0.045628681778907776]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502761363983154, 0.049723926931619644, 0.0, 0.0, 0.0, 0.0], [0.7637450695037842, 0.20073653757572174, 0.035518333315849304, 0.0, 0.0, 0.0], [0.6279101967811584, 0.0376814566552639, 0.19945232570171356, 0.13495612144470215, 0.0, 0.0], [0.6397072672843933, 0.027007289230823517, 0.09081897139549255, 0.20653867721557617, 0.035927820950746536, 0.0], [0.4559415578842163, 0.021641148254275322, 0.12939520180225372, 0.21801024675369263, 0.10379895567893982, 0.07121287286281586]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.949840784072876, 0.05015924200415611, 0.0, 0.0, 0.0, 0.0], [0.8688720464706421, 0.08722209185361862, 0.04390587657690048, 0.0, 0.0, 0.0], [0.6937950253486633, 0.06359221041202545, 0.09179061651229858, 0.15082214772701263, 0.0, 0.0], [0.726659893989563, 0.04389888793230057, 0.04683985933661461, 0.09851793199777603, 0.08408348262310028, 0.0], [0.7848988175392151, 0.037147946655750275, 0.012907883152365685, 0.010539398528635502, 0.12079229950904846, 0.03371357545256615]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9891054034233093, 0.01089461799710989, 0.0, 0.0, 0.0, 0.0], [0.8929510116577148, 0.087001271545887, 0.020047681406140327, 0.0, 0.0, 0.0], [0.7891110777854919, 0.09797301143407822, 0.0863330066204071, 0.026582954451441765, 0.0, 0.0], [0.8850634098052979, 0.03645016998052597, 0.05395463854074478, 0.012377261184155941, 0.012154542841017246, 0.0], [0.6861327290534973, 0.05720385164022446, 0.011636361479759216, 0.021660521626472473, 0.17488014698028564, 0.048486415296792984]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9396193623542786, 0.060380659997463226, 0.0, 0.0, 0.0, 0.0], [0.7851788997650146, 0.19751502573490143, 0.017306093126535416, 0.0, 0.0, 0.0], [0.7660511136054993, 0.15444667637348175, 0.03188285231590271, 0.04761938750743866, 0.0, 0.0], [0.7035228610038757, 0.05171423405408859, 0.07760986685752869, 0.15339063107967377, 0.013762437738478184, 0.0], [0.7121887803077698, 0.049942418932914734, 0.037725359201431274, 0.0864911898970604, 0.06541414558887482, 0.04823815077543259]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9740723967552185, 0.025927582755684853, 0.0, 0.0, 0.0, 0.0], [0.7925393581390381, 0.01171562448143959, 0.19574503600597382, 0.0, 0.0, 0.0], [0.5106777548789978, 0.007296778727322817, 0.03962007910013199, 0.4424054026603699, 0.0, 0.0], [0.5862482190132141, 0.012099675834178925, 0.024585217237472534, 0.06737817078828812, 0.3096887171268463, 0.0], [0.3019630014896393, 0.007723969407379627, 0.011518090032041073, 0.04694710671901703, 0.22146669030189514, 0.4103810787200928]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9744554758071899, 0.025544486939907074, 0.0, 0.0, 0.0, 0.0], [0.9769196510314941, 0.015048469416797161, 0.00803192239254713, 0.0, 0.0, 0.0], [0.9060620665550232, 0.025875359773635864, 0.025954803451895714, 0.04210779070854187, 0.0, 0.0], [0.9400083422660828, 0.005556613672524691, 0.005828283727169037, 0.031757839024066925, 0.01684902422130108, 0.0], [0.9105741381645203, 0.001975212013348937, 0.008646724745631218, 0.013360838405787945, 0.03543954715132713, 0.030003532767295837]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791666865348816, 0.020833317190408707, 0.0, 0.0, 0.0, 0.0], [0.8444864153862, 0.13507819175720215, 0.020435353741049767, 0.0, 0.0, 0.0], [0.79030841588974, 0.1455916166305542, 0.037530187517404556, 0.026569800451397896, 0.0, 0.0], [0.729892373085022, 0.05649634823203087, 0.032735567539930344, 0.10400468111038208, 0.07687091082334518, 0.0], [0.568419337272644, 0.043888114392757416, 0.026293445378541946, 0.08117109537124634, 0.24314820766448975, 0.03707980737090111]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9499869346618652, 0.05001309886574745, 0.0, 0.0, 0.0, 0.0], [0.9336172342300415, 0.058488547801971436, 0.007894248701632023, 0.0, 0.0, 0.0], [0.789783775806427, 0.11071790754795074, 0.0536017119884491, 0.04589656740427017, 0.0, 0.0], [0.8859301805496216, 0.057529836893081665, 0.013743293471634388, 0.003387755248695612, 0.03940898925065994, 0.0], [0.9337608814239502, 0.026470575481653214, 0.004523397423326969, 0.006190492305904627, 0.014132915996015072, 0.014921690337359905]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.8521365552359725e-09, 0.0, 0.0, 0.0, 0.0], [6.676049906673143e-06, 0.9999804496765137, 1.2841427633247804e-05, 0.0, 0.0, 0.0], [2.2194576843048708e-08, 2.668524290427854e-09, 0.9999971389770508, 2.8137685603724094e-06, 0.0, 0.0], [1.014530539578118e-06, 4.4639637764021245e-08, 0.00035354969440959394, 0.9993677735328674, 0.00027762516401708126, 0.0], [9.436316172894976e-10, 1.3820019781340953e-11, 5.017615212210558e-10, 2.96517832509835e-09, 0.9999971389770508, 2.864343969122274e-06]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9948632121086121, 0.005136759020388126, 0.0, 0.0, 0.0, 0.0], [0.9274215698242188, 0.018323808908462524, 0.05425465852022171, 0.0, 0.0, 0.0], [0.9678992033004761, 0.004143428988754749, 0.004314454738050699, 0.023642895743250847, 0.0, 0.0], [0.899907648563385, 0.0014671545941382647, 0.0002913349017035216, 0.0025850010570138693, 0.09574876725673676, 0.0], [0.9386117458343506, 0.00022248072491493076, 0.0006146605010144413, 0.001549550681374967, 0.030689243227243423, 0.02831234224140644]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9999959468841553, 4.042674390802858e-06, 0.0, 0.0, 0.0, 0.0], [0.9982761144638062, 3.2613336315989727e-06, 0.0017206775955855846, 0.0, 0.0, 0.0], [0.9998809099197388, 5.328825736228282e-08, 6.376306487254624e-07, 0.0001184718421427533, 0.0, 0.0], [0.9996154308319092, 3.47315932458514e-07, 3.892111877235038e-08, 4.468437566629291e-07, 0.0003836959949694574, 0.0], [0.9994840621948242, 1.6550142589721872e-08, 2.871566806561532e-08, 1.0638365210979828e-06, 0.00021266820840537548, 0.00030212124693207443]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9514135122299194, 0.04858648031949997, 0.0, 0.0, 0.0, 0.0], [0.574993908405304, 0.3902817368507385, 0.03472433611750603, 0.0, 0.0, 0.0], [0.7442327737808228, 0.1752404421567917, 0.07564744353294373, 0.004879289772361517, 0.0, 0.0], [0.5232067108154297, 0.09429281949996948, 0.11381927132606506, 0.19979366660118103, 0.06888758391141891, 0.0], [0.4747253656387329, 0.056365687400102615, 0.04530392214655876, 0.06967338174581528, 0.30980202555656433, 0.04412958398461342]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8734649419784546, 0.1265350729227066, 0.0, 0.0, 0.0, 0.0], [0.6097922921180725, 0.35417166352272034, 0.036035966128110886, 0.0, 0.0, 0.0], [0.45984140038490295, 0.38697922229766846, 0.09960115700960159, 0.05357823148369789, 0.0, 0.0], [0.572220504283905, 0.23636272549629211, 0.08344549685716629, 0.06921910494565964, 0.038752175867557526, 0.0], [0.514357328414917, 0.16723060607910156, 0.09019400179386139, 0.07654452323913574, 0.10578082501888275, 0.04589270055294037]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.981228768825531, 0.018771247938275337, 0.0, 0.0, 0.0, 0.0], [0.6142950057983398, 0.35039687156677246, 0.0353081114590168, 0.0, 0.0, 0.0], [0.5770686268806458, 0.3285844624042511, 0.05508274585008621, 0.03926417976617813, 0.0, 0.0], [0.17188261449337006, 0.011042488738894463, 0.054578930139541626, 0.7326573133468628, 0.029838675633072853, 0.0], [0.37830254435539246, 0.017070047557353973, 0.021754169836640358, 0.44096678495407104, 0.06093871220946312, 0.08096770942211151]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9923112988471985, 0.0076887100003659725, 0.0, 0.0, 0.0, 0.0], [0.9498788118362427, 0.01670972816646099, 0.03341144695878029, 0.0, 0.0, 0.0], [0.9961296319961548, 0.0008787262486293912, 0.0006868175696581602, 0.0023048429284244776, 0.0, 0.0], [0.9935756325721741, 0.0032634977251291275, 0.000999384792521596, 0.0002793250896502286, 0.0018820626428350806, 0.0], [0.9907532930374146, 0.00021344258857425302, 0.0004595253267325461, 0.0007905654492788017, 0.00442475313320756, 0.0033583587501198053]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9647741317749023, 0.03522586449980736, 0.0, 0.0, 0.0, 0.0], [0.8194136023521423, 0.13654311001300812, 0.04404330253601074, 0.0, 0.0, 0.0], [0.7584235668182373, 0.006878924090415239, 0.20653478801250458, 0.028162678703665733, 0.0, 0.0], [0.5298117995262146, 0.002678808057680726, 0.07857982069253922, 0.35983842611312866, 0.029091155156493187, 0.0], [0.754441499710083, 0.00036781918606720865, 0.0019713419023901224, 0.0032400439959019423, 0.19423416256904602, 0.045745160430669785]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9749132394790649, 0.02508677914738655, 0.0, 0.0, 0.0, 0.0], [0.9306473135948181, 0.05705645680427551, 0.012296212837100029, 0.0, 0.0, 0.0], [0.9305253624916077, 0.05277096852660179, 0.011119387112557888, 0.005584376864135265, 0.0, 0.0], [0.8863321542739868, 0.012924176640808582, 0.01772472821176052, 0.06150190532207489, 0.021517064422369003, 0.0], [0.7916851043701172, 0.015036048367619514, 0.031747814267873764, 0.03392194211483002, 0.03707972168922424, 0.09052935987710953]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9608504772186279, 0.03914956748485565, 0.0, 0.0, 0.0, 0.0], [0.9121270179748535, 0.0225764662027359, 0.06529650092124939, 0.0, 0.0, 0.0], [0.9364106059074402, 0.015584406442940235, 0.024545013904571533, 0.02345995232462883, 0.0, 0.0], [0.9454619288444519, 0.006762223318219185, 0.022026218473911285, 0.00913779903203249, 0.016611794009804726, 0.0], [0.8346170783042908, 0.0018816773081198335, 0.005609007552266121, 0.018873529508709908, 0.12449172884225845, 0.01452699489891529]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9964227080345154, 0.003577274037525058, 0.0, 0.0, 0.0, 0.0], [0.9713929891586304, 0.02445298247039318, 0.004154051188379526, 0.0, 0.0, 0.0], [0.9735793471336365, 0.019003307446837425, 0.0036644041538238525, 0.003752927528694272, 0.0, 0.0], [0.9586312174797058, 0.007116167340427637, 0.009218405932188034, 0.02272568643093109, 0.0023084785789251328, 0.0], [0.973607063293457, 0.008490562438964844, 0.003251239424571395, 0.003606440033763647, 0.0048774476163089275, 0.006167203187942505]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9759878516197205, 0.02401207946240902, 0.0, 0.0, 0.0, 0.0], [0.9460638165473938, 0.04211374744772911, 0.011822420172393322, 0.0, 0.0, 0.0], [0.8446815609931946, 0.042930979281663895, 0.052181802690029144, 0.06020564213395119, 0.0, 0.0], [0.9378370046615601, 0.03354860842227936, 0.008826478384435177, 0.0028792452067136765, 0.016908586025238037, 0.0], [0.8124936819076538, 0.0269674900919199, 0.05999195575714111, 0.03445734083652496, 0.011011873371899128, 0.05507773160934448]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9001209735870361, 0.09987906366586685, 0.0, 0.0, 0.0, 0.0], [0.6271924376487732, 0.07988706976175308, 0.29292047023773193, 0.0, 0.0, 0.0], [0.7624086141586304, 0.02734416350722313, 0.038679495453834534, 0.17156772315502167, 0.0, 0.0], [0.7995964288711548, 0.014336240477859974, 0.01437566988170147, 0.025438444688916206, 0.14625321328639984, 0.0], [0.7851970791816711, 0.04204041510820389, 0.0252536628395319, 0.029083900153636932, 0.02930637076497078, 0.08911861479282379]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954467415809631, 0.004553188104182482, 0.0, 0.0, 0.0, 0.0], [0.9356000423431396, 0.044767431914806366, 0.019632453098893166, 0.0, 0.0, 0.0], [0.5605552196502686, 0.09861954301595688, 0.29983270168304443, 0.04099256917834282, 0.0, 0.0], [0.5893716812133789, 0.11000912636518478, 0.08033595234155655, 0.16754071414470673, 0.052742499858140945, 0.0], [0.22306011617183685, 0.05680781602859497, 0.05467957630753517, 0.24734029173851013, 0.31112343072891235, 0.10698885470628738]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9301450252532959, 0.0698549896478653, 0.0, 0.0, 0.0, 0.0], [0.8936473727226257, 0.08535744249820709, 0.02099512703716755, 0.0, 0.0, 0.0], [0.8404536247253418, 0.10619217902421951, 0.023636844009160995, 0.029717376455664635, 0.0, 0.0], [0.8927386403083801, 0.024784628301858902, 0.008319017477333546, 0.05165451765060425, 0.022503133863210678, 0.0], [0.8646613359451294, 0.009503128007054329, 0.0024329759180545807, 0.04796736314892769, 0.04273195192217827, 0.032703179866075516]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9859625697135925, 0.014037422835826874, 0.0, 0.0, 0.0, 0.0], [0.9702038764953613, 0.01680702343583107, 0.012989108450710773, 0.0, 0.0, 0.0], [0.9524770379066467, 0.01606440730392933, 0.013456220738589764, 0.018002288416028023, 0.0, 0.0], [0.9332927465438843, 0.018971994519233704, 0.020146913826465607, 0.017023799940943718, 0.010564584285020828, 0.0], [0.9113592505455017, 0.012528584338724613, 0.02209623157978058, 0.017518600448966026, 0.018517879769206047, 0.017979370430111885]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9681767821311951, 0.031823206692934036, 0.0, 0.0, 0.0, 0.0], [0.909641683101654, 0.07916704565286636, 0.011191285215318203, 0.0, 0.0, 0.0], [0.8379938006401062, 0.13078218698501587, 0.012140980921685696, 0.0190830510109663, 0.0, 0.0], [0.9116524457931519, 0.054519571363925934, 0.009499364532530308, 0.0074658701196312904, 0.016862764954566956, 0.0], [0.8510292172431946, 0.07338184863328934, 0.008022483438253403, 0.00908315647393465, 0.04261001572012901, 0.015873229131102562]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9799023866653442, 0.02009764313697815, 0.0, 0.0, 0.0, 0.0], [0.9558743238449097, 0.029063154011964798, 0.015062526799738407, 0.0, 0.0, 0.0], [0.7943134903907776, 0.060740746557712555, 0.06907663494348526, 0.07586921006441116, 0.0, 0.0], [0.5494319796562195, 0.03154705464839935, 0.05482025071978569, 0.0578809417784214, 0.30631980299949646, 0.0], [0.6453986763954163, 0.010770875960588455, 0.017528092488646507, 0.021579833701252937, 0.249582439661026, 0.05514010041952133]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9506809115409851, 0.0493190623819828, 0.0, 0.0, 0.0, 0.0], [0.8553215265274048, 0.09256266057491302, 0.05211571976542473, 0.0, 0.0, 0.0], [0.8508530855178833, 0.04734566807746887, 0.04417697712779045, 0.05762428790330887, 0.0, 0.0], [0.769712507724762, 0.027885742485523224, 0.031017223373055458, 0.06842542439699173, 0.10295906662940979, 0.0], [0.7931907773017883, 0.04052164778113365, 0.029241811484098434, 0.04478137567639351, 0.048946961760520935, 0.04331738129258156]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9770310521125793, 0.022968975827097893, 0.0, 0.0, 0.0, 0.0], [0.9429819583892822, 0.017321424558758736, 0.039696723222732544, 0.0, 0.0, 0.0], [0.9144344925880432, 0.008583568967878819, 0.013035798445343971, 0.06394603103399277, 0.0, 0.0], [0.9222429990768433, 0.0036440384574234486, 0.0037402792368084192, 0.010410374030470848, 0.059962350875139236, 0.0], [0.9198879599571228, 0.0030822583939880133, 0.0034827394410967827, 0.004206802695989609, 0.021254297345876694, 0.048085931688547134]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9774582386016846, 0.022541804239153862, 0.0, 0.0, 0.0, 0.0], [0.8929325342178345, 0.07475457340478897, 0.03231294825673103, 0.0, 0.0, 0.0], [0.8423509001731873, 0.059802666306495667, 0.03740093484520912, 0.060445431619882584, 0.0, 0.0], [0.7674630284309387, 0.035363294184207916, 0.04215523228049278, 0.06658652424812317, 0.08843188732862473, 0.0], [0.6182613968849182, 0.01611054502427578, 0.020167633891105652, 0.038688939064741135, 0.23146985471248627, 0.07530158758163452]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9634859561920166, 0.036514006555080414, 0.0, 0.0, 0.0, 0.0], [0.4363924264907837, 0.5226381421089172, 0.0409693717956543, 0.0, 0.0, 0.0], [0.36086317896842957, 0.35129547119140625, 0.26551011204719543, 0.02233118936419487, 0.0, 0.0], [0.3942912817001343, 0.021704476326704025, 0.07794322818517685, 0.3716898262500763, 0.13437117636203766, 0.0], [0.6310707926750183, 0.01698385737836361, 0.025941932573914528, 0.08615948259830475, 0.21832098066806793, 0.021522969007492065]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9988250136375427, 0.0011750120902433991, 0.0, 0.0, 0.0, 0.0], [0.9944871068000793, 0.0004826366784982383, 0.00503023574128747, 0.0, 0.0, 0.0], [0.9981209635734558, 2.7051681172451936e-05, 0.00011307386739645153, 0.0017389492131769657, 0.0, 0.0], [0.9982240796089172, 6.836546526756138e-05, 0.00010199946700595319, 6.0283709899522364e-05, 0.0015453151427209377, 0.0], [0.9982888102531433, 1.0552126923357719e-06, 3.278084477642551e-05, 0.0001303898898186162, 0.0006605913513340056, 0.000886370544321835]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9936710596084595, 0.006328925024718046, 0.0, 0.0, 0.0, 0.0], [0.9727686047554016, 0.0018561407923698425, 0.02537521719932556, 0.0, 0.0, 0.0], [0.9724299907684326, 0.0019586130511015654, 0.011192455887794495, 0.01441890373826027, 0.0, 0.0], [0.9782041311264038, 0.0009589171386323869, 0.0018706527771428227, 0.006326586939394474, 0.012639714404940605, 0.0], [0.9592596888542175, 0.0024555076379328966, 0.0016124167013913393, 0.005019662901759148, 0.006687106564640999, 0.024965673685073853]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629000425338745, 0.03710000589489937, 0.0, 0.0, 0.0, 0.0], [0.368019700050354, 0.6152254939079285, 0.016754835844039917, 0.0, 0.0, 0.0], [0.3173525333404541, 0.6139997839927673, 0.05375159531831741, 0.014896123670041561, 0.0, 0.0], [0.48987340927124023, 0.2107142210006714, 0.04693015292286873, 0.20700448751449585, 0.04547775909304619, 0.0], [0.48774290084838867, 0.1769522726535797, 0.06915199756622314, 0.09849286824464798, 0.12091417610645294, 0.046745769679546356]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9794418811798096, 0.020558105781674385, 0.0, 0.0, 0.0, 0.0], [0.6677899956703186, 0.3103238642215729, 0.021886125206947327, 0.0, 0.0, 0.0], [0.7118754982948303, 0.11108537018299103, 0.14187413454055786, 0.035164978355169296, 0.0, 0.0], [0.4501444399356842, 0.04036037623882294, 0.04045833647251129, 0.38857150077819824, 0.08046536147594452, 0.0], [0.49346134066581726, 0.013696889393031597, 0.008126810193061829, 0.1307455599308014, 0.30861467123031616, 0.045354776084423065]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846054315567017, 0.015394587069749832, 0.0, 0.0, 0.0, 0.0], [0.9806739091873169, 0.007713802624493837, 0.011612347327172756, 0.0, 0.0, 0.0], [0.9326629042625427, 0.019578414037823677, 0.024103600531816483, 0.02365502528846264, 0.0, 0.0], [0.9422017931938171, 0.000953898299485445, 0.0010898022446781397, 0.003193383105099201, 0.05256117507815361, 0.0], [0.9352929592132568, 0.0010279379785060883, 0.004444452468305826, 0.0016371518140658736, 0.010591037571430206, 0.04700646921992302]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9985783100128174, 0.0014216642593964934, 0.0, 0.0, 0.0, 0.0], [0.9893348813056946, 0.001117888605222106, 0.009547184221446514, 0.0, 0.0, 0.0], [0.9979978203773499, 7.997058128239587e-05, 0.0001321886375080794, 0.001790043548680842, 0.0, 0.0], [0.9986976385116577, 4.1044077079277486e-05, 3.868332441925304e-06, 2.367625893384684e-05, 0.0012337120715528727, 0.0], [0.9971563816070557, 1.852221612352878e-05, 1.8826470977728604e-06, 2.790033795463387e-05, 0.0006533504347316921, 0.0021419881377369165]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9768233299255371, 0.02317671850323677, 0.0, 0.0, 0.0, 0.0], [0.9194679856300354, 0.05088174715638161, 0.029650233685970306, 0.0, 0.0, 0.0], [0.8474555015563965, 0.06100161001086235, 0.0437236987054348, 0.04781917482614517, 0.0, 0.0], [0.801162600517273, 0.04186687245965004, 0.043757982552051544, 0.04189474880695343, 0.07131775468587875, 0.0], [0.8031873106956482, 0.0245048888027668, 0.017323536798357964, 0.04744400829076767, 0.06109917163848877, 0.046441033482551575]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9829427003860474, 0.017057321965694427, 0.0, 0.0, 0.0, 0.0], [0.886372983455658, 0.09492714703083038, 0.01869990862905979, 0.0, 0.0, 0.0], [0.9231082797050476, 0.03696348890662193, 0.03219844773411751, 0.007729684002697468, 0.0, 0.0], [0.9068528413772583, 0.016046611592173576, 0.014310491271317005, 0.04543787240982056, 0.01735224574804306, 0.0], [0.6555972695350647, 0.050910260528326035, 0.028384851291775703, 0.1256551593542099, 0.10546848177909851, 0.03398388996720314]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502319693565369, 0.04976798966526985, 0.0, 0.0, 0.0, 0.0], [0.8829870223999023, 0.10009586066007614, 0.016917143017053604, 0.0, 0.0, 0.0], [0.80574631690979, 0.14463508129119873, 0.03018917515873909, 0.019429394975304604, 0.0, 0.0], [0.8706235885620117, 0.03244053199887276, 0.026951542124152184, 0.04410284012556076, 0.02588149905204773, 0.0], [0.6883642673492432, 0.009681438095867634, 0.016449328511953354, 0.09871133416891098, 0.08971213549375534, 0.09708155691623688]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9792683720588684, 0.020731620490550995, 0.0, 0.0, 0.0, 0.0], [0.9523285627365112, 0.02593371830880642, 0.021737750619649887, 0.0, 0.0, 0.0], [0.9144353866577148, 0.01767120137810707, 0.02235855720937252, 0.04553478583693504, 0.0, 0.0], [0.9448292851448059, 0.006467591505497694, 0.006386075634509325, 0.03263094648718834, 0.009686185047030449, 0.0], [0.9347903728485107, 0.007862498983740807, 0.007788185030221939, 0.02143281325697899, 0.008491134271025658, 0.019634833559393883]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.983370304107666, 0.016629742458462715, 0.0, 0.0, 0.0, 0.0], [0.963111400604248, 0.009229922667145729, 0.027658650651574135, 0.0, 0.0, 0.0], [0.9706627130508423, 0.004149414133280516, 0.006813078187406063, 0.01837468333542347, 0.0, 0.0], [0.987951934337616, 0.002165879588574171, 0.0003490079543553293, 0.0015838086837902665, 0.00794939324259758, 0.0], [0.9457951188087463, 0.01458354014903307, 0.0003652939631137997, 0.0009569547255523503, 0.013621637597680092, 0.024677468463778496]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9878058433532715, 0.012194144539535046, 0.0, 0.0, 0.0, 0.0], [0.87103670835495, 0.09448167681694031, 0.03448163717985153, 0.0, 0.0, 0.0], [0.6309786438941956, 0.11090352386236191, 0.19230222702026367, 0.06581560522317886, 0.0, 0.0], [0.536048412322998, 0.04618947580456734, 0.1360531747341156, 0.26455560326576233, 0.017153318971395493, 0.0], [0.8287516832351685, 0.02373271808028221, 0.020080402493476868, 0.07245294004678726, 0.030431287363171577, 0.024550972506403923]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8995685577392578, 0.10043143481016159, 0.0, 0.0, 0.0, 0.0], [0.27034327387809753, 0.6504329442977905, 0.07922383397817612, 0.0, 0.0, 0.0], [0.20541630685329437, 0.5892508625984192, 0.18085940182209015, 0.02447350136935711, 0.0, 0.0], [0.5573862195014954, 0.177413210272789, 0.088068388402462, 0.09881831705570221, 0.07831386476755142, 0.0], [0.592290997505188, 0.08700627833604813, 0.05643303692340851, 0.056858859956264496, 0.12181548029184341, 0.08559539169073105]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.931638777256012, 0.06836120039224625, 0.0, 0.0, 0.0, 0.0], [0.9572949409484863, 0.026243247091770172, 0.016461802646517754, 0.0, 0.0, 0.0], [0.9880545735359192, 0.004273299127817154, 0.002954578958451748, 0.004717648029327393, 0.0, 0.0], [0.99403977394104, 0.0009413357474841177, 0.00047398044262081385, 0.00011646909115370363, 0.004428436513990164, 0.0], [0.9806036353111267, 2.546862197050359e-05, 0.00016239289834629744, 0.00014764144725631922, 0.0013442367780953646, 0.017716728150844574]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.993178129196167, 0.006821870803833008, 0.0, 0.0, 0.0, 0.0], [0.9756524562835693, 0.013184125535190105, 0.011163492687046528, 0.0, 0.0, 0.0], [0.9418964385986328, 0.004721764009445906, 0.0023818116169422865, 0.05099993944168091, 0.0, 0.0], [0.9905040860176086, 0.0022848148364573717, 6.198474875418469e-05, 0.0005984468152746558, 0.006550672464072704, 0.0], [0.9697660207748413, 0.0008878824883140624, 0.00023466601851396263, 0.0017040767706930637, 0.004128345753997564, 0.023278921842575073]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9716231822967529, 0.02837679162621498, 0.0, 0.0, 0.0, 0.0], [0.9223621487617493, 0.028907138854265213, 0.04873068258166313, 0.0, 0.0, 0.0], [0.8426313400268555, 0.023872144520282745, 0.047481436282396317, 0.08601505309343338, 0.0, 0.0], [0.85211181640625, 0.020744264125823975, 0.04494625702500343, 0.05765020102262497, 0.02454744279384613, 0.0], [0.8800727725028992, 0.022448452189564705, 0.018235687166452408, 0.019254818558692932, 0.015854254364967346, 0.04413411021232605]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9412726759910583, 0.058727364987134933, 0.0, 0.0, 0.0, 0.0], [0.9163140058517456, 0.05759183689951897, 0.026094170287251472, 0.0, 0.0, 0.0], [0.839242160320282, 0.0576905831694603, 0.013829091563820839, 0.08923812210559845, 0.0, 0.0], [0.8987159132957458, 0.013477819971740246, 0.0003456466947682202, 0.0032987503800541162, 0.0841619074344635, 0.0], [0.8701689839363098, 0.002700856886804104, 0.0014349964912980795, 0.005666173063218594, 0.08874330669641495, 0.03128569573163986]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9656726121902466, 0.03432736173272133, 0.0, 0.0, 0.0, 0.0], [0.9178615212440491, 0.06225801631808281, 0.019880563020706177, 0.0, 0.0, 0.0], [0.8233142495155334, 0.0628240779042244, 0.03670453652739525, 0.07715719193220139, 0.0, 0.0], [0.8501748442649841, 0.03816916421055794, 0.03196500241756439, 0.05160141363739967, 0.028089623898267746, 0.0], [0.6572404503822327, 0.05877385288476944, 0.04336012154817581, 0.09013212472200394, 0.08146592229604721, 0.06902746111154556]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9162060022354126, 0.083794005215168, 0.0, 0.0, 0.0, 0.0], [0.9451773762702942, 0.040992800146341324, 0.013829780742526054, 0.0, 0.0, 0.0], [0.8928354978561401, 0.053686901926994324, 0.017596911638975143, 0.035880785435438156, 0.0, 0.0], [0.8337052464485168, 0.04799613356590271, 0.03351316228508949, 0.04680856689810753, 0.037976887077093124, 0.0], [0.8167197704315186, 0.06337109208106995, 0.013286222703754902, 0.020469728857278824, 0.025292275473475456, 0.06086099520325661]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9525134563446045, 0.047486551105976105, 0.0, 0.0, 0.0, 0.0], [0.3019862174987793, 0.6520944237709045, 0.04591928794980049, 0.0, 0.0, 0.0], [0.2855839431285858, 0.5569514036178589, 0.14447414875030518, 0.012990470044314861, 0.0, 0.0], [0.8438048958778381, 0.03225119784474373, 0.03954283148050308, 0.06848110258579254, 0.015920016914606094, 0.0], [0.6664943695068359, 0.06095915660262108, 0.04064341261982918, 0.06804454326629639, 0.09186326712369919, 0.07199524343013763]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9682655334472656, 0.03173443675041199, 0.0, 0.0, 0.0, 0.0], [0.738522469997406, 0.22856806218624115, 0.03290953114628792, 0.0, 0.0, 0.0], [0.5946678519248962, 0.23033133149147034, 0.14867620170116425, 0.026324663311243057, 0.0, 0.0], [0.633925199508667, 0.058130376040935516, 0.09654313325881958, 0.14291979372501373, 0.06848157942295074, 0.0], [0.4037562310695648, 0.08945389837026596, 0.07635080814361572, 0.2558715045452118, 0.1433037519454956, 0.03126386180520058]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9869793653488159, 0.013020625337958336, 0.0, 0.0, 0.0, 0.0], [0.8631383776664734, 0.11056670546531677, 0.026294900104403496, 0.0, 0.0, 0.0], [0.9488076567649841, 0.028615130111575127, 0.0065356092527508736, 0.016041582450270653, 0.0, 0.0], [0.9672170877456665, 0.006604980677366257, 0.00045171534293331206, 0.0048444150015711784, 0.02088177390396595, 0.0], [0.9354620575904846, 0.02047809027135372, 0.0011700252071022987, 0.007056962698698044, 0.016318202018737793, 0.019514672458171844]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846674203872681, 0.015332573093473911, 0.0, 0.0, 0.0, 0.0], [0.9052745699882507, 0.08373618870973587, 0.010989260859787464, 0.0, 0.0, 0.0], [0.8145952224731445, 0.04283708333969116, 0.10568203777074814, 0.03688560053706169, 0.0, 0.0], [0.2351973056793213, 0.012018326669931412, 0.05280076339840889, 0.6516193747520447, 0.048364195972681046, 0.0], [0.31818506121635437, 0.01863221451640129, 0.039481643587350845, 0.3755551874637604, 0.20787331461906433, 0.040272586047649384]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9811733365058899, 0.01882672682404518, 0.0, 0.0, 0.0, 0.0], [0.861893892288208, 0.06479160487651825, 0.07331453263759613, 0.0, 0.0, 0.0], [0.7664538025856018, 0.07330423593521118, 0.10353542119264603, 0.05670652166008949, 0.0, 0.0], [0.8128498196601868, 0.0321548730134964, 0.05900559946894646, 0.05416519567370415, 0.04182438552379608, 0.0], [0.8687861561775208, 0.026987716555595398, 0.02046993374824524, 0.016297345981001854, 0.03218373283743858, 0.035275135189294815]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9264581799507141, 0.07354187220335007, 0.0, 0.0, 0.0, 0.0], [0.8403536081314087, 0.06373773515224457, 0.09590869396924973, 0.0, 0.0, 0.0], [0.7330989837646484, 0.06451132148504257, 0.10380109399557114, 0.09858868271112442, 0.0, 0.0], [0.9143612384796143, 0.008257808163762093, 0.0073203956708312035, 0.017966309562325478, 0.05209435895085335, 0.0], [0.8971917033195496, 0.008555500768125057, 0.007019444834440947, 0.014860525727272034, 0.0339975580573082, 0.03837524726986885]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9180349111557007, 0.08196508139371872, 0.0, 0.0, 0.0, 0.0], [0.8328663110733032, 0.1219903975725174, 0.04514330253005028, 0.0, 0.0, 0.0], [0.7994157671928406, 0.08744129538536072, 0.03605783358216286, 0.07708507031202316, 0.0, 0.0], [0.8809850811958313, 0.02074955217540264, 0.020554589107632637, 0.01712079532444477, 0.060589853674173355, 0.0], [0.7453036904335022, 0.04433392733335495, 0.022549254819750786, 0.03315271809697151, 0.033570535480976105, 0.1210898905992508]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9867060780525208, 0.013293919153511524, 0.0, 0.0, 0.0, 0.0], [0.982177734375, 0.012414089404046535, 0.0054081035777926445, 0.0, 0.0, 0.0], [0.96304851770401, 0.015290765091776848, 0.010345722548663616, 0.011315010488033295, 0.0, 0.0], [0.921357274055481, 0.01413238886743784, 0.017639128491282463, 0.016567641869187355, 0.030303535982966423, 0.0], [0.9373326301574707, 0.009064320474863052, 0.007548393681645393, 0.006576480343937874, 0.011827622540295124, 0.0276504959911108]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9951003193855286, 0.004899636376649141, 0.0, 0.0, 0.0, 0.0], [0.9476009607315063, 0.0414077453315258, 0.010991236194968224, 0.0, 0.0, 0.0], [0.9142175316810608, 0.023523783311247826, 0.039145033806562424, 0.023113638162612915, 0.0, 0.0], [0.953473687171936, 0.008932948112487793, 0.01527277659624815, 0.007908261381089687, 0.01441231183707714, 0.0], [0.9427104592323303, 0.008233043365180492, 0.00465096440166235, 0.004178092814981937, 0.0054634977132081985, 0.03476399928331375]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9543376564979553, 0.045662373304367065, 0.0, 0.0, 0.0, 0.0], [0.9696041941642761, 0.01954755000770092, 0.01084830705076456, 0.0, 0.0, 0.0], [0.9710449576377869, 0.012425432913005352, 0.00806890707463026, 0.008460727520287037, 0.0, 0.0], [0.9726192951202393, 0.0026976638473570347, 0.0004483149677980691, 0.0013814783887937665, 0.022853175178170204, 0.0], [0.9675466418266296, 0.00961342267692089, 0.0032030262518674135, 0.0042488230392336845, 0.0074422284960746765, 0.007945878431200981]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887008666992188, 0.011299073696136475, 0.0, 0.0, 0.0, 0.0], [0.9382636547088623, 0.042042046785354614, 0.019694261252880096, 0.0, 0.0, 0.0], [0.8351991176605225, 0.03487854823470116, 0.05134488269686699, 0.07857739180326462, 0.0, 0.0], [0.9042680263519287, 0.010541534051299095, 0.016426660120487213, 0.025921886786818504, 0.0428418405354023, 0.0], [0.891313910484314, 0.008912678807973862, 0.00501069612801075, 0.008175642229616642, 0.013514752499759197, 0.07307230681180954]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8693918585777283, 0.1306082010269165, 0.0, 0.0, 0.0, 0.0], [0.35080042481422424, 0.6063504219055176, 0.042849235236644745, 0.0, 0.0, 0.0], [0.3547583222389221, 0.3502003252506256, 0.24722403287887573, 0.04781735688447952, 0.0, 0.0], [0.35370761156082153, 0.03527713194489479, 0.09567099064588547, 0.449795126914978, 0.06554913520812988, 0.0], [0.41325968503952026, 0.09055472165346146, 0.052865613251924515, 0.1746797263622284, 0.1738494336605072, 0.09479081630706787]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629756212234497, 0.037024397403001785, 0.0, 0.0, 0.0, 0.0], [0.9756426811218262, 0.019658580422401428, 0.004698706325143576, 0.0, 0.0, 0.0], [0.9775736331939697, 0.013286329805850983, 0.002559035550802946, 0.006581075489521027, 0.0, 0.0], [0.9870142936706543, 0.007388250902295113, 0.0009579145698808134, 0.0018318254733458161, 0.0028077505994588137, 0.0], [0.9409247040748596, 0.016633691266179085, 0.0022979003842920065, 0.005890632979571819, 0.005512909963726997, 0.02874019555747509]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9628273248672485, 0.03717269375920296, 0.0, 0.0, 0.0, 0.0], [0.9582238793373108, 0.024641714990139008, 0.017134323716163635, 0.0, 0.0, 0.0], [0.93513023853302, 0.01533149741590023, 0.014810922555625439, 0.0347273163497448, 0.0, 0.0], [0.9225173592567444, 0.010528713464736938, 0.011010120622813702, 0.019440004602074623, 0.036503810435533524, 0.0], [0.8420169949531555, 0.043571874499320984, 0.007488242816179991, 0.01496151927858591, 0.023852799087762833, 0.06810850650072098]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9926387071609497, 0.007361325901001692, 0.0, 0.0, 0.0, 0.0], [0.9957393407821655, 0.003346974030137062, 0.0009136885637417436, 0.0, 0.0, 0.0], [0.9869900345802307, 0.001974782906472683, 0.0015245474642142653, 0.009510640986263752, 0.0, 0.0], [0.9933527708053589, 0.0010203201090916991, 0.0003433709207456559, 0.0010291049256920815, 0.00425435695797205, 0.0], [0.9749016761779785, 0.00043480066233314574, 0.00043065325007773936, 0.0012364318827167153, 0.0015347633743658662, 0.02146158739924431]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9897475242614746, 0.010252462700009346, 0.0, 0.0, 0.0, 0.0], [0.9790638089179993, 0.01650909334421158, 0.0044271135702729225, 0.0, 0.0, 0.0], [0.9521436095237732, 0.029432358220219612, 0.008943170309066772, 0.009480937384068966, 0.0, 0.0], [0.9395942091941833, 0.021510882303118706, 0.010278509929776192, 0.004555223509669304, 0.024061109870672226, 0.0], [0.9205077886581421, 0.016153603792190552, 0.01081855595111847, 0.016644351184368134, 0.014566346071660519, 0.021309340372681618]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9898501634597778, 0.010149819776415825, 0.0, 0.0, 0.0, 0.0], [0.9820910096168518, 0.006907487753778696, 0.01100151427090168, 0.0, 0.0, 0.0], [0.9684998393058777, 0.008987552486360073, 0.015342521481215954, 0.007170091383159161, 0.0, 0.0], [0.9274123311042786, 0.00948520191013813, 0.0220660287886858, 0.03222888335585594, 0.008807597681879997, 0.0], [0.9006659388542175, 0.021623671054840088, 0.013808254152536392, 0.009843875654041767, 0.00852135755121708, 0.045536916702985764]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954444169998169, 0.004555590450763702, 0.0, 0.0, 0.0, 0.0], [0.995254397392273, 0.0024602320045232773, 0.0022854781709611416, 0.0, 0.0, 0.0], [0.9862446188926697, 0.001516818767413497, 0.0040722922421991825, 0.0081663578748703, 0.0, 0.0], [0.9889963865280151, 0.0012260394869372249, 0.0007996344938874245, 0.0006774224457331002, 0.008300562389194965, 0.0], [0.9865204095840454, 0.0003942691837437451, 0.0009571714326739311, 0.0004954344476573169, 0.0009604907245375216, 0.010672236792743206]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9821295142173767, 0.017870459705591202, 0.0, 0.0, 0.0, 0.0], [0.7489436864852905, 0.2200269252061844, 0.03102932497859001, 0.0, 0.0, 0.0], [0.2854772210121155, 0.21125562489032745, 0.4787178933620453, 0.02454923279583454, 0.0, 0.0], [0.8056645393371582, 0.026974491775035858, 0.043028220534324646, 0.06993702799081802, 0.05439576134085655, 0.0], [0.33072200417518616, 0.022326502948999405, 0.01662714220583439, 0.08019431680440903, 0.4157475531101227, 0.1343824714422226]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9697749018669128, 0.030225154012441635, 0.0, 0.0, 0.0, 0.0], [0.9800567030906677, 0.015018866397440434, 0.004924519918859005, 0.0, 0.0, 0.0], [0.9237860441207886, 0.052764892578125, 0.0063024163246154785, 0.01714668609201908, 0.0, 0.0], [0.9451844096183777, 0.036180466413497925, 0.0019892053678631783, 0.00395871140062809, 0.012687263078987598, 0.0], [0.9633329510688782, 0.018662935122847557, 0.0030418343376368284, 0.007070871535688639, 0.00500940065830946, 0.0028820636216551065]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.987324595451355, 0.012675370089709759, 0.0, 0.0, 0.0, 0.0], [0.9904569983482361, 0.005541915539652109, 0.004001101944595575, 0.0, 0.0, 0.0], [0.9814971685409546, 0.004653455223888159, 0.003725287737324834, 0.0101240836083889, 0.0, 0.0], [0.9744365811347961, 0.004632247146219015, 0.0023799941409379244, 0.006518087349832058, 0.012033028528094292, 0.0], [0.9624497294425964, 0.0033743558451533318, 0.0013198574306443334, 0.0017275019781664014, 0.0029446815606206656, 0.028183799237012863]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9807674288749695, 0.01923258975148201, 0.0, 0.0, 0.0, 0.0], [0.9664245843887329, 0.015413926914334297, 0.01816139742732048, 0.0, 0.0, 0.0], [0.9632682204246521, 0.004538126289844513, 0.0029253901448100805, 0.029268160462379456, 0.0, 0.0], [0.9562349319458008, 0.0012223595986142755, 0.0005304060177877545, 0.00867142528295517, 0.033340804278850555, 0.0], [0.9657101035118103, 0.0009808256290853024, 0.0016686187591403723, 0.002634819597005844, 0.005866350140422583, 0.023139353841543198]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639713764190674, 0.03602859377861023, 0.0, 0.0, 0.0, 0.0], [0.9562800526618958, 0.033733122050762177, 0.009986846707761288, 0.0, 0.0, 0.0], [0.8539993166923523, 0.08073055744171143, 0.03334458917379379, 0.03192553296685219, 0.0, 0.0], [0.9547491073608398, 0.009605043567717075, 0.004146174993366003, 0.0020133228972554207, 0.029486412182450294, 0.0], [0.9331135749816895, 0.028699703514575958, 0.005477482918649912, 0.006368076894432306, 0.012613032944500446, 0.013728110119700432]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9392993450164795, 0.06070062145590782, 0.0, 0.0, 0.0, 0.0], [0.9298389554023743, 0.061895567923784256, 0.008265494368970394, 0.0, 0.0, 0.0], [0.8471819758415222, 0.0903506875038147, 0.017636122182011604, 0.04483122378587723, 0.0, 0.0], [0.885770320892334, 0.03918176144361496, 0.007867710664868355, 0.02276587300002575, 0.04441443085670471, 0.0], [0.8563281893730164, 0.10088986903429031, 0.006531456485390663, 0.008485931903123856, 0.007368435151875019, 0.020396223291754723]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8353266716003418, 0.1646733283996582, 0.0, 0.0, 0.0, 0.0], [0.616086483001709, 0.313764363527298, 0.07014919072389603, 0.0, 0.0, 0.0], [0.34316375851631165, 0.27584928274154663, 0.11966024339199066, 0.26132670044898987, 0.0, 0.0], [0.5908174514770508, 0.0502907820045948, 0.041665904223918915, 0.21994897723197937, 0.0972769483923912, 0.0], [0.8481414318084717, 0.06318086385726929, 0.014733674004673958, 0.05526731535792351, 0.009015018120408058, 0.009661629796028137]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9627320766448975, 0.037267882376909256, 0.0, 0.0, 0.0, 0.0], [0.7757525444030762, 0.17996230721473694, 0.044285181909799576, 0.0, 0.0, 0.0], [0.6317052245140076, 0.24380743503570557, 0.10925715416669846, 0.01523028127849102, 0.0, 0.0], [0.9539909958839417, 0.01818225346505642, 0.011601817794144154, 0.012299083173274994, 0.003925768658518791, 0.0], [0.4035692811012268, 0.1423754096031189, 0.05661242455244064, 0.19757378101348877, 0.09299232810735703, 0.10687679052352905]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9802618026733398, 0.01973816193640232, 0.0, 0.0, 0.0, 0.0], [0.98739093542099, 0.007800442166626453, 0.0048086862079799175, 0.0, 0.0, 0.0], [0.9283917546272278, 0.008301272988319397, 0.013305719941854477, 0.05000118911266327, 0.0, 0.0], [0.8981058597564697, 0.015591289848089218, 0.010177561081945896, 0.039986737072467804, 0.03613854944705963, 0.0], [0.9753499031066895, 0.00035433052107691765, 0.0005866037681698799, 0.0011877462966367602, 0.001075089443475008, 0.021446414291858673]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9295329451560974, 0.07046709209680557, 0.0, 0.0, 0.0, 0.0], [0.9361505508422852, 0.04116687551140785, 0.022682568058371544, 0.0, 0.0, 0.0], [0.8486817479133606, 0.05802813917398453, 0.0248562078922987, 0.06843385845422745, 0.0, 0.0], [0.8661177754402161, 0.022324735298752785, 0.010369136929512024, 0.02600197307765484, 0.07518628984689713, 0.0], [0.8074417114257812, 0.04438265413045883, 0.01849714107811451, 0.03357793390750885, 0.01856130361557007, 0.07753922790288925]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9680535197257996, 0.031946420669555664, 0.0, 0.0, 0.0, 0.0], [0.9693689942359924, 0.02568492479622364, 0.004946073051542044, 0.0, 0.0, 0.0], [0.9620568156242371, 0.022552471607923508, 0.005471337586641312, 0.009919446893036366, 0.0, 0.0], [0.9727528095245361, 0.010137113742530346, 0.0007573263137601316, 0.0028828817885369062, 0.01346979383379221, 0.0], [0.9624636769294739, 0.0031109012197703123, 0.0010007594246417284, 0.0019475924782454967, 0.00826623197644949, 0.02321087382733822]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8542509078979492, 0.1457490772008896, 0.0, 0.0, 0.0, 0.0], [0.9725967645645142, 0.014116342179477215, 0.013286887668073177, 0.0, 0.0, 0.0], [0.9257619976997375, 0.032572753727436066, 0.014612194150686264, 0.027053089812397957, 0.0, 0.0], [0.7923442721366882, 0.027304992079734802, 0.018806710839271545, 0.13854004442691803, 0.023003995418548584, 0.0], [0.6152051091194153, 0.026655351743102074, 0.029353095218539238, 0.05590879172086716, 0.11611329019069672, 0.1567644327878952]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9804654121398926, 0.019534548744559288, 0.0, 0.0, 0.0, 0.0], [0.9882452487945557, 0.007509480696171522, 0.004245338030159473, 0.0, 0.0, 0.0], [0.9584203958511353, 0.010963640175759792, 0.01045613270252943, 0.020159821957349777, 0.0, 0.0], [0.9604810476303101, 0.007182626519352198, 0.0030723433010280132, 0.006898912601172924, 0.022365078330039978, 0.0], [0.9668889045715332, 0.0032812952995300293, 0.00550055131316185, 0.004234095104038715, 0.005038043484091759, 0.01505721639841795]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9498194456100464, 0.05018055811524391, 0.0, 0.0, 0.0, 0.0], [0.9781363606452942, 0.01643003709614277, 0.00543356453999877, 0.0, 0.0, 0.0], [0.8618696928024292, 0.03609364852309227, 0.07555550336837769, 0.026481153443455696, 0.0, 0.0], [0.5449840426445007, 0.0154111972078681, 0.023516522720456123, 0.25743579864501953, 0.15865245461463928, 0.0], [0.9571872353553772, 0.003080392023548484, 0.001444687251932919, 0.006861586589366198, 0.014818808995187283, 0.01660732924938202]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6156567931175232, 0.3843432068824768, 0.0, 0.0, 0.0, 0.0], [0.36760643124580383, 0.4281637966632843, 0.20422977209091187, 0.0, 0.0, 0.0], [0.16471539437770844, 0.4136793613433838, 0.2509235441684723, 0.17068175971508026, 0.0, 0.0], [0.41844621300697327, 0.1524759978055954, 0.10305390506982803, 0.1107151210308075, 0.21530869603157043, 0.0], [0.19686952233314514, 0.2014622539281845, 0.1282723993062973, 0.09203261137008667, 0.09167557209730148, 0.2896876931190491]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9027363657951355, 0.09726360440254211, 0.0, 0.0, 0.0, 0.0], [0.973663330078125, 0.014004354365170002, 0.012332331389188766, 0.0, 0.0, 0.0], [0.8504455089569092, 0.05690591782331467, 0.03206094354391098, 0.06058768928050995, 0.0, 0.0], [0.766120433807373, 0.03530407324433327, 0.03433050960302353, 0.0967523530125618, 0.06749258935451508, 0.0], [0.8650373220443726, 0.020085325464606285, 0.01149807684123516, 0.018558409065008163, 0.018430322408676147, 0.06639060378074646]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9653082489967346, 0.03469177335500717, 0.0, 0.0, 0.0, 0.0], [0.9816323518753052, 0.014176106080412865, 0.004191514104604721, 0.0, 0.0, 0.0], [0.9275255799293518, 0.047372251749038696, 0.011528252623975277, 0.013573972508311272, 0.0, 0.0], [0.9293117523193359, 0.02583330124616623, 0.007227106485515833, 0.014300605282187462, 0.023327238857746124, 0.0], [0.8895061612129211, 0.04689629003405571, 0.004717099480330944, 0.006286595948040485, 0.0060901278629899025, 0.046503696590662]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8938028812408447, 0.10619710385799408, 0.0, 0.0, 0.0, 0.0], [0.8221707344055176, 0.06304486840963364, 0.11478440463542938, 0.0, 0.0, 0.0], [0.5047385692596436, 0.15375731885433197, 0.2277035266160965, 0.11380063742399216, 0.0, 0.0], [0.4082077145576477, 0.09066348522901535, 0.11696866899728775, 0.24553164839744568, 0.1386285275220871, 0.0], [0.7291039228439331, 0.06638886779546738, 0.023112820461392403, 0.03110307827591896, 0.05714328587055206, 0.09314810484647751]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9247531890869141, 0.07524681836366653, 0.0, 0.0, 0.0, 0.0], [0.8957375288009644, 0.0698956698179245, 0.03436674177646637, 0.0, 0.0, 0.0], [0.7924939393997192, 0.0960114598274231, 0.05509104207158089, 0.0564035065472126, 0.0, 0.0], [0.789150595664978, 0.07880311459302902, 0.038401514291763306, 0.05396990105509758, 0.03967490792274475, 0.0], [0.7807855606079102, 0.07993552833795547, 0.04253167659044266, 0.032342202961444855, 0.0178169384598732, 0.04658801108598709]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9480886459350586, 0.05191135033965111, 0.0, 0.0, 0.0, 0.0], [0.863694429397583, 0.04756231606006622, 0.0887431651353836, 0.0, 0.0, 0.0], [0.9341367483139038, 0.022224243730306625, 0.02262451872229576, 0.02101437933743, 0.0, 0.0], [0.9588143229484558, 0.008020943962037563, 0.00449007423594594, 0.005862299352884293, 0.022812390699982643, 0.0], [0.938591718673706, 0.021227749064564705, 0.004872452467679977, 0.010940167121589184, 0.009524567052721977, 0.014843398705124855]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9763734340667725, 0.023626619949936867, 0.0, 0.0, 0.0, 0.0], [0.9884802103042603, 0.005189398769289255, 0.0063303615897893906, 0.0, 0.0, 0.0], [0.9477092027664185, 0.017985178157687187, 0.0101566007360816, 0.02414904534816742, 0.0, 0.0], [0.9671926498413086, 0.006552821025252342, 0.003322785021737218, 0.005563333630561829, 0.017368415370583534, 0.0], [0.9584562182426453, 0.007502950727939606, 0.00513631384819746, 0.008071641437709332, 0.005997116211801767, 0.014835822395980358]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8840707540512085, 0.11592917889356613, 0.0, 0.0, 0.0, 0.0], [0.993125319480896, 0.005070820450782776, 0.0018038044217973948, 0.0, 0.0, 0.0], [0.9534158110618591, 0.023829098790884018, 0.007748990319669247, 0.01500610914081335, 0.0, 0.0], [0.9151289463043213, 0.010873105376958847, 0.013190964236855507, 0.011050431989133358, 0.04975655674934387, 0.0], [0.8769674301147461, 0.03385206311941147, 0.008486470207571983, 0.009969159960746765, 0.03468584269285202, 0.03603913262486458]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003709544544108212, 0.999629020690918, 0.0, 0.0, 0.0, 0.0], [6.525075150420889e-05, 0.37378257513046265, 0.6261522173881531, 0.0, 0.0, 0.0], [4.606056972988881e-05, 0.21050865948200226, 0.411596417427063, 0.37784892320632935, 0.0, 0.0], [4.753119355882518e-05, 0.11616958677768707, 0.23264272511005402, 0.3985331058502197, 0.2526070475578308, 0.0], [1.2476451729526161e-06, 0.14819666743278503, 0.15813177824020386, 0.30074360966682434, 0.1193905919790268, 0.27353614568710327]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.971555769443512, 0.028444228693842888, 0.0, 0.0, 0.0, 0.0], [0.9529062509536743, 0.03233089670538902, 0.014762827195227146, 0.0, 0.0, 0.0], [0.9343125820159912, 0.023513032123446465, 0.020498063415288925, 0.02167630009353161, 0.0, 0.0], [0.9529678225517273, 0.008551443926990032, 0.004359325394034386, 0.008064579218626022, 0.026056963950395584, 0.0], [0.9653592705726624, 0.008487657643854618, 0.0034992764703929424, 0.002721586264669895, 0.0032828878611326218, 0.016649404540657997]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8630780577659607, 0.1369219720363617, 0.0, 0.0, 0.0, 0.0], [0.7696154117584229, 0.08513359725475311, 0.14525099098682404, 0.0, 0.0, 0.0], [0.7133333683013916, 0.10170912742614746, 0.1193128377199173, 0.06564464420080185, 0.0, 0.0], [0.7186221480369568, 0.054442938417196274, 0.013868182897567749, 0.07808040082454681, 0.13498634099960327, 0.0], [0.7990152835845947, 0.05805584415793419, 0.009447015821933746, 0.01777045428752899, 0.0211385078728199, 0.0945729911327362]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9518988728523254, 0.04810105636715889, 0.0, 0.0, 0.0, 0.0], [0.8580657839775085, 0.029445689171552658, 0.11248862743377686, 0.0, 0.0, 0.0], [0.6577740907669067, 0.08513446152210236, 0.12613065540790558, 0.13096080720424652, 0.0, 0.0], [0.80873703956604, 0.03230154141783714, 0.018418151885271072, 0.06856143474578857, 0.07198182493448257, 0.0], [0.6683299541473389, 0.1328137218952179, 0.021880580112338066, 0.027877412736415863, 0.04923404008150101, 0.09986433386802673]]]], "left_text": ["No", ",", " I", " am", " your", " father"], "right_text": ["No", ",", " I", " am", " your", " father"]}], "default_filter": "0", "display_mode": "dark", "root_div_id": "bertviz-35ff1a37e60b46acbd7953cb14920dca", "include_layers": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "include_heads": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "total_heads": 12} is a template marker that is replaced by actual params.
        const config = {};

        const MIN_X = 0;
        const MIN_Y = 0;
        const DIV_WIDTH = 970;
        const THUMBNAIL_PADDING = 5;
        const DETAIL_WIDTH = 300;
        const DETAIL_ATTENTION_WIDTH = 140;
        const DETAIL_BOX_WIDTH = 80;
        const DETAIL_BOX_HEIGHT = 18;
        const DETAIL_PADDING = 15;
        const ATTN_PADDING = 0;
        const DETAIL_HEADING_HEIGHT = 25;
        const HEADING_TEXT_SIZE = 15;
        const HEADING_PADDING = 5;
        const TEXT_SIZE = 13;
        const TEXT_PADDING = 5;
        const LAYER_COLORS = d3.schemeCategory10;
        const PALETTE = {
            'light': {
                'text': 'black',
                'background': 'white',
                'highlight': '#F5F5F5'
            },
            'dark': {
                'text': '#ccc',
                'background': 'black',
                'highlight': '#222'
            }
        }

        function render() {

            // Set global state variables

            var attData = config.attention[config.filter];
            config.leftText = attData.left_text;
            config.rightText = attData.right_text;
            config.attn = attData.attn;
            config.numLayers = config.attn.length;
            config.numHeads = config.attn[0].length;
            config.thumbnailBoxHeight = 7 * (12 / config.totalHeads);
            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;
            config.thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;
            config.thumbnailWidth = (DIV_WIDTH - axisSize) / config.totalHeads;
            config.detailHeight = Math.max(config.leftText.length, config.rightText.length) * DETAIL_BOX_HEIGHT + 2 * DETAIL_PADDING + DETAIL_HEADING_HEIGHT;
            config.divHeight = Math.max(config.numLayers * config.thumbnailHeight + axisSize, config.detailHeight);

            const vis = $(`#${config.rootDivId} #vis`)
            vis.empty();
            vis.attr("height", config.divHeight);
            config.svg = d3.select(`#${config.rootDivId} #vis`)
                .append('svg')
                .attr("width", DIV_WIDTH)
                .attr("height", config.divHeight)
                .attr("fill", getBackgroundColor());

            renderAxisLabels();

            var i;
            var j;
            for (i = 0; i < config.numLayers; i++) {
                for (j = 0; j < config.numHeads; j++) {
                    renderThumbnail(i, j);
                }
            }
        }

        function renderAxisLabels() {
            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;
            const tableWidth = config.thumbnailWidth * config.heads.length;
            config.svg.append("text")
                .text("Heads")
                .attr("fill", "black")
                .attr("font-weight", "bold")
                .attr("font-size", HEADING_TEXT_SIZE + "px")
                .attr("x", axisSize + tableWidth / 2)
                .attr("text-anchor", "middle")
                .attr("y", 0)
                .attr("dy", HEADING_TEXT_SIZE);
            for (let i = 0; i < config.numHeads; i++) {
                config.svg.append("text")
                    .text(config.heads[i])
                    .attr("fill", "black")
                    .attr("font-size", TEXT_SIZE + "px")
                    .attr("x", axisSize + (i + .5) * config.thumbnailWidth)
                    .attr("text-anchor", "middle")
                    .attr("y", HEADING_TEXT_SIZE + HEADING_PADDING)
                    .attr("dy", TEXT_SIZE);
            }
            let x = 0;
            let y = axisSize + config.thumbnailHeight * config.layers.length / 2;
            console.log("x", x, y)
            config.svg.append("text")
                .text("Layers")
                .attr("fill", "black")
                .attr("font-weight", "bold")
                .attr("transform", "rotate(270, " + x  + ", " + y + ")")
                .attr("font-size", HEADING_TEXT_SIZE + "px")
                .attr("x", x)
                .attr("text-anchor", "middle")
                .attr("y", y)
                .attr("dy", HEADING_TEXT_SIZE);
            for (let i = 0; i < config.numLayers; i++) {
                x = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE; // HACK
                y = axisSize + (i + .5) * config.thumbnailHeight;
                config.svg.append("text")
                    .text(config.layers[i])
                    .attr("fill", "black")
                    .attr("font-size", TEXT_SIZE + "px")
                    .attr("x", x)
                    .attr("text-anchor", "end")
                    .attr("y", y)
                    .attr("dy", TEXT_SIZE / 2);
            }
        }


        function renderThumbnail(layerIndex, headIndex) {
            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING
            const x = headIndex * config.thumbnailWidth + axisSize;
            const y = layerIndex * config.thumbnailHeight + axisSize;
            renderThumbnailAttn(x, y, config.attn[layerIndex][headIndex], layerIndex, headIndex);
        }

        function renderDetail(att, layerIndex, headIndex) {
            const axisSize = TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;
            var xOffset = .8 * config.thumbnailWidth;
            var maxX = DIV_WIDTH;
            var maxY = config.divHeight - 3;
            var leftPos = axisSize + headIndex * config.thumbnailWidth;
            var x = leftPos + THUMBNAIL_PADDING + xOffset;
            if (x < MIN_X) {
                x = MIN_X;
            } else if (x + DETAIL_WIDTH > maxX) {
                x = leftPos + THUMBNAIL_PADDING - DETAIL_WIDTH + 8;
            }
            var posLeftText = x;
            var posAttention = posLeftText + DETAIL_BOX_WIDTH;
            var posRightText = posAttention + DETAIL_ATTENTION_WIDTH;
            var thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;
            var yOffset = 20;
            var y = layerIndex * thumbnailHeight + THUMBNAIL_PADDING + yOffset;
            if (y < MIN_Y) {
                y = MIN_Y;
            } else if (y + config.detailHeight > maxY) {
                y = maxY - config.detailHeight;
            }
            renderDetailFrame(x, y, layerIndex);
            y = y + DETAIL_PADDING;
            renderDetailHeading(x, y, layerIndex, headIndex);
            y = y + DETAIL_HEADING_HEIGHT;
            renderDetailText(config.leftText, "leftText", posLeftText, y , layerIndex);
            renderDetailAttn(posAttention, y, att, layerIndex, headIndex);
            renderDetailText(config.rightText, "rightText", posRightText, y, layerIndex);
        }

        function renderDetailHeading(x, y, layerIndex, headIndex) {
            var fillColor = getTextColor();
            config.svg.append("text")
                .classed("detail", true)
                .text('Layer ' + config.layers[layerIndex] + ", Head " + config.heads[headIndex])
                .attr("font-size", TEXT_SIZE + "px")
                .attr("font-weight", "bold")
                .style("cursor", "default")
                .style("-webkit-user-select", "none")
                .attr("fill", fillColor)
                .attr("x", x + DETAIL_WIDTH / 2)
                .attr("text-anchor", "middle")
                .attr("y", y)
                .attr("height", DETAIL_HEADING_HEIGHT)
                .attr("width", DETAIL_WIDTH)
                .attr("dy", HEADING_TEXT_SIZE);
        }

        function renderDetailText(text, id, x, y, layerIndex) {
            var tokenContainer = config.svg.append("svg:g")
                .classed("detail", true)
                .selectAll("g")
                .data(text)
                .enter()
                .append("g");

            var fillColor = getTextColor();

            tokenContainer.append("rect")
                .classed("highlight", true)
                .attr("fill", fillColor)
                .style("opacity", 0.0)
                .attr("height", DETAIL_BOX_HEIGHT)
                .attr("width", DETAIL_BOX_WIDTH)
                .attr("x", x)
                .attr("y", function (d, i) {
                    return y + i * DETAIL_BOX_HEIGHT;
                });

            var textContainer = tokenContainer.append("text")
                .classed("token", true)
                .text(function (d) {
                    return d;
                })
                .attr("font-size", TEXT_SIZE + "px")
                .style("cursor", "default")
                .style("-webkit-user-select", "none")
                .attr("fill", fillColor)
                .attr("x", x)
                .attr("y", function (d, i) {
                    return i * DETAIL_BOX_HEIGHT + y;
                })
                .attr("height", DETAIL_BOX_HEIGHT)
                .attr("width", DETAIL_BOX_WIDTH)
                .attr("dy", TEXT_SIZE);

            if (id == "leftText") {
                textContainer.style("text-anchor", "end")
                    .attr("dx", DETAIL_BOX_WIDTH - 2);
                tokenContainer.on("mouseover", function (d, index) {
                    highlightSelection(index);
                });
                tokenContainer.on("mouseleave", function () {
                    unhighlightSelection();
                });
            }
        }

        function highlightSelection(index) {
            config.svg.select("#leftText")
                .selectAll(".highlight")
                .style("opacity", function (d, i) {
                    return i == index ? 1.0 : 0.0;
                });
            config.svg.selectAll(".attn-line-group")
                .style("opacity", function (d, i) {
                    return i == index ? 1.0 : 0.0;
                });
        }

        function unhighlightSelection() {
            config.svg.select("#leftText")
                .selectAll(".highlight")
                .style("opacity", 0.0);
            config.svg.selectAll(".attn-line-group")
                .style("opacity", 1);
        }

        function renderThumbnailAttn(x, y, att, layerIndex, headIndex) {

            var attnContainer = config.svg.append("svg:g");

            var attnBackground = attnContainer.append("rect")
                .attr("id", 'attn_background_' + layerIndex + "_" + headIndex)
                .classed("attn_background", true)
                .attr("x", x)
                .attr("y", y)
                .attr("height", config.thumbnailHeight)
                .attr("width", config.thumbnailWidth)
                .attr("stroke-width", 2)
                .attr("stroke", getLayerColor(layerIndex))
                .attr("stroke-opacity", 0)
                .attr("fill", getBackgroundColor());
            var x1 = x + THUMBNAIL_PADDING;
            var x2 = x1 + config.thumbnailWidth - 14;
            var y1 = y + THUMBNAIL_PADDING;

            attnContainer.selectAll("g")
                .data(att)
                .enter()
                .append("g") // Add group for each source token
                .attr("source-index", function (d, i) { // Save index of source token
                    return i;
                })
                .selectAll("line")
                .data(function (d) { // Loop over all target tokens
                    return d;
                })
                .enter() // When entering
                .append("line")
                .attr("x1", x1)
                .attr("y1", function (d) {
                    var sourceIndex = +this.parentNode.getAttribute("source-index");
                    return y1 + (sourceIndex + .5) * config.thumbnailBoxHeight;
                })
                .attr("x2", x2)
                .attr("y2", function (d, targetIndex) {
                    return y1 + (targetIndex + .5) * config.thumbnailBoxHeight;
                })
                .attr("stroke-width", 2.2)
                .attr("stroke", getLayerColor(layerIndex))
                .attr("stroke-opacity", function (d) {
                    return d;
                });

            var clickRegion = attnContainer.append("rect")
                .attr("x", x)
                .attr("y", y)
                .attr("height", config.thumbnailHeight)
                .attr("width", config.thumbnailWidth)
                .style("opacity", 0);

            clickRegion.on("click", function (d, index) {
                var attnBackgroundOther = config.svg.selectAll(".attn_background");
                attnBackgroundOther.attr("fill", getBackgroundColor());
                attnBackgroundOther.attr("stroke-opacity", 0);

                config.svg.selectAll(".detail").remove();
                if (config.detail_layer != layerIndex || config.detail_head != headIndex) {
                    renderDetail(att, layerIndex, headIndex);
                    config.detail_layer = layerIndex;
                    config.detail_head = headIndex;
                    attnBackground.attr("fill", getHighlightColor());
                    attnBackground.attr("stroke-opacity", .8);
                } else {
                    config.detail_layer = null;
                    config.detail_head = null;
                    attnBackground.attr("fill", getBackgroundColor());
                    attnBackground.attr("stroke-opacity", 0);
                }
            });

            clickRegion.on("mouseover", function (d) {
                d3.select(this).style("cursor", "pointer");
            });
        }

        function renderDetailFrame(x, y, layerIndex) {
            var detailFrame = config.svg.append("rect")
                .classed("detail", true)
                .attr("x", x)
                .attr("y", y)
                .attr("height", config.detailHeight)
                .attr("width", DETAIL_WIDTH)
                .style("opacity", 1)
                .attr("stroke-width", 1.5)
                .attr("stroke-opacity", 0.7)
                .attr("stroke", getLayerColor(layerIndex));
        }

        function renderDetailAttn(x, y, att, layerIndex) {
            var attnContainer = config.svg.append("svg:g")
                .classed("detail", true)
                .attr("pointer-events", "none");
            attnContainer.selectAll("g")
                .data(att)
                .enter()
                .append("g") // Add group for each source token
                .classed('attn-line-group', true)
                .attr("source-index", function (d, i) { // Save index of source token
                    return i;
                })
                .selectAll("line")
                .data(function (d) { // Loop over all target tokens
                    return d;
                })
                .enter()
                .append("line")
                .attr("x1", x + ATTN_PADDING)
                .attr("y1", function (d) {
                    var sourceIndex = +this.parentNode.getAttribute("source-index");
                    return y + (sourceIndex + .5) * DETAIL_BOX_HEIGHT;
                })
                .attr("x2", x + DETAIL_ATTENTION_WIDTH - ATTN_PADDING)
                .attr("y2", function (d, targetIndex) {
                    return y + (targetIndex + .5) * DETAIL_BOX_HEIGHT;
                })
                .attr("stroke-width", 2.2)
                .attr("stroke", getLayerColor(layerIndex))
                .attr("stroke-opacity", function (d) {
                    return d;
                });
        }

        function getLayerColor(layer) {
          return LAYER_COLORS[config.layers[layer] % 10];
        }

        function getTextColor() {
            return PALETTE[config.mode]['text']
        }

        function getBackgroundColor() {
           return PALETTE[config.mode]['background']
        }

        function getHighlightColor() {
           return PALETTE[config.mode]['highlight']
        }

        function initialize() {
            config.attention = params['attention'];
            config.filter = params['default_filter'];
            config.mode = params['display_mode'];
            config.layers = params['include_layers']
            config.heads = params['include_heads']
            config.totalHeads = params['total_heads']
            config.rootDivId = params['root_div_id'];
            $(`#${config.rootDivId} #filter`).on('change', function (e) {
                config.filter = e.currentTarget.value;
                render();
            });
        }

        initialize();
        render();

    });
</script>
</div>
</div>
</section>
</section>
</section>
<section id="pipeline-using-huggingface" class="level2">
<h2 class="anchored" data-anchor-id="pipeline-using-huggingface">Pipeline using HuggingFace</h2>
<p>Now, let’s see a practical application of LLMs using a HuggingFace pipeline for classification.</p>
<p>This involves a few steps including: 1. Setting up a prompt 2. Loading in a pretrained model 3. Loading in the tokenizer and tokenizing input text 4. Performing model inference 5. Interpreting inference output</p>
<div id="343550f6" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="co"># STEP 0 : Installations and imports</span></span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForSequenceClassification, AutoConfig</span>
<span id="cb14-3"><a href="#cb14-3"></a><span class="im">import</span> torch</span>
<span id="cb14-4"><a href="#cb14-4"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="setting-up-a-prompt" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-a-prompt">1. Setting up a prompt</h3>
<p>A “prompt” refers to a specific input or query provided to a language model. They guide the text processing and generation by providing the context for the model to generate coherent and relevant text based on the given input.</p>
<p>The choice and structure of the prompt depends on the specific task, the context and desired output. Prompts can be “discrete” or “instructive” where they are explicit instructions or questions directed to the language model. They can also be more nuanced by more providing suggestions, directions and contexts to the model.</p>
<p>We will use very simple prompts in this tutorial section, but we will learn more about prompt engineering and how it helps in optimizing the performance of the model for a given use case in the following tutorials.</p>
<div id="5d814b82" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># STEP 1 : Set up the prompt</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>input_text <span class="op">=</span> <span class="st">"The panoramic view of the ocean was breathtaking."</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="loading-pretrained-models" class="level3">
<h3 class="anchored" data-anchor-id="loading-pretrained-models">2. Loading Pretrained Models</h3>
<p>The AutoModelForSequenceClassification from_pretrained() method instantiates a sequence classification model.</p>
<p>Refer to https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodels for the list of model classes supported.</p>
<p>“from_pretrained” method downloads the pre-trained weights from the Hugging Face Model Hub or the specified URL if the model is not already cached locally. It then loads the weights into the instantiated model, initializing the model parameters with the pre-trained values.</p>
<p>The model cache contains:</p>
<ul>
<li>model configuration (config.json)</li>
<li>pretrained model weights (model.safetensors)</li>
<li>tokenizer information (tokenizer.json, vocab.json, merges.txt, tokenizer.model)</li>
</ul>
<div id="bc63c257" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:671,&quot;referenced_widgets&quot;:[&quot;bd858574deec4181bb0ba443be9a2088&quot;,&quot;b025f1fa9b61436f93515742e324b86f&quot;,&quot;5073ff96876f4e04b246e466be18c186&quot;,&quot;0c9e9d1ae5ed4ae4a7e37513d35f9c77&quot;,&quot;b29ed3c3e599460cbd3d693d71f456d2&quot;,&quot;5b43c22a0fa94a7690d637dcef8f9e0a&quot;,&quot;51736c335dd349e7bfcc529ada3c6f6a&quot;,&quot;4791ebe7b3cc4444afdcbfd8d1b5967a&quot;,&quot;93c49acd409c4616b22b28ef971c9dd6&quot;,&quot;4777bd67c1e64692b294830f05c7143d&quot;,&quot;aeb6c3883112462c8ade793f34ebc4da&quot;,&quot;82ebac62b0954db1b9fa3e3ae4025a04&quot;,&quot;88bc6af07dc244b8bb3ea694ed31d2ae&quot;,&quot;21364965ba974886ad36c5d9670e7a70&quot;,&quot;00c33e96a50b4993a9f3387402fbccfe&quot;,&quot;9de7b337c654426fac8f86b97c7ca7dd&quot;,&quot;d3d454f41f164fc5af66adf8d2caa53c&quot;,&quot;06cd5703c7134fd49a66a259d0154b85&quot;,&quot;f6131609549c4e7eb89cbb3f430d8aca&quot;,&quot;e5a7799affd54336b99042ae28609630&quot;,&quot;5ee4ce555b264be4becae07722f88a25&quot;,&quot;4e181643cf4e449ca06776ecbba79c5e&quot;]}}" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># STEP 2 : Load the pretrained model.</span></span>
<span id="cb16-2"><a href="#cb16-2"></a>model_name <span class="op">=</span> <span class="st">"distilbert-base-uncased-finetuned-sst-2-english"</span></span>
<span id="cb16-3"><a href="#cb16-3"></a>model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(model_name)</span>
<span id="cb16-4"><a href="#cb16-4"></a>config <span class="op">=</span> AutoConfig.from_pretrained(model_name)</span>
<span id="cb16-5"><a href="#cb16-5"></a><span class="bu">print</span>(config)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>DistilBertConfig {
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "sst-2",
  "hidden_dim": 3072,
  "id2label": {
    "0": "NEGATIVE",
    "1": "POSITIVE"
  },
  "initializer_range": 0.02,
  "label2id": {
    "NEGATIVE": 0,
    "POSITIVE": 1
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "output_past": true,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.53.3",
  "vocab_size": 30522
}
</code></pre>
</div>
</div>
</section>
<section id="loading-in-the-tokenizer-and-tokenizing-input-text" class="level3">
<h3 class="anchored" data-anchor-id="loading-in-the-tokenizer-and-tokenizing-input-text">3. Loading in the tokenizer and tokenizing input text</h3>
<p>Here, we load in a pretrained tokenizer associated with this model.</p>
<div id="9a923fa2" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:116,&quot;referenced_widgets&quot;:[&quot;067f49dd461a44fabf2b2fbc48c38ea3&quot;,&quot;8081fe15a05f4d93a263016d65f7c867&quot;,&quot;db4f4633a272445fb65b31b81cc13a47&quot;,&quot;683187173eee4b4f84c42ee1b030041e&quot;,&quot;9b356d4bb2584404b40a53e84ffd08ad&quot;,&quot;f8844d328728454cabd0c601f1dfac4c&quot;,&quot;999973bfe8554a6b89b2214b39348331&quot;,&quot;80f90f7e6d644d1d8b7df13c9e85225d&quot;,&quot;683ed139b6384e78a7d16d8375640c74&quot;,&quot;aaca325b53f449519e4193981a4b3865&quot;,&quot;d1a67e3364a04697a740fac83669d9f9&quot;,&quot;9489672461b54621bdb8bb24d0eb9934&quot;,&quot;2862c54dff8c4f83829e946c69962221&quot;,&quot;95367cef20634367b463b8f0a95e0503&quot;,&quot;fac8d31cc40e4c9e80f8ca93a9075077&quot;,&quot;e61474e5f4784cf6ab8a47decc035ae0&quot;,&quot;b28301ee0a554b009b4a04ab5c3bcaf0&quot;,&quot;8a29da2807084308bd8ccf9414150e31&quot;,&quot;545d6ab11dc744099b608face7404d0e&quot;,&quot;6c4a6d9277f148c6a575b1b7c6c008f7&quot;,&quot;cac928cad64b40cb8d0ef32f3731451a&quot;,&quot;a0e7b739e9a44660b0b8b65448d89a93&quot;]}}" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="co">#STEP 3 : Load the tokenizer and tokenize the input text</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>tokenizer  <span class="op">=</span>  AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb18-3"><a href="#cb18-3"></a>input_ids <span class="op">=</span> tokenizer(input_text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)[<span class="st">"input_ids"</span>]</span>
<span id="cb18-4"><a href="#cb18-4"></a><span class="bu">print</span>(input_ids)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[  101,  1996,  6090,  6525,  7712,  3193,  1997,  1996,  4153,  2001,
          3052, 17904,  1012,   102]])</code></pre>
</div>
</div>
</section>
<section id="performing-inference-and-interpreting" class="level3">
<h3 class="anchored" data-anchor-id="performing-inference-and-interpreting">4. Performing inference and interpreting</h3>
<p>Here, we: * load data into the model, * perform inference to obtain logits, * Convert logits into probabilities * According to probabilities assign label</p>
<p>The end result is that we can predict whether the input phrase is positive or negative.</p>
<div id="2b7736ce" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="co"># STEP 5 : Perform inference</span></span>
<span id="cb20-2"><a href="#cb20-2"></a>outputs <span class="op">=</span> model(input_ids)</span>
<span id="cb20-3"><a href="#cb20-3"></a>result <span class="op">=</span> outputs.logits</span>
<span id="cb20-4"><a href="#cb20-4"></a><span class="bu">print</span>(result)</span>
<span id="cb20-5"><a href="#cb20-5"></a></span>
<span id="cb20-6"><a href="#cb20-6"></a><span class="co"># STEP 6 :  Interpret the output.</span></span>
<span id="cb20-7"><a href="#cb20-7"></a>probabilities <span class="op">=</span> F.softmax(result, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb20-8"><a href="#cb20-8"></a><span class="bu">print</span>(probabilities)</span>
<span id="cb20-9"><a href="#cb20-9"></a>predicted_class <span class="op">=</span> torch.argmax(probabilities, dim<span class="op">=-</span><span class="dv">1</span>).item()</span>
<span id="cb20-10"><a href="#cb20-10"></a>labels <span class="op">=</span> [<span class="st">"NEGATIVE"</span>, <span class="st">"POSITIVE"</span>]</span>
<span id="cb20-11"><a href="#cb20-11"></a>out_string <span class="op">=</span> <span class="st">"[{'label': '"</span> <span class="op">+</span> <span class="bu">str</span>(labels[predicted_class]) <span class="op">+</span> <span class="st">"', 'score': "</span> <span class="op">+</span> <span class="bu">str</span>(probabilities[<span class="dv">0</span>][predicted_class].tolist()) <span class="op">+</span> <span class="st">"}]"</span></span>
<span id="cb20-12"><a href="#cb20-12"></a><span class="bu">print</span>(out_string)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[-4.2767,  4.5486]], grad_fn=&lt;AddmmBackward0&gt;)
tensor([[1.4695e-04, 9.9985e-01]], grad_fn=&lt;SoftmaxBackward0&gt;)
[{'label': 'POSITIVE', 'score': 0.9998530149459839}]</code></pre>
</div>
</div>
</section>
<section id="saving-and-loading-models" class="level3">
<h3 class="anchored" data-anchor-id="saving-and-loading-models">Saving and loading models</h3>
<p>Model can be saved and loaded to and from a local model directory.</p>
<div id="0e2bb4f7" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:81,&quot;referenced_widgets&quot;:[&quot;5c584862c1714236be9a4d72eacb7e8c&quot;,&quot;878ac7474c814c56a1f8e4ca1307b446&quot;,&quot;61d31a811e704e0b940337009892c774&quot;,&quot;90d95ae3318b4f4bbda0f5d016347e91&quot;,&quot;c603255a544f47b280fd61e9e45d9204&quot;,&quot;bc69805bf372491d931e20940cf99cfc&quot;,&quot;9eb1cc51fc0348069875ce5c52c992b9&quot;,&quot;a36bbfb2408944c39232013f53811f08&quot;,&quot;16994401181f43238336a07a06be119b&quot;,&quot;dc6892bac378490abd7f628bca35906c&quot;,&quot;1dcd4ce0732240aab355557623a19d59&quot;,&quot;3933d404330d4d02913a7e075819f807&quot;,&quot;f286b77c68ce47ec9a5177f2dd35ea48&quot;,&quot;d15d08eb7374438f8edbcc2d78beafc8&quot;,&quot;a9a334e404fe47da819a7e97d0dbf958&quot;,&quot;ed261fb3eb6b4661a0dba67a3fc9858b&quot;,&quot;95d2b19696f44d099e530f82ad213328&quot;,&quot;725a53042a724dc0a9a658bd3dd3cb83&quot;,&quot;e4dd55f635a74dbeb0067d1c47769123&quot;,&quot;832adb3112224a51ba9376a50076dd8c&quot;,&quot;c953196990624b0ea79c2a3c74cd1de9&quot;,&quot;46f89e0a5ae84c6c97265c795fae9f88&quot;]}}" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoModelForCausalLM</span>
<span id="cb22-2"><a href="#cb22-2"></a></span>
<span id="cb22-3"><a href="#cb22-3"></a><span class="co"># Instantiate and train or fine-tune a model</span></span>
<span id="cb22-4"><a href="#cb22-4"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span>
<span id="cb22-5"><a href="#cb22-5"></a></span>
<span id="cb22-6"><a href="#cb22-6"></a><span class="co"># Train or fine-tune the model...</span></span>
<span id="cb22-7"><a href="#cb22-7"></a></span>
<span id="cb22-8"><a href="#cb22-8"></a><span class="co"># Save the model to a local directory</span></span>
<span id="cb22-9"><a href="#cb22-9"></a>directory <span class="op">=</span> <span class="st">"my_local_model"</span></span>
<span id="cb22-10"><a href="#cb22-10"></a>model.save_pretrained(directory)</span>
<span id="cb22-11"><a href="#cb22-11"></a></span>
<span id="cb22-12"><a href="#cb22-12"></a><span class="co"># Load a pre-trained model from a local directory</span></span>
<span id="cb22-13"><a href="#cb22-13"></a>loaded_model <span class="op">=</span> AutoModel.from_pretrained(directory)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[2025-07-23 11:36:55,005] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to mps (auto detect)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
W0723 11:36:55.254000 76909 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[2025-07-23 11:36:55,793] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False</code></pre>
</div>
</div>
</section>
</section>
<section id="model-hub" class="level2">
<h2 class="anchored" data-anchor-id="model-hub">Model Hub</h2>
<p>The Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing.</p>
<ul>
<li>Download pre-trained models with the huggingface_hub client library, with Transformers for fine-tuning.</li>
<li>Make use of Inference API to use models in production settings.</li>
<li>You can filter for different models for different tasks, frameworks used, datasets used, and many more.</li>
<li>You can select any model, that will show the model card.</li>
<li>Model card contains information of the model, including the description, usage, limitations etc. Some models also have inference API’s that can be used directly.</li>
</ul>
<p>Model Hub Link : https://huggingface.co/docs/hub/en/models-the-hub</p>
<p>Example of a model card : https://huggingface.co/bert-base-uncased/tree/main</p>
</section>
<section id="recommended-reading" class="level2">
<h2 class="anchored" data-anchor-id="recommended-reading">Recommended reading</h2>
<ul>
<li><a href="https://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer” by Jay Alammar</a></li>
<li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">“Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)” by Jay Alammar</a></li>
<li><a href="https://jalammar.github.io/illustrated-gpt2/">“The Illustrated GPT-2 (Visualizing Transformer Language Models)”</a></li>
<li><a href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/">“A gentle introduction to positional encoding”</a></li>
<li><a href="https://github.com/brettin/llm_tutorial">“LLM Tutorial Workshop (Argonne National Laboratory)”</a></li>
<li><a href="https://github.com/argonne-lcf/llm-workshop">“LLM Tutorial Workshop Part 2 (Argonne National Laboratory)”</a></li>
</ul>
</section>
<section id="homework" class="level2">
<h2 class="anchored" data-anchor-id="homework">Homework</h2>
<ol type="1">
<li>Load in a generative model using the HuggingFace pipeline and generate text using a batch of prompts.</li>
</ol>
<ul>
<li>Play with generative parameters such as temperature, max_new_tokens, and the model itself and explain the effect on the legibility of the model response. Try at least 4 different parameter/model combinations.</li>
<li>Models that can be used include:
<ul>
<li><code>google/gemma-2-2b-it</code></li>
<li><code>microsoft/Phi-3-mini-4k-instruct</code></li>
<li><code>meta-llama/Llama-3.2-1B</code></li>
<li>Any model from this list: <a href="https://huggingface.co/models?pipeline_tag=text-generation">Text-generation models</a></li>
<li><code>gpt2</code> if having trouble loading these models in</li>
</ul></li>
<li>This guide should help! <a href="https://huggingface.co/docs/transformers/en/generation_strategies">Text-generation strategies</a></li>
</ul>
<ol start="2" type="1">
<li>Load in 2 models of different parameter size (e.g.&nbsp;GPT2, meta-llama/Llama-2-7b-chat-hf, or distilbert/distilgpt2) and analyze the BertViz for each. How does the attention mechanisms change depending on model size?</li>
</ol>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{foreman2025,
  author = {Foreman, Sam},
  title = {Hands {On:} {Introduction} to {Large} {Language} {Models}
    {(LLMs)}},
  date = {2025-07-23},
  url = {https://saforem2.github.io/hpc-bootcamp-2025/02-llms/1-hands-on-llms/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-foreman2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Foreman, Sam. 2025. <span>“Hands On: Introduction to Large Language
Models (LLMs).”</span> July 23, 2025. <a href="https://saforem2.github.io/hpc-bootcamp-2025/02-llms/1-hands-on-llms/">https://saforem2.github.io/hpc-bootcamp-2025/02-llms/1-hands-on-llms/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/saforem2\.github\.io\/hpc-bootcamp-2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="dark">
<input type="hidden" id="giscus-alt-theme" value="light">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "saforem2/intro-hpc-bootcamp-2025";
    script.dataset.repoId = "";
    script.dataset.category = "General";
    script.dataset.categoryId = "";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../02-llms/0-intro-to-llms/index.html" class="pagination-link" aria-label="[0] Intro to LLMs">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">[0] Intro to LLMs</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../02-llms/4-evaluating-llms/index.html" class="pagination-link" aria-label="[4] Evaluating LLMs">
        <span class="nav-page-text">[4] Evaluating LLMs</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb26" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb26-1"><a href="#cb26-1"></a><span class="co">---</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="co"># jupyter: python3</span></span>
<span id="cb26-3"><a href="#cb26-3"></a><span class="an">title:</span><span class="co"> "Hands On: Introduction to Large Language Models (LLMs)"</span></span>
<span id="cb26-4"><a href="#cb26-4"></a><span class="an">description:</span><span class="co"> "An introduction to Large Language Models (LLMs), their architecture, and how they can be applied to scientific applications."</span></span>
<span id="cb26-5"><a href="#cb26-5"></a><span class="an">date:</span><span class="co"> 2025-07-23</span></span>
<span id="cb26-6"><a href="#cb26-6"></a><span class="an">date-modified:</span><span class="co"> last-modified</span></span>
<span id="cb26-7"><a href="#cb26-7"></a><span class="co">---</span></span>
<span id="cb26-8"><a href="#cb26-8"></a></span>
<span id="cb26-9"><a href="#cb26-9"></a><span class="dt">&lt;</span><span class="kw">a</span><span class="ot"> href</span><span class="op">=</span><span class="st">"https://colab.research.google.com/github/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/IntroLLMs.ipynb"</span><span class="ot"> target</span><span class="op">=</span><span class="st">"_parent"</span><span class="dt">&gt;&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"https://colab.research.google.com/assets/colab-badge.svg"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">"Open In Colab"</span><span class="dt">/&gt;&lt;/</span><span class="kw">a</span><span class="dt">&gt;</span></span>
<span id="cb26-10"><a href="#cb26-10"></a></span>
<span id="cb26-11"><a href="#cb26-11"></a>::: {.callout-note title="Authors" collapse="false"}</span>
<span id="cb26-12"><a href="#cb26-12"></a></span>
<span id="cb26-13"><a href="#cb26-13"></a>Content in this notebook is modified from content originally written by:</span>
<span id="cb26-14"><a href="#cb26-14"></a></span>
<span id="cb26-15"><a href="#cb26-15"></a><span class="ss">- </span>Archit Vasan, Huihuo Zheng, Marieme Ngom, Bethany Lusch, Taylor Childers,</span>
<span id="cb26-16"><a href="#cb26-16"></a>  Venkat Vishwanath</span>
<span id="cb26-17"><a href="#cb26-17"></a></span>
<span id="cb26-18"><a href="#cb26-18"></a>Inspiration from the blog posts "The Illustrated Transformer" and "The</span>
<span id="cb26-19"><a href="#cb26-19"></a>Illustrated GPT2" by Jay Alammar, highly recommended reading.</span>
<span id="cb26-20"><a href="#cb26-20"></a></span>
<span id="cb26-21"><a href="#cb26-21"></a>:::</span>
<span id="cb26-22"><a href="#cb26-22"></a></span>
<span id="cb26-23"><a href="#cb26-23"></a>Although the name "language models" is derived from Natural Language</span>
<span id="cb26-24"><a href="#cb26-24"></a>Processing, the models used in these approaches can be applied to diverse</span>
<span id="cb26-25"><a href="#cb26-25"></a>scientific applications as illustrated below.</span>
<span id="cb26-26"><a href="#cb26-26"></a></span>
<span id="cb26-27"><a href="#cb26-27"></a></span>
<span id="cb26-28"><a href="#cb26-28"></a><span class="fu">## Outline</span></span>
<span id="cb26-29"><a href="#cb26-29"></a></span>
<span id="cb26-30"><a href="#cb26-30"></a>During this session I will cover:</span>
<span id="cb26-31"><a href="#cb26-31"></a></span>
<span id="cb26-32"><a href="#cb26-32"></a><span class="ss">1. </span>Scientific applications for language models</span>
<span id="cb26-33"><a href="#cb26-33"></a><span class="ss">2. </span>General overview of Transformers</span>
<span id="cb26-34"><a href="#cb26-34"></a><span class="ss">3. </span>Tokenization</span>
<span id="cb26-35"><a href="#cb26-35"></a><span class="ss">4. </span>Model Architecture</span>
<span id="cb26-36"><a href="#cb26-36"></a><span class="ss">5. </span>Pipeline using HuggingFace</span>
<span id="cb26-37"><a href="#cb26-37"></a><span class="ss">6. </span>Model loading</span>
<span id="cb26-38"><a href="#cb26-38"></a></span>
<span id="cb26-39"><a href="#cb26-39"></a><span class="fu">## Modeling Sequential Data</span></span>
<span id="cb26-40"><a href="#cb26-40"></a></span>
<span id="cb26-41"><a href="#cb26-41"></a>Sequences are variable-length lists with data in subsequent iterations that</span>
<span id="cb26-42"><a href="#cb26-42"></a>depends on previous iterations (or tokens).</span>
<span id="cb26-43"><a href="#cb26-43"></a></span>
<span id="cb26-44"><a href="#cb26-44"></a>Mathematically:</span>
<span id="cb26-45"><a href="#cb26-45"></a></span>
<span id="cb26-46"><a href="#cb26-46"></a>A sequence is a list of tokens:</span>
<span id="cb26-47"><a href="#cb26-47"></a></span>
<span id="cb26-48"><a href="#cb26-48"></a>$$T = <span class="co">[</span><span class="ot">t_1, t_2, t_3,...,t_N</span><span class="co">]</span>$$</span>
<span id="cb26-49"><a href="#cb26-49"></a></span>
<span id="cb26-50"><a href="#cb26-50"></a>where each token within the list depends on the others with a particular</span>
<span id="cb26-51"><a href="#cb26-51"></a>probability:</span>
<span id="cb26-52"><a href="#cb26-52"></a></span>
<span id="cb26-53"><a href="#cb26-53"></a>$$P(t_N | t_{N-1}, ..., t_3, t_2, t_1)$$</span>
<span id="cb26-54"><a href="#cb26-54"></a></span>
<span id="cb26-55"><a href="#cb26-55"></a>The purpose of sequential modeling is to learn these probabilities for possible</span>
<span id="cb26-56"><a href="#cb26-56"></a>tokens in a distribution to perform various tasks including:</span>
<span id="cb26-57"><a href="#cb26-57"></a></span>
<span id="cb26-58"><a href="#cb26-58"></a><span class="ss">- </span>Sequence generation based on a prompt</span>
<span id="cb26-59"><a href="#cb26-59"></a><span class="ss">- </span>Language translation (e.g. English --&gt; French)</span>
<span id="cb26-60"><a href="#cb26-60"></a><span class="ss">- </span>Property prediction (predicting a property based on an entire sequence)</span>
<span id="cb26-61"><a href="#cb26-61"></a><span class="ss">- </span>Identifying mistakes or missing elements in sequential data</span>
<span id="cb26-62"><a href="#cb26-62"></a></span>
<span id="cb26-63"><a href="#cb26-63"></a><span class="fu">## Scientific sequential data modeling examples</span></span>
<span id="cb26-64"><a href="#cb26-64"></a></span>
<span id="cb26-65"><a href="#cb26-65"></a><span class="fu">### Nucleic acid sequences \+ genomic data</span></span>
<span id="cb26-66"><a href="#cb26-66"></a></span>
<span id="cb26-67"><a href="#cb26-67"></a>::: {#fig-RNA-codons}</span>
<span id="cb26-68"><a href="#cb26-68"></a></span>
<span id="cb26-69"><a href="#cb26-69"></a><span class="al">![](https://github.com/architvasan/ai_science_local/blob/main/images/RNA-codons.svg.png?raw=1)</span></span>
<span id="cb26-70"><a href="#cb26-70"></a></span>
<span id="cb26-71"><a href="#cb26-71"></a>RNA Codons</span>
<span id="cb26-72"><a href="#cb26-72"></a>:::</span>
<span id="cb26-73"><a href="#cb26-73"></a></span>
<span id="cb26-74"><a href="#cb26-74"></a>Nucleic acid sequences can be used to predict translation of proteins, mutations, and gene expression levels.</span>
<span id="cb26-75"><a href="#cb26-75"></a></span>
<span id="cb26-76"><a href="#cb26-76"></a>Here is an image of GenSLM. This is a language model developed by Argonne researchers that can model genomic information in a single model. It was shown to model the evolution of SARS-COV2 without expensive experiments.</span>
<span id="cb26-77"><a href="#cb26-77"></a></span>
<span id="cb26-78"><a href="#cb26-78"></a></span>
<span id="cb26-79"><a href="#cb26-79"></a>::: {#fig-genslm}</span>
<span id="cb26-80"><a href="#cb26-80"></a></span>
<span id="cb26-81"><a href="#cb26-81"></a><span class="al">![](https://github.com/architvasan/ai_science_local/blob/main/images/genslm.png?raw=1)</span></span>
<span id="cb26-82"><a href="#cb26-82"></a></span>
<span id="cb26-83"><a href="#cb26-83"></a>Genomic Scale Language Models (GenSLM)</span>
<span id="cb26-84"><a href="#cb26-84"></a><span class="co">[</span><span class="ot">Zvyagin et. al 2022. BioRXiv</span><span class="co">](https://www.biorxiv.org/content/10.1101/2022.10.10.511571v1)</span></span>
<span id="cb26-85"><a href="#cb26-85"></a>:::</span>
<span id="cb26-86"><a href="#cb26-86"></a></span>
<span id="cb26-87"><a href="#cb26-87"></a><span class="fu">### Protein sequences</span></span>
<span id="cb26-88"><a href="#cb26-88"></a></span>
<span id="cb26-89"><a href="#cb26-89"></a>Protein sequences can be used to predict folding structure, protein-protein interactions, chemical/binding properties, protein function and many more properties.</span>
<span id="cb26-90"><a href="#cb26-90"></a></span>
<span id="cb26-91"><a href="#cb26-91"></a>::: {#fig-protein-structure}</span>
<span id="cb26-92"><a href="#cb26-92"></a></span>
<span id="cb26-93"><a href="#cb26-93"></a><span class="al">![](https://github.com/architvasan/ai_science_local/blob/main/images/Protein-Structure-06.png?raw=1)</span></span>
<span id="cb26-94"><a href="#cb26-94"></a></span>
<span id="cb26-95"><a href="#cb26-95"></a>Protein Structure</span>
<span id="cb26-96"><a href="#cb26-96"></a>:::</span>
<span id="cb26-97"><a href="#cb26-97"></a></span>
<span id="cb26-98"><a href="#cb26-98"></a></span>
<span id="cb26-99"><a href="#cb26-99"></a>::: {#fig-esmfold}</span>
<span id="cb26-100"><a href="#cb26-100"></a></span>
<span id="cb26-101"><a href="#cb26-101"></a><span class="al">![](https://github.com/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/images/ESMFold.png?raw=1)</span></span>
<span id="cb26-102"><a href="#cb26-102"></a></span>
<span id="cb26-103"><a href="#cb26-103"></a>ESMFold</span>
<span id="cb26-104"><a href="#cb26-104"></a><span class="co">[</span><span class="ot">Lin et. al. 2023. Science</span><span class="co">](https://www.science.org/doi/10.1126/science.ade2574)</span></span>
<span id="cb26-105"><a href="#cb26-105"></a>:::</span>
<span id="cb26-106"><a href="#cb26-106"></a></span>
<span id="cb26-107"><a href="#cb26-107"></a></span>
<span id="cb26-108"><a href="#cb26-108"></a><span class="fu">### Other applications</span></span>
<span id="cb26-109"><a href="#cb26-109"></a></span>
<span id="cb26-110"><a href="#cb26-110"></a><span class="ss">* </span>Biomedical text</span>
<span id="cb26-111"><a href="#cb26-111"></a><span class="ss">* </span>SMILES strings</span>
<span id="cb26-112"><a href="#cb26-112"></a><span class="ss">* </span>Weather predictions</span>
<span id="cb26-113"><a href="#cb26-113"></a><span class="ss">* </span>Interfacing with simulations such as molecular dynamics simulation</span>
<span id="cb26-114"><a href="#cb26-114"></a></span>
<span id="cb26-115"><a href="#cb26-115"></a><span class="fu">## Overview of Language models</span></span>
<span id="cb26-116"><a href="#cb26-116"></a></span>
<span id="cb26-117"><a href="#cb26-117"></a>We will now briefly talk about the progression of language models.</span>
<span id="cb26-118"><a href="#cb26-118"></a></span>
<span id="cb26-119"><a href="#cb26-119"></a><span class="fu">### Transformers</span></span>
<span id="cb26-120"><a href="#cb26-120"></a></span>
<span id="cb26-121"><a href="#cb26-121"></a>The most common LMs base their design on the Transformer architecture that was introduced in 2017 in the "Attention is all you need" paper.</span>
<span id="cb26-122"><a href="#cb26-122"></a></span>
<span id="cb26-123"><a href="#cb26-123"></a></span>
<span id="cb26-124"><a href="#cb26-124"></a>::: {#fig-attention-is-all-you-need}</span>
<span id="cb26-125"><a href="#cb26-125"></a></span>
<span id="cb26-126"><a href="#cb26-126"></a><span class="al">![](https://github.com/architvasan/ai_science_local/blob/main/images/attention_is_all_you_need.png?raw=1)</span></span>
<span id="cb26-127"><a href="#cb26-127"></a></span>
<span id="cb26-128"><a href="#cb26-128"></a>Attention is all you need</span>
<span id="cb26-129"><a href="#cb26-129"></a><span class="co">[</span><span class="ot">Vaswani 2017. Advances in Neural Information Processing Systems</span><span class="co">](https://arxiv.org/pdf/1706.03762)</span></span>
<span id="cb26-130"><a href="#cb26-130"></a>:::</span>
<span id="cb26-131"><a href="#cb26-131"></a></span>
<span id="cb26-132"><a href="#cb26-132"></a></span>
<span id="cb26-133"><a href="#cb26-133"></a>Since then a multitude of LLM architectures have been designed.</span>
<span id="cb26-134"><a href="#cb26-134"></a></span>
<span id="cb26-135"><a href="#cb26-135"></a>::: {#fig-ch1-transformers}</span>
<span id="cb26-136"><a href="#cb26-136"></a></span>
<span id="cb26-137"><a href="#cb26-137"></a><span class="al">![](https://github.com/architvasan/ai_science_local/blob/main/images/en_chapter1_transformers_chrono.svg?raw=1)</span></span>
<span id="cb26-138"><a href="#cb26-138"></a></span>
<span id="cb26-139"><a href="#cb26-139"></a>Transformers, chronologically</span>
<span id="cb26-140"><a href="#cb26-140"></a>:::</span>
<span id="cb26-141"><a href="#cb26-141"></a></span>
<span id="cb26-142"><a href="#cb26-142"></a><span class="co">[</span><span class="ot">HuggingFace NLP Course</span><span class="co">](https://huggingface.co/learn/nlp-course/chapter1/4)</span></span>
<span id="cb26-143"><a href="#cb26-143"></a></span>
<span id="cb26-144"><a href="#cb26-144"></a><span class="fu">## Coding example of LLMs in action!</span></span>
<span id="cb26-145"><a href="#cb26-145"></a></span>
<span id="cb26-146"><a href="#cb26-146"></a>Let's look at an example of running inference with a LLM as a block box to</span>
<span id="cb26-147"><a href="#cb26-147"></a>generate text given a prompt and we will also initiate a training loop for an</span>
<span id="cb26-148"><a href="#cb26-148"></a>LLM</span>
<span id="cb26-149"><a href="#cb26-149"></a></span>
<span id="cb26-150"><a href="#cb26-150"></a>Here, we will use the <span class="in">`transformers`</span> library which is as part of HuggingFace, a</span>
<span id="cb26-151"><a href="#cb26-151"></a>repository of different models, tokenizers and information on how to apply</span>
<span id="cb26-152"><a href="#cb26-152"></a>these models</span>
<span id="cb26-153"><a href="#cb26-153"></a></span>
<span id="cb26-154"><a href="#cb26-154"></a>::: {.callout-warning collapse="false" title="🦜 Stochastic Parrots"}</span>
<span id="cb26-155"><a href="#cb26-155"></a></span>
<span id="cb26-156"><a href="#cb26-156"></a>**Warning**: _Large Language Models are only as good as their training data_.</span>
<span id="cb26-157"><a href="#cb26-157"></a></span>
<span id="cb26-158"><a href="#cb26-158"></a>They have no ethics, judgement, or editing ability.</span>
<span id="cb26-159"><a href="#cb26-159"></a></span>
<span id="cb26-160"><a href="#cb26-160"></a>We will be using some pretrained models from Hugging Face which used wide</span>
<span id="cb26-161"><a href="#cb26-161"></a>samples of internet hosted text.</span>
<span id="cb26-162"><a href="#cb26-162"></a></span>
<span id="cb26-163"><a href="#cb26-163"></a>The datasets have not been strictly filtered to restrict all malign content so</span>
<span id="cb26-164"><a href="#cb26-164"></a>the generated text may be surprisingly dark or questionable.</span>
<span id="cb26-165"><a href="#cb26-165"></a></span>
<span id="cb26-166"><a href="#cb26-166"></a>They do not reflect our core values and are only used for demonstration</span>
<span id="cb26-167"><a href="#cb26-167"></a>purposes.</span>
<span id="cb26-168"><a href="#cb26-168"></a>:::</span>
<span id="cb26-169"><a href="#cb26-169"></a></span>
<span id="cb26-172"><a href="#cb26-172"></a><span class="in">```{python}</span></span>
<span id="cb26-173"><a href="#cb26-173"></a><span class="co">#| colab: {base_uri: https://localhost:8080/, height: 35}</span></span>
<span id="cb26-174"><a href="#cb26-174"></a><span class="co">'''</span></span>
<span id="cb26-175"><a href="#cb26-175"></a><span class="co">Uncomment below section if running on sophia jupyter notebook</span></span>
<span id="cb26-176"><a href="#cb26-176"></a><span class="co">'''</span></span>
<span id="cb26-177"><a href="#cb26-177"></a><span class="co"># import os</span></span>
<span id="cb26-178"><a href="#cb26-178"></a><span class="co"># os.environ["HTTP_PROXY"]="proxy.alcf.anl.gov:3128"</span></span>
<span id="cb26-179"><a href="#cb26-179"></a><span class="co"># os.environ["HTTPS_PROXY"]="proxy.alcf.anl.gov:3128"</span></span>
<span id="cb26-180"><a href="#cb26-180"></a><span class="co"># os.environ["http_proxy"]="proxy.alcf.anl.gov:3128"</span></span>
<span id="cb26-181"><a href="#cb26-181"></a><span class="co"># os.environ["https_proxy"]="proxy.alcf.anl.gov:3128"</span></span>
<span id="cb26-182"><a href="#cb26-182"></a><span class="co"># os.environ["ftp_proxy"]="proxy.alcf.anl.gov:3128"</span></span>
<span id="cb26-183"><a href="#cb26-183"></a><span class="in">```</span></span>
<span id="cb26-184"><a href="#cb26-184"></a></span>
<span id="cb26-187"><a href="#cb26-187"></a><span class="in">```{python}</span></span>
<span id="cb26-188"><a href="#cb26-188"></a><span class="co">#| colab: {base_uri: https://localhost:8080/}</span></span>
<span id="cb26-189"><a href="#cb26-189"></a><span class="co">#| output: false</span></span>
<span id="cb26-190"><a href="#cb26-190"></a><span class="op">!</span>pip install transformers</span>
<span id="cb26-191"><a href="#cb26-191"></a><span class="op">!</span>pip install pandas</span>
<span id="cb26-192"><a href="#cb26-192"></a><span class="op">!</span>pip install torch</span>
<span id="cb26-193"><a href="#cb26-193"></a><span class="in">```</span></span>
<span id="cb26-194"><a href="#cb26-194"></a></span>
<span id="cb26-197"><a href="#cb26-197"></a><span class="in">```{python}</span></span>
<span id="cb26-198"><a href="#cb26-198"></a><span class="im">import</span> ambivalent</span>
<span id="cb26-199"><a href="#cb26-199"></a></span>
<span id="cb26-200"><a href="#cb26-200"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb26-201"><a href="#cb26-201"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb26-202"><a href="#cb26-202"></a></span>
<span id="cb26-203"><a href="#cb26-203"></a>plt.style.use(ambivalent.STYLES[<span class="st">'ambivalent'</span>])</span>
<span id="cb26-204"><a href="#cb26-204"></a>sns.set_context(<span class="st">"notebook"</span>)</span>
<span id="cb26-205"><a href="#cb26-205"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> [<span class="fl">6.4</span>, <span class="fl">4.8</span>]</span>
<span id="cb26-206"><a href="#cb26-206"></a><span class="in">```</span></span>
<span id="cb26-207"><a href="#cb26-207"></a></span>
<span id="cb26-210"><a href="#cb26-210"></a><span class="in">```{python}</span></span>
<span id="cb26-211"><a href="#cb26-211"></a><span class="co">#| colab: {base_uri: https://localhost:8080/}</span></span>
<span id="cb26-212"><a href="#cb26-212"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer,AutoModelForCausalLM, AutoConfig</span>
<span id="cb26-213"><a href="#cb26-213"></a>input_text <span class="op">=</span> <span class="st">"My dog really wanted to eat icecream because"</span></span>
<span id="cb26-214"><a href="#cb26-214"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb26-215"><a href="#cb26-215"></a>generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>, model<span class="op">=</span><span class="st">"gpt2"</span>)</span>
<span id="cb26-216"><a href="#cb26-216"></a>pipe <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>, model<span class="op">=</span><span class="st">"gpt2"</span>)</span>
<span id="cb26-217"><a href="#cb26-217"></a>generator(input_text, max_length<span class="op">=</span><span class="dv">20</span>, num_return_sequences<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb26-218"><a href="#cb26-218"></a><span class="in">```</span></span>
<span id="cb26-219"><a href="#cb26-219"></a></span>
<span id="cb26-220"><a href="#cb26-220"></a><span class="fu">## What's going on under the hood?</span></span>
<span id="cb26-221"><a href="#cb26-221"></a></span>
<span id="cb26-222"><a href="#cb26-222"></a>There are two components that are "black-boxes" here:</span>
<span id="cb26-223"><a href="#cb26-223"></a></span>
<span id="cb26-224"><a href="#cb26-224"></a><span class="ss">1. </span>The method for tokenization</span>
<span id="cb26-225"><a href="#cb26-225"></a><span class="ss">2. </span>The model that generates novel text.</span>
<span id="cb26-226"><a href="#cb26-226"></a></span>
<span id="cb26-227"><a href="#cb26-227"></a><span class="fu">## Tokenization and embedding of sequential data</span></span>
<span id="cb26-228"><a href="#cb26-228"></a></span>
<span id="cb26-229"><a href="#cb26-229"></a>Humans can inherently understand language data because they previously learned phonetic sounds.</span>
<span id="cb26-230"><a href="#cb26-230"></a></span>
<span id="cb26-231"><a href="#cb26-231"></a>Machines don’t have phonetic knowledge so they need to be told how to break text into standard units to process it.</span>
<span id="cb26-232"><a href="#cb26-232"></a></span>
<span id="cb26-233"><a href="#cb26-233"></a>They use a system called “tokenization”, where sequences of text are broken into smaller parts, or “tokens”, and then fed as input.</span>
<span id="cb26-234"><a href="#cb26-234"></a></span>
<span id="cb26-235"><a href="#cb26-235"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb26-236"><a href="#cb26-236"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"https://github.com/architvasan/ai_science_local/blob/main/images/text-processing---machines-vs-humans.png?raw=1"</span><span class="ot"> width</span><span class="op">=</span><span class="st">"400"</span><span class="dt">/&gt;</span></span>
<span id="cb26-237"><a href="#cb26-237"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb26-238"><a href="#cb26-238"></a></span>
<span id="cb26-239"><a href="#cb26-239"></a>Tokenization is a data preprocessing step which transforms the raw text data into a format suitable for machine learning models. Tokenizers break down raw text into smaller units called tokens. These tokens are what is fed into the language models. Based on the type and configuration of the tokenizer, these tokens can be words, subwords, or characters.</span>
<span id="cb26-240"><a href="#cb26-240"></a></span>
<span id="cb26-241"><a href="#cb26-241"></a>Types of tokenizers:</span>
<span id="cb26-242"><a href="#cb26-242"></a></span>
<span id="cb26-243"><a href="#cb26-243"></a><span class="ss">1. </span>Character Tokenizers: Split text into individual characters.</span>
<span id="cb26-244"><a href="#cb26-244"></a><span class="ss">2. </span>Word Tokenizers: Split text into words based on whitespace or punctuation.</span>
<span id="cb26-245"><a href="#cb26-245"></a><span class="ss">3. </span>Subword Tokenizers: Split text into subword units, such as morphemes or character n-grams. Common subword tokenization algorithms include:</span>
<span id="cb26-246"><a href="#cb26-246"></a><span class="ss">  1. </span>Byte-Pair Encoding (BPE),</span>
<span id="cb26-247"><a href="#cb26-247"></a><span class="ss">  2. </span>SentencePiece,</span>
<span id="cb26-248"><a href="#cb26-248"></a><span class="ss">  3. </span>WordPiece.</span>
<span id="cb26-249"><a href="#cb26-249"></a></span>
<span id="cb26-250"><a href="#cb26-250"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb26-251"><a href="#cb26-251"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"https://github.com/architvasan/ai_science_local/blob/main/images/tokenization_image.webp?raw=1"</span><span class="ot"> width</span><span class="op">=</span><span class="st">"400"</span><span class="dt">/&gt;</span></span>
<span id="cb26-252"><a href="#cb26-252"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb26-253"><a href="#cb26-253"></a></span>
<span id="cb26-254"><a href="#cb26-254"></a><span class="co">[</span><span class="ot">nlpiation</span><span class="co">](https://nlpiation.medium.com/how-to-use-huggingfaces-transformers-pre-trained-tokenizers-e029e8d6d1fa)</span></span>
<span id="cb26-255"><a href="#cb26-255"></a></span>
<span id="cb26-256"><a href="#cb26-256"></a><span class="fu">### Example of tokenization</span></span>
<span id="cb26-257"><a href="#cb26-257"></a>Let's look at an example of tokenization using byte-pair encoding.</span>
<span id="cb26-258"><a href="#cb26-258"></a></span>
<span id="cb26-261"><a href="#cb26-261"></a><span class="in">```{python}</span></span>
<span id="cb26-262"><a href="#cb26-262"></a><span class="co">#| colab: {base_uri: https://localhost:8080/}</span></span>
<span id="cb26-263"><a href="#cb26-263"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb26-264"><a href="#cb26-264"></a></span>
<span id="cb26-265"><a href="#cb26-265"></a><span class="co"># A utility function to tokenize a sequence and print out some information about it.</span></span>
<span id="cb26-266"><a href="#cb26-266"></a></span>
<span id="cb26-267"><a href="#cb26-267"></a><span class="kw">def</span> tokenization_summary(tokenizer, sequence):</span>
<span id="cb26-268"><a href="#cb26-268"></a></span>
<span id="cb26-269"><a href="#cb26-269"></a>    <span class="co"># get the vocabulary</span></span>
<span id="cb26-270"><a href="#cb26-270"></a>    vocab <span class="op">=</span> tokenizer.vocab</span>
<span id="cb26-271"><a href="#cb26-271"></a>    <span class="co"># Number of entries to print</span></span>
<span id="cb26-272"><a href="#cb26-272"></a>    n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb26-273"><a href="#cb26-273"></a></span>
<span id="cb26-274"><a href="#cb26-274"></a>    <span class="co"># Print subset of the vocabulary</span></span>
<span id="cb26-275"><a href="#cb26-275"></a>    <span class="bu">print</span>(<span class="st">"Subset of tokenizer.vocab:"</span>)</span>
<span id="cb26-276"><a href="#cb26-276"></a>    <span class="cf">for</span> i, (token, index) <span class="kw">in</span> <span class="bu">enumerate</span>(tokenizer.vocab.items()):</span>
<span id="cb26-277"><a href="#cb26-277"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>token<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>index<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb26-278"><a href="#cb26-278"></a>        <span class="cf">if</span> i <span class="op">&gt;=</span> n <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb26-279"><a href="#cb26-279"></a>            <span class="cf">break</span></span>
<span id="cb26-280"><a href="#cb26-280"></a></span>
<span id="cb26-281"><a href="#cb26-281"></a>    <span class="bu">print</span>(<span class="st">"Vocab size of the tokenizer = "</span>, <span class="bu">len</span>(vocab))</span>
<span id="cb26-282"><a href="#cb26-282"></a>    <span class="bu">print</span>(<span class="st">"------------------------------------------"</span>)</span>
<span id="cb26-283"><a href="#cb26-283"></a></span>
<span id="cb26-284"><a href="#cb26-284"></a>    <span class="co"># .tokenize chunks the existing sequence into different tokens based on the rules and vocab of the tokenizer.</span></span>
<span id="cb26-285"><a href="#cb26-285"></a>    tokens <span class="op">=</span> tokenizer.tokenize(sequence)</span>
<span id="cb26-286"><a href="#cb26-286"></a>    <span class="bu">print</span>(<span class="st">"Tokens : "</span>, tokens)</span>
<span id="cb26-287"><a href="#cb26-287"></a>    <span class="bu">print</span>(<span class="st">"------------------------------------------"</span>)</span>
<span id="cb26-288"><a href="#cb26-288"></a></span>
<span id="cb26-289"><a href="#cb26-289"></a>    <span class="co"># .convert_tokens_to_ids or .encode or .tokenize converts the tokens to their corresponding numerical representation.</span></span>
<span id="cb26-290"><a href="#cb26-290"></a>    <span class="co">#  .convert_tokens_to_ids has a 1-1 mapping between tokens and numerical representation</span></span>
<span id="cb26-291"><a href="#cb26-291"></a>    <span class="co"># ids = tokenizer.convert_tokens_to_ids(tokens)</span></span>
<span id="cb26-292"><a href="#cb26-292"></a>    <span class="co"># print("encoded Ids: ", ids)</span></span>
<span id="cb26-293"><a href="#cb26-293"></a></span>
<span id="cb26-294"><a href="#cb26-294"></a>    <span class="co"># .encode also adds additional information like Start of sequence tokens and End of sequene</span></span>
<span id="cb26-295"><a href="#cb26-295"></a>    <span class="bu">print</span>(<span class="st">"tokenized sequence : "</span>, tokenizer.encode(sequence))</span>
<span id="cb26-296"><a href="#cb26-296"></a></span>
<span id="cb26-297"><a href="#cb26-297"></a>    <span class="co"># .tokenizer has additional information about attention_mask.</span></span>
<span id="cb26-298"><a href="#cb26-298"></a>    <span class="co"># encode = tokenizer(sequence)</span></span>
<span id="cb26-299"><a href="#cb26-299"></a>    <span class="co"># print("Encode sequence : ", encode)</span></span>
<span id="cb26-300"><a href="#cb26-300"></a>    <span class="co"># print("------------------------------------------")</span></span>
<span id="cb26-301"><a href="#cb26-301"></a></span>
<span id="cb26-302"><a href="#cb26-302"></a>    <span class="co"># .decode decodes the ids to raw text</span></span>
<span id="cb26-303"><a href="#cb26-303"></a>    ids <span class="op">=</span> tokenizer.convert_tokens_to_ids(tokens)</span>
<span id="cb26-304"><a href="#cb26-304"></a>    decode <span class="op">=</span> tokenizer.decode(ids)</span>
<span id="cb26-305"><a href="#cb26-305"></a>    <span class="bu">print</span>(<span class="st">"Decode sequence : "</span>, decode)</span>
<span id="cb26-306"><a href="#cb26-306"></a></span>
<span id="cb26-307"><a href="#cb26-307"></a></span>
<span id="cb26-308"><a href="#cb26-308"></a>tokenizer_1  <span class="op">=</span>  AutoTokenizer.from_pretrained(<span class="st">"gpt2"</span>) <span class="co"># GPT-2 uses "Byte-Pair Encoding (BPE)"</span></span>
<span id="cb26-309"><a href="#cb26-309"></a></span>
<span id="cb26-310"><a href="#cb26-310"></a>sequence <span class="op">=</span> <span class="st">"Counselor, please adjust your Zoom filter to appear as a human, rather than as a cat"</span></span>
<span id="cb26-311"><a href="#cb26-311"></a></span>
<span id="cb26-312"><a href="#cb26-312"></a>tokenization_summary(tokenizer_1, sequence)</span>
<span id="cb26-313"><a href="#cb26-313"></a><span class="in">```</span></span>
<span id="cb26-314"><a href="#cb26-314"></a></span>
<span id="cb26-315"><a href="#cb26-315"></a><span class="fu">### Token embedding:</span></span>
<span id="cb26-316"><a href="#cb26-316"></a></span>
<span id="cb26-317"><a href="#cb26-317"></a>Words are turned into vectors based on their location within a vocabulary.</span>
<span id="cb26-318"><a href="#cb26-318"></a></span>
<span id="cb26-319"><a href="#cb26-319"></a>The strategy of choice for learning language structure from tokenized text is to find a clever way to map each token into a moderate-dimension vector space, adjusting the mapping so that</span>
<span id="cb26-320"><a href="#cb26-320"></a></span>
<span id="cb26-321"><a href="#cb26-321"></a>Similar, or associated tokens take up residence nearby each other, and different regions of the space correspond to different position in the sequence.</span>
<span id="cb26-322"><a href="#cb26-322"></a>Such a mapping from token ID to a point in a vector space is called a token embedding. The dimension of the vector space is often high (e.g. 1024-dimensional), but much smaller than the vocabulary size (30,000--500,000).</span>
<span id="cb26-323"><a href="#cb26-323"></a></span>
<span id="cb26-324"><a href="#cb26-324"></a>Various approaches have been attempted for generating such embeddings, including static algorithms that operate on a corpus of tokenized data as preprocessors for NLP tasks. Transformers, however, adjust their embeddings during training.</span>
<span id="cb26-325"><a href="#cb26-325"></a></span>
<span id="cb26-326"><a href="#cb26-326"></a><span class="fu">## Transformer Model Architecture</span></span>
<span id="cb26-327"><a href="#cb26-327"></a></span>
<span id="cb26-328"><a href="#cb26-328"></a>Now let's look at the base elements that</span>
<span id="cb26-329"><a href="#cb26-329"></a>make up a Transformer by dissecting the popular GPT2 model</span>
<span id="cb26-330"><a href="#cb26-330"></a></span>
<span id="cb26-333"><a href="#cb26-333"></a><span class="in">```{python}</span></span>
<span id="cb26-334"><a href="#cb26-334"></a><span class="co">#| colab: {base_uri: https://localhost:8080/}</span></span>
<span id="cb26-335"><a href="#cb26-335"></a><span class="im">from</span> transformers <span class="im">import</span> GPT2Tokenizer, GPT2LMHeadModel</span>
<span id="cb26-336"><a href="#cb26-336"></a>model <span class="op">=</span> GPT2LMHeadModel.from_pretrained(<span class="st">'gpt2'</span>)</span>
<span id="cb26-337"><a href="#cb26-337"></a><span class="bu">print</span>(model)</span>
<span id="cb26-338"><a href="#cb26-338"></a><span class="in">```</span></span>
<span id="cb26-339"><a href="#cb26-339"></a></span>
<span id="cb26-340"><a href="#cb26-340"></a>GPT2 is an example of a Transformer Decoder which is used to generate novel text.</span>
<span id="cb26-341"><a href="#cb26-341"></a></span>
<span id="cb26-342"><a href="#cb26-342"></a>Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models. The pretraining of decoder models usually revolves around predicting the next word in the sentence.</span>
<span id="cb26-343"><a href="#cb26-343"></a></span>
<span id="cb26-344"><a href="#cb26-344"></a>These models are best suited for tasks involving text generation.</span>
<span id="cb26-345"><a href="#cb26-345"></a></span>
<span id="cb26-346"><a href="#cb26-346"></a>The architecture of GPT-2 is inspired by the paper: "Generating Wikipedia by Summarizing Long Sequences" which is another arrangement of the transformer block that can do language modeling. This model threw away the encoder and thus is known as the “Transformer-Decoder”.</span>
<span id="cb26-347"><a href="#cb26-347"></a></span>
<span id="cb26-348"><a href="#cb26-348"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb26-349"><a href="#cb26-349"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"https://github.com/architvasan/ai_science_local/blob/main/images/transformer-decoder-intro.png?raw=1"</span><span class="ot"> width</span><span class="op">=</span><span class="st">"500"</span><span class="dt">/&gt;</span></span>
<span id="cb26-350"><a href="#cb26-350"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb26-351"><a href="#cb26-351"></a></span>
<span id="cb26-352"><a href="#cb26-352"></a><span class="co">[</span><span class="ot">Illustrated GPT2</span><span class="co">](https://jalammar.github.io/illustrated-gpt2/)</span></span>
<span id="cb26-353"><a href="#cb26-353"></a></span>
<span id="cb26-354"><a href="#cb26-354"></a>Key components of the transformer architecture include:</span>
<span id="cb26-355"><a href="#cb26-355"></a></span>
<span id="cb26-356"><a href="#cb26-356"></a><span class="ss">* </span>Input Embeddings: Word embedding or word vectors help us represent words or text as a numeric vector where words with similar meanings have the similar representation.</span>
<span id="cb26-357"><a href="#cb26-357"></a></span>
<span id="cb26-358"><a href="#cb26-358"></a><span class="ss">* </span>Positional Encoding: Injects information about the position of words in a sequence, helping the model understand word order.</span>
<span id="cb26-359"><a href="#cb26-359"></a></span>
<span id="cb26-360"><a href="#cb26-360"></a><span class="ss">* </span>Self-Attention Mechanism: Allows the model to weigh the importance of different words in a sentence, enabling it to effectively capture contextual information.</span>
<span id="cb26-361"><a href="#cb26-361"></a></span>
<span id="cb26-362"><a href="#cb26-362"></a><span class="ss">* </span>Feedforward Neural Networks: Process information from self-attention layers to generate output for each word/token.</span>
<span id="cb26-363"><a href="#cb26-363"></a></span>
<span id="cb26-364"><a href="#cb26-364"></a><span class="ss">* </span>Layer Normalization and Residual Connections: Aid in stabilizing training and mitigating the vanishing gradient problem.</span>
<span id="cb26-365"><a href="#cb26-365"></a></span>
<span id="cb26-366"><a href="#cb26-366"></a><span class="ss">* </span>Transformer Blocks: Comprised of multiple layers of self-attention and feedforward neural networks, stacked together to form the model.</span>
<span id="cb26-367"><a href="#cb26-367"></a></span>
<span id="cb26-368"><a href="#cb26-368"></a><span class="fu">### Attention mechanisms</span></span>
<span id="cb26-369"><a href="#cb26-369"></a></span>
<span id="cb26-370"><a href="#cb26-370"></a>Since attention mechanisms are arguably the most powerful component of the Transformer, let's discuss this in a little more detail.</span>
<span id="cb26-371"><a href="#cb26-371"></a></span>
<span id="cb26-372"><a href="#cb26-372"></a>Suppose the following sentence is an input sentence we want to translate using an LLM:</span>
<span id="cb26-373"><a href="#cb26-373"></a></span>
<span id="cb26-374"><a href="#cb26-374"></a><span class="in">`”The animal didn't cross the street because it was too tired”`</span></span>
<span id="cb26-375"><a href="#cb26-375"></a></span>
<span id="cb26-376"><a href="#cb26-376"></a>To understand a full sentence, the model needs to understand what each word means in relation to other words.</span>
<span id="cb26-377"><a href="#cb26-377"></a></span>
<span id="cb26-378"><a href="#cb26-378"></a>For example, when we read the sentence:</span>
<span id="cb26-379"><a href="#cb26-379"></a><span class="in">`”The animal didn't cross the street because it was too tired”`</span></span>
<span id="cb26-380"><a href="#cb26-380"></a>we know intuitively that the word <span class="in">`"it"`</span> refers to <span class="in">`"animal"`</span>, the state for <span class="in">`"it"`</span> is <span class="in">`"tired"`</span>, and the associated action is <span class="in">`"didn't cross"`</span>.</span>
<span id="cb26-381"><a href="#cb26-381"></a></span>
<span id="cb26-382"><a href="#cb26-382"></a>However, the model needs a way to learn all of this information in a simple yet generalizable way.</span>
<span id="cb26-383"><a href="#cb26-383"></a>What makes Transformers particularly powerful compared to earlier sequential architectures is how it encodes context with the **self-attention mechanism**.</span>
<span id="cb26-384"><a href="#cb26-384"></a></span>
<span id="cb26-385"><a href="#cb26-385"></a>As the model processes each word in the input sequence, attention looks at other positions in the input sequence for clues to a better understanding for this word.</span>
<span id="cb26-386"><a href="#cb26-386"></a></span>
<span id="cb26-387"><a href="#cb26-387"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb26-388"><a href="#cb26-388"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"https://github.com/architvasan/ai_science_local/blob/main/images/transformer_self-attention_visualization.png?raw=1"</span><span class="ot"> width</span><span class="op">=</span><span class="st">"400"</span><span class="dt">/&gt;</span></span>
<span id="cb26-389"><a href="#cb26-389"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb26-390"><a href="#cb26-390"></a></span>
<span id="cb26-391"><a href="#cb26-391"></a><span class="co">[</span><span class="ot">The Illustrated Transformer</span><span class="co">](https://jalammar.github.io/illustrated-transformer/)</span></span>
<span id="cb26-392"><a href="#cb26-392"></a></span>
<span id="cb26-393"><a href="#cb26-393"></a><span class="fu">#### Multi-head attention</span></span>
<span id="cb26-394"><a href="#cb26-394"></a>In practice, multiple attention heads are used simultaneously.</span>
<span id="cb26-395"><a href="#cb26-395"></a></span>
<span id="cb26-396"><a href="#cb26-396"></a>This:</span>
<span id="cb26-397"><a href="#cb26-397"></a><span class="ss">* </span>Expands the model’s ability to focus on different positions.</span>
<span id="cb26-398"><a href="#cb26-398"></a><span class="ss">* </span>Prevents the attention to be dominated by the word itself.</span>
<span id="cb26-399"><a href="#cb26-399"></a></span>
<span id="cb26-400"><a href="#cb26-400"></a><span class="fu">#### Let's see multi-head attention mechanisms in action!</span></span>
<span id="cb26-401"><a href="#cb26-401"></a></span>
<span id="cb26-402"><a href="#cb26-402"></a>We are going to use the powerful visualization tool bertviz, which allows an interactive experience of the attention mechanisms. Normally these mechanisms are abstracted away but this will allow us to inspect our model in more detail.</span>
<span id="cb26-403"><a href="#cb26-403"></a></span>
<span id="cb26-406"><a href="#cb26-406"></a><span class="in">```{python}</span></span>
<span id="cb26-407"><a href="#cb26-407"></a><span class="co">#| colab: {base_uri: https://localhost:8080/}</span></span>
<span id="cb26-408"><a href="#cb26-408"></a><span class="co">#| output: false</span></span>
<span id="cb26-409"><a href="#cb26-409"></a><span class="op">!</span>pip install bertviz</span>
<span id="cb26-410"><a href="#cb26-410"></a><span class="in">```</span></span>
<span id="cb26-411"><a href="#cb26-411"></a></span>
<span id="cb26-412"><a href="#cb26-412"></a>Let's load in the model, GPT2 and look at the attention mechanisms.</span>
<span id="cb26-413"><a href="#cb26-413"></a></span>
<span id="cb26-414"><a href="#cb26-414"></a>**Hint... click on the different blocks in the visualization to see the attention**</span>
<span id="cb26-415"><a href="#cb26-415"></a></span>
<span id="cb26-418"><a href="#cb26-418"></a><span class="in">```{python}</span></span>
<span id="cb26-419"><a href="#cb26-419"></a><span class="co">#| colab: {base_uri: https://localhost:8080/, height: 907, referenced_widgets: [ad748b4c592645cb90d427c9012d7368, a34cdc4592cf46ad9ccf69caa52363bd, c1eac1886ea84c87bed161c960569d3d, ca50498e237843709d8c44e7ea0e94b5, 8f4387f0bb3e402ab1ab4d443c693f96, 47316e9804804c8d9c9b031fa3d86052, 6cc5bd956c694bc5b63c0c7bba2a29f8, 93728bade1714bcabbe90f1b9b95b7c2, 83cea646339b4e74a9582827afdf5753, c765d253aa974bc59654f0398afb964d, 3d71478078bc4743a580dc6200cccbd0, 099e1574f4a14c308be2364d0bba2a76, 6f822f4ae7734ce09736d50bf17ad99e, 93fbd0ce38df488fa725d7c584344e6b, ee56c1c4c2be4086883a95e15adc655d, 846bb66c196540febf6a7bc32b60dc3a, 37dcd77ea3154208be44bfa35a88fa90, fdd9ba7947794f4aa0e2782e3b379f61, 64fa9056ffef4ab3a4e2db085a48cbfb, acee154ece2e4a939f42236ec38de871, 46acb20f74174029962ecb8142c576fb, b58662cfa5fb4406a171135e01d25368, c770f4a4cad0435ebedbb932c1431f52, 14dab4fee3d34783bf838cfbd4bdbcaa, d00cd656bc4f4f0d9cf52b942fceb155, d5fbc43ad5204bb88b11ddceaedc5d6d, 8d825e3d51524720ada2f1ecd9a9ffd5, 9a17934807c140eeb8c0a5cf80c4cb50, 7d2254be42234d24b044f6c9c0d49119, 9eddddc3c9034e6da5bf612da55f5714, 9548afdd0d154788ba3edb96c326c85f, 2213cf9588c545f2b200186766ac425d, d56ccfc975d949b7a6cbe7b9a89a1101, ff3907fe32a14ae681a6a01933e6eaf5, f08eac6bbad843ca95157d035c1ef043, 7e8af1c44d984d99a2b27e80c05cec75, 0a381955a75148d5a59815efb2b352bd, 7ac42549074746b48926a9cffe0a542c, 55b65d9fa1e44149bfc763a8c99e7c2a, cd7ca2bbf9b04de4842b316ea7a5535f, 0c0b4f2c4ef14413bc24340afc040431, 6442fa46c934428998cf8e3d5835e8bc, 8da520e3f829425b83e9b92149f792ad, 78c5836dbb6b4538abcc3cd348c5b958, 3f9fa0a2a0614bae87f7c698d043a3b5, 6c5fb86539584d6dad2af92729b9a1ef, 1eb460d0dad44857a14646782366c7c1, 67c1873c51d34503833366af483744a3, 713fef79f5d441b18f372c64d9569551, 4b5d59490f264d03a9ac52897ae94d33, 42519c4cca864ad78595214e86079c8f, bd3444ba231347378ae1a7ffa4181648, d701e46a5472481ea942b613e3506f03, e2effa7f78034f5c98a2c7f2d12b7ec3, 8ee1c3f4f84f49b9aa78590d20e03005, 00d232fe28f74a1ea9fad0510586ff9c, e2e8c12232854688802988193dd6bdc8, 3129da774c2c4298afbf3d766ca7f428, 0d36758d65ed404e8181bdc4e2a9d26b, 3e75401bc3bf42da8f9e1a377401b69d, fbd25c280a554d50ace7384d5939e335, 3371add482984a3bab5c31379484af71, 001363aab0a143a683a07e48cbbf5594, c87375d637f74dc4ab4bab00852c758c, b5f27258ef4a4b9e8b7d13387b14d9a1, d201d648b932443ca9141aa9129e5661, 0a2f04862b3a4c36becf5949c67a14a5, 8f57a09202cf4cddb2ba697ecfc78daf, a9fae2f4af744d33bddeeb48420cda3e, 2994daa8857945509c97e3f2cdb4ee97, 593dfccfa5ca4e56a32e9cead70714d2, 5f3d428e20334a3da4c82d0fc2e3eb3e, 5e028ae3b1824c74a0d610129dcc8246, 28ba4c7a70294e589cb2ced5766072e4, d3a9c7752afb49a5bb1ea70bdb8744fe, 09eed5891aa647da9f7c9beceac50622, 4868e95bf5ec4cecab53e5e65862aea6]}</span></span>
<span id="cb26-420"><a href="#cb26-420"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModel, utils, AutoModelForCausalLM</span>
<span id="cb26-421"><a href="#cb26-421"></a></span>
<span id="cb26-422"><a href="#cb26-422"></a><span class="im">from</span> bertviz <span class="im">import</span> model_view</span>
<span id="cb26-423"><a href="#cb26-423"></a>utils.logging.set_verbosity_error()  <span class="co"># Suppress standard warnings</span></span>
<span id="cb26-424"><a href="#cb26-424"></a></span>
<span id="cb26-425"><a href="#cb26-425"></a>model_name <span class="op">=</span> <span class="st">'openai-community/gpt2'</span></span>
<span id="cb26-426"><a href="#cb26-426"></a>input_text <span class="op">=</span> <span class="st">"No, I am your father"</span></span>
<span id="cb26-427"><a href="#cb26-427"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_name, output_attentions<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-428"><a href="#cb26-428"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb26-429"><a href="#cb26-429"></a>inputs <span class="op">=</span> tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)  <span class="co"># Tokenize input text</span></span>
<span id="cb26-430"><a href="#cb26-430"></a>outputs <span class="op">=</span> model(inputs)  <span class="co"># Run model</span></span>
<span id="cb26-431"><a href="#cb26-431"></a>attention <span class="op">=</span> outputs[<span class="op">-</span><span class="dv">1</span>]  <span class="co"># Retrieve attention from model outputs</span></span>
<span id="cb26-432"><a href="#cb26-432"></a>tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(inputs[<span class="dv">0</span>])  <span class="co"># Convert input ids to token strings</span></span>
<span id="cb26-433"><a href="#cb26-433"></a>model_view(attention, tokens)  <span class="co"># Display model view</span></span>
<span id="cb26-434"><a href="#cb26-434"></a><span class="in">```</span></span>
<span id="cb26-435"><a href="#cb26-435"></a></span>
<span id="cb26-436"><a href="#cb26-436"></a><span class="fu">## Pipeline using HuggingFace</span></span>
<span id="cb26-437"><a href="#cb26-437"></a></span>
<span id="cb26-438"><a href="#cb26-438"></a>Now, let's see a practical application of LLMs using a HuggingFace pipeline for classification.</span>
<span id="cb26-439"><a href="#cb26-439"></a></span>
<span id="cb26-440"><a href="#cb26-440"></a>This involves a few steps including:</span>
<span id="cb26-441"><a href="#cb26-441"></a><span class="ss">1. </span>Setting up a prompt</span>
<span id="cb26-442"><a href="#cb26-442"></a><span class="ss">2. </span>Loading in a pretrained model</span>
<span id="cb26-443"><a href="#cb26-443"></a><span class="ss">3. </span>Loading in the tokenizer and tokenizing input text</span>
<span id="cb26-444"><a href="#cb26-444"></a><span class="ss">4. </span>Performing model inference</span>
<span id="cb26-445"><a href="#cb26-445"></a><span class="ss">5. </span>Interpreting inference output</span>
<span id="cb26-446"><a href="#cb26-446"></a></span>
<span id="cb26-449"><a href="#cb26-449"></a><span class="in">```{python}</span></span>
<span id="cb26-450"><a href="#cb26-450"></a><span class="co"># STEP 0 : Installations and imports</span></span>
<span id="cb26-451"><a href="#cb26-451"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForSequenceClassification, AutoConfig</span>
<span id="cb26-452"><a href="#cb26-452"></a><span class="im">import</span> torch</span>
<span id="cb26-453"><a href="#cb26-453"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb26-454"><a href="#cb26-454"></a><span class="in">```</span></span>
<span id="cb26-455"><a href="#cb26-455"></a></span>
<span id="cb26-456"><a href="#cb26-456"></a><span class="fu">### 1. Setting up a prompt</span></span>
<span id="cb26-457"><a href="#cb26-457"></a></span>
<span id="cb26-458"><a href="#cb26-458"></a>A "prompt" refers to a specific input or query provided to a language model. They guide the text processing and generation by providing the context for the model to generate coherent and relevant text based on the given input.</span>
<span id="cb26-459"><a href="#cb26-459"></a></span>
<span id="cb26-460"><a href="#cb26-460"></a>The choice and structure of the prompt depends on the specific task, the context and desired output. Prompts can be "discrete" or "instructive" where they are explicit instructions or questions directed to the language model. They can also be more nuanced by more providing suggestions, directions and contexts to the model.</span>
<span id="cb26-461"><a href="#cb26-461"></a></span>
<span id="cb26-462"><a href="#cb26-462"></a>We will use very simple prompts in this tutorial section, but we will learn more about prompt engineering and how it helps in optimizing the performance of the model for a given use case in the following tutorials.</span>
<span id="cb26-463"><a href="#cb26-463"></a></span>
<span id="cb26-466"><a href="#cb26-466"></a><span class="in">```{python}</span></span>
<span id="cb26-467"><a href="#cb26-467"></a><span class="co"># STEP 1 : Set up the prompt</span></span>
<span id="cb26-468"><a href="#cb26-468"></a>input_text <span class="op">=</span> <span class="st">"The panoramic view of the ocean was breathtaking."</span></span>
<span id="cb26-469"><a href="#cb26-469"></a><span class="in">```</span></span>
<span id="cb26-470"><a href="#cb26-470"></a></span>
<span id="cb26-471"><a href="#cb26-471"></a><span class="fu">### 2. Loading Pretrained Models</span></span>
<span id="cb26-472"><a href="#cb26-472"></a></span>
<span id="cb26-473"><a href="#cb26-473"></a>The AutoModelForSequenceClassification from_pretrained() method instantiates a sequence classification model.</span>
<span id="cb26-474"><a href="#cb26-474"></a></span>
<span id="cb26-475"><a href="#cb26-475"></a>Refer to https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodels for the list of model classes supported.</span>
<span id="cb26-476"><a href="#cb26-476"></a></span>
<span id="cb26-477"><a href="#cb26-477"></a>"from_pretrained" method downloads the pre-trained weights from the Hugging Face Model Hub or the specified URL if the model is not already cached locally. It then loads the weights into the instantiated model, initializing the model parameters with the pre-trained values.</span>
<span id="cb26-478"><a href="#cb26-478"></a></span>
<span id="cb26-479"><a href="#cb26-479"></a>The model cache contains:</span>
<span id="cb26-480"><a href="#cb26-480"></a></span>
<span id="cb26-481"><a href="#cb26-481"></a><span class="ss">* </span>model configuration (config.json)</span>
<span id="cb26-482"><a href="#cb26-482"></a><span class="ss">* </span>pretrained model weights (model.safetensors)</span>
<span id="cb26-483"><a href="#cb26-483"></a><span class="ss">* </span>tokenizer information (tokenizer.json, vocab.json, merges.txt, tokenizer.model)</span>
<span id="cb26-484"><a href="#cb26-484"></a></span>
<span id="cb26-487"><a href="#cb26-487"></a><span class="in">```{python}</span></span>
<span id="cb26-488"><a href="#cb26-488"></a><span class="co">#| colab: {base_uri: https://localhost:8080/, height: 671, referenced_widgets: [bd858574deec4181bb0ba443be9a2088, b025f1fa9b61436f93515742e324b86f, 5073ff96876f4e04b246e466be18c186, 0c9e9d1ae5ed4ae4a7e37513d35f9c77, b29ed3c3e599460cbd3d693d71f456d2, 5b43c22a0fa94a7690d637dcef8f9e0a, 51736c335dd349e7bfcc529ada3c6f6a, 4791ebe7b3cc4444afdcbfd8d1b5967a, 93c49acd409c4616b22b28ef971c9dd6, 4777bd67c1e64692b294830f05c7143d, aeb6c3883112462c8ade793f34ebc4da, 82ebac62b0954db1b9fa3e3ae4025a04, 88bc6af07dc244b8bb3ea694ed31d2ae, 21364965ba974886ad36c5d9670e7a70, 00c33e96a50b4993a9f3387402fbccfe, 9de7b337c654426fac8f86b97c7ca7dd, d3d454f41f164fc5af66adf8d2caa53c, 06cd5703c7134fd49a66a259d0154b85, f6131609549c4e7eb89cbb3f430d8aca, e5a7799affd54336b99042ae28609630, 5ee4ce555b264be4becae07722f88a25, 4e181643cf4e449ca06776ecbba79c5e]}</span></span>
<span id="cb26-489"><a href="#cb26-489"></a><span class="co"># STEP 2 : Load the pretrained model.</span></span>
<span id="cb26-490"><a href="#cb26-490"></a>model_name <span class="op">=</span> <span class="st">"distilbert-base-uncased-finetuned-sst-2-english"</span></span>
<span id="cb26-491"><a href="#cb26-491"></a>model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(model_name)</span>
<span id="cb26-492"><a href="#cb26-492"></a>config <span class="op">=</span> AutoConfig.from_pretrained(model_name)</span>
<span id="cb26-493"><a href="#cb26-493"></a><span class="bu">print</span>(config)</span>
<span id="cb26-494"><a href="#cb26-494"></a><span class="in">```</span></span>
<span id="cb26-495"><a href="#cb26-495"></a></span>
<span id="cb26-496"><a href="#cb26-496"></a><span class="fu">### 3. Loading in the tokenizer and tokenizing input text</span></span>
<span id="cb26-497"><a href="#cb26-497"></a></span>
<span id="cb26-498"><a href="#cb26-498"></a>Here, we load in a pretrained tokenizer associated with this model.</span>
<span id="cb26-499"><a href="#cb26-499"></a></span>
<span id="cb26-502"><a href="#cb26-502"></a><span class="in">```{python}</span></span>
<span id="cb26-503"><a href="#cb26-503"></a><span class="co">#| colab: {base_uri: https://localhost:8080/, height: 116, referenced_widgets: [067f49dd461a44fabf2b2fbc48c38ea3, 8081fe15a05f4d93a263016d65f7c867, db4f4633a272445fb65b31b81cc13a47, 683187173eee4b4f84c42ee1b030041e, 9b356d4bb2584404b40a53e84ffd08ad, f8844d328728454cabd0c601f1dfac4c, 999973bfe8554a6b89b2214b39348331, 80f90f7e6d644d1d8b7df13c9e85225d, 683ed139b6384e78a7d16d8375640c74, aaca325b53f449519e4193981a4b3865, d1a67e3364a04697a740fac83669d9f9, 9489672461b54621bdb8bb24d0eb9934, 2862c54dff8c4f83829e946c69962221, 95367cef20634367b463b8f0a95e0503, fac8d31cc40e4c9e80f8ca93a9075077, e61474e5f4784cf6ab8a47decc035ae0, b28301ee0a554b009b4a04ab5c3bcaf0, 8a29da2807084308bd8ccf9414150e31, 545d6ab11dc744099b608face7404d0e, 6c4a6d9277f148c6a575b1b7c6c008f7, cac928cad64b40cb8d0ef32f3731451a, a0e7b739e9a44660b0b8b65448d89a93]}</span></span>
<span id="cb26-504"><a href="#cb26-504"></a><span class="co">#STEP 3 : Load the tokenizer and tokenize the input text</span></span>
<span id="cb26-505"><a href="#cb26-505"></a>tokenizer  <span class="op">=</span>  AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb26-506"><a href="#cb26-506"></a>input_ids <span class="op">=</span> tokenizer(input_text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)[<span class="st">"input_ids"</span>]</span>
<span id="cb26-507"><a href="#cb26-507"></a><span class="bu">print</span>(input_ids)</span>
<span id="cb26-508"><a href="#cb26-508"></a><span class="in">```</span></span>
<span id="cb26-509"><a href="#cb26-509"></a></span>
<span id="cb26-510"><a href="#cb26-510"></a><span class="fu">### 4. Performing inference and interpreting</span></span>
<span id="cb26-511"><a href="#cb26-511"></a></span>
<span id="cb26-512"><a href="#cb26-512"></a>Here, we:</span>
<span id="cb26-513"><a href="#cb26-513"></a><span class="ss">* </span>load data into the model,</span>
<span id="cb26-514"><a href="#cb26-514"></a><span class="ss">* </span>perform inference to obtain logits,</span>
<span id="cb26-515"><a href="#cb26-515"></a><span class="ss">* </span>Convert logits into probabilities</span>
<span id="cb26-516"><a href="#cb26-516"></a><span class="ss">* </span>According to probabilities assign label</span>
<span id="cb26-517"><a href="#cb26-517"></a></span>
<span id="cb26-518"><a href="#cb26-518"></a>The end result is that we can predict whether the input phrase is positive or negative.</span>
<span id="cb26-519"><a href="#cb26-519"></a></span>
<span id="cb26-522"><a href="#cb26-522"></a><span class="in">```{python}</span></span>
<span id="cb26-523"><a href="#cb26-523"></a><span class="co">#| colab: {base_uri: https://localhost:8080/}</span></span>
<span id="cb26-524"><a href="#cb26-524"></a><span class="co"># STEP 5 : Perform inference</span></span>
<span id="cb26-525"><a href="#cb26-525"></a>outputs <span class="op">=</span> model(input_ids)</span>
<span id="cb26-526"><a href="#cb26-526"></a>result <span class="op">=</span> outputs.logits</span>
<span id="cb26-527"><a href="#cb26-527"></a><span class="bu">print</span>(result)</span>
<span id="cb26-528"><a href="#cb26-528"></a></span>
<span id="cb26-529"><a href="#cb26-529"></a><span class="co"># STEP 6 :  Interpret the output.</span></span>
<span id="cb26-530"><a href="#cb26-530"></a>probabilities <span class="op">=</span> F.softmax(result, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb26-531"><a href="#cb26-531"></a><span class="bu">print</span>(probabilities)</span>
<span id="cb26-532"><a href="#cb26-532"></a>predicted_class <span class="op">=</span> torch.argmax(probabilities, dim<span class="op">=-</span><span class="dv">1</span>).item()</span>
<span id="cb26-533"><a href="#cb26-533"></a>labels <span class="op">=</span> [<span class="st">"NEGATIVE"</span>, <span class="st">"POSITIVE"</span>]</span>
<span id="cb26-534"><a href="#cb26-534"></a>out_string <span class="op">=</span> <span class="st">"[{'label': '"</span> <span class="op">+</span> <span class="bu">str</span>(labels[predicted_class]) <span class="op">+</span> <span class="st">"', 'score': "</span> <span class="op">+</span> <span class="bu">str</span>(probabilities[<span class="dv">0</span>][predicted_class].tolist()) <span class="op">+</span> <span class="st">"}]"</span></span>
<span id="cb26-535"><a href="#cb26-535"></a><span class="bu">print</span>(out_string)</span>
<span id="cb26-536"><a href="#cb26-536"></a><span class="in">```</span></span>
<span id="cb26-537"><a href="#cb26-537"></a></span>
<span id="cb26-538"><a href="#cb26-538"></a><span class="fu">### Saving and loading models</span></span>
<span id="cb26-539"><a href="#cb26-539"></a></span>
<span id="cb26-540"><a href="#cb26-540"></a>Model can be saved and loaded to and from a local model directory.</span>
<span id="cb26-541"><a href="#cb26-541"></a></span>
<span id="cb26-544"><a href="#cb26-544"></a><span class="in">```{python}</span></span>
<span id="cb26-545"><a href="#cb26-545"></a><span class="co">#| colab: {base_uri: https://localhost:8080/, height: 81, referenced_widgets: [5c584862c1714236be9a4d72eacb7e8c, 878ac7474c814c56a1f8e4ca1307b446, 61d31a811e704e0b940337009892c774, 90d95ae3318b4f4bbda0f5d016347e91, c603255a544f47b280fd61e9e45d9204, bc69805bf372491d931e20940cf99cfc, 9eb1cc51fc0348069875ce5c52c992b9, a36bbfb2408944c39232013f53811f08, 16994401181f43238336a07a06be119b, dc6892bac378490abd7f628bca35906c, 1dcd4ce0732240aab355557623a19d59, 3933d404330d4d02913a7e075819f807, f286b77c68ce47ec9a5177f2dd35ea48, d15d08eb7374438f8edbcc2d78beafc8, a9a334e404fe47da819a7e97d0dbf958, ed261fb3eb6b4661a0dba67a3fc9858b, 95d2b19696f44d099e530f82ad213328, 725a53042a724dc0a9a658bd3dd3cb83, e4dd55f635a74dbeb0067d1c47769123, 832adb3112224a51ba9376a50076dd8c, c953196990624b0ea79c2a3c74cd1de9, 46f89e0a5ae84c6c97265c795fae9f88]}</span></span>
<span id="cb26-546"><a href="#cb26-546"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoModelForCausalLM</span>
<span id="cb26-547"><a href="#cb26-547"></a></span>
<span id="cb26-548"><a href="#cb26-548"></a><span class="co"># Instantiate and train or fine-tune a model</span></span>
<span id="cb26-549"><a href="#cb26-549"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span>
<span id="cb26-550"><a href="#cb26-550"></a></span>
<span id="cb26-551"><a href="#cb26-551"></a><span class="co"># Train or fine-tune the model...</span></span>
<span id="cb26-552"><a href="#cb26-552"></a></span>
<span id="cb26-553"><a href="#cb26-553"></a><span class="co"># Save the model to a local directory</span></span>
<span id="cb26-554"><a href="#cb26-554"></a>directory <span class="op">=</span> <span class="st">"my_local_model"</span></span>
<span id="cb26-555"><a href="#cb26-555"></a>model.save_pretrained(directory)</span>
<span id="cb26-556"><a href="#cb26-556"></a></span>
<span id="cb26-557"><a href="#cb26-557"></a><span class="co"># Load a pre-trained model from a local directory</span></span>
<span id="cb26-558"><a href="#cb26-558"></a>loaded_model <span class="op">=</span> AutoModel.from_pretrained(directory)</span>
<span id="cb26-559"><a href="#cb26-559"></a><span class="in">```</span></span>
<span id="cb26-560"><a href="#cb26-560"></a></span>
<span id="cb26-561"><a href="#cb26-561"></a><span class="fu">## Model Hub</span></span>
<span id="cb26-562"><a href="#cb26-562"></a>The Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing.</span>
<span id="cb26-563"><a href="#cb26-563"></a></span>
<span id="cb26-564"><a href="#cb26-564"></a><span class="ss">* </span>Download pre-trained models with the huggingface_hub client library, with Transformers for fine-tuning.</span>
<span id="cb26-565"><a href="#cb26-565"></a><span class="ss">* </span>Make use of Inference API to use models in production settings.</span>
<span id="cb26-566"><a href="#cb26-566"></a><span class="ss">* </span>You can filter for different models for different tasks, frameworks used, datasets used, and many more.</span>
<span id="cb26-567"><a href="#cb26-567"></a><span class="ss">* </span>You can select any model, that will show the model card.</span>
<span id="cb26-568"><a href="#cb26-568"></a><span class="ss">* </span>Model card contains information of the model, including the description, usage, limitations etc. Some models also have inference API's that can be used directly.</span>
<span id="cb26-569"><a href="#cb26-569"></a></span>
<span id="cb26-570"><a href="#cb26-570"></a>Model Hub Link : https://huggingface.co/docs/hub/en/models-the-hub</span>
<span id="cb26-571"><a href="#cb26-571"></a></span>
<span id="cb26-572"><a href="#cb26-572"></a>Example of a model card : https://huggingface.co/bert-base-uncased/tree/main</span>
<span id="cb26-573"><a href="#cb26-573"></a></span>
<span id="cb26-574"><a href="#cb26-574"></a><span class="fu">## Recommended reading</span></span>
<span id="cb26-575"><a href="#cb26-575"></a></span>
<span id="cb26-576"><a href="#cb26-576"></a><span class="ss">* </span><span class="co">[</span><span class="ot">"The Illustrated Transformer" by Jay Alammar</span><span class="co">](https://jalammar.github.io/illustrated-transformer/)</span></span>
<span id="cb26-577"><a href="#cb26-577"></a><span class="ss">* </span><span class="co">[</span><span class="ot">"Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)" by Jay Alammar</span><span class="co">](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)</span></span>
<span id="cb26-578"><a href="#cb26-578"></a><span class="ss">* </span><span class="co">[</span><span class="ot">"The Illustrated GPT-2 (Visualizing Transformer Language Models)"</span><span class="co">](https://jalammar.github.io/illustrated-gpt2/)</span></span>
<span id="cb26-579"><a href="#cb26-579"></a><span class="ss">* </span><span class="co">[</span><span class="ot">"A gentle introduction to positional encoding"</span><span class="co">](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)</span></span>
<span id="cb26-580"><a href="#cb26-580"></a><span class="ss">* </span><span class="co">[</span><span class="ot">"LLM Tutorial Workshop (Argonne National Laboratory)"</span><span class="co">](https://github.com/brettin/llm_tutorial)</span></span>
<span id="cb26-581"><a href="#cb26-581"></a><span class="ss">* </span><span class="co">[</span><span class="ot">"LLM Tutorial Workshop Part 2 (Argonne National Laboratory)"</span><span class="co">](https://github.com/argonne-lcf/llm-workshop)</span></span>
<span id="cb26-582"><a href="#cb26-582"></a></span>
<span id="cb26-583"><a href="#cb26-583"></a><span class="fu">## Homework</span></span>
<span id="cb26-584"><a href="#cb26-584"></a></span>
<span id="cb26-585"><a href="#cb26-585"></a><span class="ss">1. </span>Load in a generative model using the HuggingFace pipeline and generate text using a batch of prompts.</span>
<span id="cb26-586"><a href="#cb26-586"></a><span class="ss">  * </span>Play with generative parameters such as temperature, max_new_tokens, and the model itself and explain the effect on the legibility of the model response. Try at least 4 different parameter/model combinations.</span>
<span id="cb26-587"><a href="#cb26-587"></a><span class="ss">  * </span>Models that can be used include:</span>
<span id="cb26-588"><a href="#cb26-588"></a><span class="ss">    * </span><span class="in">`google/gemma-2-2b-it`</span></span>
<span id="cb26-589"><a href="#cb26-589"></a><span class="ss">    * </span><span class="in">`microsoft/Phi-3-mini-4k-instruct`</span></span>
<span id="cb26-590"><a href="#cb26-590"></a><span class="ss">    * </span><span class="in">`meta-llama/Llama-3.2-1B`</span></span>
<span id="cb26-591"><a href="#cb26-591"></a><span class="ss">    * </span>Any model from this list: <span class="co">[</span><span class="ot">Text-generation models</span><span class="co">](https://huggingface.co/models?pipeline_tag=text-generation)</span></span>
<span id="cb26-592"><a href="#cb26-592"></a><span class="ss">    * </span><span class="in">`gpt2`</span> if having trouble loading these models in</span>
<span id="cb26-593"><a href="#cb26-593"></a><span class="ss">  * </span>This guide should help! <span class="co">[</span><span class="ot">Text-generation strategies</span><span class="co">](https://huggingface.co/docs/transformers/en/generation_strategies)</span></span>
<span id="cb26-594"><a href="#cb26-594"></a><span class="ss">2. </span>Load in 2 models of different parameter size (e.g. GPT2, meta-llama/Llama-2-7b-chat-hf, or distilbert/distilgpt2) and analyze the BertViz for each. How does the attention mechanisms change depending on model size?</span>
<span id="cb26-595"><a href="#cb26-595"></a></span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://saforem2.github.io/intro-hpc-bootcamp-2025/">
<p>saforem2.github.io/intro-hpc-bootcamp-2025</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/blob/main/02-llms/1-hands-on-llms/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/edit/main/02-llms/1-hands-on-llms/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/saforem2">
      <i class="bi bi-twitter" role="img" aria-label="Sam Foreman Twitter">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link active" href="https://github.com/saforem2/intro-hpc-bootcamp-2025" aria-current="page">
      <i class="bi bi-github" role="img" aria-label="Sam Foreman GitHub">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>