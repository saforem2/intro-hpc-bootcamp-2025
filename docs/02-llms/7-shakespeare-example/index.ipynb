{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [`wordplay` ğŸ® ğŸ’¬](https://github.com/saforem2/wordplay): Shakespeare\n",
        "\n",
        "âœï¸\n",
        "\n",
        "[Sam Foreman](https://samforeman.me)\n",
        "(\\[[ALCF](https://alcf.anl.gov/about/people/sam-foreman)\\](<https://alcf.anl.gov/about/people/sam-foreman>))  \n",
        "2025-07-22\n",
        "\n",
        "We will be using the [Shakespeare\n",
        "dataset](https://github.com/saforem2/wordplay/blob/main/data/shakespeare/readme.md)\n",
        "to train a (~ small) 10M param LLM *from scratch*.\n",
        "\n",
        "<img src=\"https://github.com/saforem2/wordplay/blob/main/assets/shakespeare.jpeg?raw=true\" width=\"45%\" align=\"center\" /><br>\n",
        "\n",
        "Image generated from\n",
        "[stabilityai/stable-diffusion](https://huggingface.co/spaces/stabilityai/stable-diffusion)\n",
        "on [ğŸ¤— Spaces](https://huggingface.co/spaces).<br>\n",
        "\n",
        "<details closed>\n",
        "\n",
        "<summary>\n",
        "\n",
        "Prompt Details\n",
        "\n",
        "</summary>\n",
        "\n",
        "<ul>\n",
        "\n",
        "<li>\n",
        "\n",
        "Prompt:\n",
        "\n",
        "</li>\n",
        "\n",
        "<t><q> Shakespeare himself, dressed in full Shakespearean garb, writing\n",
        "code at a modern workstation with multiple monitors, hacking away\n",
        "profusely, backlit, high quality for publication </q></t>\n",
        "\n",
        "<li>\n",
        "\n",
        "Negative Prompt:\n",
        "\n",
        "</li>\n",
        "\n",
        "<t><q> low quality, 3d, photorealistic, ugly </q></t>\n",
        "\n",
        "</ul>\n",
        "\n",
        "</details>\n",
        "\n",
        "## Install / Setup\n",
        "\n",
        "<b>Warning!</b><br>\n",
        "\n",
        "**IF YOU ARE EXECUTING ON GOOGLE COLAB**:\n",
        "\n",
        "You will need to restart your runtime (`Runtime` $\\rightarrow\\,$\n",
        "`Restart runtime`)  \n",
        "*after* executing the following cell:"
      ],
      "id": "fd42fb51-00fe-4f1e-8e64-ec5eb2cb228b"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "quarto-private-1": {
          "key": "jupyter",
          "value": {
            "outputs_hidden": false,
            "source_hidden": false
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/Users/samforeman/projects/saforem2/wordplay/src/wordplay/__init__.py\n",
            "Has wordplay installed. Nothing to do."
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "python3 -c 'import wordplay; print(wordplay.__file__)' 2> '/dev/null'\n",
        "\n",
        "if [[ $? -eq 0 ]]; then\n",
        "    echo \"Has wordplay installed. Nothing to do.\"\n",
        "else\n",
        "    echo \"Does not have wordplay installed. Installing...\"\n",
        "    git clone 'https://github.com/saforem2/wordplay'\n",
        "    python3 wordplay/data/shakespeare_char/prepare.py\n",
        "    python3 wordplay/data/shakespeare/prepare.py\n",
        "    python3 -m pip install deepspeed\n",
        "    python3 -m pip install -e wordplay\n",
        "fi"
      ],
      "id": "18e044b3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post Install\n",
        "\n",
        "If installed correctly, you should be able to:\n",
        "\n",
        "``` python\n",
        ">>> import wordplay\n",
        ">>> wordplay.__file__\n",
        "'/path/to/wordplay/src/wordplay/__init__.py'\n",
        "```"
      ],
      "id": "41103d27-f459-4d95-86c7-f34a9aa69367"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "quarto-private-1": {
          "key": "jupyter",
          "value": {
            "outputs_hidden": false,
            "source_hidden": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import os\n",
        "import sys\n",
        "import ezpz\n",
        "\n",
        "os.environ['COLORTERM'] = 'truecolor'\n",
        "if sys.platform == 'darwin':\n",
        "    # If running on MacOS:\n",
        "    # os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
        "    os.environ['TORCH_DEVICE'] = 'cpu'\n",
        "# -----------------------------------------------\n",
        "\n",
        "logger = ezpz.get_logger()\n",
        "\n",
        "import wordplay\n",
        "logger.info(wordplay.__file__)"
      ],
      "id": "5dbf6aed"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Trainer\n",
        "\n",
        "Explicitly, we:\n",
        "\n",
        "1.  `setup_torch(...)`\n",
        "2.  Build `cfg: DictConfig = get_config(...)`\n",
        "3.  Instnatiate `config: ExperimentConfig = instantiate(cfg)`\n",
        "4.  Build `trainer = Trainer(config)`"
      ],
      "id": "2c87f180-1734-46c1-8dc1-c19ec6523bf9"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "quarto-private-1": {
          "key": "jupyter",
          "value": {
            "outputs_hidden": false,
            "source_hidden": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from ezpz import setup\n",
        "from hydra.utils import instantiate\n",
        "from wordplay.configs import get_config, PROJECT_ROOT\n",
        "from wordplay.trainer import Trainer\n",
        "\n",
        "HF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\n",
        "HF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "os.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n",
        "\n",
        "BACKEND = 'DDP'\n",
        "\n",
        "rank = setup(\n",
        "    framework='pytorch',\n",
        "    backend=BACKEND,\n",
        "    seed=1234,\n",
        ")\n",
        "\n",
        "cfg = get_config(\n",
        "    [\n",
        "        'data=shakespeare',\n",
        "        'model=shakespeare',\n",
        "        'model.batch_size=1',\n",
        "        'model.block_size=128',\n",
        "        'optimizer=shakespeare',\n",
        "        'train=shakespeare',\n",
        "        f'train.backend={BACKEND}',\n",
        "        'train.compile=false',\n",
        "        'train.dtype=bfloat16',\n",
        "        'train.max_iters=500',\n",
        "        'train.log_interval=10',\n",
        "        'train.eval_interval=50',\n",
        "    ]\n",
        ")\n",
        "config = instantiate(cfg)"
      ],
      "id": "e8571145"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build `Trainer` object"
      ],
      "id": "d93cd11a-1d05-4c97-b056-423a7cdb3125"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "quarto-private-1": {
          "key": "jupyter",
          "value": {
            "outputs_hidden": false,
            "source_hidden": false
          }
        }
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(config)"
      ],
      "id": "600bdfad"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt (**prior** to training)"
      ],
      "id": "c5599e08-4e57-4522-b0bf-fd52e33e9282"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "quarto-private-1": {
          "key": "jupyter",
          "value": {
            "outputs_hidden": false,
            "source_hidden": false
          }
        }
      },
      "outputs": [],
      "source": [
        "query = \"What is an LLM?\"\n",
        "outputs = trainer.evaluate(\n",
        "    query,\n",
        "    num_samples=1,\n",
        "    max_new_tokens=256,\n",
        "    top_k=16,\n",
        "    display=False\n",
        ")\n",
        "logger.info(f\"['prompt']: '{query}'\")\n",
        "logger.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")"
      ],
      "id": "18c7e430"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model\n",
        "\n",
        "|  name  |        description         |\n",
        "|:------:|:--------------------------:|\n",
        "| `step` |   Current training step    |\n",
        "| `loss` |         Loss value         |\n",
        "|  `dt`  | Time per step (in **ms**)  |\n",
        "| `sps`  |     Samples per second     |\n",
        "| `mtps` |  (million) Tokens per sec  |\n",
        "| `mfu`  | Model Flops utilization[1] |\n",
        "\n",
        "^legend: #tbl-legend\n",
        "\n",
        "[1] in units of A100 `bfloat16` peak FLOPS"
      ],
      "id": "390378c1-a4c9-45ea-9766-2aa1b09eae24"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "quarto-private-1": {
          "key": "jupyter",
          "value": {
            "outputs_hidden": false,
            "source_hidden": false
          }
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          }
        }
      ],
      "source": [
        "trainer.config.device_type"
      ],
      "id": "84730cca"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "quarto-private-1": {
          "key": "colab",
          "value": {
            "base_uri": "https://localhost:8080/",
            "height": 449
          }
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPT</span><span style=\"font-weight: bold\">(</span>\n",
              "  <span style=\"font-weight: bold\">(</span>transformer<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModuleDict</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"font-weight: bold\">(</span>wte<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span><span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">(</span>wpe<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span><span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">(</span>drop<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">(</span>h<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModuleList</span><span style=\"font-weight: bold\">(</span>\n",
              "      <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> x <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Block</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"font-weight: bold\">(</span>ln_1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">()</span>\n",
              "        <span style=\"font-weight: bold\">(</span>attn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CausalSelfAttention</span><span style=\"font-weight: bold\">(</span>\n",
              "          <span style=\"font-weight: bold\">(</span>c_attn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1152</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "          <span style=\"font-weight: bold\">(</span>c_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "          <span style=\"font-weight: bold\">(</span>attn_dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "          <span style=\"font-weight: bold\">(</span>resid_dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "        <span style=\"font-weight: bold\">)</span>\n",
              "        <span style=\"font-weight: bold\">(</span>ln_2<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">()</span>\n",
              "        <span style=\"font-weight: bold\">(</span>mlp<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MLP</span><span style=\"font-weight: bold\">(</span>\n",
              "          <span style=\"font-weight: bold\">(</span>c_fc<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "          <span style=\"font-weight: bold\">(</span>act_fn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GELU</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">approximate</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'none'</span><span style=\"font-weight: bold\">)</span>\n",
              "          <span style=\"font-weight: bold\">(</span>c_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "          <span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "        <span style=\"font-weight: bold\">)</span>\n",
              "      <span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">(</span>ln_f<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">()</span>\n",
              "  <span style=\"font-weight: bold\">)</span>\n",
              "  <span style=\"font-weight: bold\">(</span>lm_head<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>"
            ]
          }
        }
      ],
      "source": [
        "from rich import print\n",
        "\n",
        "print(trainer.model)"
      ],
      "id": "7b201de3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (partial) Training:\n",
        "\n",
        "Weâ€™ll first train for 500 iterations and then evaluate the models\n",
        "performance on the same prompt:\n",
        "\n",
        "> What is an LLM?"
      ],
      "id": "0f9a3469-5be3-4262-afef-2e6c318638cd"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "quarto-private-1": {
          "key": "jupyter",
          "value": {
            "outputs_hidden": false,
            "source_hidden": false
          }
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                Training Legend                 </span>\n",
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\">        abbr </span>â”ƒ<span style=\"font-weight: bold\"> desc                           </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚<span style=\"color: #008000; text-decoration-color: #008000\">        step </span>â”‚ Current training iteration     â”‚\n",
              "â”‚<span style=\"color: #008000; text-decoration-color: #008000\">        loss </span>â”‚ Loss value                     â”‚\n",
              "â”‚<span style=\"color: #008000; text-decoration-color: #008000\">          dt </span>â”‚ Elapsed time per training step â”‚\n",
              "â”‚<span style=\"color: #008000; text-decoration-color: #008000\">         dtf </span>â”‚ Elapsed time per forward step  â”‚\n",
              "â”‚<span style=\"color: #008000; text-decoration-color: #008000\">         dtb </span>â”‚ Elapsed time per backward step â”‚\n",
              "â”‚<span style=\"color: #008000; text-decoration-color: #008000\">         sps </span>â”‚ Samples per second             â”‚\n",
              "â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> sps_per_gpu </span>â”‚ Samples per second (per GPU)   â”‚\n",
              "â”‚<span style=\"color: #008000; text-decoration-color: #008000\">         tps </span>â”‚ Tokens per second              â”‚\n",
              "â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> tps_per_gpu </span>â”‚ Tokens per second (per GPU)    â”‚\n",
              "â”‚<span style=\"color: #008000; text-decoration-color: #008000\">         mfu </span>â”‚ Model flops utilization        â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>"
            ]
          }
        }
      ],
      "source": [
        "trainer.train(train_iters=500)"
      ],
      "id": "2401b6ee"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "quarto-private-1": {
          "key": "colab",
          "value": {
            "base_uri": "https://localhost:8080/",
            "height": 321
          }
        }
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "query = \"What is an LLM?\"\n",
        "t0 = time.perf_counter()\n",
        "outputs = trainer.evaluate(\n",
        "    query,\n",
        "    num_samples=1,\n",
        "    max_new_tokens=256,\n",
        "    top_k=16,\n",
        "    display=False\n",
        ")\n",
        "logger.info(f'took: {time.perf_counter() - t0:.4f}s')\n",
        "logger.info(f\"['prompt']: '{query}'\")\n",
        "logger.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")"
      ],
      "id": "d15b3b16"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resume Trainingâ€¦"
      ],
      "id": "47a77859-04c9-4c4f-a079-e60615e64333"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "quarto-private-1": {
          "key": "colab",
          "value": {
            "base_uri": "https://localhost:8080/",
            "height": 832
          }
        }
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ],
      "id": "580bf7a1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Model"
      ],
      "id": "2ea5b13a-4e7e-43ba-aacb-2ef04d622ae4"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "quarto-private-1": {
          "key": "jupyter",
          "value": {
            "outputs_hidden": false,
            "source_hidden": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "query = \"What is an LLM?\"\n",
        "t0 = time.perf_counter()\n",
        "outputs = trainer.evaluate(\n",
        "    query,\n",
        "    num_samples=1,\n",
        "    max_new_tokens=256,\n",
        "    top_k=2,\n",
        "    display=False\n",
        ")\n",
        "logger.info(f'took: {time.perf_counter() - t0:.4f}s')\n",
        "logger.info(f\"['prompt']: '{query}'\")\n",
        "logger.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")"
      ],
      "id": "bba4e0e5"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  }
}