# Introduction to Large Language Models
Sam Foreman
2025-07-15

<link rel="preconnect" href="https://fonts.googleapis.com">

- [Brief overview](#brief-overview)
- [References](#references)

> [!NOTE]
>
> ### Authors
>
> Content modified from original content written by Archit Vasan ,
> including materials on LLMs by Varuni Sastri and Carlo Graziani at
> Argonne, and discussion/editorial work by Taylor Childers, Bethany
> Lusch, and Venkat Vishwanath (Argonne)

Inspiration from the blog posts ‚ÄúThe Illustrated Transformer‚Äù and ‚ÄúThe
Illustrated GPT2‚Äù by Jay Alammar, highly recommended reading.

This tutorial covers the some fundamental concepts necessary to to study
of large language models (LLMs).

## Brief overview

- Scientific applications for language models
- General overview of Transformers
- Tokenization
- Model Architecture
- Pipeline using HuggingFace
- Model loading

## References

I strongly recommend reading:

- [‚ÄúThe Illustrated
  Transformer‚Äù](https://jalammar.github.io/illustrated-transformer/) by
  Jay AlammarAlammar also has a useful post dedicated more generally to
  Sequence-to-Sequence modeling
- [LLM Course by ü§ó
  HuggingFace](https://huggingface.co/learn/llm-course/chapter1/1)
- [‚ÄúVisualizing A Neural Machine Translation Model (Mechanics of Seq2seq
  Models With
  Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/),
  which illustrates the attention mechanism in the context of a more
  generic language translation model.
- [GPT in 60 Lines of
  NumPy](https://jaykmody.com/blog/gpt-from-scratch/)
