<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.18">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sam Foreman">
<meta name="dcterms.date" content="2025-08-05">
<meta name="description" content="An introduction to High Performance Computing (HPC) and Artificial Intelligence (AI) for scientific applications, with a focus on practical skills and hands-on experience.">

<title>Language models (LMs) – Intro to HPC Bootcamp 2025</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../02-llms/01-hands-on-llms/index.html" rel="next">
<link href="../../02-llms/index.html" rel="prev">
<link href="../../assets/favicon-sf.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-9bf6f933b760cbb4c99ada293d62cb55.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-22c6c940514060718923bc7c0517e30f.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-1a95a006fa96163f90bf11620203e68d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-174353d2fa111ba48747fab321614398.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=GTM-MRCG68M5"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'GTM-MRCG68M5', { 'anonymize_ip': true});
</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script type="text/javascript">
window.PlotlyConfig = {MathJaxConfig: 'local'};
if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
</script>
<script type="module">import "https://cdn.plot.ly/plotly-3.0.1.min"</script>

<script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous">
<script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js" integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6" crossorigin="anonymous"></script>
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;family=IBM+Plex+Sans+Condensed:ital,wght@0,400;0,500;0,600;0,700&amp;family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans&amp;family=IBM+Plex+Sans+Condensed&amp;family=IBM+Plex+Mono&amp;display=swap" rel="stylesheet">
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-TC329HJ');</script>
<!-- End Google Tag Manager -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link href="https://iosevka-webfonts.github.io/iosevka/iosevka.css" rel="stylesheet">

  <script defer="" src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Language models (LMs) – Intro to HPC Bootcamp 2025">
<meta property="og:description" content="An introduction to High Performance Computing (HPC) and Artificial Intelligence (AI) for scientific applications, with a focus on practical skills and hands-on experience.">
<meta property="og:image" content="https://saforem2.github.io/hpc-bootcamp-2025/02-llms/00-intro-to-llms/images/RNA-codons.svg.png">
<meta property="og:site_name" content="Intro to HPC Bootcamp 2025">
<meta property="og:image:height" content="467">
<meta property="og:image:width" content="330">
<meta name="twitter:title" content="Language models (LMs) – Intro to HPC Bootcamp 2025">
<meta name="twitter:description" content="An introduction to High Performance Computing (HPC) and Artificial Intelligence (AI) for scientific applications, with a focus on practical skills and hands-on experience.">
<meta name="twitter:image" content="https://saforem2.github.io/hpc-bootcamp-2025/02-llms/00-intro-to-llms/images/RNA-codons.svg.png">
<meta name="twitter:site" content="saforem2">
<meta name="twitter:image-height" content="467">
<meta name="twitter:image-width" content="330">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="Language models (LMs)">
<meta name="citation_author" content="Sam Foreman">
<meta name="citation_publication_date" content="2025-08-05">
<meta name="citation_cover_date" content="2025-08-05">
<meta name="citation_year" content="2025">
<meta name="citation_online_date" content="2025-08-05">
<meta name="citation_fulltext_html_url" content="https://saforem2.github.io/hpc-bootcamp-2025/02-llms/00-intro-to-llms/">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=The hugging face course, 2022;,citation_author=Hugging Face;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_publisher=https://huggingface.co/course;">
<meta name="citation_reference" content="citation_title=HiPerRAG: High-performance retrieval augmented generation for scientific insights;,citation_author=Ozan Gokdemir;,citation_author=Carlo Siebenschuh;,citation_author=Alexander Brace;,citation_author=Azton Wells;,citation_author=Brian Hsu;,citation_author=Kyle Hippe;,citation_author=Priyanka V. Setty;,citation_author=Aswathy Ajith;,citation_author=J. Gregory Pauloski;,citation_author=Varuni Sastry;,citation_author=Sam Foreman;,citation_author=Huihuo Zheng;,citation_author=Heng Ma;,citation_author=Bharat Kale;,citation_author=Nicholas Chia;,citation_author=Thomas Gibbs;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Francis J. Alexander;,citation_author=Anima Anandkumar;,citation_author=Ian Foster;,citation_author=Rick Stevens;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://arxiv.org/abs/2505.04846;">
<meta name="citation_reference" content="citation_title=MOFA: Discovering materials for carbon capture with a GenAI- and simulation-based workflow;,citation_author=Xiaoli Yan;,citation_author=Nathaniel Hudson;,citation_author=Hyun Park;,citation_author=Daniel Grzenda;,citation_author=J. Gregory Pauloski;,citation_author=Marcus Schwarting;,citation_author=Haochen Pan;,citation_author=Hassan Harb;,citation_author=Samuel Foreman;,citation_author=Chris Knight;,citation_author=Tom Gibbs;,citation_author=Kyle Chard;,citation_author=Santanu Chaudhuri;,citation_author=Emad Tajkhorshid;,citation_author=Ian Foster;,citation_author=Mohamad Moosavi;,citation_author=Logan Ward;,citation_author=E. A. Huerta;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://arxiv.org/abs/2501.10651;">
<meta name="citation_reference" content="citation_title=MProt-DPO: Breaking the ExaFLOPS barrier for multimodal protein design workflows with direct preference optimization;,citation_abstract=We present a scalable, end-to-end workflow for protein design. By augmenting protein sequences with natural language descriptions of their biochemical properties, we train generative models that can be preferentially aligned with protein fitness landscapes. Through complex experimental- and simulation-based observations, we integrate these measures as preferred parameters for generating new protein variants and demonstrate our workflow on five diverse supercomputers. We achieve &amp;amp;amp;gt;1 ExaFLOPS sustained performance in mixed precision on each supercomputer and a maximum sustained performance of 4.11 ExaFLOPS and peak performance of 5.57 ExaFLOPS. We establish the scientific performance of our model on two tasks: (1) across a predetermined benchmark dataset of deep mutational scanning experiments to optimize the fitness-determining mutations in the yeast protein HIS7, and (2) in optimizing the design of the enzyme malate dehydrogenase to achieve lower activation barriers (and therefore increased catalytic rates) using simulation data. Our implementation thus sets high watermarks for multimodal protein design workflows.;,citation_author=Gautham Dharuman;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Sam Foreman;,citation_author=Väinö Hatanpää;,citation_author=Varuni K. Sastry;,citation_author=Huihuo Zheng;,citation_author=Logan Ward;,citation_author=Servesh Muralidharan;,citation_author=Archit Vasan;,citation_author=Bharat Kale;,citation_author=Carla M. Mann;,citation_author=Heng Ma;,citation_author=Yun-Hsuan Cheng;,citation_author=Yuliana Zamora;,citation_author=Shengchao Liu;,citation_author=Chaowei Xiao;,citation_author=Murali Emani;,citation_author=Tom Gibbs;,citation_author=Mahidhar Tatineni;,citation_author=Deepak Canchi;,citation_author=Jerome Mitchell;,citation_author=Koichi Yamada;,citation_author=Maria Garzaran;,citation_author=Michael E. Papka;,citation_author=Ian Foster;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://doi.org/10.1109/SC41406.2024.00013;,citation_doi=10.1109/SC41406.2024.00013;,citation_isbn=9798350352917;,citation_conference_title=Proceedings of the international conference for high performance computing, networking, storage, and analysis;,citation_conference=IEEE Press;,citation_series_title=SC ’24;">
<meta name="citation_reference" content="citation_title=Quality measures for dynamic graph generative models;,citation_author=Ryien Hosseini;,citation_author=Filippo Simini;,citation_author=Venkatram Vishwanath;,citation_author=Rebecca Willett;,citation_author=Henry Hoffmann;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://openreview.net/forum?id=8bjspmAMBk;,citation_conference_title=The thirteenth international conference on learning representations;">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=RG-inspired machine learning for lattice field theory;,citation_author=Sam Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_volume=175;,citation_conference_title=EPJ web of conferences;,citation_conference=EDP Sciences;">
<meta name="citation_reference" content="citation_title=Large energy density in three-plate nanocapacitors due to coulomb blockade;,citation_author=A Hubler;,citation_author=S Foreman;,citation_author=J Liu;,citation_author=L Wortsmann;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=10;,citation_volume=123;,citation_journal_title=Journal of Applied Physics;,citation_publisher=AIP Publishing;">
<meta name="citation_reference" content="citation_title=Examples of renormalization group transformations for image sets;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=5;,citation_volume=98;,citation_journal_title=Physical Review E;,citation_publisher=American Physical Society;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the Ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.22323/1.334.0245;,citation_volume=LATTICE2018;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=Lattice 2018;">
<meta name="citation_reference" content="citation_title=Learning better physics: A machine learning approach to lattice gauge theory;,citation_author=Samuel Alfred Foreman;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_dissertation_institution=University of Iowa;">
<meta name="citation_reference" content="citation_title=Machine learning and neural networks for field theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=HMC with normalizing flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_journal_title=arXiv preprint arXiv:2112.01586;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy storage in quantum resonators;,citation_author=Jiaqi Liu;,citation_author=Alfred W Hubler;,citation_author=Samuel Alfred Foreman;,citation_author=Katharina Ott;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Applications of machine learning to lattice quantum field theory;,citation_author=Denis Boyda;,citation_author=Salvatore Calı̀;,citation_author=Sam Foreman;,citation_author=Lena Funcke;,citation_author=Daniel C Hackett;,citation_author=Yin Lin;,citation_author=Gert Aarts;,citation_author=Andrei Alexandru;,citation_author=Xiao-Yong Jin;,citation_author=Biagio Lucini;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_journal_title=arXiv preprint arXiv:2202.05838;">
<meta name="citation_reference" content="citation_title=Lattice QCD and particle physics;,citation_author=Andreas S Kronfeld;,citation_author=Tanmoy Bhattacharya;,citation_author=Thomas Blum;,citation_author=Norman H Christ;,citation_author=Carleton DeTar;,citation_author=William Detmold;,citation_author=Robert Edwards;,citation_author=Anna Hasenfratz;,citation_author=Huey-Wen Lin;,citation_author=Swagato Mukherjee;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2207.07641;,citation_journal_title=arXiv preprint arXiv:2207.07641;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=6;,citation_volume=37;,citation_journal_title=The International Journal of High Performance Computing Applications;,citation_publisher=SAGE Publications Sage UK: London, England;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=The international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=A comprehensive performance study of large language models on novel AI accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04607;,citation_journal_title=arXiv preprint arXiv:2310.04607;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04610;,citation_journal_title=arXiv preprint arXiv:2310.04610;">
<meta name="citation_reference" content="citation_title=Protein generation via genome-scale language models with bio-physical scoring;,citation_author=Gautham Dharuman;,citation_author=Logan Ward;,citation_author=Heng Ma;,citation_author=Priyanka V Setty;,citation_author=Ozan Gokdemir;,citation_author=Sam Foreman;,citation_author=Murali Emani;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Kristopher Keipert;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=Proceedings of the SC’23 workshops of the international conference on high performance computing, network, storage, and analysis;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2312.08936;,citation_journal_title=arXiv preprint arXiv:2312.08936;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 computational frontier CompF03 topical group report: Machine learning;,citation_author=Phiala Shanahan;,citation_author=Kazuhiro Terao;,citation_author=Daniel Whiteson;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;,citation_journal_title=arXiv preprint arXiv:2209.07559;">
<meta name="citation_reference" content="citation_title=Thorough characterization and analysis of large transformer model training at-scale;,citation_author=Scott Cheng;,citation_author=Jun-Liang Lin;,citation_author=Murali Emani;,citation_author=Siddhisanket Raskar;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Venkatram Vishwanath;,citation_author=Mahmut Taylan Kandemir;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=8;,citation_journal_title=Proceedings of the ACM on Measurement and Analysis of Computing Systems;,citation_publisher=ACM New York, NY, USA;">
<meta name="citation_reference" content="citation_title=Communities through energy justice projects;,citation_author=Mary Ann Leung;,citation_author=Katharine Cahill;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois Curfman McInnes;,citation_author=Suzanne Parete-Koon;,citation_author=Subil Abraham;,citation_author=Lacy Beach Barrier;,citation_author=Gladys Chen;,citation_author=Lizanne DeStefano;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science;">
<meta name="citation_reference" content="citation_title=Applications of a foundation model approach for weather and climate;,citation_author=Troy Arcomano;,citation_author=Alexander Wikner;,citation_author=Romit Maulik;,citation_author=Veerabhadra Rao Kotamarthi;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_volume=2023;,citation_conference_title=AGU fall meeting abstracts;">
<meta name="citation_reference" content="citation_title=Toward a holistic performance evaluation of large language models across diverse ai accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_author=Sanjif Shanmugavelu;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 IEEE international parallel and distributed processing symposium workshops (IPDPSW);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Intro to HPC bootcamp: Engaging new communities through energy justice projects;,citation_author=Suzanne Parete-Koon;,citation_author=Michael Sandoval;,citation_author=Kellen Leland;,citation_author=Subil Abraham;,citation_author=Mary Ann Leung;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois McInnes;,citation_author=Sreeranjani Ramprakash;,citation_author=Lacy Beach Barrier;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science Education;,citation_publisher=Oak Ridge National Laboratory (ORNL), Oak Ridge, TN (United States);">
<meta name="citation_reference" content="citation_title=MProt-DPO: Breaking the ExaFLOPS barrier for multimodal protein design workflows with direct preference optimization;,citation_author=Gautham Dharuman;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Sam Foreman;,citation_author=Väinä Hatanpää;,citation_author=Varuni K Sastry;,citation_author=Huihuo Zheng;,citation_author=Logan Ward;,citation_author=Servesh Muralidharan;,citation_author=Archit Vasan;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 SC24: International conference for high performance computing, networking, storage and analysis SC;,citation_conference=IEEE Computer Society;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=Connor Holmes;,citation_author=Martin Cai;,citation_author=Adam Ghanem;,citation_author=Zhongzhu Zhou;,citation_author=Yuxiong He;,citation_author=Pete Luferenko;,citation_author=Divya Kumar;,citation_author=Jonathan Weyn;,citation_author=Ruixiong Zhang;,citation_author=Sylwester Klocek;,citation_author=Volodymyr Vragov;,citation_author=Mohammed AlQuraishi;,citation_author=Gustaf Ahdritz;,citation_author=Christina Floristean;,citation_author=Cristina Negri;,citation_author=Rao Kotamarthi;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_author=Sam Foreman;,citation_author=Kyle Hippe;,citation_author=Troy Arcomano;,citation_author=Romit Maulik;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot;,citation_author=Murali Emani;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Prasanna Balaprakash;,citation_author=Gina Tourassi;,citation_author=John Gounley;,citation_author=Heidi Hanson;,citation_author=Thomas E Potok;,citation_author=Massimiliano Lupo Pasini;,citation_author=Kate Evans;,citation_author=Dan Lu;,citation_author=Dalton Lunga;,citation_author=Junqi Yin;,citation_author=Sajal Dash;,citation_author=Feiyi Wang;,citation_author=Mallikarjun Shankar;,citation_author=Isaac Lyngaas;,citation_author=Xiao Wang;,citation_author=Guojing Cong;,citation_author=Pei Zhang;,citation_author=Ming Fan;,citation_author=Siyan Liu;,citation_author=Adolfy Hoisie;,citation_author=Shinjae Yoo;,citation_author=Yihui Ren;,citation_author=William Tang;,citation_author=Kyle Felker;,citation_author=Alexey Svyatkovskiy;,citation_author=Hang Liu;,citation_author=Ashwin Aji;,citation_author=Angela Dalton;,citation_author=Michael Schulte;,citation_author=Karl Schulz;,citation_author=Yuntian Deng;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Anima Anandkumar;,citation_author=Rick Stevens;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04610;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=The climate risk &amp;amp;amp; resilience portal (ClimRR) metadata and data dictionary;,citation_author=C. Burdi;,citation_author=Wall. T Branham;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://dub.sh/ClimRR-Metadata;">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2105.03418;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy Justice Analysis of Climate Data with ClimRR;,citation_author=Sam Foreman;,citation_publication_date=2023-08-07;,citation_cover_date=2023-08-07;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/climate-analysis;,citation_language=en;">
<meta name="citation_reference" content="citation_author=Sam Foreman;,citation_publication_date=2023-08-19;,citation_cover_date=2023-08-19;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/l2hmc-qcd;,citation_language=en;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James Osborn;,citation_publication_date=00;,citation_cover_date=00;,citation_year=0;,citation_conference_title=40th international symposium on lattice field theory (lattice 2023) (batavia, IL, united states, 07/31/2023 - 08/04/2023);">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=S. Foreman;,citation_author=X. Jin;,citation_author=J. Osborn;,citation_publication_date=2022-07;,citation_cover_date=2022-07;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_conference_title=The 38th international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Mastering language models;,citation_author=Samuel Montgomery;,citation_publication_date=2023-10;,citation_cover_date=2023-10;,citation_year=2023;,citation_fulltext_html_url=https://towardsdatascience.com/mastering-language-models-32e1d891511a
           ;,citation_journal_title=Medium;,citation_publisher=Towards Data Science;">
<meta name="citation_reference" content="citation_title=Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond;,citation_author=Jingfeng Yang;,citation_author=Hongye Jin;,citation_author=Ruixiang Tang;,citation_author=Xiaotian Han;,citation_author=Qizhang Feng;,citation_author=Haoming Jiang;,citation_author=Bing Yin;,citation_author=Xia Hu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2304.13712;">
<meta name="citation_reference" content="citation_title=Training tips for the transformer model;,citation_author=Martin Popel;,citation_author=Ondřej Bojar;,citation_publication_date=2018-04;,citation_cover_date=2018-04;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.2478%2Fpralin-2018-0002;,citation_issue=1;,citation_doi=10.2478/pralin-2018-0002;,citation_volume=110;,citation_journal_title=The Prague Bulletin of Mathematical Linguistics;,citation_publisher=Charles University in Prague, Karolinum Press;">
<meta name="citation_reference" content="citation_title=Attention is all you need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N. Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03762;">
<meta name="citation_reference" content="citation_title=Tree of thoughts: Deliberate problem solving with large language models;,citation_author=Shunyu Yao;,citation_author=Dian Yu;,citation_author=Jeffrey Zhao;,citation_author=Izhak Shafran;,citation_author=Thomas L. Griffiths;,citation_author=Yuan Cao;,citation_author=Karthik Narasimhan;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2305.10601;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_abstract=We seek to transform how new and emergent variants of pandemiccausing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.Competing Interest StatementThe authors have declared no competing interest.;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot-Sasson;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.biorxiv.org/content/early/2022/11/23/2022.10.10.511571;,citation_doi=10.1101/2022.10.10.511571;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
</head>

<body class="nav-sidebar floating quarto-dark"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = true;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../02-llms/index.html">[02] Large Language Models</a></li><li class="breadcrumb-item"><a href="../../02-llms/00-intro-to-llms/index.html">[0] Intro to LLMs</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Intro to HPC Bootcamp 2025</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="../../index.html" title="Home" class="quarto-navigation-tool px-1" aria-label="Home"><i class="bi bi-home"></i></a>
    <a href="https://github.com/saforem2/intro-hpc-bootcamp-2025" title="GitHub" class="quarto-navigation-tool px-1" aria-label="GitHub"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../00-intro-AI-HPC/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[00] Intro to AI and HPC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/0-compute-systems/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] Compute Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/1-shared-resources/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[1] Shared Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/2-jupyter-notebooks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[2] Jupyter Notebooks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/3-python/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[3] Using Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/4-data/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[4] Working with Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/5-mcmc-example/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[5] MCMC Example</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/6-linear-regression/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[6] Linear Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/7-statistical-learning/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[7] Statistical Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/8-clustering/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[8] Clustering</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../01-neural-networks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[01] Neural Networks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/0-intro/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] Intro to NNs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/1-mnist/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[1] MNIST Example</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/1-mnist-ipynb/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[1] MNIST Example (ipynb)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/2-advanced/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[2] Advanced</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/3-conv-nets/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[3] Conv. Nets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/4-representation-learning/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[4] Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/5-distributed-training/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[5] Distributed Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../02-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[02] Large Language Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/00-intro-to-llms/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">[0] Intro to LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/01-hands-on-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[1] Hands-on LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/06-parallel-training/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[6] Parallel Training</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/08-shakespeare-example-colab/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[8] Shakespeare Example (Colab)</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#modeling-sequential-data" id="toc-modeling-sequential-data" class="nav-link" data-scroll-target="#modeling-sequential-data">Modeling Sequential Data</a></li>
  <li><a href="#scientific-sequential-data-modeling-examples" id="toc-scientific-sequential-data-modeling-examples" class="nav-link" data-scroll-target="#scientific-sequential-data-modeling-examples">Scientific sequential data modeling examples</a>
  <ul class="collapse">
  <li><a href="#nucleic-acid-sequences-genomic-data" id="toc-nucleic-acid-sequences-genomic-data" class="nav-link" data-scroll-target="#nucleic-acid-sequences-genomic-data">Nucleic acid sequences + genomic data</a></li>
  <li><a href="#protein-sequences" id="toc-protein-sequences" class="nav-link" data-scroll-target="#protein-sequences">Protein sequences</a></li>
  <li><a href="#other-applications" id="toc-other-applications" class="nav-link" data-scroll-target="#other-applications">Other applications:</a></li>
  </ul></li>
  <li><a href="#overview-of-language-models" id="toc-overview-of-language-models" class="nav-link" data-scroll-target="#overview-of-language-models">Overview of Language models</a>
  <ul class="collapse">
  <li><a href="#rnns" id="toc-rnns" class="nav-link" data-scroll-target="#rnns">RNNs</a></li>
  <li><a href="#transformers" id="toc-transformers" class="nav-link" data-scroll-target="#transformers">Transformers</a></li>
  </ul></li>
  <li><a href="#coding-example-of-llms-in-action" id="toc-coding-example-of-llms-in-action" class="nav-link" data-scroll-target="#coding-example-of-llms-in-action">Coding example of LLMs in action!</a></li>
  <li><a href="#whats-going-on-under-the-hood" id="toc-whats-going-on-under-the-hood" class="nav-link" data-scroll-target="#whats-going-on-under-the-hood">What’s going on under the hood?</a>
  <ul class="collapse">
  <li><a href="#tokenization" id="toc-tokenization" class="nav-link" data-scroll-target="#tokenization">Tokenization</a></li>
  <li><a href="#token-embedding" id="toc-token-embedding" class="nav-link" data-scroll-target="#token-embedding">Token embedding:</a></li>
  <li><a href="#we-can-visualize-these-embeddings-of-the-popular-bert-model-using-pca" id="toc-we-can-visualize-these-embeddings-of-the-popular-bert-model-using-pca" class="nav-link" data-scroll-target="#we-can-visualize-these-embeddings-of-the-popular-bert-model-using-pca">We can visualize these embeddings of the popular BERT model using PCA!</a></li>
  </ul></li>
  <li><a href="#elements-of-a-transformer" id="toc-elements-of-a-transformer" class="nav-link" data-scroll-target="#elements-of-a-transformer">Elements of a Transformer</a></li>
  <li><a href="#attention-mechanisms" id="toc-attention-mechanisms" class="nav-link" data-scroll-target="#attention-mechanisms">Attention mechanisms</a>
  <ul class="collapse">
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention">Multi-head attention</a></li>
  <li><a href="#lets-see-attention-mechanisms-in-action" id="toc-lets-see-attention-mechanisms-in-action" class="nav-link" data-scroll-target="#lets-see-attention-mechanisms-in-action">Let’s see attention mechanisms in action!</a></li>
  </ul></li>
  <li><a href="#positional-encoding" id="toc-positional-encoding" class="nav-link" data-scroll-target="#positional-encoding">Positional encoding</a></li>
  <li><a href="#output-layers" id="toc-output-layers" class="nav-link" data-scroll-target="#output-layers">Output layers</a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training">Training</a></li>
  <li><a href="#lets-train-a-mini-llm-from-scratch" id="toc-lets-train-a-mini-llm-from-scratch" class="nav-link" data-scroll-target="#lets-train-a-mini-llm-from-scratch">Let’s train a mini-LLM from scratch</a>
  <ul class="collapse">
  <li><a href="#set-up-hyperparameters" id="toc-set-up-hyperparameters" class="nav-link" data-scroll-target="#set-up-hyperparameters">Set up hyperparameters:</a></li>
  <li><a href="#load-in-data-and-create-train-and-test-datasets" id="toc-load-in-data-and-create-train-and-test-datasets" class="nav-link" data-scroll-target="#load-in-data-and-create-train-and-test-datasets">Load in data and create train and test datasets</a></li>
  <li><a href="#set-up-the-components-of-the-decoder-block" id="toc-set-up-the-components-of-the-decoder-block" class="nav-link" data-scroll-target="#set-up-the-components-of-the-decoder-block">Set up the components of the Decoder block:</a></li>
  <li><a href="#combine-components-into-the-decoder-block" id="toc-combine-components-into-the-decoder-block" class="nav-link" data-scroll-target="#combine-components-into-the-decoder-block">Combine components into the Decoder block</a></li>
  <li><a href="#set-up-the-full-transformer-model" id="toc-set-up-the-full-transformer-model" class="nav-link" data-scroll-target="#set-up-the-full-transformer-model">Set up the full Transformer model</a></li>
  </ul></li>
  <li><a href="#homework" id="toc-homework" class="nav-link" data-scroll-target="#homework">Homework</a></li>
  <li><a href="#different-types-of-transformers" id="toc-different-types-of-transformers" class="nav-link" data-scroll-target="#different-types-of-transformers">Different types of Transformers</a>
  <ul class="collapse">
  <li><a href="#encoder-decoder-architecture" id="toc-encoder-decoder-architecture" class="nav-link" data-scroll-target="#encoder-decoder-architecture">Encoder-Decoder architecture</a></li>
  <li><a href="#encoder-only-transformers" id="toc-encoder-only-transformers" class="nav-link" data-scroll-target="#encoder-only-transformers">Encoder-only Transformers</a></li>
  <li><a href="#bidirectional-encoder-representations-from-transformers-bert-model" id="toc-bidirectional-encoder-representations-from-transformers-bert-model" class="nav-link" data-scroll-target="#bidirectional-encoder-representations-from-transformers-bert-model">Bidirectional Encoder Representations from Transformers (BERT) model</a></li>
  <li><a href="#decoder-only-models" id="toc-decoder-only-models" class="nav-link" data-scroll-target="#decoder-only-models">Decoder-only models</a></li>
  <li><a href="#advantages-and-disadvantages-1" id="toc-advantages-and-disadvantages-1" class="nav-link" data-scroll-target="#advantages-and-disadvantages-1">Advantages and disadvantages</a></li>
  </ul></li>
  <li><a href="#additional-architectures" id="toc-additional-architectures" class="nav-link" data-scroll-target="#additional-architectures">Additional architectures</a>
  <ul class="collapse">
  <li><a href="#vision-transformers" id="toc-vision-transformers" class="nav-link" data-scroll-target="#vision-transformers">Vision Transformers</a></li>
  <li><a href="#graph-transformers" id="toc-graph-transformers" class="nav-link" data-scroll-target="#graph-transformers">Graph Transformers</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/blob/main/02-llms/00-intro-to-llms/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/edit/main/02-llms/00-intro-to-llms/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.ipynb" download="index.ipynb"><i class="bi bi-journal-code"></i>Jupyter</a></li><li><a href="README.md"><i class="bi bi-file-code"></i>Github (GFM)</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TC329HJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../02-llms/index.html">[02] Large Language Models</a></li><li class="breadcrumb-item"><a href="../../02-llms/00-intro-to-llms/index.html">[0] Intro to LLMs</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Language models (LMs)</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>

<div>
  <div class="description">
    An introduction to High Performance Computing (HPC) and Artificial Intelligence (AI) for scientific applications, with a focus on practical skills and hands-on experience.
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://samforeman.me">Sam Foreman</a> <a href="mailto:foremans@anl.gov" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://alcf.anl.gov/about/people/sam-foreman">
            </a><a href="https://alcf.anl.gov/about/people/sam-foreman">ALCF</a>
            
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 5, 2025</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">August 5, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<div class="callout callout-style-simple callout-note callout-titled" title="Authors">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Authors
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Author: Archit Vasan , including materials on LLMs by Varuni Sastri and Carlo Graziani at Argonne, and discussion/editorial work by Taylor Childers, Bethany Lusch, and Venkat Vishwanath (Argonne)</p>
<p>Modification by Huihuo Zheng on August 1, 2025</p>
</div>
</div>
</div>
<p>Inspiration from the blog posts “The Illustrated Transformer” and “The Illustrated GPT2” by Jay Alammar, highly recommended reading.</p>
<p>Although the name “language models” is derived from Natural Language Processing, the models used in these approaches can be applied to diverse scientific applications as illustrated below.</p>
<p>This session is dedicated to setting out the basics of sequential data modeling, and introducing a few key elements required for DL approaches to such modeling—principally Transformers.</p>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>During this session I will cover:</p>
<ol type="1">
<li>Scientific applications modeling sequential data</li>
<li>Brief History of Language Models</li>
<li>Tokenization and embedding of sequential data</li>
<li>Elements of a Transformer</li>
<li>Attention mechanisms</li>
<li>Output layers</li>
<li>Training loops</li>
<li>Different types of Transformers</li>
</ol>
</section>
<section id="modeling-sequential-data" class="level2">
<h2 class="anchored" data-anchor-id="modeling-sequential-data">Modeling Sequential Data</h2>
<p>Sequences are variable-length lists with data in subsequent iterations that depends on previous iterations (or tokens).</p>
<p>Mathematically: A sequence is a list of tokens:</p>
<p><span class="math display">T = [t_1, t_2, t_3,...,t_N]</span></p>
<p>where each token within the list depends on the others with a particular probability:</p>
<p><span class="math display">P(t_2 | t_1, t_3, t_4, ..., t_N)</span></p>
<p>The purpose of sequential modeling is to learn these probabilities for possible tokens in a distribution to perform various tasks including:</p>
<ul>
<li>Sequence generation based on a prompt</li>
<li>Language translation (e.g.&nbsp;English –&gt; French)</li>
<li>Property prediction (predicting a property based on an entire sequence)</li>
<li>Identifying mistakes or missing elements in sequential data</li>
</ul>
</section>
<section id="scientific-sequential-data-modeling-examples" class="level2">
<h2 class="anchored" data-anchor-id="scientific-sequential-data-modeling-examples">Scientific sequential data modeling examples</h2>
<section id="nucleic-acid-sequences-genomic-data" class="level3">
<h3 class="anchored" data-anchor-id="nucleic-acid-sequences-genomic-data">Nucleic acid sequences + genomic data</h3>
<p>Nucleic acid sequences can be used to predict translation of proteins, mutations, and gene expression levels.</p>
<div id="fig-rna-sequences" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rna-sequences-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/RNA-codons.svg.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: RNA sequences"><img src="images/RNA-codons.svg.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rna-sequences-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: RNA sequences
</figcaption>
</figure>
</div>
<p>Here is an image of GenSLM. This is a language model developed by Argonne researchers that can model genomic information in a single model. It was shown to model the evolution of SARS-COV2 without expensive experiments.</p>
<div id="fig-genslm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genslm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/genslm.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: GenSLM. Image credit: Zvyagin et. al 2022. BioRXiv"><img src="images/genslm.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genslm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: GenSLM. Image credit: Zvyagin et. al 2022. BioRXiv
</figcaption>
</figure>
</div>
</section>
<section id="protein-sequences" class="level3">
<h3 class="anchored" data-anchor-id="protein-sequences">Protein sequences</h3>
<p>Protein sequences can be used to predict folding structure, protein-protein interactions, chemical/binding properties, protein function and many more properties.</p>
<div id="fig-protein-sequences" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-protein-sequences-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/Protein-Structure-06.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Protein sequences"><img src="images/Protein-Structure-06.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-protein-sequences-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Protein sequences
</figcaption>
</figure>
</div>
</section>
<section id="other-applications" class="level3">
<h3 class="anchored" data-anchor-id="other-applications">Other applications:</h3>
<ul>
<li>Biomedical text</li>
<li>SMILES strings</li>
<li>Weather predictions</li>
<li>Interfacing with simulations such as molecular dynamics simulation</li>
</ul>
</section>
</section>
<section id="overview-of-language-models" class="level2">
<h2 class="anchored" data-anchor-id="overview-of-language-models">Overview of Language models</h2>
<p>We will now briefly talk about the progression of language models.</p>
<section id="rnns" class="level3">
<h3 class="anchored" data-anchor-id="rnns">RNNs</h3>
<p>Recurrent Neural Newtorks(RNNs) were a traditional model used to determine temporal dependencies within data.</p>
<p>In RNNs, the hidden state from the previous time step is fed back into the network, allowing it to maintain a “memory” of past inputs.</p>
<p>They were ideal for tasks with short sequences such as natural language processing and time-series prediction.</p>
<div id="fig-rnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/recurrent_nn.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: RNN"><img src="images/recurrent_nn.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: RNN
</figcaption>
</figure>
</div>
<p>However, these networks had significant challenges.</p>
<ul>
<li><strong>Slow to train</strong>: RNNs process data one element at a time, maintaining an internal hidden state that is updated at each step. They operate recurrently, where each output depends on the previous hidden state and the current input; thus, parallel computation is not possible.</li>
<li><strong>Cannot handle large sequences</strong>: Exploding and vanishing gradients limit the RNN modelling of long sequences. Some variants of RNNs such as LSTM and GRU addressed this problem, they cannot engage with very large sequences.</li>
</ul>
</section>
<section id="transformers" class="level3">
<h3 class="anchored" data-anchor-id="transformers">Transformers</h3>
<p>The newest LMs referred to as “large language models” (since they have large parameter size) were developed to address many of these challenges.</p>
<p>These new models base their desin on the Transformer architecture that was introduced in 2017 in the “Attention is all you need” paper.</p>
<p>Since then a multitude of LLM architectures have been designed.</p>
<div id="fig-chapter1-transformers-chrono" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chapter1-transformers-chrono-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/en_chapter1_transformers_chrono.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: "><img src="images/en_chapter1_transformers_chrono.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-chapter1-transformers-chrono-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5
</figcaption>
</figure>
</div>
<p>The power of these models comes from the “attention mechanism” defined in the Vaswani 2017 seminal paper.</p>
<div id="fig-transformer-arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/attention_is_all_you_need.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Transformer architecture"><img src="images/attention_is_all_you_need.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Transformer architecture
</figcaption>
</figure>
</div>
</section>
</section>
<section id="coding-example-of-llms-in-action" class="level2">
<h2 class="anchored" data-anchor-id="coding-example-of-llms-in-action">Coding example of LLMs in action!</h2>
<p>Let’s look at an example of running inference with a LLM as a block box to generate text given a prompt and we will also initiate a training loop for an LLM:</p>
<p>Here, we will use the <code>transformers</code> library which is as part of HuggingFace, a repository of different models, tokenizers and information on how to apply these models</p>
<p><em>Warning: Large Language Models are only as good as their training data. They have no ethics, no judgement, or editing ability. We will be using some pretrained models from Hugging Face which used wide samples of internet hosted text. The datasets have not been strictly filtered to restrict all malign content so the generated text may be surprisingly dark or questionable. They do not reflect our core values and are only used for demonstration purposes.</em></p>
<div id="b32d0c02" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># !pip install transformers</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="co"># !pip install pandas</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co"># !pip install torch</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="a6649ab4" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="op">%</span>autoreload <span class="dv">2</span></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="co"># settings for jupyter book: svg for html version, high-resolution png for pdf</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="im">import</span> matplotlib_inline.backend_inline</span>
<span id="cb2-6"><a href="#cb2-6"></a>matplotlib_inline.backend_inline.set_matplotlib_formats(<span class="st">'retina'</span>, <span class="st">'svg'</span>, <span class="st">'png'</span>)</span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="co"># mpl.rcParams['figure.dpi'] = 400</span></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="im">from</span> rich <span class="im">import</span> <span class="bu">print</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="6218cc12" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoConfig, AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb3-2"><a href="#cb3-2"></a></span>
<span id="cb3-3"><a href="#cb3-3"></a>input_text <span class="op">=</span> <span class="st">"I got an A+ in my final exam; I am very"</span></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb3-5"><a href="#cb3-5"></a></span>
<span id="cb3-6"><a href="#cb3-6"></a>generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>, model<span class="op">=</span><span class="st">"openai-community/gpt2"</span>)</span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="bu">print</span>(</span>
<span id="cb3-8"><a href="#cb3-8"></a>    [</span>
<span id="cb3-9"><a href="#cb3-9"></a>        i[<span class="st">"generated_text"</span>]</span>
<span id="cb3-10"><a href="#cb3-10"></a>        <span class="cf">for</span> i <span class="kw">in</span> generator(input_text, max_length<span class="op">=</span><span class="dv">20</span>, num_return_sequences<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb3-11"><a href="#cb3-11"></a>    ]</span>
<span id="cb3-12"><a href="#cb3-12"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Device set to use mps:0
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[</span>
    <span style="color: #008000; text-decoration-color: #008000">"I got an A+ in my final exam; I am very disappointed.\n\nI'm not sure if I'll ever get my grades back, but I'm</span>
<span style="color: #008000; text-decoration-color: #008000">sure it's a good thing!\n\nI'm still looking for other ways to help, and I hope you'll join in!\n\n-Joe\n\nUPDATE: </span>
<span style="color: #008000; text-decoration-color: #008000">I got this email from a reader:\n\nDear Joe,\n\nI was just looking for a way to get my C grades back and I received</span>
<span style="color: #008000; text-decoration-color: #008000">this email from a reader stating that they are good because they gave me an A+ in my final exam. I was surprised to</span>
<span style="color: #008000; text-decoration-color: #008000">see that they gave me a B+ in my final exam, as I am a pretty good student.\n\nThanks for your email, though. I'm </span>
<span style="color: #008000; text-decoration-color: #008000">glad I have received a letter from you, too.\n\nIf you're interested in getting your A+ in your final exam, you can</span>
<span style="color: #008000; text-decoration-color: #008000">get it today by clicking here.\n\nThank you for taking the time to send me this email, but please know that you may</span>
<span style="color: #008000; text-decoration-color: #008000">want to consider making a donation to help me.\n\nUntil next time!\n\n-Tom"</span>,
    <span style="color: #008000; text-decoration-color: #008000">'I got an A+ in my final exam; I am very happy with my results. I would like to thank the faculty at the </span>
<span style="color: #008000; text-decoration-color: #008000">University of Arkansas for their time and effort.\n\nThank you,\n\nColleen F.'</span>,
    <span style="color: #008000; text-decoration-color: #008000">"I got an A+ in my final exam; I am very happy with my study as it has allowed me to take the A-level with an </span>
<span style="color: #008000; text-decoration-color: #008000">emphasis on study and teaching.\n\nI've also received many positive feedback from my students, and I have been able</span>
<span style="color: #008000; text-decoration-color: #008000">to gain valuable feedback on my writing skills. I have been given a lot of support from my faculty and staff, </span>
<span style="color: #008000; text-decoration-color: #008000">including teachers and students, to help me develop my writing techniques and focus and build a better writing </span>
<span style="color: #008000; text-decoration-color: #008000">vocabulary.\n\nI am very grateful to all of you for your support and encouragement.\n\nSincerely,\n\nBarry"</span>,
    <span style="color: #008000; text-decoration-color: #008000">'I got an A+ in my final exam; I am very happy I did. I am very proud of my performance and I hope I am getting</span>
<span style="color: #008000; text-decoration-color: #008000">a second chance at the same level I did.\n\n(3/17/14: I had a second chance at my second attempt and I am very </span>
<span style="color: #008000; text-decoration-color: #008000">proud of it.)\n\n(3/17/14: I am very happy with my second attempt and I am very proud of it.)\n\n(3/17/14: I am </span>
<span style="color: #008000; text-decoration-color: #008000">very happy with my second attempt and I am very proud of it.)\n\n(3/17/14: I am very happy with my second attempt </span>
<span style="color: #008000; text-decoration-color: #008000">and I am very proud of it.)\n\n(3/17/14: I am extremely happy with my second attempt and I am very proud of </span>
<span style="color: #008000; text-decoration-color: #008000">it.)\n\n(3/17/14: I am extremely happy with my second attempt and I am very proud of it.)\n\n(3/17/14: I am </span>
<span style="color: #008000; text-decoration-color: #008000">extremely happy with my second attempt and I am very proud of it.)\n\n(3/17/14: I am extremely happy with my second</span>
<span style="color: #008000; text-decoration-color: #008000">attempt and I am very proud of it.)\n\n(3/17/14: I am extremely happy with my second attempt and I am very proud of</span>
<span style="color: #008000; text-decoration-color: #008000">it.)\n\n('</span>,
    <span style="color: #008000; text-decoration-color: #008000">"I got an A+ in my final exam; I am very pleased with my grades. I think this year is going to be a tough year </span>
<span style="color: #008000; text-decoration-color: #008000">for me. I could use a few more years of the hard work of getting my mind off the subject. I could also use some </span>
<span style="color: #008000; text-decoration-color: #008000">more experience in the field of psychology, or even more time with my team. I have never been a good teacher, and I</span>
<span style="color: #008000; text-decoration-color: #008000">have no way to make my students feel comfortable about that.\n\nWhat do you do when you're an outlier?\n\nI am very</span>
<span style="color: #008000; text-decoration-color: #008000">excited to have a great career and to be able to continue to produce at a high level for the next five years. I am </span>
<span style="color: #008000; text-decoration-color: #008000">excited to have a partner that will be able to help me with the next five years. I have been working on my career </span>
<span style="color: #008000; text-decoration-color: #008000">as a writer, and I hope to be working with other writers soon.\n\nWhat's your career plan like?\n\nI have been </span>
<span style="color: #008000; text-decoration-color: #008000">working my way up the ladder for about three years and have been fortunate enough to have an amazing team of people</span>
<span style="color: #008000; text-decoration-color: #008000">who are amazing at what they do. I am very happy with my situation and very happy to be working with </span>
<span style="color: #008000; text-decoration-color: #008000">them.\n\nYou've been writing for a long time. How did you get started writing for a publisher?\n\nI started writing</span>
<span style="color: #008000; text-decoration-color: #008000">after college. I started"</span>
<span style="font-weight: bold">]</span>
</pre>
</div>
</div>
<p>We can also train a language model given input data:</p>
</section>
<section id="whats-going-on-under-the-hood" class="level2">
<h2 class="anchored" data-anchor-id="whats-going-on-under-the-hood">What’s going on under the hood?</h2>
<p>There are two components that are “black-boxes” here:</p>
<ol type="1">
<li>The method for tokenization</li>
<li>The model that generates novel text.</li>
</ol>
<p>Image credit: https://blog.floydhub.com/tokenization-nlp/</p>
<section id="tokenization" class="level3">
<h3 class="anchored" data-anchor-id="tokenization">Tokenization</h3>
<div id="c3434d6c" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb5-2"><a href="#cb5-2"></a></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="co"># A utility function to tokenize a sequence and print out some information about it.</span></span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a></span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="kw">def</span> tokenization_summary(tokenizer, sequence):</span>
<span id="cb5-7"><a href="#cb5-7"></a>    <span class="co"># get the vocabulary</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>    vocab <span class="op">=</span> tokenizer.vocab</span>
<span id="cb5-9"><a href="#cb5-9"></a>    <span class="co"># Number of entries to print</span></span>
<span id="cb5-10"><a href="#cb5-10"></a>    n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb5-11"><a href="#cb5-11"></a></span>
<span id="cb5-12"><a href="#cb5-12"></a>    <span class="co"># Print subset of the vocabulary</span></span>
<span id="cb5-13"><a href="#cb5-13"></a>    <span class="bu">print</span>(<span class="st">"Subset of tokenizer.vocab:"</span>)</span>
<span id="cb5-14"><a href="#cb5-14"></a>    <span class="cf">for</span> i, (token, index) <span class="kw">in</span> <span class="bu">enumerate</span>(tokenizer.vocab.items()):</span>
<span id="cb5-15"><a href="#cb5-15"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>token<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>index<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-16"><a href="#cb5-16"></a>        <span class="cf">if</span> i <span class="op">&gt;=</span> n <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb5-17"><a href="#cb5-17"></a>            <span class="cf">break</span></span>
<span id="cb5-18"><a href="#cb5-18"></a></span>
<span id="cb5-19"><a href="#cb5-19"></a>    <span class="bu">print</span>(<span class="st">"Vocab size of the tokenizer = "</span>, <span class="bu">len</span>(vocab))</span>
<span id="cb5-20"><a href="#cb5-20"></a>    <span class="bu">print</span>(<span class="st">"------------------------------------------"</span>)</span>
<span id="cb5-21"><a href="#cb5-21"></a></span>
<span id="cb5-22"><a href="#cb5-22"></a>    <span class="co"># .tokenize chunks the existing sequence into different tokens based on the rules and vocab of the tokenizer.</span></span>
<span id="cb5-23"><a href="#cb5-23"></a>    tokens <span class="op">=</span> tokenizer.tokenize(sequence)</span>
<span id="cb5-24"><a href="#cb5-24"></a>    <span class="bu">print</span>(<span class="st">"Tokens : "</span>, tokens)</span>
<span id="cb5-25"><a href="#cb5-25"></a>    <span class="bu">print</span>(<span class="st">"------------------------------------------"</span>)</span>
<span id="cb5-26"><a href="#cb5-26"></a></span>
<span id="cb5-27"><a href="#cb5-27"></a>    <span class="co"># .convert_tokens_to_ids or .encode or .tokenize converts the tokens to their corresponding numerical representation.</span></span>
<span id="cb5-28"><a href="#cb5-28"></a>    <span class="co">#  .convert_tokens_to_ids has a 1-1 mapping between tokens and numerical representation</span></span>
<span id="cb5-29"><a href="#cb5-29"></a>    <span class="co"># ids = tokenizer.convert_tokens_to_ids(tokens)</span></span>
<span id="cb5-30"><a href="#cb5-30"></a>    <span class="co"># print("encoded Ids: ", ids)</span></span>
<span id="cb5-31"><a href="#cb5-31"></a></span>
<span id="cb5-32"><a href="#cb5-32"></a>    <span class="co"># .encode also adds additional information like Start of sequence tokens and End of sequene</span></span>
<span id="cb5-33"><a href="#cb5-33"></a>    <span class="bu">print</span>(<span class="st">"tokenized sequence : "</span>, tokenizer.encode(sequence))</span>
<span id="cb5-34"><a href="#cb5-34"></a></span>
<span id="cb5-35"><a href="#cb5-35"></a>    <span class="co"># .tokenizer has additional information about attention_mask.</span></span>
<span id="cb5-36"><a href="#cb5-36"></a>    <span class="co"># encode = tokenizer(sequence)</span></span>
<span id="cb5-37"><a href="#cb5-37"></a>    <span class="co"># print("Encode sequence : ", encode)</span></span>
<span id="cb5-38"><a href="#cb5-38"></a>    <span class="co"># print("------------------------------------------")</span></span>
<span id="cb5-39"><a href="#cb5-39"></a></span>
<span id="cb5-40"><a href="#cb5-40"></a>    <span class="co"># .decode decodes the ids to raw text</span></span>
<span id="cb5-41"><a href="#cb5-41"></a>    ids <span class="op">=</span> tokenizer.convert_tokens_to_ids(tokens)</span>
<span id="cb5-42"><a href="#cb5-42"></a>    decode <span class="op">=</span> tokenizer.decode(ids)</span>
<span id="cb5-43"><a href="#cb5-43"></a>    <span class="bu">print</span>(<span class="st">"Decode sequence : "</span>, decode)</span>
<span id="cb5-44"><a href="#cb5-44"></a></span>
<span id="cb5-45"><a href="#cb5-45"></a></span>
<span id="cb5-46"><a href="#cb5-46"></a>tokenizer_1 <span class="op">=</span> AutoTokenizer.from_pretrained(</span>
<span id="cb5-47"><a href="#cb5-47"></a>    <span class="st">"gpt2"</span></span>
<span id="cb5-48"><a href="#cb5-48"></a>)  <span class="co"># GPT-2 uses "Byte-Pair Encoding (BPE)"</span></span>
<span id="cb5-49"><a href="#cb5-49"></a></span>
<span id="cb5-50"><a href="#cb5-50"></a>sequence <span class="op">=</span> <span class="st">"I got an A+ in my final exam; I am very"</span></span>
<span id="cb5-51"><a href="#cb5-51"></a></span>
<span id="cb5-52"><a href="#cb5-52"></a>tokenization_summary(tokenizer_1, sequence)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Subset of tokenizer.vocab:
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">ĠSikh: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">34629</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">wcs: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12712</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">force: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3174</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">ount: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">608</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">either: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">31336</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Ġsmear: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">35180</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">ĠPhoenix: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9643</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Ġpreacher: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">39797</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">imum: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2847</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Susan: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">45842</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Vocab size of the tokenizer =  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">50257</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">------------------------------------------
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Tokens : 
<span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'I'</span>, <span style="color: #008000; text-decoration-color: #008000">'Ġgot'</span>, <span style="color: #008000; text-decoration-color: #008000">'Ġan'</span>, <span style="color: #008000; text-decoration-color: #008000">'ĠA'</span>, <span style="color: #008000; text-decoration-color: #008000">'+'</span>, <span style="color: #008000; text-decoration-color: #008000">'Ġin'</span>, <span style="color: #008000; text-decoration-color: #008000">'Ġmy'</span>, <span style="color: #008000; text-decoration-color: #008000">'Ġfinal'</span>, <span style="color: #008000; text-decoration-color: #008000">'Ġexam'</span>, <span style="color: #008000; text-decoration-color: #008000">';'</span>, <span style="color: #008000; text-decoration-color: #008000">'ĠI'</span>, <span style="color: #008000; text-decoration-color: #008000">'Ġam'</span>, <span style="color: #008000; text-decoration-color: #008000">'Ġvery'</span><span style="font-weight: bold">]</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">------------------------------------------
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">tokenized sequence : 
<span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">40</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1392</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">281</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">317</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">287</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">616</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2457</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2814</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">314</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">716</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">845</span><span style="font-weight: bold">]</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Decode sequence :  I got an A+ in my final exam; I am very
</pre>
</div>
</div>
</section>
<section id="token-embedding" class="level3">
<h3 class="anchored" data-anchor-id="token-embedding">Token embedding:</h3>
<p>Words are turned into vectors based on their location within a vocabulary.</p>
<p>The strategy of choice for learning language structure from tokenized text is to find a clever way to map each token into a moderate-dimension vector space, adjusting the mapping so that</p>
<p>Similar, or associated tokens take up residence nearby each other, and different regions of the space correspond to different position in the sequence. Such a mapping from token ID to a point in a vector space is called a token embedding. The dimension of the vector space is often high (e.g.&nbsp;1024-dimensional), but much smaller than the vocabulary size (30,000–500,000).</p>
<p>Various approaches have been attempted for generating such embeddings, including static algorithms that operate on a corpus of tokenized data as preprocessors for NLP tasks. Transformers, however, adjust their embeddings during training.</p>
</section>
<section id="we-can-visualize-these-embeddings-of-the-popular-bert-model-using-pca" class="level3">
<h3 class="anchored" data-anchor-id="we-can-visualize-these-embeddings-of-the-popular-bert-model-using-pca">We can visualize these embeddings of the popular BERT model using PCA!</h3>
<div id="c8fc97de" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># !pip install umap</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="co"># !pip install plotly</span></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="co"># !pip install scikit-learn</span></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="co"># !pip install nltk</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="625386d9" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="im">import</span> nltk</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="im">import</span> umap</span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb7-8"><a href="#cb7-8"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb7-9"><a href="#cb7-9"></a><span class="im">from</span> transformers <span class="im">import</span> BertModel, BertTokenizer</span>
<span id="cb7-10"><a href="#cb7-10"></a></span>
<span id="cb7-11"><a href="#cb7-11"></a>nltk.download(<span class="st">"stopwords"</span>)</span>
<span id="cb7-12"><a href="#cb7-12"></a><span class="im">import</span> torch</span>
<span id="cb7-13"><a href="#cb7-13"></a></span>
<span id="cb7-14"><a href="#cb7-14"></a><span class="co"># Load BERT model and tokenizer</span></span>
<span id="cb7-15"><a href="#cb7-15"></a>model_name <span class="op">=</span> <span class="st">"bert-base-uncased"</span></span>
<span id="cb7-16"><a href="#cb7-16"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(model_name)</span>
<span id="cb7-17"><a href="#cb7-17"></a>model <span class="op">=</span> BertModel.from_pretrained(model_name)</span>
<span id="cb7-18"><a href="#cb7-18"></a></span>
<span id="cb7-19"><a href="#cb7-19"></a><span class="cf">if</span> <span class="va">True</span>:</span>
<span id="cb7-20"><a href="#cb7-20"></a>    text <span class="op">=</span> <span class="st">"The diligent student diligently studied hard for his upcoming exams He was incredibly conscientious in his efforts and committed himself to mastering every subject"</span></span>
<span id="cb7-21"><a href="#cb7-21"></a></span>
<span id="cb7-22"><a href="#cb7-22"></a>    <span class="co"># Tokenize and get BERT embeddings</span></span>
<span id="cb7-23"><a href="#cb7-23"></a>    tokens <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-24"><a href="#cb7-24"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-25"><a href="#cb7-25"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>tokens)</span>
<span id="cb7-26"><a href="#cb7-26"></a>        embeddings <span class="op">=</span> outputs.last_hidden_state.squeeze(</span>
<span id="cb7-27"><a href="#cb7-27"></a>            <span class="dv">0</span></span>
<span id="cb7-28"><a href="#cb7-28"></a>        ).numpy()  <span class="co"># Shape: (num_tokens, 768) for BERT-base</span></span>
<span id="cb7-29"><a href="#cb7-29"></a></span>
<span id="cb7-30"><a href="#cb7-30"></a>    <span class="co"># Get the list of token labels without special tokens and subword tokens</span></span>
<span id="cb7-31"><a href="#cb7-31"></a>    labels <span class="op">=</span> [</span>
<span id="cb7-32"><a href="#cb7-32"></a>        tokenizer.convert_ids_to_tokens(<span class="bu">id</span>) <span class="cf">for</span> <span class="bu">id</span> <span class="kw">in</span> tokens.input_ids[<span class="dv">0</span>].tolist()</span>
<span id="cb7-33"><a href="#cb7-33"></a>    ]</span>
<span id="cb7-34"><a href="#cb7-34"></a>    filtered_labels <span class="op">=</span> [</span>
<span id="cb7-35"><a href="#cb7-35"></a>        label</span>
<span id="cb7-36"><a href="#cb7-36"></a>        <span class="cf">for</span> label <span class="kw">in</span> labels</span>
<span id="cb7-37"><a href="#cb7-37"></a>        <span class="cf">if</span> <span class="kw">not</span> (label.startswith(<span class="st">"["</span>) <span class="kw">and</span> label.endswith(<span class="st">"]"</span>)) <span class="kw">and</span> <span class="st">"##"</span> <span class="kw">not</span> <span class="kw">in</span> label</span>
<span id="cb7-38"><a href="#cb7-38"></a>    ]</span>
<span id="cb7-39"><a href="#cb7-39"></a></span>
<span id="cb7-40"><a href="#cb7-40"></a>    <span class="co"># Remove stopwords from labels and embeddings</span></span>
<span id="cb7-41"><a href="#cb7-41"></a>    stop_words <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">"english"</span>))</span>
<span id="cb7-42"><a href="#cb7-42"></a>    filtered_labels <span class="op">=</span> [</span>
<span id="cb7-43"><a href="#cb7-43"></a>        label <span class="cf">for</span> label <span class="kw">in</span> filtered_labels <span class="cf">if</span> label.lower() <span class="kw">not</span> <span class="kw">in</span> stop_words</span>
<span id="cb7-44"><a href="#cb7-44"></a>    ]</span>
<span id="cb7-45"><a href="#cb7-45"></a>    filtered_embeddings <span class="op">=</span> embeddings[: <span class="bu">len</span>(filtered_labels)]</span>
<span id="cb7-46"><a href="#cb7-46"></a></span>
<span id="cb7-47"><a href="#cb7-47"></a>    <span class="co"># Perform PCA for dimensionality reduction (3D)</span></span>
<span id="cb7-48"><a href="#cb7-48"></a>    pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb7-49"><a href="#cb7-49"></a>    embeddings_pca <span class="op">=</span> pca.fit_transform(filtered_embeddings)</span>
<span id="cb7-50"><a href="#cb7-50"></a></span>
<span id="cb7-51"><a href="#cb7-51"></a>    <span class="co"># Convert embeddings and labels to DataFrame for Plotly</span></span>
<span id="cb7-52"><a href="#cb7-52"></a>    data_pca <span class="op">=</span> {</span>
<span id="cb7-53"><a href="#cb7-53"></a>        <span class="st">"x"</span>: embeddings_pca[:, <span class="dv">0</span>],</span>
<span id="cb7-54"><a href="#cb7-54"></a>        <span class="st">"y"</span>: embeddings_pca[:, <span class="dv">1</span>],</span>
<span id="cb7-55"><a href="#cb7-55"></a>        <span class="st">"z"</span>: embeddings_pca[:, <span class="dv">2</span>],</span>
<span id="cb7-56"><a href="#cb7-56"></a>        <span class="st">"label"</span>: filtered_labels,</span>
<span id="cb7-57"><a href="#cb7-57"></a>    }</span>
<span id="cb7-58"><a href="#cb7-58"></a>    df_pca <span class="op">=</span> pd.DataFrame(data_pca)</span>
<span id="cb7-59"><a href="#cb7-59"></a></span>
<span id="cb7-60"><a href="#cb7-60"></a>    <span class="co"># Plot PCA in 3D with Plotly (interactive)</span></span>
<span id="cb7-61"><a href="#cb7-61"></a>    fig_pca <span class="op">=</span> px.scatter_3d(</span>
<span id="cb7-62"><a href="#cb7-62"></a>        df_pca,</span>
<span id="cb7-63"><a href="#cb7-63"></a>        x<span class="op">=</span><span class="st">"x"</span>,</span>
<span id="cb7-64"><a href="#cb7-64"></a>        y<span class="op">=</span><span class="st">"y"</span>,</span>
<span id="cb7-65"><a href="#cb7-65"></a>        z<span class="op">=</span><span class="st">"z"</span>,</span>
<span id="cb7-66"><a href="#cb7-66"></a>        text<span class="op">=</span><span class="st">"label"</span>,</span>
<span id="cb7-67"><a href="#cb7-67"></a>        title<span class="op">=</span><span class="st">"PCA 3D Visualization of Token Embeddings"</span>,</span>
<span id="cb7-68"><a href="#cb7-68"></a>        labels<span class="op">=</span>{<span class="st">"x"</span>: <span class="st">"Dimension 1"</span>, <span class="st">"y"</span>: <span class="st">"Dimension 2"</span>, <span class="st">"z"</span>: <span class="st">"Dimension 3"</span>},</span>
<span id="cb7-69"><a href="#cb7-69"></a>        hover_name<span class="op">=</span><span class="st">"label"</span>,</span>
<span id="cb7-70"><a href="#cb7-70"></a>    )</span>
<span id="cb7-71"><a href="#cb7-71"></a>    fig_pca.update_traces(marker<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">5</span>), textfont<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">8</span>))</span>
<span id="cb7-72"><a href="#cb7-72"></a>    fig_pca.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to
[nltk_data]     /Users/samforeman/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.0.1.min.js" integrity="sha256-oy6Be7Eh6eiQFs5M7oXuPxxm9qbJXEtTpfSI93dW16Q=" crossorigin="anonymous"></script>                <div id="a61b27dd-a12c-4668-8a1a-f33951945916" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("a61b27dd-a12c-4668-8a1a-f33951945916")) {                    Plotly.newPlot(                        "a61b27dd-a12c-4668-8a1a-f33951945916",                        [{"hovertemplate":"\u003cb\u003e%{hovertext}\u003c\u002fb\u003e\u003cbr\u003e\u003cbr\u003eDimension 1=%{x}\u003cbr\u003eDimension 2=%{y}\u003cbr\u003eDimension 3=%{z}\u003cbr\u003elabel=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","hovertext":["dil","student","dil","studied","hard","upcoming","exams","incredibly","con","efforts","committed","mastering","every","subject"],"legendgroup":"","marker":{"color":"#636efa","symbol":"circle","size":5},"mode":"markers+text","name":"","scene":"scene","showlegend":false,"text":["dil","student","dil","studied","hard","upcoming","exams","incredibly","con","efforts","committed","mastering","every","subject"],"x":{"dtype":"f4","bdata":"7D4hwTZOgMDDdQBBgKLVQAXUc0B3lyLAwYz2QNB+4kAe+bY\u002fnzE\u002fwMU6+b\u002fT9pfAqJJ2wHgCksA="},"y":{"dtype":"f4","bdata":"2boPwXyP\u002fsDmpIXASpQ3vWKWp7\u002f\u002fNDNA\u002fHCYvyYIxz\u002f3wWa\u002ft2CHQGwtHkCWyJFASq+BQIGnnEA="},"z":{"dtype":"f4","bdata":"xrPqP0yCXj+Q54y\u002fMQgLQRJAdMD+8ri\u002f5jWqwKnTk0DQXGrA6eFwv9SdDsAARSW\u002fYJTtP5QhpT8="},"type":"scatter3d","textfont":{"size":8}}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"scene":{"domain":{"x":[0.0,1.0],"y":[0.0,1.0]},"xaxis":{"title":{"text":"Dimension 1"}},"yaxis":{"title":{"text":"Dimension 2"}},"zaxis":{"title":{"text":"Dimension 3"}}},"legend":{"tracegroupgap":0},"title":{"text":"PCA 3D Visualization of Token Embeddings"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('a61b27dd-a12c-4668-8a1a-f33951945916');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };            </script>        </div>
</div>
</div>
<p>You should see common words grouped together!</p>
</section>
</section>
<section id="elements-of-a-transformer" class="level2">
<h2 class="anchored" data-anchor-id="elements-of-a-transformer">Elements of a Transformer</h2>
<p>Now let’s look at the base elements that make up a Transformer by dissecting the popular GPT2 model</p>
<div id="0a12be49" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="im">from</span> transformers <span class="im">import</span> GPT2LMHeadModel, GPT2Tokenizer</span>
<span id="cb9-2"><a href="#cb9-2"></a></span>
<span id="cb9-3"><a href="#cb9-3"></a>model <span class="op">=</span> GPT2LMHeadModel.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="bu">print</span>(model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GPT2LMHeadModel</span><span style="font-weight: bold">(</span>
  <span style="font-weight: bold">(</span>transformer<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GPT2Model</span><span style="font-weight: bold">(</span>
    <span style="font-weight: bold">(</span>wte<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Embedding</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">50257</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span><span style="font-weight: bold">)</span>
    <span style="font-weight: bold">(</span>wpe<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Embedding</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1024</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span><span style="font-weight: bold">)</span>
    <span style="font-weight: bold">(</span>drop<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
    <span style="font-weight: bold">(</span>h<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">ModuleList</span><span style="font-weight: bold">(</span>
      <span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span><span style="font-weight: bold">)</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span> x <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GPT2Block</span><span style="font-weight: bold">(</span>
        <span style="font-weight: bold">(</span>ln_1<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">LayerNorm</span><span style="font-weight: bold">((</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span>,<span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-05</span>, <span style="color: #808000; text-decoration-color: #808000">elementwise_affine</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
        <span style="font-weight: bold">(</span>attn<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GPT2Attention</span><span style="font-weight: bold">(</span>
          <span style="font-weight: bold">(</span>c_attn<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Conv1D</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">nf</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2304</span>, <span style="color: #808000; text-decoration-color: #808000">nx</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span><span style="font-weight: bold">)</span>
          <span style="font-weight: bold">(</span>c_proj<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Conv1D</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">nf</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span>, <span style="color: #808000; text-decoration-color: #808000">nx</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span><span style="font-weight: bold">)</span>
          <span style="font-weight: bold">(</span>attn_dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
          <span style="font-weight: bold">(</span>resid_dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
        <span style="font-weight: bold">)</span>
        <span style="font-weight: bold">(</span>ln_2<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">LayerNorm</span><span style="font-weight: bold">((</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span>,<span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-05</span>, <span style="color: #808000; text-decoration-color: #808000">elementwise_affine</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
        <span style="font-weight: bold">(</span>mlp<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GPT2MLP</span><span style="font-weight: bold">(</span>
          <span style="font-weight: bold">(</span>c_fc<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Conv1D</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">nf</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3072</span>, <span style="color: #808000; text-decoration-color: #808000">nx</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span><span style="font-weight: bold">)</span>
          <span style="font-weight: bold">(</span>c_proj<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Conv1D</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">nf</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span>, <span style="color: #808000; text-decoration-color: #808000">nx</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3072</span><span style="font-weight: bold">)</span>
          <span style="font-weight: bold">(</span>act<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">NewGELUActivation</span><span style="font-weight: bold">()</span>
          <span style="font-weight: bold">(</span>dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
        <span style="font-weight: bold">)</span>
      <span style="font-weight: bold">)</span>
    <span style="font-weight: bold">)</span>
    <span style="font-weight: bold">(</span>ln_f<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">LayerNorm</span><span style="font-weight: bold">((</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span>,<span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-05</span>, <span style="color: #808000; text-decoration-color: #808000">elementwise_affine</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
  <span style="font-weight: bold">)</span>
  <span style="font-weight: bold">(</span>lm_head<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">50257</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="font-weight: bold">)</span>
</pre>
</div>
</div>
<p>GPT2 is an example of a Transformer Decoder which is used to generate novel text.</p>
<p>Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.</p>
<p>The pretraining of decoder models usually revolves around predicting the next word in the sentence.</p>
<p>These models are best suited for tasks involving text generation.</p>
<p>Examples of these include: * CTRL * GPT * GPT-2 * Transformer XL</p>
<p>Let’s discuss one of the most popular models, GPT-2 in a little more detail.</p>
<p>The architecture of GPT-2 is inspired by the paper: “Generating Wikipedia by Summarizing Long Sequences” which is another arrangement of the transformer block that can do language modeling. This model threw away the encoder and thus is known as the “Transformer-Decoder”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/transformer-decoder-intro.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="transformer-decoder-intro.png"><img src="images/transformer-decoder-intro.png" class="img-fluid figure-img" alt="transformer-decoder-intro.png"></a></p>
<figcaption>transformer-decoder-intro.png</figcaption>
</figure>
</div>
<p>Image credit: https://jalammar.github.io/illustrated-gpt2/</p>
<p>The Transformer-Decoder is composed of Decoder blocks stacked ontop of each other where each contains two types of layers: 1. Masked Self-Attention and 2. Feed Forward Neural Networks.</p>
<p>In this lecture, we will * First, discuss attention mechanisms at length as this is arguably the greatest contribution by Transformers. * Second, extend the discussion from last week (https://github.com/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/Sequential_Data_Models.ipynb) on embedding input data while taking into account position. * Third, discuss outputting real text/sequences from the models. * Fourth, build a training loop for a mini-LLM.</p>
<div id="6b0a7686" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co">## IMPORTS</span></span>
<span id="cb10-2"><a href="#cb10-2"></a></span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="im">import</span> torch</span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb10-6"><a href="#cb10-6"></a></span>
<span id="cb10-7"><a href="#cb10-7"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb10-8"><a href="#cb10-8"></a><span class="co"># hyperparameters</span></span>
<span id="cb10-9"><a href="#cb10-9"></a>batch_size <span class="op">=</span> <span class="dv">16</span>  <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb10-10"><a href="#cb10-10"></a>block_size <span class="op">=</span> <span class="dv">32</span>  <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb10-11"><a href="#cb10-11"></a>max_iters <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb10-12"><a href="#cb10-12"></a>eval_interval <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb10-13"><a href="#cb10-13"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb10-14"><a href="#cb10-14"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb10-15"><a href="#cb10-15"></a>eval_iters <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb10-16"><a href="#cb10-16"></a>n_embd <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb10-17"><a href="#cb10-17"></a>n_head <span class="op">=</span> <span class="dv">4</span>  <span class="co">## so head_size = 16</span></span>
<span id="cb10-18"><a href="#cb10-18"></a>n_layer <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb10-19"><a href="#cb10-19"></a>dropout <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb10-20"><a href="#cb10-20"></a><span class="co"># ------------</span></span>
<span id="cb10-21"><a href="#cb10-21"></a></span>
<span id="cb10-22"><a href="#cb10-22"></a>torch.manual_seed(<span class="dv">1337</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>&lt;torch._C.Generator at 0x11d729970&gt;</code></pre>
</div>
</div>
</section>
<section id="attention-mechanisms" class="level2">
<h2 class="anchored" data-anchor-id="attention-mechanisms">Attention mechanisms</h2>
<p>Suppose the following sentence is an input sentence we want to translate using an LLM:</p>
<p><code>”The animal didn't cross the street because it was too tired”</code></p>
<p>Earlier, we mentioned that the Transformer learns an embedding of all words allowing interpretation of meanings of words.</p>
<p><img src="images/viz-bert-voc-verbs.png" alt="Drawing" style="width: 400px;"></p>
<p>So, if the model did a good job in token embedding, it will “know” what all the words in this sentence mean.</p>
<p>But to understand a full sentence, the model also need to understand what each word means in relation to other words.</p>
<p>For example, when we read the sentence: <code>”The animal didn't cross the street because it was too tired”</code> we know intuitively that the word <code>"it"</code> refers to <code>"animal"</code>, the state for <code>"it"</code> is <code>"tired"</code>, and the associated action is <code>"didn't cross"</code>.</p>
<p>However, the model needs a way to learn all of this information in a simple yet generalizable way. What makes Transformers particularly powerful compared to earlier sequential architectures is how it encodes context with the <strong>self-attention mechanism</strong>.</p>
<p>As the model processes each word in the input sequence, attention looks at other positions in the input sequence for clues to a better understanding for this word.</p>
<p><img src="images/transformer_self-attention_visualization.png" alt="Drawing" style="width: 300px;"></p>
<p>Image credit: https://jalammar.github.io/illustrated-transformer/</p>
<p>Self-attention mechanisms use 3 vectors to encode the context of a word in a sequence with another word: 1. Query: the word representation we score other words against using the other word’s keys 2. Key: labels for the words in a sequence that we match against the query 3. Value: actual word representation. We will use the queries and keys to score the word’s relevance to the query, and multiply this by the value.</p>
<p>An analogy provided by Jay Alammar is thinking about attention as choosing a file from a file cabinet according to information on a post-it note. You can use the post-it note (query) to identify the folder (key) that most matches the topic you are looking up. Then you access the contents of the file (value) according to its relevance to your query.</p>
<p><img src="images/self-attention-example-folders-3.png" alt="Drawing" style="width: 500px;"> Image credit: https://jalammar.github.io/illustrated-gpt2/</p>
<p>In our models, we can encode queries, keys, and values using simple linear layers with the same size (<code>sequence length, head_size</code>). During the training process, these layers will be updated to best encode context.</p>
<div id="7f4412f4" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>C <span class="op">=</span> <span class="dv">32</span>  <span class="co"># channels</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>head_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb12-3"><a href="#cb12-3"></a></span>
<span id="cb12-4"><a href="#cb12-4"></a>key <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-5"><a href="#cb12-5"></a>query <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-6"><a href="#cb12-6"></a>value <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The algorithm for self-attention is as follows:</p>
<ol type="1">
<li>Generate query, key and value vectors for each word</li>
<li>Calculate a score for each word in the input sentence against each other.</li>
<li>Divide the scores by the square root of the dimension of the key vectors to stabilize the gradients. This is then passed through a softmax operation.</li>
<li>Multiply each value vector by the softmax score.</li>
<li>Sum up the weighted value vectors to produce the output.</li>
</ol>
<p><img src="images/self-attention-output.png" alt="Drawing" style="width: 450px;"></p>
<p>Image credit: https://jalammar.github.io/illustrated-transformer/</p>
<p>Let’s see how attention is performed in the code.</p>
<div id="0c7de941" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb13-4"><a href="#cb13-4"></a></span>
<span id="cb13-5"><a href="#cb13-5"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb13-6"><a href="#cb13-6"></a>B, T, C <span class="op">=</span> <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">32</span>  <span class="co"># batch, time, channels</span></span>
<span id="cb13-7"><a href="#cb13-7"></a>x <span class="op">=</span> torch.randn(B, T, C)</span>
<span id="cb13-8"><a href="#cb13-8"></a></span>
<span id="cb13-9"><a href="#cb13-9"></a><span class="co"># Here we want the wei to be data dependent - ie gather info from the past but in a data dependant way</span></span>
<span id="cb13-10"><a href="#cb13-10"></a></span>
<span id="cb13-11"><a href="#cb13-11"></a>head_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb13-12"><a href="#cb13-12"></a>key <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-13"><a href="#cb13-13"></a>query <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-14"><a href="#cb13-14"></a>value <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-15"><a href="#cb13-15"></a>k <span class="op">=</span> key(</span>
<span id="cb13-16"><a href="#cb13-16"></a>    x</span>
<span id="cb13-17"><a href="#cb13-17"></a>)  <span class="co"># (B, T, 16) # each token here (totally B*T) produce a key and query in parallel and independently</span></span>
<span id="cb13-18"><a href="#cb13-18"></a>q <span class="op">=</span> query(x)  <span class="co"># (B, T, 16)</span></span>
<span id="cb13-19"><a href="#cb13-19"></a>v <span class="op">=</span> value(x)</span>
<span id="cb13-20"><a href="#cb13-20"></a></span>
<span id="cb13-21"><a href="#cb13-21"></a>wei <span class="op">=</span> (</span>
<span id="cb13-22"><a href="#cb13-22"></a>    q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> head_size<span class="op">**-</span><span class="fl">0.5</span></span>
<span id="cb13-23"><a href="#cb13-23"></a>)  <span class="co"># (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T). #</span></span>
<span id="cb13-24"><a href="#cb13-24"></a>wei <span class="op">=</span> F.softmax(</span>
<span id="cb13-25"><a href="#cb13-25"></a>    wei, dim<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb13-26"><a href="#cb13-26"></a>)  <span class="co"># exponentiate and normalize giving a nice distibution that sums to 1 and</span></span>
<span id="cb13-27"><a href="#cb13-27"></a><span class="co"># now it tells us that in a data dependent manner how much of info to aggregate from</span></span>
<span id="cb13-28"><a href="#cb13-28"></a></span>
<span id="cb13-29"><a href="#cb13-29"></a>out <span class="op">=</span> wei <span class="op">@</span> v  <span class="co"># aggregate the attention scores and value vector.</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="1dbb60db" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="bu">print</span>(out[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0618</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0091</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3488</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3208</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2971</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1573</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0561</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1068</span>,
          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0368</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0139</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0017</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3110</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1404</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0158</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1853</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4290</span><span style="font-weight: bold">]</span>,
        <span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1578</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0971</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4256</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3538</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3621</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2392</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0536</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1759</span>,
          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1115</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0282</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0649</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3641</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1928</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0261</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2162</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3758</span><span style="font-weight: bold">]</span>,
        <span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1293</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0759</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2946</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2292</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2215</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0710</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0107</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1616</span>,
         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0930</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0877</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0567</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1899</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0311</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0894</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0309</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5471</span><span style="font-weight: bold">]</span>,
        <span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1247</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1400</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2436</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1819</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1976</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0338</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0028</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1124</span>,
         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1477</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0748</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0650</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1392</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0314</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0989</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0613</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5433</span><span style="font-weight: bold">]</span>,
        <span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0667</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1845</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2135</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2813</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2064</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0873</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0084</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2055</span>,
         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1130</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1466</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0459</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1923</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0275</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1107</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0065</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4674</span><span style="font-weight: bold">]</span>,
        <span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1924</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1693</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1568</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2284</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1620</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0737</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0443</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2519</span>,
         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1912</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1979</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0832</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0713</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0826</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0848</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1047</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6089</span><span style="font-weight: bold">]</span>,
        <span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1184</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0884</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2652</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2560</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1840</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0284</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0621</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1181</span>,
         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0880</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0104</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1123</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1850</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0369</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0730</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0663</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5242</span><span style="font-weight: bold">]</span>,
        <span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1243</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0453</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3412</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2709</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2335</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0948</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0421</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2143</span>,
         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0330</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0313</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0520</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2378</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1084</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0959</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0300</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4707</span><span style="font-weight: bold">]]</span>,
       <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">SelectBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div>
</div>
<section id="multi-head-attention" class="level3">
<h3 class="anchored" data-anchor-id="multi-head-attention">Multi-head attention</h3>
<p>In practice, multiple attention heads are used which 1. Expands the model’s ability to focus on different positions and prevent the attention to be dominated by the word itself. 2. Have multiple “representation subspaces”. Have multiple sets of Query/Key/Value weight matrices</p>
<p><img src="images/transformer_multi-headed_self-attention-recap.png" alt="Drawing" style="width: 700px;"></p>
<p>Image credit: https://jalammar.github.io/illustrated-transformer/</p>
</section>
<section id="lets-see-attention-mechanisms-in-action" class="level3">
<h3 class="anchored" data-anchor-id="lets-see-attention-mechanisms-in-action">Let’s see attention mechanisms in action!</h3>
<p>We are going to use the powerful visualization tool bertviz, which allows an interactive experience of the attention mechanisms. Normally these mechanisms are abstracted away but this will allow us to inspect our model in more detail.</p>
<div id="760cda8b" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="op">!</span>pip install bertviz</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Requirement already satisfied: bertviz in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (1.4.1)
Requirement already satisfied: transformers&gt;=2.0 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from bertviz) (4.53.3)
Requirement already satisfied: torch&gt;=1.0 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from bertviz) (2.7.1)
Requirement already satisfied: tqdm in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from bertviz) (4.67.1)
Requirement already satisfied: boto3 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from bertviz) (1.39.11)
Requirement already satisfied: requests in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from bertviz) (2.32.4)
Requirement already satisfied: regex in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from bertviz) (2024.11.6)
Requirement already satisfied: sentencepiece in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from bertviz) (0.2.0)
Requirement already satisfied: IPython&gt;=7.14 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from bertviz) (9.4.0)
Requirement already satisfied: decorator in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from IPython&gt;=7.14-&gt;bertviz) (5.2.1)
Requirement already satisfied: ipython-pygments-lexers in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from IPython&gt;=7.14-&gt;bertviz) (1.1.1)
Requirement already satisfied: jedi&gt;=0.16 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from IPython&gt;=7.14-&gt;bertviz) (0.19.2)
Requirement already satisfied: matplotlib-inline in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from IPython&gt;=7.14-&gt;bertviz) (0.1.7)
Requirement already satisfied: pexpect&gt;4.3 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from IPython&gt;=7.14-&gt;bertviz) (4.9.0)
Requirement already satisfied: prompt_toolkit&lt;3.1.0,&gt;=3.0.41 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from IPython&gt;=7.14-&gt;bertviz) (3.0.51)
Requirement already satisfied: pygments&gt;=2.4.0 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from IPython&gt;=7.14-&gt;bertviz) (2.19.2)
Requirement already satisfied: stack_data in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from IPython&gt;=7.14-&gt;bertviz) (0.6.3)
Requirement already satisfied: traitlets&gt;=5.13.0 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from IPython&gt;=7.14-&gt;bertviz) (5.14.3)
Requirement already satisfied: wcwidth in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from prompt_toolkit&lt;3.1.0,&gt;=3.0.41-&gt;IPython&gt;=7.14-&gt;bertviz) (0.2.13)
Requirement already satisfied: parso&lt;0.9.0,&gt;=0.8.4 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from jedi&gt;=0.16-&gt;IPython&gt;=7.14-&gt;bertviz) (0.8.4)
Requirement already satisfied: ptyprocess&gt;=0.5 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from pexpect&gt;4.3-&gt;IPython&gt;=7.14-&gt;bertviz) (0.7.0)
Requirement already satisfied: filelock in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from torch&gt;=1.0-&gt;bertviz) (3.18.0)
Requirement already satisfied: typing-extensions&gt;=4.10.0 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from torch&gt;=1.0-&gt;bertviz) (4.14.1)
Requirement already satisfied: setuptools in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from torch&gt;=1.0-&gt;bertviz) (80.9.0)
Requirement already satisfied: sympy&gt;=1.13.3 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from torch&gt;=1.0-&gt;bertviz) (1.14.0)
Requirement already satisfied: networkx in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from torch&gt;=1.0-&gt;bertviz) (3.5)
Requirement already satisfied: jinja2 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from torch&gt;=1.0-&gt;bertviz) (3.1.6)
Requirement already satisfied: fsspec in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from torch&gt;=1.0-&gt;bertviz) (2025.7.0)
Requirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from sympy&gt;=1.13.3-&gt;torch&gt;=1.0-&gt;bertviz) (1.3.0)
Requirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.30.0 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from transformers&gt;=2.0-&gt;bertviz) (0.33.4)
Requirement already satisfied: numpy&gt;=1.17 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from transformers&gt;=2.0-&gt;bertviz) (2.3.1)
Requirement already satisfied: packaging&gt;=20.0 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from transformers&gt;=2.0-&gt;bertviz) (25.0)
Requirement already satisfied: pyyaml&gt;=5.1 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from transformers&gt;=2.0-&gt;bertviz) (6.0.2)
Requirement already satisfied: tokenizers&lt;0.22,&gt;=0.21 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from transformers&gt;=2.0-&gt;bertviz) (0.21.2)
Requirement already satisfied: safetensors&gt;=0.4.3 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from transformers&gt;=2.0-&gt;bertviz) (0.5.3)
Requirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.2 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from huggingface-hub&lt;1.0,&gt;=0.30.0-&gt;transformers&gt;=2.0-&gt;bertviz) (1.1.5)
Requirement already satisfied: botocore&lt;1.40.0,&gt;=1.39.11 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from boto3-&gt;bertviz) (1.39.11)
Requirement already satisfied: jmespath&lt;2.0.0,&gt;=0.7.1 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from boto3-&gt;bertviz) (1.0.1)
Requirement already satisfied: s3transfer&lt;0.14.0,&gt;=0.13.0 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from boto3-&gt;bertviz) (0.13.1)
Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from botocore&lt;1.40.0,&gt;=1.39.11-&gt;boto3-&gt;bertviz) (2.9.0.post0)
Requirement already satisfied: urllib3!=2.2.0,&lt;3,&gt;=1.25.4 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from botocore&lt;1.40.0,&gt;=1.39.11-&gt;boto3-&gt;bertviz) (2.5.0)
Requirement already satisfied: six&gt;=1.5 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.40.0,&gt;=1.39.11-&gt;boto3-&gt;bertviz) (1.17.0)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from jinja2-&gt;torch&gt;=1.0-&gt;bertviz) (3.0.2)
Requirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from requests-&gt;bertviz) (3.4.2)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from requests-&gt;bertviz) (3.10)
Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from requests-&gt;bertviz) (2025.8.3)
Requirement already satisfied: executing&gt;=1.2.0 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from stack_data-&gt;IPython&gt;=7.14-&gt;bertviz) (2.2.0)
Requirement already satisfied: asttokens&gt;=2.1.0 in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from stack_data-&gt;IPython&gt;=7.14-&gt;bertviz) (3.0.0)
Requirement already satisfied: pure-eval in /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages (from stack_data-&gt;IPython&gt;=7.14-&gt;bertviz) (0.2.3)</code></pre>
</div>
</div>
<p>Let’s load in the model, GPT2 and look at the attention mechanisms.</p>
<p><strong>Hint… click on the different blocks in the visualization to see the attention</strong></p>
<div id="f8c6df68" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="im">from</span> bertviz <span class="im">import</span> model_view</span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoModelForCausalLM, AutoTokenizer, utils</span>
<span id="cb18-3"><a href="#cb18-3"></a></span>
<span id="cb18-4"><a href="#cb18-4"></a>utils.logging.set_verbosity_error()  <span class="co"># Suppress standard warnings</span></span>
<span id="cb18-5"><a href="#cb18-5"></a></span>
<span id="cb18-6"><a href="#cb18-6"></a>model_name <span class="op">=</span> <span class="st">"openai-community/gpt2"</span></span>
<span id="cb18-7"><a href="#cb18-7"></a>input_text <span class="op">=</span> <span class="st">"The animal didn't cross the street because it was too tired"</span></span>
<span id="cb18-8"><a href="#cb18-8"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_name, output_attentions<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-9"><a href="#cb18-9"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb18-10"><a href="#cb18-10"></a>inputs <span class="op">=</span> tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)  <span class="co"># Tokenize input text</span></span>
<span id="cb18-11"><a href="#cb18-11"></a>outputs <span class="op">=</span> model(inputs)  <span class="co"># Run model</span></span>
<span id="cb18-12"><a href="#cb18-12"></a>attention <span class="op">=</span> outputs[<span class="op">-</span><span class="dv">1</span>]  <span class="co"># Retrieve attention from model outputs</span></span>
<span id="cb18-13"><a href="#cb18-13"></a>tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(</span>
<span id="cb18-14"><a href="#cb18-14"></a>    inputs[<span class="dv">0</span>]</span>
<span id="cb18-15"><a href="#cb18-15"></a>)  <span class="co"># Convert input ids to token strings</span></span>
<span id="cb18-16"><a href="#cb18-16"></a>model_view(attention, tokens)  <span class="co"># Display model view</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
</div>
<div class="cell-output cell-output-display">
      
        <div id="bertviz-6d5f6c190c0d473a97e9e93b0a3cd83d" style="font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;">
            <span style="user-select:none">
                
            </span>
            <div id="vis"></div>
        </div>
    
</div>
<div class="cell-output cell-output-display">
<script type="application/javascript">
/**
 * @fileoverview Transformer Visualization D3 javascript code.
 *
 * Based on: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/visualization/attention.js
 *
 * Change log:
 *
 * 02/01/19  Jesse Vig   Initial implementation
 * 12/31/20  Jesse Vig   Support multiple visualizations in single notebook.
 * 01/19/21  Jesse Vig   Support light/dark modes
 * 02/06/21  Jesse Vig   Move require config from separate jupyter notebook step
 * 05/03/21  Jesse Vig   Adjust visualization height dynamically
 * 03/23/22  Daniel SC   Update requirement URLs for d3 and jQuery (source of bug not allowing end result to be displayed on browsers)
 **/

require.config({
  paths: {
      d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',
    jquery: 'https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min',
  }
});

requirejs(['jquery', 'd3'], function($, d3) {

        const params = {"attention": [{"name": null, "attn": [[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8728697299957275, 0.12713032960891724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5175417065620422, 0.11140565574169159, 0.3710525929927826, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5058048367500305, 0.11489029228687286, 0.3174988925457001, 0.06180591136217117, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3867604434490204, 0.220637246966362, 0.1409534066915512, 0.096247099339962, 0.1554018259048462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.45108598470687866, 0.10241292417049408, 0.16359977424144745, 0.10497768968343735, 0.13571642339229584, 0.04220709949731827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.41864293813705444, 0.18242023885250092, 0.0589672289788723, 0.060029298067092896, 0.07290387153625488, 0.11826886236667633, 0.08876752853393555, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22391554713249207, 0.11939653754234314, 0.18790480494499207, 0.09788685292005539, 0.06084481254220009, 0.07281451672315598, 0.05366430804133415, 0.18357263505458832, 0.0, 0.0, 0.0, 0.0], [0.34604179859161377, 0.061977166682481766, 0.175901859998703, 0.07132770121097565, 0.09440907835960388, 0.044403597712516785, 0.053248897194862366, 0.08974883705377579, 0.06294109672307968, 0.0, 0.0, 0.0], [0.1479593962430954, 0.023442042991518974, 0.404382586479187, 0.03131354972720146, 0.025051061064004898, 0.0123198963701725, 0.03870793431997299, 0.05227704346179962, 0.04183676466345787, 0.22270964086055756, 0.0, 0.0], [0.21109521389007568, 0.12199978530406952, 0.09744543582201004, 0.05828961730003357, 0.08130468428134918, 0.04482971876859665, 0.046579837799072266, 0.09903564304113388, 0.05852463096380234, 0.12149063497781754, 0.059404801577329636, 0.0], [0.18719828128814697, 0.10780622065067291, 0.07435780763626099, 0.03466513380408287, 0.10310203582048416, 0.0584256649017334, 0.09578841924667358, 0.06054351478815079, 0.051962342113256454, 0.053742628544569016, 0.029733479022979736, 0.142674520611763]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [6.108285015216097e-05, 0.99993896484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00021011529315728694, 0.00099235400557518, 0.9987975358963013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0008750453707762063, 0.006663358770310879, 0.20087231695652008, 0.7915893197059631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003292917972430587, 0.0015007079346105456, 0.000736874935682863, 0.0002482582931406796, 0.9971848130226135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.064983069896698, 0.002671959577128291, 0.00010563300020294264, 0.00027576537104323506, 0.00063684320775792, 0.9313266277313232, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00012113703996874392, 0.003704436356201768, 0.0001494394091423601, 0.000288489944068715, 0.001142423483543098, 7.132500468287617e-05, 0.9945228099822998, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00021930693765170872, 0.0002042065025307238, 0.001590381609275937, 0.0001218211546074599, 3.5917539662477793e-06, 6.844720337539911e-05, 1.4673828445666004e-05, 0.9977776408195496, 0.0, 0.0, 0.0, 0.0], [0.008068851195275784, 0.004223143216222525, 0.0006737743970006704, 0.0005198557046242058, 9.300156671088189e-05, 0.017658421769738197, 1.895686364150606e-05, 0.0021341184619814157, 0.96660977602005, 0.0, 0.0, 0.0], [0.0033355734776705503, 0.0030058480333536863, 0.07951563596725464, 0.0015125838108360767, 0.00020864261023234576, 0.0010847202502191067, 4.4731208618031815e-05, 0.0004880736523773521, 0.0009698860230855644, 0.9098342657089233, 0.0, 0.0], [0.0003093187988270074, 0.0008345430251210928, 0.0035225353203713894, 0.0011243205517530441, 6.018464773660526e-05, 0.00027331075398251414, 6.361877603922039e-05, 0.007032756227999926, 0.0023551885969936848, 9.43261111387983e-05, 0.984329879283905, 0.0], [2.6519946914049797e-05, 0.00031819401192478836, 0.0008608945645391941, 0.00017407081031706184, 6.26821129117161e-05, 2.2431158868130296e-05, 0.00019366737978998572, 0.00196713930927217, 3.900060983141884e-05, 2.604008659545798e-05, 0.0005589863285422325, 0.9957504272460938]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8076128363609314, 0.19238720834255219, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7564974427223206, 0.13910256326198578, 0.10439995676279068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6285322308540344, 0.09183657914400101, 0.17913036048412323, 0.10050079226493835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6490501761436462, 0.13083405792713165, 0.09088309109210968, 0.07949371635913849, 0.04973892122507095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4437858462333679, 0.14629819989204407, 0.17920459806919098, 0.11442861706018448, 0.04963948577642441, 0.06664326786994934, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4212614893913269, 0.1986178457736969, 0.06085188314318657, 0.042993731796741486, 0.10431601852178574, 0.11300401389598846, 0.058954983949661255, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4052816927433014, 0.13476884365081787, 0.11341717839241028, 0.09819232672452927, 0.030604762956500053, 0.06306599080562592, 0.02787695825099945, 0.12679223716259003, 0.0, 0.0, 0.0, 0.0], [0.27947285771369934, 0.10286424309015274, 0.167910635471344, 0.0832909643650055, 0.02562699466943741, 0.05126956105232239, 0.039714645594358444, 0.19665390253067017, 0.05319620296359062, 0.0, 0.0, 0.0], [0.30790916085243225, 0.05029373615980148, 0.12240590900182724, 0.07851825654506683, 0.02606876753270626, 0.05091991275548935, 0.02441977709531784, 0.17152781784534454, 0.05295291170477867, 0.11498378217220306, 0.0, 0.0], [0.2208244502544403, 0.03443547710776329, 0.06055028364062309, 0.0898970440030098, 0.028440022841095924, 0.06481733918190002, 0.043153103440999985, 0.2896708548069, 0.05559169128537178, 0.06887681782245636, 0.043742936104536057, 0.0], [0.20316891372203827, 0.053842198103666306, 0.07006055116653442, 0.07429607212543488, 0.08070989698171616, 0.09928829222917557, 0.12549568712711334, 0.09593525528907776, 0.06501322984695435, 0.058736518025398254, 0.04788060486316681, 0.025572730228304863]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06895631551742554, 0.9310437440872192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.016044210642576218, 0.003348082536831498, 0.9806077480316162, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002597825601696968, 0.001177250756882131, 0.8540812134742737, 0.14214369654655457, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.003115046303719282, 0.0021718258503824472, 0.0005197911523282528, 0.0002590379153843969, 0.9939343333244324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06561258435249329, 0.030216623097658157, 0.03344912827014923, 0.01978001929819584, 0.2708275318145752, 0.5801140666007996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.003000790486112237, 0.0014867207501083612, 0.00024781335378065705, 0.00015431756037287414, 0.07085542380809784, 0.00043144385563209653, 0.9238234758377075, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009311525151133537, 0.001187278307043016, 0.016908884048461914, 0.003651350736618042, 0.0024482421576976776, 0.006121673621237278, 0.006837640423327684, 0.953533411026001, 0.0, 0.0, 0.0, 0.0], [0.007349053397774696, 0.00036790885496884584, 0.0009323050617240369, 0.0007442481582984328, 0.002626796020194888, 0.013326027430593967, 0.009429228492081165, 0.03102949447929859, 0.9341950416564941, 0.0, 0.0, 0.0], [0.009169088676571846, 0.0003112829872407019, 0.004256051033735275, 0.0006666898261755705, 0.0002504764124751091, 0.010006257332861423, 0.0019485640805214643, 0.04430656507611275, 0.13488054275512695, 0.7942044138908386, 0.0, 0.0], [0.0021743429824709892, 4.400549005367793e-05, 0.0011337270261719823, 0.0005299060139805079, 0.00038564420538023114, 0.0005951530183665454, 0.0004676577518694103, 0.002957117510959506, 0.029903478920459747, 0.017577586695551872, 0.9442313313484192, 0.0], [0.0003444154281169176, 4.677717697632033e-06, 3.269095759605989e-05, 3.8404403312597424e-05, 2.4318798750755377e-05, 2.835653504007496e-06, 0.00035965166171081364, 0.00018638838082551956, 8.838604117045179e-05, 0.00023993004288058728, 0.00679047591984272, 0.9918877482414246]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4607641398906708, 0.5392358303070068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4751085638999939, 0.11302156001329422, 0.4118698835372925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010727260261774063, 0.010956241749227047, 0.8002898693084717, 0.17802657186985016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06713563203811646, 0.11317005753517151, 0.014099083840847015, 0.0032748980447649956, 0.8023203015327454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19567757844924927, 0.07725901156663895, 0.19520670175552368, 0.0607624426484108, 0.34884950518608093, 0.12224484235048294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.023007115349173546, 0.03443444147706032, 0.002561410889029503, 0.0019284840673208237, 0.09173764288425446, 0.006991400849074125, 0.8393394947052002, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09177704900503159, 0.03550707921385765, 0.18329711258411407, 0.021718988195061684, 0.03723064064979553, 0.07004774361848831, 0.09536559879779816, 0.46505582332611084, 0.0, 0.0, 0.0, 0.0], [0.09844592958688736, 0.023700494319200516, 0.06559878587722778, 0.016079658642411232, 0.07972504198551178, 0.05119725316762924, 0.11837273091077805, 0.2941359281539917, 0.2527441084384918, 0.0, 0.0, 0.0], [0.12338249385356903, 0.020907437428832054, 0.06487789005041122, 0.017104074358940125, 0.02788626402616501, 0.03818007931113243, 0.08251627534627914, 0.15584000945091248, 0.150512233376503, 0.31879323720932007, 0.0, 0.0], [0.02439802512526512, 0.006507936865091324, 0.052472010254859924, 0.016575146466493607, 0.01601746492087841, 0.015596074052155018, 0.012763152830302715, 0.07024283707141876, 0.09645550698041916, 0.19563028216362, 0.4933415949344635, 0.0], [0.004754412453621626, 0.0012759313685819507, 0.0036332185845822096, 0.0007064667297527194, 0.001813474460504949, 0.0010349522344768047, 0.00175969407428056, 0.005657439585775137, 0.003501236904412508, 0.016981450840830803, 0.018394963815808296, 0.9404867887496948]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0350920744240284, 0.9649078845977783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.017149852588772774, 9.22839535633102e-05, 0.9827578663825989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.016695363447070122, 0.00017185522301588207, 0.4026584327220917, 0.5804743766784668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.024308882653713226, 7.026863750070333e-05, 7.09742880644626e-06, 1.3527117062039906e-06, 0.9756124019622803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.369503915309906, 0.08078742772340775, 0.04147397726774216, 0.08062081784009933, 0.048944707959890366, 0.3786691725254059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005368008278310299, 0.0011901067337021232, 1.1595958540056017e-06, 2.2413612441596342e-06, 0.00023564137518405914, 3.6005621950607747e-06, 0.9931991696357727, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010830705985426903, 0.0008522263378836215, 0.0017843025270849466, 0.0003264769911766052, 2.373226743657142e-05, 0.00010243060387438163, 1.073618568625534e-05, 0.9860694408416748, 0.0, 0.0, 0.0, 0.0], [0.08394403755664825, 0.011172972619533539, 0.03143863379955292, 0.014028087258338928, 0.00071554264286533, 0.012287710793316364, 0.0019427718361839652, 0.005782771855592728, 0.8386874198913574, 0.0, 0.0, 0.0], [0.03297869861125946, 0.0009347641025669873, 0.049889467656612396, 0.0013223604764789343, 0.0005436119390651584, 0.0016309588681906462, 0.00023146480089053512, 0.0007070330320857465, 0.0012526813661679626, 0.9105090498924255, 0.0, 0.0], [0.010116259567439556, 0.00015297441859729588, 0.0026823931839317083, 0.0005950400372967124, 6.250196020118892e-05, 1.939378671522718e-05, 9.410779966856353e-06, 0.0016189247835427523, 0.0001303377066506073, 1.8262175217387266e-05, 0.984594464302063, 0.0], [0.0013144222320988774, 5.157303894520737e-05, 0.0028327396139502525, 0.00014328223187476397, 1.7027043213602155e-05, 2.3940978621794784e-07, 9.704353033157531e-06, 5.872030305908993e-05, 8.171849117388774e-07, 2.892753343530785e-07, 0.0002102044818457216, 0.9953610301017761]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7840536236763, 0.21594637632369995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.46175748109817505, 0.2752997875213623, 0.26294267177581787, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.347594290971756, 0.19824321568012238, 0.3449990153312683, 0.10916347801685333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3028227388858795, 0.34311631321907043, 0.08300741761922836, 0.07853998988866806, 0.19251346588134766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1498660296201706, 0.30764830112457275, 0.19471493363380432, 0.10896730422973633, 0.23311977088451385, 0.0056836288422346115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24475958943367004, 0.31070372462272644, 0.03855603560805321, 0.03197957202792168, 0.16650812327861786, 0.028320904821157455, 0.1791720688343048, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12895876169204712, 0.25211194157600403, 0.1414985954761505, 0.058149829506874084, 0.18921132385730743, 0.03846234828233719, 0.13460887968540192, 0.056998275220394135, 0.0, 0.0, 0.0, 0.0], [0.1107507273554802, 0.20980755984783173, 0.13674712181091309, 0.0653587058186531, 0.1750716269016266, 0.008427752181887627, 0.22881995141506195, 0.05217902362346649, 0.01283758133649826, 0.0, 0.0, 0.0], [0.09499160200357437, 0.2059698849916458, 0.09614235162734985, 0.043118856847286224, 0.2156355381011963, 0.009005552157759666, 0.23591184616088867, 0.06384054571390152, 0.014153259806334972, 0.021230628713965416, 0.0, 0.0], [0.10118942707777023, 0.1541300266981125, 0.12498695403337479, 0.07628810405731201, 0.17016486823558807, 0.01900625042617321, 0.16963225603103638, 0.06684330850839615, 0.02166706509888172, 0.03310520946979523, 0.0629865825176239, 0.0], [0.09340905398130417, 0.156208336353302, 0.05762823671102524, 0.0384947769343853, 0.10130387544631958, 0.05469154566526413, 0.20964321494102478, 0.05482856184244156, 0.04473245516419411, 0.03423313423991203, 0.0662151500582695, 0.0886116698384285]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9780775308609009, 0.02192249335348606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6736375093460083, 0.11471686512231827, 0.21164564788341522, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4995661675930023, 0.05211051180958748, 0.303146094083786, 0.1451772302389145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33038750290870667, 0.13192273676395416, 0.31486958265304565, 0.1981905847787857, 0.024629658088088036, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16962504386901855, 0.05462287738919258, 0.13125978410243988, 0.13574470579624176, 0.17157454788684845, 0.33717310428619385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10292115807533264, 0.09207423031330109, 0.05589307099580765, 0.07956819236278534, 0.29818037152290344, 0.28000280261039734, 0.09136013686656952, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14299601316452026, 0.04475784674286842, 0.053097810596227646, 0.06785091012716293, 0.047260161489248276, 0.2249143272638321, 0.11457084864377975, 0.3045520782470703, 0.0, 0.0, 0.0, 0.0], [0.0789123997092247, 0.008341478183865547, 0.026632847264409065, 0.03329024836421013, 0.027116699144244194, 0.10302349925041199, 0.05209316313266754, 0.41128402948379517, 0.2593056559562683, 0.0, 0.0, 0.0], [0.04155365005135536, 0.005958626512438059, 0.01734502613544464, 0.021420443430542946, 0.008064493536949158, 0.062019798904657364, 0.04794998839497566, 0.28655534982681274, 0.25646114349365234, 0.2526715397834778, 0.0, 0.0], [0.07974296063184738, 0.005500698462128639, 0.023976197466254234, 0.049523379653692245, 0.009776486083865166, 0.05415314808487892, 0.011103822849690914, 0.13373155891895294, 0.19987474381923676, 0.28547707200050354, 0.14713986217975616, 0.0], [0.05216839909553528, 0.008333834819495678, 0.012947536073625088, 0.009802688844501972, 0.008580652996897697, 0.04168024659156799, 0.02927258238196373, 0.06936177611351013, 0.11549323797225952, 0.18446815013885498, 0.3898080587387085, 0.0780828669667244]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8899824619293213, 0.1100175753235817, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7910976409912109, 0.13273528218269348, 0.07616700977087021, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5870044231414795, 0.18726570904254913, 0.11894597113132477, 0.10678387433290482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6070454716682434, 0.13816702365875244, 0.08815921097993851, 0.06497915834188461, 0.10164909809827805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11922377347946167, 0.021004417911171913, 0.05086641013622284, 0.019459102302789688, 0.03123021312057972, 0.7582160234451294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4150432348251343, 0.08174128830432892, 0.038821496069431305, 0.04890870675444603, 0.06506572663784027, 0.1816372573375702, 0.1687822937965393, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18959496915340424, 0.037007544189691544, 0.11742503941059113, 0.055452268570661545, 0.037552639842033386, 0.38132354617118835, 0.0895354151725769, 0.09210861474275589, 0.0, 0.0, 0.0, 0.0], [0.1284029632806778, 0.015749910846352577, 0.050582192838191986, 0.017391901463270187, 0.02099684625864029, 0.38308435678482056, 0.03219332918524742, 0.05255797505378723, 0.2990405857563019, 0.0, 0.0, 0.0], [0.11030983179807663, 0.07149005681276321, 0.0278136245906353, 0.01897183246910572, 0.03508180007338524, 0.2551598846912384, 0.03378261625766754, 0.05971577763557434, 0.1384403258562088, 0.2492341697216034, 0.0, 0.0], [0.13853077590465546, 0.04226505756378174, 0.061700064688920975, 0.026694128289818764, 0.030155055224895477, 0.1933630406856537, 0.030162764713168144, 0.06526393443346024, 0.18384860455989838, 0.18518555164337158, 0.04283098876476288, 0.0], [0.18808332085609436, 0.08665277063846588, 0.06464262306690216, 0.05770152062177658, 0.07556959986686707, 0.07506731897592545, 0.12305204570293427, 0.09461119771003723, 0.07921434193849564, 0.050303906202316284, 0.06363522261381149, 0.04146607592701912]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9383811950683594, 0.06161879375576973, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7096535563468933, 0.12009122222661972, 0.17025528848171234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6245108246803284, 0.11435782164335251, 0.14431102573871613, 0.1168203055858612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5780115723609924, 0.14060483872890472, 0.12913811206817627, 0.117121122777462, 0.03512433543801308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4283130466938019, 0.10099487006664276, 0.12637944519519806, 0.10225489735603333, 0.07366866618394852, 0.16838905215263367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38447701930999756, 0.17786811292171478, 0.09393604844808578, 0.08223118633031845, 0.09574440866708755, 0.15477706491947174, 0.010966161265969276, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30859634280204773, 0.07401464879512787, 0.12571625411510468, 0.10065291076898575, 0.07448951154947281, 0.12167088687419891, 0.05235442891716957, 0.14250510931015015, 0.0, 0.0, 0.0, 0.0], [0.2724604904651642, 0.07329657673835754, 0.09941134601831436, 0.07452664524316788, 0.05941953510046005, 0.11507894098758698, 0.04539712518453598, 0.15814168751239777, 0.10226769000291824, 0.0, 0.0, 0.0], [0.21520473062992096, 0.0612168051302433, 0.0975215807557106, 0.06979615241289139, 0.05536844953894615, 0.10722650587558746, 0.048566557466983795, 0.14410267770290375, 0.10709703713655472, 0.09389952570199966, 0.0, 0.0], [0.22600826621055603, 0.05705907195806503, 0.081429623067379, 0.055139318108558655, 0.057059574872255325, 0.09463922679424286, 0.04760555550456047, 0.1230005994439125, 0.09137232601642609, 0.09042489528656006, 0.07626152783632278, 0.0], [0.20337553322315216, 0.06865828484296799, 0.07323914766311646, 0.07007752358913422, 0.0471903495490551, 0.07860611379146576, 0.07656153291463852, 0.1007312759757042, 0.07251296192407608, 0.07606791704893112, 0.09149494767189026, 0.041484393179416656]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7349589467048645, 0.26504114270210266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6350484490394592, 0.07779435068368912, 0.28715723752975464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4731510281562805, 0.0820721983909607, 0.1918269395828247, 0.2529498338699341, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5389819741249084, 0.09492167830467224, 0.05702498182654381, 0.053141962736845016, 0.25592944025993347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5582868456840515, 0.09634020924568176, 0.058614153414964676, 0.05053364485502243, 0.04481681436300278, 0.19140827655792236, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3542708456516266, 0.09667258709669113, 0.04984694719314575, 0.057038456201553345, 0.06632541120052338, 0.11566125601530075, 0.26018455624580383, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2819109559059143, 0.06420554965734482, 0.139084592461586, 0.07838134467601776, 0.02713886648416519, 0.09132640808820724, 0.02755514159798622, 0.29039716720581055, 0.0, 0.0, 0.0, 0.0], [0.25015780329704285, 0.07111040502786636, 0.07981448620557785, 0.0587257444858551, 0.03078356198966503, 0.1258556991815567, 0.024951079860329628, 0.09503911435604095, 0.2635622024536133, 0.0, 0.0, 0.0], [0.19839204847812653, 0.07056555896997452, 0.14053791761398315, 0.057803548872470856, 0.02710641920566559, 0.09296996146440506, 0.024104677140712738, 0.0847659781575203, 0.09305982291698456, 0.21069404482841492, 0.0, 0.0], [0.21326251327991486, 0.045818574726581573, 0.10826554149389267, 0.06558641791343689, 0.0304541178047657, 0.07516511529684067, 0.028256254270672798, 0.08140641450881958, 0.07022910565137863, 0.052346743643283844, 0.22920916974544525, 0.0], [0.14975866675376892, 0.045595683157444, 0.09922152012586594, 0.06701158732175827, 0.0272412970662117, 0.06311651319265366, 0.039232827723026276, 0.08968015760183334, 0.04797147586941719, 0.04765913635492325, 0.08759023994207382, 0.23592089116573334]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7732552289962769, 0.22674483060836792, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7912213802337646, 0.09266883879899979, 0.11610986292362213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5883223414421082, 0.0983569547533989, 0.1432572901248932, 0.17006340622901917, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48703742027282715, 0.1118689700961113, 0.1425783932209015, 0.12304051965475082, 0.13547472655773163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3959304988384247, 0.08376403898000717, 0.13426341116428375, 0.11199831962585449, 0.0767139121890068, 0.19732977449893951, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3574475646018982, 0.09473831206560135, 0.11103996634483337, 0.09657836705446243, 0.09167300164699554, 0.09265297651290894, 0.15586984157562256, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3627004623413086, 0.07596524059772491, 0.08705926686525345, 0.08842978626489639, 0.06313448399305344, 0.054963767528533936, 0.07222599536180496, 0.1955210268497467, 0.0, 0.0, 0.0, 0.0], [0.2852361798286438, 0.06665609031915665, 0.08555057644844055, 0.09368794411420822, 0.055589959025382996, 0.062020909041166306, 0.07374361157417297, 0.17022117972373962, 0.10729356110095978, 0.0, 0.0, 0.0], [0.2766849398612976, 0.0571669340133667, 0.10888494551181793, 0.07267199456691742, 0.058557722717523575, 0.0566844716668129, 0.05878359079360962, 0.14458636939525604, 0.06512584537267685, 0.10085316002368927, 0.0, 0.0], [0.22204309701919556, 0.05345913767814636, 0.08119961619377136, 0.10553348809480667, 0.0626453086733818, 0.0358818955719471, 0.0488056018948555, 0.182176411151886, 0.052070748060941696, 0.03290907293558121, 0.12327562272548676, 0.0], [0.17174239456653595, 0.043927084654569626, 0.09396396577358246, 0.07147853821516037, 0.03878185898065567, 0.032659366726875305, 0.03652893751859665, 0.18229392170906067, 0.073262058198452, 0.05900287628173828, 0.10664978623390198, 0.08970915526151657]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.979947030544281, 0.020053040236234665, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9064003229141235, 0.04455103352665901, 0.049048569053411484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2723913788795471, 0.01160728931427002, 0.696338415145874, 0.019662918522953987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14442968368530273, 0.04174760729074478, 0.5063616633415222, 0.2511478364467621, 0.05631326884031296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3186441659927368, 0.02709266170859337, 0.13278278708457947, 0.17500078678131104, 0.1905987560749054, 0.15588079392910004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1709321141242981, 0.035454489290714264, 0.03797627612948418, 0.05808206647634506, 0.3221748471260071, 0.2711712121963501, 0.10420902073383331, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1971396654844284, 0.03036244586110115, 0.18091575801372528, 0.17116889357566833, 0.059210654348134995, 0.13205456733703613, 0.045205872505903244, 0.18394222855567932, 0.0, 0.0, 0.0, 0.0], [0.13221381604671478, 0.019768381491303444, 0.06372471153736115, 0.06933099776506424, 0.06309947371482849, 0.09296020120382309, 0.030701227486133575, 0.3849904537200928, 0.14321079850196838, 0.0, 0.0, 0.0], [0.06980615854263306, 0.00614056084305048, 0.008247887715697289, 0.012550330720841885, 0.013560635037720203, 0.03587082028388977, 0.01228304859250784, 0.16868868470191956, 0.601778507232666, 0.07107339799404144, 0.0, 0.0], [0.0608648881316185, 0.0038375426083803177, 0.04279155656695366, 0.10038410127162933, 0.026612071320414543, 0.04333258792757988, 0.01065671257674694, 0.08342711627483368, 0.1718553900718689, 0.38817715644836426, 0.06806083768606186, 0.0], [0.014886012300848961, 0.0018363917479291558, 0.01209049392491579, 0.010846177116036415, 0.003093752544373274, 0.016056740656495094, 0.0017555614467710257, 0.020901760086417198, 0.030798381194472313, 0.23313745856285095, 0.5502512454986572, 0.10434607416391373]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9551265239715576, 0.04487345367670059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8799847960472107, 0.08408363908529282, 0.03593158349394798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8051981925964355, 0.08723460137844086, 0.051951903849840164, 0.055615298449993134, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5739929676055908, 0.04412629082798958, 0.1285352110862732, 0.14267240464687347, 0.11067305505275726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.44761157035827637, 0.05743060261011124, 0.08780208975076675, 0.10678548365831375, 0.16958056390285492, 0.1307896375656128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.41970279812812805, 0.024734431877732277, 0.08340764045715332, 0.07035268843173981, 0.08245468884706497, 0.23245976865291595, 0.08688792586326599, 0.0, 0.0, 0.0, 0.0, 0.0], [0.42025232315063477, 0.04738416522741318, 0.048687130212783813, 0.06443719565868378, 0.06721972674131393, 0.12863847613334656, 0.09138281643390656, 0.131998211145401, 0.0, 0.0, 0.0, 0.0], [0.22643543779850006, 0.02606033906340599, 0.02284727618098259, 0.029372673481702805, 0.05598103627562523, 0.08910596370697021, 0.08331067115068436, 0.34287595748901367, 0.12401062995195389, 0.0, 0.0, 0.0], [0.17026743292808533, 0.015088323503732681, 0.009379584342241287, 0.026578467339277267, 0.04159500077366829, 0.07079079747200012, 0.045527368783950806, 0.32141274213790894, 0.18145078420639038, 0.11790948361158371, 0.0, 0.0], [0.1350724846124649, 0.016675980761647224, 0.013373014517128468, 0.01955663040280342, 0.05289120227098465, 0.05380149930715561, 0.05511067435145378, 0.1726294755935669, 0.1553964465856552, 0.19214408099651337, 0.1333485245704651, 0.0], [0.07148509472608566, 0.00749995606020093, 0.009808235801756382, 0.016794810071587563, 0.022439315915107727, 0.047335729002952576, 0.031363796442747116, 0.20149238407611847, 0.1096927598118782, 0.2121623009443283, 0.17322920262813568, 0.09669644385576248]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9840556383132935, 0.015944339334964752, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8753244280815125, 0.029910502955317497, 0.09476509690284729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7785478830337524, 0.03376587852835655, 0.08547811210155487, 0.10220813751220703, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6078203916549683, 0.037794966250658035, 0.09666099399328232, 0.10369881242513657, 0.15402479469776154, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.43524253368377686, 0.03078000620007515, 0.06343892216682434, 0.07255823910236359, 0.10661431401968002, 0.29136598110198975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.37486594915390015, 0.028744203969836235, 0.06359034031629562, 0.06930729746818542, 0.095894955098629, 0.265307754278183, 0.10228946059942245, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28450676798820496, 0.025613868609070778, 0.05001720413565636, 0.055339615792036057, 0.07876990735530853, 0.19157899916172028, 0.08155117928981781, 0.2326224148273468, 0.0, 0.0, 0.0, 0.0], [0.22293546795845032, 0.0230584554374218, 0.038695383816957474, 0.04352843761444092, 0.06766421347856522, 0.1633482426404953, 0.06899033486843109, 0.19938266277313232, 0.17239682376384735, 0.0, 0.0, 0.0], [0.17135027050971985, 0.021583618596196175, 0.03695805370807648, 0.038456737995147705, 0.05774207413196564, 0.12768331170082092, 0.06149352714419365, 0.16471555829048157, 0.12349909543991089, 0.19651781022548676, 0.0, 0.0], [0.15861618518829346, 0.01985713094472885, 0.029123583808541298, 0.0323592871427536, 0.0519796721637249, 0.1182146742939949, 0.054614391177892685, 0.1511715203523636, 0.11388956755399704, 0.15963254868984222, 0.1105414479970932, 0.0], [0.12451773881912231, 0.024423295632004738, 0.0329747349023819, 0.03324553370475769, 0.04873286932706833, 0.10237710922956467, 0.0552690364420414, 0.13593509793281555, 0.10538710653781891, 0.13878096640110016, 0.09587018191814423, 0.10248634219169617]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6076840758323669, 0.39231589436531067, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5740119218826294, 0.22664694488048553, 0.1993410587310791, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5490851402282715, 0.16984055936336517, 0.14071883261203766, 0.1403554379940033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5604156851768494, 0.10848326981067657, 0.10910706967115402, 0.10159764438867569, 0.12039632350206375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5460138320922852, 0.08237838745117188, 0.0802190974354744, 0.07730230689048767, 0.09164692461490631, 0.1224394366145134, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5261914730072021, 0.0677928552031517, 0.06919460743665695, 0.06449531018733978, 0.07680013030767441, 0.09788374602794647, 0.09764182567596436, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5131775736808777, 0.05574741214513779, 0.051236238330602646, 0.049592673778533936, 0.06212632358074188, 0.07881930470466614, 0.07740150392055511, 0.11189895868301392, 0.0, 0.0, 0.0, 0.0], [0.526820719242096, 0.04039672017097473, 0.039656419306993484, 0.038343656808137894, 0.049558475613594055, 0.06518222391605377, 0.06257982552051544, 0.09358059614896774, 0.08388140052556992, 0.0, 0.0, 0.0], [0.529583215713501, 0.0338798463344574, 0.03318331390619278, 0.03119697980582714, 0.04034705460071564, 0.05263498052954674, 0.05049256235361099, 0.08114458620548248, 0.06535084545612335, 0.08218658715486526, 0.0, 0.0], [0.5429602861404419, 0.027156753465533257, 0.025428326800465584, 0.024930426850914955, 0.03425537422299385, 0.045957356691360474, 0.044235460460186005, 0.07109387218952179, 0.05870058387517929, 0.0700608491897583, 0.055220719426870346, 0.0], [0.5506265759468079, 0.022231576964259148, 0.021587437018752098, 0.020916588604450226, 0.02898082509636879, 0.03786320611834526, 0.03647599741816521, 0.059800852090120316, 0.050082769244909286, 0.06114479899406433, 0.050266657024621964, 0.060022708028554916]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8702662587165833, 0.12973372638225555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8472357392311096, 0.09009046107530594, 0.06267386674880981, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.782696545124054, 0.09648110717535019, 0.06159596145153046, 0.05922645330429077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7116276025772095, 0.0798899456858635, 0.05689363181591034, 0.049426473677158356, 0.10216224938631058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.601807177066803, 0.09040296077728271, 0.054780904203653336, 0.05295631289482117, 0.10202852636575699, 0.09802403301000595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6155767440795898, 0.06183405965566635, 0.042170554399490356, 0.04380261152982712, 0.07423573732376099, 0.0720072090625763, 0.09037303924560547, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5020349621772766, 0.0681060329079628, 0.04269871860742569, 0.039827678352594376, 0.08607848733663559, 0.0705646499991417, 0.09173229336738586, 0.0989571139216423, 0.0, 0.0, 0.0, 0.0], [0.45950639247894287, 0.059754569083452225, 0.0354926735162735, 0.03442965820431709, 0.07413391023874283, 0.06531193852424622, 0.08086792379617691, 0.09575708955526352, 0.09474585205316544, 0.0, 0.0, 0.0], [0.4242171049118042, 0.055863428860902786, 0.03791385889053345, 0.03367750346660614, 0.06485656648874283, 0.05714495852589607, 0.06973648816347122, 0.08327874541282654, 0.06851263344287872, 0.10479874163866043, 0.0, 0.0], [0.4251870810985565, 0.04572660103440285, 0.025779863819479942, 0.027172980830073357, 0.058162983506917953, 0.05110367760062218, 0.06293302029371262, 0.07560289651155472, 0.0698137879371643, 0.08496536314487457, 0.07355175912380219, 0.0], [0.3971458375453949, 0.04355883598327637, 0.02725597843527794, 0.023847106844186783, 0.046799108386039734, 0.044024109840393066, 0.0540614128112793, 0.06523529440164566, 0.058209050446748734, 0.07153372466564178, 0.06782133132219315, 0.10050824284553528]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9218317270278931, 0.07816830277442932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8978624939918518, 0.01866903342306614, 0.08346851915121078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7895718812942505, 0.014598335139453411, 0.028746197000145912, 0.16708362102508545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6952294111251831, 0.11708205193281174, 0.03865331411361694, 0.0947684720158577, 0.05426681786775589, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7230411171913147, 0.03417448699474335, 0.025864381343126297, 0.08141802996397018, 0.04185297340154648, 0.09364903718233109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5197308659553528, 0.09920022636651993, 0.01570837013423443, 0.024404294788837433, 0.07784514129161835, 0.0959615558385849, 0.16714954376220703, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4812636375427246, 0.026900850236415863, 0.02467389963567257, 0.06395314633846283, 0.013288728892803192, 0.0744088664650917, 0.0035473397001624107, 0.3119634687900543, 0.0, 0.0, 0.0, 0.0], [0.4392852485179901, 0.050179239362478256, 0.027471525594592094, 0.027292031794786453, 0.010739617049694061, 0.08148384094238281, 0.007755375932902098, 0.06490097939968109, 0.2908921539783478, 0.0, 0.0, 0.0], [0.4640936553478241, 0.016027523204684258, 0.095416359603405, 0.0877053514122963, 0.006881945766508579, 0.08289765566587448, 0.0044562420807778835, 0.1074572503566742, 0.040652040392160416, 0.0944119319319725, 0.0, 0.0], [0.36210981011390686, 0.017365405336022377, 0.020258352160453796, 0.2149999439716339, 0.01839406043291092, 0.07594575732946396, 0.003529764711856842, 0.15045060217380524, 0.04410959407687187, 0.03428013250231743, 0.05855671688914299, 0.0], [0.20636208355426788, 0.08818274736404419, 0.053778547793626785, 0.1101035624742508, 0.0295663233846426, 0.08989465236663818, 0.009544558823108673, 0.11749652028083801, 0.044293664395809174, 0.06098956614732742, 0.13603566586971283, 0.05375209078192711]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7630558013916016, 0.23694424331188202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8104128241539001, 0.16593848168849945, 0.02364877052605152, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8013966083526611, 0.05908661335706711, 0.08803565055131912, 0.051481135189533234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6542818546295166, 0.08055344223976135, 0.07231783121824265, 0.093824103474617, 0.09902267903089523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6464332342147827, 0.09665412455797195, 0.06958901137113571, 0.06390134245157242, 0.05082916095852852, 0.07259304821491241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5932385325431824, 0.07786495983600616, 0.04866541549563408, 0.026224525645375252, 0.0359153114259243, 0.06502923369407654, 0.15306195616722107, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4648856222629547, 0.08402153849601746, 0.1546728014945984, 0.14775429666042328, 0.02312767319381237, 0.06596957892179489, 0.02099539339542389, 0.03857317194342613, 0.0, 0.0, 0.0, 0.0], [0.4725463092327118, 0.28130587935447693, 0.031354039907455444, 0.045714255422353745, 0.035225410014390945, 0.05393796041607857, 0.020315248519182205, 0.043893616646528244, 0.015707310289144516, 0.0, 0.0, 0.0], [0.4615643620491028, 0.06369355320930481, 0.09353318810462952, 0.07067464292049408, 0.04094003885984421, 0.05316901579499245, 0.0341290645301342, 0.08444718271493912, 0.045428041368722916, 0.05242094025015831, 0.0, 0.0], [0.3452893793582916, 0.06941735744476318, 0.04384656995534897, 0.13930663466453552, 0.05961472913622856, 0.0849718376994133, 0.025742970407009125, 0.05666351690888405, 0.07251816987991333, 0.0729379802942276, 0.0296908151358366, 0.0], [0.4520183801651001, 0.03357364982366562, 0.05398453027009964, 0.035573069006204605, 0.0353504978120327, 0.07412748783826828, 0.05191688612103462, 0.04278302937746048, 0.06815274804830551, 0.050793930888175964, 0.0564747154712677, 0.04525120183825493]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.972019374370575, 0.027980569750070572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.766235888004303, 0.16074949502944946, 0.07301460951566696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7264899015426636, 0.09499865770339966, 0.09379664063453674, 0.08471482992172241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6163113117218018, 0.12251747399568558, 0.08084671199321747, 0.12036369740962982, 0.05996077135205269, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5449367165565491, 0.08718199282884598, 0.09454697370529175, 0.09300707280635834, 0.08671275526285172, 0.09361441433429718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.581535816192627, 0.13962030410766602, 0.04561341553926468, 0.07957355678081512, 0.031826429069042206, 0.06989289075136185, 0.05193748697638512, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4521293044090271, 0.06534257531166077, 0.06223730370402336, 0.08815828710794449, 0.073446124792099, 0.05534704774618149, 0.08042723685503006, 0.12291213124990463, 0.0, 0.0, 0.0, 0.0], [0.5168890953063965, 0.05495457723736763, 0.04661063477396965, 0.04772607609629631, 0.054437849670648575, 0.054699886590242386, 0.08823376893997192, 0.09498725831508636, 0.04146074503660202, 0.0, 0.0, 0.0], [0.45206570625305176, 0.07043009251356125, 0.03654498606920242, 0.046732399612665176, 0.062179744243621826, 0.04561404883861542, 0.07900413870811462, 0.12783221900463104, 0.04042116552591324, 0.03917555510997772, 0.0, 0.0], [0.4915085434913635, 0.046815503388643265, 0.0224485844373703, 0.054474372416734695, 0.06866524368524551, 0.043236277997493744, 0.058763518929481506, 0.08669673651456833, 0.0398537777364254, 0.04208027571439743, 0.045457061380147934, 0.0], [0.3946869373321533, 0.06832289695739746, 0.02945793606340885, 0.03097362071275711, 0.07954288274049759, 0.05126291513442993, 0.10539431124925613, 0.08275118470191956, 0.0355067178606987, 0.04803762957453728, 0.04208837449550629, 0.0319746732711792]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9688529372215271, 0.031147044152021408, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8570253849029541, 0.062306661158800125, 0.0806679055094719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8029581904411316, 0.04412786290049553, 0.05619947612285614, 0.09671443700790405, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7389590740203857, 0.040562234818935394, 0.07495889812707901, 0.09387699514627457, 0.0516427606344223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5993561744689941, 0.051472924649715424, 0.049909934401512146, 0.07945625483989716, 0.05817067623138428, 0.16163399815559387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5680141448974609, 0.036817993968725204, 0.05957496166229248, 0.08119294792413712, 0.05851511284708977, 0.1539078652858734, 0.04197695478796959, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48524320125579834, 0.043800197541713715, 0.05856427922844887, 0.07296425849199295, 0.05363108590245247, 0.13263963162899017, 0.04517727345228195, 0.10798004269599915, 0.0, 0.0, 0.0, 0.0], [0.4732201099395752, 0.02950545959174633, 0.04531603679060936, 0.06493087112903595, 0.037241507321596146, 0.12293045222759247, 0.030782992020249367, 0.10388990491628647, 0.09218274801969528, 0.0, 0.0, 0.0], [0.4268310070037842, 0.030795885249972343, 0.04433230683207512, 0.06361854076385498, 0.03509882465004921, 0.11961077153682709, 0.031144065782427788, 0.08233419060707092, 0.08275330811738968, 0.08348111063241959, 0.0, 0.0], [0.3738822042942047, 0.03556617721915245, 0.03818495571613312, 0.05299520492553711, 0.03664463385939598, 0.11148156225681305, 0.042840056121349335, 0.0729127749800682, 0.10209716111421585, 0.0776481181383133, 0.05574718862771988, 0.0], [0.3833029270172119, 0.026266418397426605, 0.046954914927482605, 0.04724700748920441, 0.03048652410507202, 0.0995405837893486, 0.029856879264116287, 0.08838009089231491, 0.0776040181517601, 0.09249158948659897, 0.052949681878089905, 0.024919403716921806]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9848400950431824, 0.015159910544753075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9608851671218872, 0.014262314885854721, 0.02485252544283867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9154780507087708, 0.009755338542163372, 0.022220587357878685, 0.052546028047800064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8640547394752502, 0.022632481530308723, 0.03669228032231331, 0.0643068477511406, 0.012313623912632465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7343919277191162, 0.01643194817006588, 0.03234057500958443, 0.05545142665505409, 0.02478897012770176, 0.13659514486789703, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7495422959327698, 0.02294948138296604, 0.0315098874270916, 0.06747899204492569, 0.016226673498749733, 0.08359023183584213, 0.028702476993203163, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6013484597206116, 0.02158457413315773, 0.037559930235147476, 0.061698153614997864, 0.02471565641462803, 0.1450560837984085, 0.0265207439661026, 0.08151637762784958, 0.0, 0.0, 0.0, 0.0], [0.5195896029472351, 0.013673385605216026, 0.02469421736896038, 0.05734190344810486, 0.023027218878269196, 0.1530330926179886, 0.017911896109580994, 0.08565989881753922, 0.10506880283355713, 0.0, 0.0, 0.0], [0.49409326910972595, 0.019376453012228012, 0.030479811131954193, 0.056374695152044296, 0.026226021349430084, 0.1314636766910553, 0.02314276620745659, 0.08470799773931503, 0.08495794236660004, 0.04917730763554573, 0.0, 0.0], [0.4547763466835022, 0.027728945016860962, 0.020163388922810555, 0.048880938440561295, 0.02525123581290245, 0.13165032863616943, 0.031582314521074295, 0.04620339721441269, 0.11664771288633347, 0.03626316413283348, 0.060852326452732086, 0.0], [0.4784049689769745, 0.023097189143300056, 0.035647861659526825, 0.04248501732945442, 0.0198901928961277, 0.1021590456366539, 0.03027435392141342, 0.06000401824712753, 0.09482362866401672, 0.04951559379696846, 0.04960549250245094, 0.01409261766821146]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00023010728182271123, 0.9997698664665222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00040440820157527924, 0.5760704278945923, 0.4235251545906067, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0006699447403661907, 0.4373238682746887, 0.29587674140930176, 0.2661294639110565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0007628405001014471, 0.2933131456375122, 0.21884940564632416, 0.19266118109226227, 0.2944134473800659, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0013304917374625802, 0.22389216721057892, 0.16386117041110992, 0.15429355204105377, 0.22289718687534332, 0.23372547328472137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0007811126415617764, 0.17180658876895905, 0.13584180176258087, 0.1269332766532898, 0.16967187821865082, 0.17110370099544525, 0.2238616943359375, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0011363985249772668, 0.1293402761220932, 0.09965262562036514, 0.0909978374838829, 0.13142170011997223, 0.13494838774204254, 0.17862166464328766, 0.23388105630874634, 0.0, 0.0, 0.0, 0.0], [0.0018828052561730146, 0.09856385737657547, 0.07240129262208939, 0.06875976920127869, 0.11190880089998245, 0.11563269048929214, 0.14963652193546295, 0.21002690494060516, 0.17118731141090393, 0.0, 0.0, 0.0], [0.0017585208406671882, 0.083652563393116, 0.06346957385540009, 0.05951950326561928, 0.09480505436658859, 0.09138507395982742, 0.12164143472909927, 0.17406365275382996, 0.13257531821727753, 0.1771293431520462, 0.0, 0.0], [0.0017982524586841464, 0.07162737101316452, 0.05087161809206009, 0.04886774346232414, 0.08079848438501358, 0.08193361014127731, 0.11045757681131363, 0.1608104556798935, 0.11930069327354431, 0.15031492710113525, 0.1232193112373352, 0.0], [0.0021407450549304485, 0.05850968137383461, 0.04200172796845436, 0.04180064797401428, 0.07000071555376053, 0.06888872385025024, 0.0903313085436821, 0.13403572142124176, 0.10497260093688965, 0.12410125881433487, 0.1082136482000351, 0.15500320494174957]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4703635573387146, 0.5296363830566406, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09644557535648346, 0.00046337247476913035, 0.9030910730361938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1662207841873169, 0.0012884103925898671, 0.029675235971808434, 0.8028154969215393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07393239438533783, 0.002491587307304144, 0.0011616703122854233, 0.0016388826770707965, 0.9207754731178284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39317989349365234, 0.0016690263291820884, 0.0022225764114409685, 0.00592972245067358, 0.0005455518257804215, 0.5964532494544983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09042319655418396, 0.007328287232667208, 0.0010630202013999224, 0.0016174084739759564, 0.003815952455624938, 0.0030302933882921934, 0.8927218317985535, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11827344447374344, 0.0023825904354453087, 0.006021094508469105, 0.00265209237113595, 0.0002592849195934832, 0.0024089738726615906, 0.0005828177672810853, 0.8674197196960449, 0.0, 0.0, 0.0, 0.0], [0.09650203585624695, 0.003430234966799617, 0.003914874047040939, 0.0025358637794852257, 0.0004887667600996792, 0.021757174283266068, 0.0018492029048502445, 0.0022155013866722584, 0.8673063516616821, 0.0, 0.0, 0.0], [0.07428883016109467, 0.001193568343296647, 0.04355399310588837, 0.011594090610742569, 0.00019835159764625132, 0.006407168693840504, 0.0007153560291044414, 0.006364504806697369, 0.016579916700720787, 0.8391042351722717, 0.0, 0.0], [0.03366722911596298, 0.00040106652886606753, 0.0025250082835555077, 0.004791175946593285, 0.00028016557916998863, 0.0007122975657694042, 0.000632882583886385, 0.008528497070074081, 0.0016292856307700276, 0.001422783941961825, 0.9454097151756287, 0.0], [0.055681731551885605, 0.0009613372385501862, 0.0024059081915766, 0.0019102273508906364, 0.0005529737682081759, 0.0019506729440763593, 0.0010463668731972575, 0.010695857927203178, 0.0003851715009659529, 0.0011233465047553182, 0.01299989316612482, 0.9102866053581238]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8781078457832336, 0.12189211696386337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6164735555648804, 0.19103063642978668, 0.19249577820301056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6650122404098511, 0.12475281208753586, 0.15640965104103088, 0.05382528528571129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5632638931274414, 0.09697529673576355, 0.11476663500070572, 0.0953347235918045, 0.12965941429138184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3389432430267334, 0.07471293210983276, 0.08159822225570679, 0.14115813374519348, 0.31209391355514526, 0.05149354785680771, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4212939143180847, 0.06571400910615921, 0.04560428485274315, 0.11548054218292236, 0.2409122735261917, 0.08692669123411179, 0.02406831830739975, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23083534836769104, 0.04499159753322601, 0.07112487405538559, 0.14779967069625854, 0.28178486227989197, 0.03674672544002533, 0.09242646396160126, 0.09429051727056503, 0.0, 0.0, 0.0, 0.0], [0.2732803225517273, 0.031674861907958984, 0.0702551081776619, 0.0804426521062851, 0.076832115650177, 0.027498045936226845, 0.05925281345844269, 0.2976400852203369, 0.08312404900789261, 0.0, 0.0, 0.0], [0.2441769689321518, 0.022628450766205788, 0.09720192104578018, 0.0733688548207283, 0.09339921921491623, 0.01914278417825699, 0.04514671117067337, 0.28711599111557007, 0.05994873866438866, 0.05787045508623123, 0.0, 0.0], [0.17422117292881012, 0.01404612883925438, 0.053765296936035156, 0.1819821149110794, 0.09341471642255783, 0.022614533081650734, 0.034402161836624146, 0.17333924770355225, 0.042598556727170944, 0.07513485103845596, 0.13448117673397064, 0.0], [0.26459449529647827, 0.018104610964655876, 0.04464280605316162, 0.07766652852296829, 0.06467735767364502, 0.026884667575359344, 0.04807349666953087, 0.14864404499530792, 0.04848519340157509, 0.03841494396328926, 0.12109217047691345, 0.09871960431337357]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9332111477851868, 0.06678880751132965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.823287844657898, 0.142595112323761, 0.03411710634827614, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7973390221595764, 0.15168169140815735, 0.032679665833711624, 0.018299609422683716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8104780912399292, 0.058685604482889175, 0.04641744866967201, 0.05605899915099144, 0.02835986204445362, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8055204749107361, 0.06521234661340714, 0.044814109802246094, 0.043998170644044876, 0.020638369023799896, 0.019816480576992035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7404448390007019, 0.033930033445358276, 0.011560495011508465, 0.042004719376564026, 0.07076054811477661, 0.03288567066192627, 0.06841365247964859, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8444225788116455, 0.07805097848176956, 0.009932824410498142, 0.011446095071732998, 0.012716361321508884, 0.010639584623277187, 0.016574356704950333, 0.01621723733842373, 0.0, 0.0, 0.0, 0.0], [0.7817609310150146, 0.0868074968457222, 0.019452117383480072, 0.017004400491714478, 0.013132546097040176, 0.013470613397657871, 0.040290169417858124, 0.015268928371369839, 0.01281281653791666, 0.0, 0.0, 0.0], [0.7691628336906433, 0.06588425487279892, 0.020462455227971077, 0.01662065088748932, 0.016864582896232605, 0.013528340496122837, 0.033799927681684494, 0.024586357176303864, 0.009423044510185719, 0.029667558148503304, 0.0, 0.0], [0.8633047342300415, 0.038117025047540665, 0.007750991731882095, 0.018767759203910828, 0.005141099449247122, 0.005130196921527386, 0.005833812523633242, 0.033420782536268234, 0.0038051342125982046, 0.013171013444662094, 0.005557485856115818, 0.0], [0.7065597772598267, 0.04700806364417076, 0.02238195389509201, 0.01639433391392231, 0.03139510750770569, 0.01754208654165268, 0.02735884301364422, 0.039188142865896225, 0.008682917803525925, 0.051923058927059174, 0.010514219291508198, 0.021051548421382904]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9379645586013794, 0.06203542649745941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7079555988311768, 0.24472026526927948, 0.047324102371931076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03715604543685913, 0.018268484622240067, 0.9409029483795166, 0.0036725325044244528, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22538582980632782, 0.005044805351644754, 0.6956296563148499, 0.0491027794778347, 0.024836944416165352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04836393520236015, 0.000954647024627775, 0.030809493735432625, 0.02715085633099079, 0.8526884913444519, 0.04003256559371948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02019609697163105, 0.0015556745929643512, 0.036298081278800964, 0.014755629003047943, 0.8196247220039368, 0.08057141304016113, 0.026998432353138924, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16762953996658325, 0.000587791670113802, 0.012316002510488033, 0.04345786198973656, 0.03088608756661415, 0.40523314476013184, 0.06328568607568741, 0.27660390734672546, 0.0, 0.0, 0.0, 0.0], [0.04366714134812355, 0.00022709081531502306, 0.00296855834312737, 0.002727970713749528, 0.011186012998223305, 0.06274452805519104, 0.08613348007202148, 0.7055478692054749, 0.08479733765125275, 0.0, 0.0, 0.0], [0.019737225025892258, 3.866089537041262e-05, 7.319901487790048e-05, 0.0008717062883079052, 0.0006596603197976947, 0.005625881254673004, 0.004456520080566406, 0.053980883210897446, 0.8280009627342224, 0.08655531704425812, 0.0, 0.0], [0.016411777585744858, 7.675777851545718e-06, 6.868194759590551e-05, 8.072177297435701e-05, 0.00040444653131999075, 0.0033121800515800714, 0.0008003880502656102, 0.012667200528085232, 0.43930771946907043, 0.4885188937187195, 0.03842031955718994, 0.0], [0.060993436723947525, 7.77978784753941e-05, 0.0004060136852785945, 0.00020667102944571525, 0.0006278342334553599, 0.0022513840813189745, 0.0010990225709974766, 0.012721770443022251, 0.07257095724344254, 0.28912219405174255, 0.4415247440338135, 0.11839824169874191]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9912973642349243, 0.008702684193849564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8648866415023804, 0.09594505280256271, 0.0391683392226696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8359034657478333, 0.11275361478328705, 0.024740837514400482, 0.026602065190672874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7440686821937561, 0.06632532924413681, 0.026145070791244507, 0.03581717982888222, 0.1276436746120453, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.31843435764312744, 0.013179956935346127, 0.0267920084297657, 0.043430984020233154, 0.4265963137149811, 0.17156633734703064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14179986715316772, 0.004731707274913788, 0.0218634195625782, 0.013440828770399094, 0.12196134775876999, 0.6652882695198059, 0.03091456927359104, 0.0, 0.0, 0.0, 0.0, 0.0], [0.221032977104187, 0.008466227911412716, 0.07445935904979706, 0.0396663174033165, 0.1500423103570938, 0.29216456413269043, 0.07217342406511307, 0.14199481904506683, 0.0, 0.0, 0.0, 0.0], [0.09185487776994705, 0.002501819981262088, 0.006770754233002663, 0.007829136215150356, 0.04507555440068245, 0.16715432703495026, 0.029191777110099792, 0.564083456993103, 0.0855383649468422, 0.0, 0.0, 0.0], [0.12729421257972717, 0.0010979826329275966, 0.004122352227568626, 0.00581877026706934, 0.02883356250822544, 0.11280454695224762, 0.020912544801831245, 0.4405272603034973, 0.21420541405677795, 0.04438337683677673, 0.0, 0.0], [0.057975538074970245, 0.0002739007177297026, 0.0013533690944314003, 0.0030177205335348845, 0.02802245505154133, 0.0673532485961914, 0.0065277134999632835, 0.4956493675708771, 0.23367507755756378, 0.07504769414663315, 0.03110402449965477, 0.0], [0.10563216358423233, 0.0009625510429032147, 0.0020227984059602022, 0.0014707164373248816, 0.019648760557174683, 0.0892830565571785, 0.010372592136263847, 0.07168444991111755, 0.32808971405029297, 0.03909464180469513, 0.14632777869701385, 0.18541084229946136]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.93415766954422, 0.06584230810403824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8781709671020508, 0.02717074751853943, 0.09465830773115158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7062388062477112, 0.03728875145316124, 0.24489866197109222, 0.01157375704497099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8126732707023621, 0.07964880019426346, 0.08058363944292068, 0.007200552150607109, 0.019893664866685867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4347286820411682, 0.017637748271226883, 0.02783961221575737, 0.10469397157430649, 0.29196664690971375, 0.12313339114189148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23461021482944489, 0.013393808156251907, 0.028551394119858742, 0.011351880617439747, 0.40035349130630493, 0.3019402325153351, 0.00979908648878336, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48288485407829285, 0.003346700919792056, 0.08956298977136612, 0.15661408007144928, 0.023249968886375427, 0.1860189288854599, 0.0016905857482925057, 0.05663188174366951, 0.0, 0.0, 0.0, 0.0], [0.39638856053352356, 0.007130577228963375, 0.016744393855333328, 0.013370837084949017, 0.006458314601331949, 0.06643921136856079, 0.012001914903521538, 0.4646482467651367, 0.016817938536405563, 0.0, 0.0, 0.0], [0.34083396196365356, 0.0006720253149978817, 0.006472840439528227, 0.014820754528045654, 0.005580693017691374, 0.03332879766821861, 0.0011610648361966014, 0.2690477669239044, 0.24367675185203552, 0.08440529555082321, 0.0, 0.0], [0.20390523970127106, 0.000153121814946644, 0.017549574375152588, 0.3541663885116577, 0.0009578660828992724, 0.027641739696264267, 8.7510677985847e-05, 0.07152913510799408, 0.05104142799973488, 0.2649223804473877, 0.008045703172683716, 0.0], [0.2707310616970062, 0.00037032441468909383, 0.006185768637806177, 0.007864363491535187, 0.0009796275990083814, 0.040453121066093445, 0.006254659499973059, 0.021984709426760674, 0.011691758409142494, 0.15498913824558258, 0.43243300914764404, 0.046062443405389786]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9590114951133728, 0.040988512337207794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5909857749938965, 0.29490604996681213, 0.11410824209451675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7424482107162476, 0.07264469563961029, 0.1396339237689972, 0.045273177325725555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5215661525726318, 0.05047183856368065, 0.18099644780158997, 0.19013552367687225, 0.05682998150587082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3786081373691559, 0.06172652542591095, 0.15618978440761566, 0.2035628855228424, 0.10177110135555267, 0.09814158082008362, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24982644617557526, 0.017605241388082504, 0.14165279269218445, 0.11939530819654465, 0.05709546059370041, 0.28054073452949524, 0.13388407230377197, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12028275430202484, 0.016026677563786507, 0.03646903485059738, 0.051655691117048264, 0.12589678168296814, 0.19984306395053864, 0.18626657128334045, 0.26355937123298645, 0.0, 0.0, 0.0, 0.0], [0.09398014098405838, 0.004285357892513275, 0.004469349980354309, 0.0072449506260454655, 0.006311268545687199, 0.04345414787530899, 0.059877097606658936, 0.6592835783958435, 0.12109418958425522, 0.0, 0.0, 0.0], [0.09061430394649506, 0.0013481259811669588, 0.0008830557344481349, 0.002764447359368205, 0.007852896116673946, 0.04917768016457558, 0.03815363347530365, 0.4651167690753937, 0.21454475820064545, 0.12954437732696533, 0.0, 0.0], [0.056537192314863205, 0.001229340210556984, 0.0018526456551626325, 0.0013827537186443806, 0.0038750397507101297, 0.02679772675037384, 0.018294908106327057, 0.2956799268722534, 0.21861840784549713, 0.34043604135513306, 0.03529601916670799, 0.0], [0.06780596077442169, 0.0018421338172629476, 0.002587857423350215, 0.0027698760386556387, 0.003574688220396638, 0.017963245511054993, 0.03389628976583481, 0.12340184301137924, 0.18436744809150696, 0.26132410764694214, 0.24650393426418304, 0.05396261066198349]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9736940860748291, 0.026305966079235077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9209668636322021, 0.030091503635048866, 0.04894161969423294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8649247288703918, 0.03205401077866554, 0.04293571412563324, 0.06008556857705116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8901171088218689, 0.013402741402387619, 0.02664656564593315, 0.03422350063920021, 0.03561001271009445, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8113662004470825, 0.011968432925641537, 0.03363415226340294, 0.037792086601257324, 0.032452091574668884, 0.07278704643249512, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8191323280334473, 0.010194334201514721, 0.028570281341671944, 0.03664132207632065, 0.020372258499264717, 0.061434555798769, 0.023654859513044357, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7478240728378296, 0.010737818665802479, 0.02998773753643036, 0.026183634996414185, 0.029361456632614136, 0.050624873489141464, 0.01880902796983719, 0.08647137135267258, 0.0, 0.0, 0.0, 0.0], [0.7198331952095032, 0.009542803280055523, 0.022422324866056442, 0.021884340792894363, 0.01618967205286026, 0.03239963948726654, 0.016953716054558754, 0.07919379323720932, 0.08158048987388611, 0.0, 0.0, 0.0], [0.7244294881820679, 0.006173714529722929, 0.014071539975702763, 0.016882434487342834, 0.014386113733053207, 0.026978041976690292, 0.014173042960464954, 0.07686232775449753, 0.06194736808538437, 0.04409588873386383, 0.0, 0.0], [0.5530064702033997, 0.00840730406343937, 0.017932601273059845, 0.0255845095962286, 0.02281295694410801, 0.0459609217941761, 0.013337604701519012, 0.11343717575073242, 0.0837775468826294, 0.03909250721335411, 0.07665035128593445, 0.0], [0.6812024712562561, 0.005081912502646446, 0.011029524728655815, 0.01381220668554306, 0.008537770248949528, 0.02969549410045147, 0.009517206810414791, 0.07080018520355225, 0.057230833917856216, 0.022029759362339973, 0.05572707578539848, 0.0353354811668396]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7174798250198364, 0.2825200855731964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5321810245513916, 0.148194819688797, 0.319624125957489, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49307969212532043, 0.10650792717933655, 0.21861152350902557, 0.18180087208747864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.40419021248817444, 0.07372257858514786, 0.13404139876365662, 0.11584916710853577, 0.2721967101097107, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2947199046611786, 0.04160097986459732, 0.10102204978466034, 0.08106310665607452, 0.20569005608558655, 0.2759039103984833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24270351231098175, 0.041247762739658356, 0.07888814806938171, 0.0601796992123127, 0.15548306703567505, 0.16124945878982544, 0.2602483034133911, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15333375334739685, 0.0247360672801733, 0.05465046688914299, 0.039793603122234344, 0.11203107237815857, 0.12232091277837753, 0.1755170226097107, 0.3176170885562897, 0.0, 0.0, 0.0, 0.0], [0.13877515494823456, 0.01843683235347271, 0.04112793132662773, 0.028804156929254532, 0.06946352124214172, 0.08256173133850098, 0.10517999529838562, 0.2463608831167221, 0.26928985118865967, 0.0, 0.0, 0.0], [0.1199040338397026, 0.01487675029784441, 0.031828153878450394, 0.022936033084988594, 0.054138340055942535, 0.06396904587745667, 0.07627788186073303, 0.1862259954214096, 0.19804146885871887, 0.23180240392684937, 0.0, 0.0], [0.12630590796470642, 0.015686102211475372, 0.028281889855861664, 0.02116563729941845, 0.05268620327115059, 0.059369105845689774, 0.068016417324543, 0.14470727741718292, 0.17204780876636505, 0.19920207560062408, 0.1125316247344017, 0.0], [0.14840739965438843, 0.01661476492881775, 0.025700176134705544, 0.01897691935300827, 0.04167233034968376, 0.047815874218940735, 0.06246653199195862, 0.12499336898326874, 0.1255367547273636, 0.14674519002437592, 0.08876433968544006, 0.15230637788772583]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9369667172431946, 0.06303325295448303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7100542187690735, 0.1202579215168953, 0.169687882065773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13701392710208893, 0.01138237863779068, 0.8253353834152222, 0.026268305256962776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3931029438972473, 0.010305263102054596, 0.37625741958618164, 0.08749116212129593, 0.13284313678741455, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12200688570737839, 0.008694413118064404, 0.0985313206911087, 0.0681489109992981, 0.6705349087715149, 0.03208352252840996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19365546107292175, 0.009477543644607067, 0.04317101463675499, 0.05981353670358658, 0.5394402742385864, 0.10020936280488968, 0.054232798516750336, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13985542953014374, 0.010571797378361225, 0.09438531845808029, 0.16049186885356903, 0.34686002135276794, 0.10269001126289368, 0.05728447437286377, 0.0878610759973526, 0.0, 0.0, 0.0, 0.0], [0.19576559960842133, 0.0031221627723425627, 0.053796298801898956, 0.03173494338989258, 0.1254771649837494, 0.08873596787452698, 0.0685482993721962, 0.3735332489013672, 0.05928633362054825, 0.0, 0.0, 0.0], [0.24252833425998688, 0.001316024106927216, 0.03527138754725456, 0.021554838865995407, 0.07222966849803925, 0.06741803139448166, 0.016597477719187737, 0.22136302292346954, 0.03975214436650276, 0.2819690704345703, 0.0, 0.0], [0.06255188584327698, 0.00034409918589517474, 0.0016009265091270208, 0.003741259453818202, 0.025818554684519768, 0.033243875950574875, 0.005965142976492643, 0.10557804256677628, 0.046737879514694214, 0.6736611723899841, 0.040757182985544205, 0.0], [0.1558845043182373, 0.0005898639792576432, 0.0017844068352133036, 0.004590149968862534, 0.009998415596783161, 0.02667095884680748, 0.003405632684007287, 0.037321269512176514, 0.05449407547712326, 0.4541740417480469, 0.13782721757888794, 0.1132594645023346]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7902512550354004, 0.2097487598657608, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16302646696567535, 0.8188311457633972, 0.018142392858862877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21496212482452393, 0.7441778779029846, 0.028864892199635506, 0.011995168402791023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2576298713684082, 0.5512527823448181, 0.06071155518293381, 0.03953644260764122, 0.09086935967206955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1743863970041275, 0.17801572382450104, 0.03725041449069977, 0.02944694086909294, 0.5401284098625183, 0.040772080421447754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1690203845500946, 0.08488938212394714, 0.04634728282690048, 0.03722001239657402, 0.29029759764671326, 0.17155815660953522, 0.20066718757152557, 0.0, 0.0, 0.0, 0.0, 0.0], [0.052473701536655426, 0.0397942028939724, 0.017460301518440247, 0.011766470968723297, 0.16572000086307526, 0.065220907330513, 0.6219889521598816, 0.025575540959835052, 0.0, 0.0, 0.0, 0.0], [0.09761087596416473, 0.02446305751800537, 0.009704435244202614, 0.008824140764772892, 0.08726299554109573, 0.04076192528009415, 0.35046592354774475, 0.09055014699697495, 0.29035651683807373, 0.0, 0.0, 0.0], [0.0891936719417572, 0.016631746664643288, 0.004590357653796673, 0.0025735972449183464, 0.060088884085416794, 0.018169047310948372, 0.2089393585920334, 0.07626218348741531, 0.45058995485305786, 0.07296118885278702, 0.0, 0.0], [0.06603395938873291, 0.007157415151596069, 0.004313490353524685, 0.003372266422957182, 0.0903496965765953, 0.01674657315015793, 0.0638502910733223, 0.06875314563512802, 0.41355639696121216, 0.16250348091125488, 0.10336321592330933, 0.0], [0.07272497564554214, 0.0305596012622118, 0.0023116839583963156, 0.0009517553844489157, 0.01897965744137764, 0.006342370994389057, 0.09928803890943527, 0.028887443244457245, 0.4353678226470947, 0.03425458446145058, 0.14930544793605804, 0.12102659791707993]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8651140928268433, 0.13488595187664032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7069637179374695, 0.13403700292110443, 0.1589992642402649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6579862236976624, 0.10959306359291077, 0.11564951390028, 0.11677118390798569, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5272865295410156, 0.08985163271427155, 0.08828489482402802, 0.0921960100531578, 0.2023809254169464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4782703220844269, 0.08764707297086716, 0.08274314552545547, 0.07801370322704315, 0.1678975224494934, 0.10542823374271393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3823850154876709, 0.06394025683403015, 0.059817925095558167, 0.05747097358107567, 0.14363016188144684, 0.08353633433580399, 0.2092193067073822, 0.0, 0.0, 0.0, 0.0, 0.0], [0.34527620673179626, 0.048548709601163864, 0.06020848825573921, 0.05394059419631958, 0.11346247792243958, 0.06415403634309769, 0.12884928286075592, 0.18556033074855804, 0.0, 0.0, 0.0, 0.0], [0.2516897916793823, 0.04683006927371025, 0.05075565353035927, 0.04517180100083351, 0.08541659265756607, 0.057843394577503204, 0.10375666618347168, 0.18626761436462402, 0.17226842045783997, 0.0, 0.0, 0.0], [0.23134355247020721, 0.03772030025720596, 0.04131350293755531, 0.04102708026766777, 0.08295273780822754, 0.04716109856963158, 0.07933615148067474, 0.15425843000411987, 0.13029208779335022, 0.1545950472354889, 0.0, 0.0], [0.18546068668365479, 0.03300140053033829, 0.03182963281869888, 0.033861931413412094, 0.06720530986785889, 0.044443778693675995, 0.07274916023015976, 0.12987762689590454, 0.12584485113620758, 0.1473376303911209, 0.1283879429101944, 0.0], [0.1627577245235443, 0.024810824543237686, 0.025144211947917938, 0.028414679691195488, 0.05169982090592384, 0.038615357130765915, 0.06539981067180634, 0.10991564393043518, 0.1000967025756836, 0.10192864388227463, 0.10087453573942184, 0.1903419941663742]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.932966411113739, 0.067033551633358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8618248105049133, 0.06330804526805878, 0.07486721128225327, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8436559438705444, 0.05087690055370331, 0.049410294741392136, 0.05605683848261833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8058751821517944, 0.040660977363586426, 0.03847081586718559, 0.03894269838929176, 0.07605024427175522, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7697471976280212, 0.035759881138801575, 0.035084210336208344, 0.032907795161008835, 0.05759386345744133, 0.06890703737735748, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7374449372291565, 0.02757931873202324, 0.029758905991911888, 0.028281286358833313, 0.036742087453603745, 0.056699249893426895, 0.0834941491484642, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6900503039360046, 0.025390060618519783, 0.02861245721578598, 0.025389475747942924, 0.03885332867503166, 0.03964974731206894, 0.07139166444540024, 0.08066301047801971, 0.0, 0.0, 0.0, 0.0], [0.6860940456390381, 0.016467422246932983, 0.017849231138825417, 0.01583140715956688, 0.02867799811065197, 0.028850499540567398, 0.046248577535152435, 0.07272989302873611, 0.08725086599588394, 0.0, 0.0, 0.0], [0.6597928404808044, 0.014094915241003036, 0.013287689536809921, 0.014738171361386776, 0.02721545472741127, 0.026123249903321266, 0.036225587129592896, 0.06444042921066284, 0.06819253414869308, 0.07588916271924973, 0.0, 0.0], [0.6366190910339355, 0.015182534232735634, 0.011112377047538757, 0.012331817299127579, 0.02593938633799553, 0.024615852162241936, 0.03139957785606384, 0.048720043152570724, 0.058031268417835236, 0.07494818419218063, 0.06109994277358055, 0.0], [0.6339138150215149, 0.010275943204760551, 0.009939133189618587, 0.009871595539152622, 0.019191591069102287, 0.019791187718510628, 0.028154002502560616, 0.049737464636564255, 0.0529216043651104, 0.04739025607705116, 0.05127650499343872, 0.06753694266080856]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9462036490440369, 0.05379636585712433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9859046339988708, 0.0010813204571604729, 0.013014083728194237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9427915811538696, 0.0010751297231763601, 0.017473088577389717, 0.03866022825241089, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9755438566207886, 0.00025894722784869373, 0.00010120125079993159, 0.00107152434065938, 0.023024415597319603, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9841082096099854, 0.005002154968678951, 0.0006323120323941112, 0.0002135200338670984, 3.7998248444637284e-05, 0.010005765594542027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9657293558120728, 0.00037993284058757126, 0.00010600060340948403, 0.00011835432087536901, 2.70258078671759e-05, 0.004543195944279432, 0.029096106067299843, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9533812403678894, 0.00031049156677909195, 0.0007170988828875124, 0.0010181042598560452, 1.0633904821588658e-05, 0.007014530245214701, 1.0494230991753284e-05, 0.03753732889890671, 0.0, 0.0, 0.0, 0.0], [0.8992465734481812, 0.02959398366510868, 0.000716496491804719, 0.0005821764934808016, 3.1428640795638785e-05, 0.010119626298546791, 0.00021349822054617107, 0.0005901998956687748, 0.05890617147088051, 0.0, 0.0, 0.0], [0.9564356207847595, 0.0017393501475453377, 0.004936703015118837, 0.0003567417443264276, 4.017451647087e-05, 0.0077008577063679695, 5.147924093762413e-05, 0.0004949343856424093, 0.002805377123877406, 0.025438854470849037, 0.0, 0.0], [0.9276311993598938, 0.0005981308058835566, 0.002999079879373312, 0.002071782946586609, 2.0481427782215178e-05, 0.00794089213013649, 4.463898221729323e-05, 0.0002562312292866409, 0.0007683400763198733, 0.00035894676693715155, 0.05731029808521271, 0.0], [0.8226107954978943, 0.0002930026385001838, 0.005195077508687973, 0.0036377517972141504, 8.86755806277506e-05, 0.017185620963573456, 0.0004103920073248446, 0.0003644432872533798, 0.00020085895084775984, 0.00509214261546731, 0.00046304764691740274, 0.14445827901363373]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.934181272983551, 0.06581870466470718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.946675717830658, 0.0348019003868103, 0.01852232962846756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8878981471061707, 0.046532485634088516, 0.026716630905866623, 0.03885267674922943, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7278714776039124, 0.03480902686715126, 0.07431179285049438, 0.12827998399734497, 0.034727707505226135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6610495448112488, 0.017683668062090874, 0.06894843280315399, 0.1414782702922821, 0.018861697986721992, 0.0919782891869545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6017756462097168, 0.019233180209994316, 0.10405001789331436, 0.1481013298034668, 0.0127408467233181, 0.0989888608455658, 0.015110144391655922, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6149452328681946, 0.009367543272674084, 0.06363137066364288, 0.12537536025047302, 0.03152664750814438, 0.035621579736471176, 0.005293888505548239, 0.11423838138580322, 0.0, 0.0, 0.0, 0.0], [0.5520167946815491, 0.009736021049320698, 0.031615182757377625, 0.02587008662521839, 0.00751398503780365, 0.07077289372682571, 0.016069753095507622, 0.23858679831027985, 0.047818493098020554, 0.0, 0.0, 0.0], [0.5053149461746216, 0.005238383077085018, 0.005833242554217577, 0.011323629878461361, 0.005753076169639826, 0.05265110731124878, 0.015692707151174545, 0.2686934471130371, 0.0671958401799202, 0.062303610146045685, 0.0, 0.0], [0.365776389837265, 0.00967499241232872, 0.013094278052449226, 0.0065970695577561855, 0.007869667373597622, 0.021574942395091057, 0.002572321565821767, 0.16977234184741974, 0.2834668755531311, 0.0911540687084198, 0.02844703011214733, 0.0], [0.43649736046791077, 0.002584250411018729, 0.008319142274558544, 0.009658015333116055, 0.006572813261300325, 0.01012723334133625, 0.00230936030857265, 0.1450168639421463, 0.25118187069892883, 0.08513636142015457, 0.01961982250213623, 0.02297699823975563]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8475502133369446, 0.15244972705841064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4268895387649536, 0.45276033878326416, 0.12035007774829865, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3262058198451996, 0.2552734911441803, 0.37729567289352417, 0.041224971413612366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10651334375143051, 0.03409997373819351, 0.4198276400566101, 0.22789905965328217, 0.21165995299816132, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04040772467851639, 0.0028312441427260637, 0.0426565557718277, 0.03170691803097725, 0.8076382875442505, 0.07475925236940384, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0806674212217331, 0.006583390757441521, 0.016676386818289757, 0.017218464985489845, 0.2559136748313904, 0.6083461046218872, 0.014594513922929764, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11650186777114868, 0.016504643484950066, 0.05102887377142906, 0.056034691631793976, 0.4465913772583008, 0.16767466068267822, 0.004269096069037914, 0.14139483869075775, 0.0, 0.0, 0.0, 0.0], [0.27949729561805725, 0.004680721089243889, 0.004073943477123976, 0.027938948944211006, 0.007497725542634726, 0.03518160805106163, 0.004174832254648209, 0.5182017683982849, 0.11875316500663757, 0.0, 0.0, 0.0], [0.1720791608095169, 0.0009431965299881995, 0.00011011768219759688, 0.002658707555383444, 0.0014664753107354045, 0.026084113866090775, 0.0023385584354400635, 0.4547872841358185, 0.29068970680236816, 0.048842739313840866, 0.0, 0.0], [0.2584436237812042, 0.0006138923345133662, 0.0002999557473231107, 0.0020178784616291523, 0.0032765865325927734, 0.008634144440293312, 0.000887721311300993, 0.08229543268680573, 0.1740889698266983, 0.43950697779655457, 0.0299347136169672, 0.0], [0.042027682065963745, 5.580105789704248e-05, 0.00014719620230607688, 0.0007677275571040809, 0.00029123813146725297, 0.0019376394338905811, 9.673462045611814e-05, 0.022064529359340668, 0.04079076275229454, 0.08050014823675156, 0.8002156615257263, 0.011104898527264595]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9778113961219788, 0.022188598290085793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8856669664382935, 0.10262762010097504, 0.011705362237989902, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5711549520492554, 0.020835628733038902, 0.34595465660095215, 0.06205468252301216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8675779104232788, 0.0029287526849657297, 0.023038797080516815, 0.008203993551433086, 0.09825059771537781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17063988745212555, 0.0038937849458307028, 0.010593392886221409, 0.00941922701895237, 0.7828241586685181, 0.02262955904006958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18013757467269897, 0.007006799802184105, 0.0061373659409582615, 0.009514699690043926, 0.7311381101608276, 0.062246814370155334, 0.0038186535239219666, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8846362233161926, 0.001095319981686771, 0.0009580539190210402, 0.004804512020200491, 0.010888881981372833, 0.01123096514493227, 0.001137223676778376, 0.08524884283542633, 0.0, 0.0, 0.0, 0.0], [0.43809089064598083, 0.00025616688071750104, 0.0025699555408209562, 0.0036727874539792538, 0.007057029753923416, 0.001999892061576247, 0.0016585997072979808, 0.5273739695549011, 0.017320577055215836, 0.0, 0.0, 0.0], [0.4491119384765625, 0.00023905841226223856, 0.0007206295849755406, 0.0007735940162092447, 0.0017132335342466831, 0.0015869137132540345, 0.000425097270635888, 0.22148583829402924, 0.21787409484386444, 0.10606969147920609, 0.0, 0.0], [0.227415531873703, 2.0085386495338753e-05, 0.0005525473388843238, 0.0070381760597229, 0.0017499318346381187, 0.0025515174493193626, 8.639314182801172e-05, 0.08004384487867355, 0.41885268688201904, 0.23752480745315552, 0.02416449971497059, 0.0], [0.8164998888969421, 0.00037504153442569077, 0.0005529123009182513, 0.00023904019326437265, 0.004184219986200333, 0.0007241279818117619, 0.0005008660373277962, 0.010204427875578403, 0.00695461593568325, 0.05242817848920822, 0.07450960576534271, 0.03282708674669266]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9973061084747314, 0.0026938924565911293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9833701848983765, 0.015229934826493263, 0.0013999724760651588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9694795608520508, 0.01317347027361393, 0.011636162176728249, 0.005710878409445286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9588772058486938, 0.023039821535348892, 0.011424114927649498, 0.006120182573795319, 0.0005386777338571846, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9631021022796631, 0.018249068409204483, 0.0030617681331932545, 0.0027161743491888046, 0.002595692640170455, 0.01027526892721653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9579458236694336, 0.018401505425572395, 0.002447160193696618, 0.006091397255659103, 0.00419163191691041, 0.010504133999347687, 0.0004182756529189646, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9676074385643005, 0.004889665637165308, 0.004013672471046448, 0.003380576381459832, 0.002149938140064478, 0.005730265751481056, 0.0037748857866972685, 0.00845354050397873, 0.0, 0.0, 0.0, 0.0], [0.9491918087005615, 0.01031600870192051, 0.007181162945926189, 0.0016818776493892074, 0.001220678212121129, 0.011364162899553776, 0.003179075662046671, 0.0069913859479129314, 0.00887387152761221, 0.0, 0.0, 0.0], [0.940422773361206, 0.010778812691569328, 0.004137901123613119, 0.0023584766313433647, 0.0018587415106594563, 0.010733678005635738, 0.003805146785452962, 0.006860995665192604, 0.010862688533961773, 0.008180786855518818, 0.0, 0.0], [0.956041693687439, 0.006083235610276461, 0.004101088736206293, 0.002086978405714035, 0.0027238531038165092, 0.007339073810726404, 0.0031898566521704197, 0.005603219848126173, 0.005675863474607468, 0.006961008068174124, 0.00019410581444390118, 0.0], [0.9106211066246033, 0.027075840160250664, 0.0018459341954439878, 0.003990750294178724, 0.013256181962788105, 0.015319242142140865, 0.009530570358037949, 0.0026450955774635077, 0.0034587529953569174, 0.004586934112012386, 0.0037229591980576515, 0.0039466251619160175]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9244049787521362, 0.07559501379728317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8413457274436951, 0.12676087021827698, 0.031893447041511536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7983846664428711, 0.11939822882413864, 0.06120268628001213, 0.021014433354139328, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.617295503616333, 0.26533445715904236, 0.09038365632295609, 0.006888146046549082, 0.020098160952329636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7058608531951904, 0.13515134155750275, 0.04769744724035263, 0.019744670018553734, 0.055128056555986404, 0.03641761094331741, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7561709880828857, 0.039225853979587555, 0.037663765251636505, 0.018122108653187752, 0.048967063426971436, 0.06614930182695389, 0.03370086848735809, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5732577443122864, 0.05628063902258873, 0.054805539548397064, 0.025832965970039368, 0.058667197823524475, 0.06369299441576004, 0.12524616718292236, 0.042216770350933075, 0.0, 0.0, 0.0, 0.0], [0.43667295575141907, 0.29494547843933105, 0.07805941998958588, 0.02462742105126381, 0.022452501580119133, 0.04949364438652992, 0.03455051779747009, 0.026887422427535057, 0.032310664653778076, 0.0, 0.0, 0.0], [0.46524375677108765, 0.1231870949268341, 0.13833998143672943, 0.05199529975652695, 0.03753659129142761, 0.04482344537973404, 0.05072709918022156, 0.02859198860824108, 0.02094249427318573, 0.03861227259039879, 0.0, 0.0], [0.6167864799499512, 0.08427728712558746, 0.06628365069627762, 0.02360384352505207, 0.016323436051607132, 0.03751397877931595, 0.02886350452899933, 0.03820088878273964, 0.05038609728217125, 0.03162790462374687, 0.0061329505406320095, 0.0], [0.32179737091064453, 0.1377774327993393, 0.06780994683504105, 0.0073193153366446495, 0.04428977146744728, 0.03387845680117607, 0.1433209776878357, 0.029624588787555695, 0.05317685008049011, 0.040202025324106216, 0.019032984972000122, 0.1017703041434288]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6848293542861938, 0.3151707053184509, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.405917227268219, 0.5538331866264343, 0.04024963825941086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5064757466316223, 0.2512700855731964, 0.1836850941181183, 0.058569081127643585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18066567182540894, 0.38471657037734985, 0.3004553020000458, 0.09840826690196991, 0.035754136741161346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.36469215154647827, 0.20566584169864655, 0.047985345125198364, 0.07275111973285675, 0.19145463407039642, 0.11745092272758484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30597013235092163, 0.08811716735363007, 0.0689636617898941, 0.08613263815641403, 0.07111217081546783, 0.2875581383705139, 0.09214616566896439, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15204812586307526, 0.3995952904224396, 0.033719711005687714, 0.0835268422961235, 0.03886743262410164, 0.12229737639427185, 0.10339587181806564, 0.06654931604862213, 0.0, 0.0, 0.0, 0.0], [0.3858775794506073, 0.054666146636009216, 0.016103658825159073, 0.021154530346393585, 0.008699505589902401, 0.012611358426511288, 0.042083490639925, 0.12304147332906723, 0.33576229214668274, 0.0, 0.0, 0.0], [0.2563883066177368, 0.006225172430276871, 0.0031406923662871122, 0.009269846603274345, 0.014981541782617569, 0.0029313445556908846, 0.0051747821271419525, 0.1047133207321167, 0.472259521484375, 0.12491544336080551, 0.0, 0.0], [0.22488461434841156, 0.006588257849216461, 0.0030539147555828094, 0.005225357599556446, 0.003515986492857337, 0.0018982253968715668, 0.003341678297147155, 0.05538947135210037, 0.41693413257598877, 0.1977931410074234, 0.08137518167495728, 0.0], [0.13688454031944275, 0.01187199167907238, 0.003071862505748868, 0.0035710057709366083, 0.002265369053930044, 0.0014930753968656063, 0.006160548888146877, 0.05137980356812477, 0.34265607595443726, 0.1919621378183365, 0.12970131635665894, 0.11898223310709]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9180123805999756, 0.08198761194944382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3949938416481018, 0.5839554667472839, 0.021050643175840378, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3877755403518677, 0.5568139553070068, 0.02831251733005047, 0.027097946032881737, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3355889320373535, 0.5343419313430786, 0.059505440294742584, 0.03628109395503998, 0.034282635897397995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3992776572704315, 0.1104472428560257, 0.019662750884890556, 0.016560113057494164, 0.33270061016082764, 0.12135162949562073, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1380329579114914, 0.06448762118816376, 0.18865221738815308, 0.13999728858470917, 0.12433087825775146, 0.25660088658332825, 0.08789817243814468, 0.0, 0.0, 0.0, 0.0, 0.0], [0.42494866251945496, 0.021410871297121048, 0.02127196453511715, 0.011130346916615963, 0.156648650765419, 0.07137682288885117, 0.22730380296707153, 0.06590884923934937, 0.0, 0.0, 0.0, 0.0], [0.4433141350746155, 0.005116062238812447, 0.0028766710311174393, 0.004301641136407852, 0.02347750775516033, 0.016305193305015564, 0.017478210851550102, 0.32150524854660034, 0.16562539339065552, 0.0, 0.0, 0.0], [0.40416419506073, 0.004320567008107901, 0.002450478496029973, 0.0015975813148543239, 0.024319665506482124, 0.011469296179711819, 0.011005894280970097, 0.10314484685659409, 0.3152018189430237, 0.12232562899589539, 0.0, 0.0], [0.40542247891426086, 0.0005554989329539239, 0.0021443134173750877, 0.00036240200279280543, 0.03350996598601341, 0.005181205924600363, 0.003017916576936841, 0.03355051204562187, 0.24882422387599945, 0.23244649171829224, 0.03498493507504463, 0.0], [0.5131354331970215, 0.005842556711286306, 0.0005023505073040724, 0.0001326373458141461, 0.017861418426036835, 0.005983739160001278, 0.010271260514855385, 0.016466530039906502, 0.1642664521932602, 0.08876743167638779, 0.03159203380346298, 0.14517810940742493]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9614267349243164, 0.03857320174574852, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6386594772338867, 0.08767986297607422, 0.2736606299877167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7111484408378601, 0.10255236178636551, 0.1398307830095291, 0.04646850377321243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.34518516063690186, 0.06946074962615967, 0.3812117874622345, 0.09928049147129059, 0.1048617959022522, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23724718391895294, 0.027826305478811264, 0.06943078339099884, 0.03797012194991112, 0.5739367604255676, 0.053588803857564926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16556432843208313, 0.00540589215233922, 0.05101452395319939, 0.039844341576099396, 0.6768275499343872, 0.051274027675390244, 0.010069329291582108, 0.0, 0.0, 0.0, 0.0, 0.0], [0.32803288102149963, 0.0047265635803341866, 0.04449059069156647, 0.05225992575287819, 0.2915233373641968, 0.029499247670173645, 0.05724119022488594, 0.19222630560398102, 0.0, 0.0, 0.0, 0.0], [0.13233308494091034, 0.0003766294976230711, 0.002018038649111986, 0.008727647364139557, 0.0022303592413663864, 0.00039485719753429294, 0.002645060420036316, 0.8244432806968689, 0.026830947026610374, 0.0, 0.0, 0.0], [0.24178439378738403, 0.0005728285759687424, 0.0016106656985357404, 0.00904602650552988, 0.008923226036131382, 0.0020288946107029915, 0.004533377010375261, 0.5979434847831726, 0.035811930894851685, 0.09774508327245712, 0.0, 0.0], [0.3394607901573181, 0.0003767766756936908, 0.0011075539514422417, 0.0007220124243758619, 0.003524676663801074, 0.0004385215579532087, 0.004461744800209999, 0.3389549255371094, 0.07574637979269028, 0.2162509560585022, 0.01895555853843689, 0.0], [0.410976380109787, 0.0007699335110373795, 0.002405997598543763, 0.001362967537716031, 0.029753968119621277, 0.0006104142521508038, 0.004897443577647209, 0.06702261418104172, 0.09554196894168854, 0.35167041420936584, 0.024177778512239456, 0.010810214094817638]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9539893865585327, 0.04601064324378967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9122704863548279, 0.06606083363294601, 0.021668674424290657, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5102095603942871, 0.04938278719782829, 0.40510088205337524, 0.03530684486031532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2631271779537201, 0.04231131821870804, 0.43990498781204224, 0.021146390587091446, 0.23351016640663147, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09409203380346298, 0.004995512776076794, 0.02625870332121849, 0.016330450773239136, 0.8483418822288513, 0.009981410577893257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1013886108994484, 0.001290985383093357, 0.024362735450267792, 0.025670234113931656, 0.8180003762245178, 0.019878540188074112, 0.009408443234860897, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2809317708015442, 0.009141347371041775, 0.36079326272010803, 0.17584992945194244, 0.08852569013834, 0.01629437319934368, 0.004121338948607445, 0.06434230506420135, 0.0, 0.0, 0.0, 0.0], [0.5295581221580505, 0.0033636451698839664, 0.02279689535498619, 0.0416388139128685, 0.02408556453883648, 0.00494986679404974, 0.0030154669657349586, 0.27245450019836426, 0.09813707321882248, 0.0, 0.0, 0.0], [0.5156345963478088, 0.007988205179572105, 0.0018925534095615149, 0.014108862727880478, 0.00636411365121603, 0.002628510119393468, 0.0013930859277024865, 0.2506510317325592, 0.053619176149368286, 0.14571987092494965, 0.0, 0.0], [0.25978711247444153, 0.0008039651438593864, 0.012721340171992779, 0.030898092314600945, 0.0009560766047798097, 0.001091594691388309, 0.002776163397356868, 0.08005262166261673, 0.13635307550430298, 0.4495926797389984, 0.024967320263385773, 0.0], [0.3658088445663452, 0.0019930177368223667, 0.011018095538020134, 0.02275533601641655, 0.008609471842646599, 0.005063106305897236, 0.003827338805422187, 0.022504078224301338, 0.0874822661280632, 0.3931390941143036, 0.034847453236579895, 0.04295191541314125]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9732004404067993, 0.02679956704378128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9314186573028564, 0.02178899012506008, 0.046792302280664444, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9197826385498047, 0.02465265803039074, 0.01591983623802662, 0.03964490070939064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.761898934841156, 0.01197794172912836, 0.014251968823373318, 0.025125592947006226, 0.1867455095052719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6390997171401978, 0.014319099485874176, 0.00945218838751316, 0.016696708276867867, 0.08699866384267807, 0.23343367874622345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7963140606880188, 0.01284806989133358, 0.009065455757081509, 0.010788903571665287, 0.021879462525248528, 0.026973184198141098, 0.12213091552257538, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7814188003540039, 0.004653508309274912, 0.010081375949084759, 0.042759984731674194, 0.033155471086502075, 0.02240855060517788, 0.016249017789959908, 0.08927334100008011, 0.0, 0.0, 0.0, 0.0], [0.706521213054657, 0.01534980721771717, 0.01677507720887661, 0.01400816347450018, 0.031778085976839066, 0.020790819078683853, 0.008240246213972569, 0.05242399126291275, 0.13411255180835724, 0.0, 0.0, 0.0], [0.6862983703613281, 0.009663430973887444, 0.018400929868221283, 0.01113735232502222, 0.031281713396310806, 0.02631983533501625, 0.0071691470220685005, 0.03737906739115715, 0.07027172297239304, 0.10207834839820862, 0.0, 0.0], [0.7075713276863098, 0.004273268859833479, 0.014776094816625118, 0.02503817342221737, 0.028900407254695892, 0.004385778214782476, 0.005319984629750252, 0.04460887983441353, 0.02679380215704441, 0.03303541988134384, 0.1052967980504036, 0.0], [0.6584796905517578, 0.0037740927655249834, 0.012275700457394123, 0.014592116698622704, 0.026735622435808182, 0.005563528276979923, 0.005506359972059727, 0.03028920479118824, 0.014621717855334282, 0.024132221937179565, 0.02886994369328022, 0.17515972256660461]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9183754920959473, 0.08162446320056915, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.613378643989563, 0.2889190912246704, 0.0977022796869278, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.616844654083252, 0.13575176894664764, 0.14538876712322235, 0.10201482474803925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2811371982097626, 0.05428781732916832, 0.4604642689228058, 0.14522957801818848, 0.058881163597106934, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24744997918605804, 0.015795528888702393, 0.052028827369213104, 0.1084214299917221, 0.5497640371322632, 0.02654021605849266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29004088044166565, 0.03145422041416168, 0.047215357422828674, 0.07205928862094879, 0.44222787022590637, 0.0394941009581089, 0.07750827819108963, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1786317080259323, 0.014454253017902374, 0.18418608605861664, 0.2883809804916382, 0.23233045637607574, 0.02106829360127449, 0.011311350390315056, 0.06963685154914856, 0.0, 0.0, 0.0, 0.0], [0.3969332277774811, 0.0118337357416749, 0.021994926035404205, 0.06408385932445526, 0.010975580662488937, 0.009779717773199081, 0.007898708805441856, 0.45046669244766235, 0.02603352628648281, 0.0, 0.0, 0.0], [0.33482107520103455, 0.02425556071102619, 0.020623283460736275, 0.020534556359052658, 0.008364737033843994, 0.009489926509559155, 0.01035021897405386, 0.48126745223999023, 0.028121022507548332, 0.06217218190431595, 0.0, 0.0], [0.3868201673030853, 0.0015774857020005584, 0.12976935505867004, 0.15973088145256042, 0.002939123660326004, 0.012929966673254967, 0.001030257437378168, 0.15698453783988953, 0.03071729838848114, 0.10136435925960541, 0.016136525198817253, 0.0], [0.4904960095882416, 0.0021130170207470655, 0.01666315086185932, 0.06591114401817322, 0.0136982137337327, 0.0051941336132586, 0.00356202176772058, 0.08004001528024673, 0.01657048426568508, 0.0762484073638916, 0.14045222103595734, 0.08905114233493805]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9916268587112427, 0.008373113349080086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9785059094429016, 0.011739584617316723, 0.009754432365298271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6891879439353943, 0.00971236638724804, 0.24130822718143463, 0.05979152023792267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6533017754554749, 0.003963386174291372, 0.035399891436100006, 0.04757675901055336, 0.25975820422172546, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.046492040157318115, 0.0021350018214434385, 0.011684387922286987, 0.005742967594414949, 0.9073606133460999, 0.026584923267364502, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1958036720752716, 0.0006484208279289305, 0.021577604115009308, 0.012443572282791138, 0.5194180607795715, 0.24188034236431122, 0.008228300139307976, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6254817247390747, 0.005490442737936974, 0.07993101328611374, 0.07406028360128403, 0.07481696456670761, 0.10413049906492233, 0.007200698833912611, 0.028888434171676636, 0.0, 0.0, 0.0, 0.0], [0.7911684513092041, 0.0008952774805948138, 0.016441810876131058, 0.0070267487317323685, 0.008342365734279156, 0.0026900889351963997, 0.0005639050505124032, 0.07687400281429291, 0.09599728882312775, 0.0, 0.0, 0.0], [0.8617139458656311, 0.0011017915094271302, 0.002620236948132515, 0.0016413311241194606, 0.002533764112740755, 0.0017492101760581136, 0.0006824972224421799, 0.030591562390327454, 0.036979787051677704, 0.06038583442568779, 0.0, 0.0], [0.38036587834358215, 0.0004438881005626172, 0.005241252947598696, 0.0016626837896183133, 0.0037095157895237207, 0.0019490160048007965, 3.2135510991793126e-05, 0.005728769116103649, 0.06048043444752693, 0.2509708106517792, 0.2894155979156494, 0.0], [0.18602709472179413, 0.00046482813195325434, 0.001229702029377222, 0.0019439341267570853, 0.0010734319221228361, 0.0011136098764836788, 0.00010452136484673247, 0.005022715777158737, 0.02124914340674877, 0.041930727660655975, 0.6888179183006287, 0.05102228373289108]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.971875011920929, 0.02812495082616806, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8864855766296387, 0.06220969185233116, 0.05130479484796524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.827964186668396, 0.027522273361682892, 0.10500240325927734, 0.03951115906238556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5721298456192017, 0.012099802494049072, 0.21772344410419464, 0.1826125532388687, 0.0154343843460083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4619048237800598, 0.024185946211218834, 0.13522516191005707, 0.24856916069984436, 0.056687574833631516, 0.07342737913131714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48561906814575195, 0.02038731426000595, 0.07830341160297394, 0.28824254870414734, 0.01786840334534645, 0.07493211328983307, 0.03464711084961891, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6177240014076233, 0.01831837370991707, 0.02144678123295307, 0.2346264272928238, 0.010465016588568687, 0.013433333486318588, 0.006588229443877935, 0.07739772647619247, 0.0, 0.0, 0.0, 0.0], [0.17064309120178223, 0.006338080391287804, 0.01714596152305603, 0.032212868332862854, 0.0028467108495533466, 0.0024092330131679773, 0.003001743694767356, 0.7317686676979065, 0.033633649349212646, 0.0, 0.0, 0.0], [0.07694181054830551, 0.005245609674602747, 0.0021100379526615143, 0.009623479098081589, 0.005406350828707218, 0.004624790512025356, 0.005937275476753712, 0.8445192575454712, 0.027675507590174675, 0.017915910109877586, 0.0, 0.0], [0.0698530524969101, 3.533143535605632e-05, 0.0012332746991887689, 0.01619541458785534, 0.0026448427233844995, 0.005400719121098518, 0.00024533135001547635, 0.8326626420021057, 0.023648645728826523, 0.04280540719628334, 0.00527527229860425, 0.0], [0.13982002437114716, 0.0008886256837286055, 0.004758882336318493, 0.03886324539780617, 0.003979677800089121, 0.005322924815118313, 0.0022382508032023907, 0.5608633756637573, 0.04811626300215721, 0.04522097110748291, 0.12907394766807556, 0.020853793248534203]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9745988249778748, 0.02540120854973793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9508765339851379, 0.032303523272275925, 0.01681988313794136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9217000603675842, 0.03772292286157608, 0.013875641860067844, 0.0267014242708683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9527413845062256, 0.026541007682681084, 0.009563452564179897, 0.010573301464319229, 0.0005808805581182241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.904239296913147, 0.02000918984413147, 0.016210485249757767, 0.029593635350465775, 0.009068161249160767, 0.020879333838820457, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9011470675468445, 0.020221704617142677, 0.018964337185025215, 0.018589358776807785, 0.009834060445427895, 0.017685649916529655, 0.01355784758925438, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8786922097206116, 0.016607340425252914, 0.011513836681842804, 0.0172122772783041, 0.006645049434155226, 0.014896126464009285, 0.031353238970041275, 0.02308001182973385, 0.0, 0.0, 0.0, 0.0], [0.795646071434021, 0.011176262982189655, 0.0206607598811388, 0.0238161850720644, 0.009468181058764458, 0.01179464627057314, 0.04857921227812767, 0.06296893954277039, 0.01588977500796318, 0.0, 0.0, 0.0], [0.7824582457542419, 0.012342039495706558, 0.020356297492980957, 0.020906617864966393, 0.007481393404304981, 0.012584630399942398, 0.05386853218078613, 0.04721526801586151, 0.01686694473028183, 0.0259200818836689, 0.0, 0.0], [0.7650517225265503, 0.0214928537607193, 0.02113945223391056, 0.035109832882881165, 0.008372221142053604, 0.012828447856009007, 0.047981925308704376, 0.05252368375658989, 0.009112877771258354, 0.009026141837239265, 0.01736074686050415, 0.0], [0.701083242893219, 0.034963514655828476, 0.02630223147571087, 0.020700929686427116, 0.010743987746536732, 0.01811748556792736, 0.057569947093725204, 0.0501299649477005, 0.025239212438464165, 0.02598019316792488, 0.01451875176280737, 0.014650581404566765]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8976650238037109, 0.10233494639396667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10251972079277039, 0.8853189945220947, 0.01216126512736082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1602431982755661, 0.7964072823524475, 0.036199018359184265, 0.007150525227189064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08985596895217896, 0.8738166093826294, 0.023656487464904785, 0.003785331267863512, 0.008885587565600872, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6170852184295654, 0.2821478843688965, 0.024813106283545494, 0.005818724166601896, 0.054924171417951584, 0.0152110131457448, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5874587893486023, 0.2845360338687897, 0.028187856078147888, 0.01620645634829998, 0.043142169713974, 0.0321548767387867, 0.008313828147947788, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12517106533050537, 0.20123349130153656, 0.07289708405733109, 0.017301220446825027, 0.4789695143699646, 0.055220186710357666, 0.01895202323794365, 0.030255381017923355, 0.0, 0.0, 0.0, 0.0], [0.08492691069841385, 0.8460666537284851, 0.018965408205986023, 0.003410051576793194, 0.01023133099079132, 0.005475788842886686, 0.0036752752494066954, 0.010594513267278671, 0.016654010862112045, 0.0, 0.0, 0.0], [0.15052379667758942, 0.5567432045936584, 0.007086496334522963, 0.001208747737109661, 0.014648091979324818, 0.004902316723018885, 0.011388173326849937, 0.0379825197160244, 0.16715434193611145, 0.04836226627230644, 0.0, 0.0], [0.28910037875175476, 0.16966500878334045, 0.007687435019761324, 0.002422981895506382, 0.020892659202218056, 0.010301738977432251, 0.01143357902765274, 0.07083204388618469, 0.2093251347541809, 0.15542054176330566, 0.05291854962706566, 0.0], [0.05920978635549545, 0.34150439500808716, 0.003558492986485362, 0.0006763122510164976, 0.004894494079053402, 0.001986775314435363, 0.006104869768023491, 0.006767371203750372, 0.47138598561286926, 0.059042539447546005, 0.011387810111045837, 0.03348105028271675]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9582101702690125, 0.041789811104536057, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.926227867603302, 0.06026637181639671, 0.013505754061043262, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.883263885974884, 0.010961456224322319, 0.05239122360944748, 0.05338342487812042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9666603803634644, 0.006177782081067562, 0.015207424759864807, 0.00967143103480339, 0.0022830236703157425, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9481725692749023, 0.00530355516821146, 0.004518948495388031, 0.006387536413967609, 0.012660246342420578, 0.02295706421136856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.864551842212677, 0.007598440628498793, 0.0017070939065888524, 0.0012534863781183958, 0.02837173268198967, 0.05752791836857796, 0.0389895923435688, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7952762246131897, 0.019959373399615288, 0.030558692291378975, 0.08939388394355774, 0.0074524907395243645, 0.01457296684384346, 0.0036466901656240225, 0.03913963958621025, 0.0, 0.0, 0.0, 0.0], [0.7641720771789551, 0.05240252614021301, 0.015671435743570328, 0.035485975444316864, 0.0014632222009822726, 0.01181564200669527, 0.0021043207962065935, 0.051919125020504, 0.06496578454971313, 0.0, 0.0, 0.0], [0.703173816204071, 0.027284841984510422, 0.012088809162378311, 0.04124690219759941, 0.0009836162207648158, 0.010432172566652298, 0.0020317097660154104, 0.043465469032526016, 0.09161631017923355, 0.06767626851797104, 0.0, 0.0], [0.745573103427887, 0.006055185571312904, 0.01108713448047638, 0.06693821400403976, 0.001724982401356101, 0.006130802445113659, 0.001709687989205122, 0.02814086712896824, 0.05905423313379288, 0.05313568189740181, 0.020450187847018242, 0.0], [0.4453561305999756, 0.008541356772184372, 0.006410693749785423, 0.017810961231589317, 0.0005199032020755112, 0.012691737152636051, 0.0015004739398136735, 0.017321180552244186, 0.028959030285477638, 0.05490780249238014, 0.3651728332042694, 0.0408078096807003]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9283452033996582, 0.07165483385324478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8323249816894531, 0.16098429262638092, 0.006690754555165768, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9476935267448425, 0.031869273632764816, 0.01847018487751484, 0.001967017538845539, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.821929931640625, 0.021780718117952347, 0.12256638705730438, 0.005033751949667931, 0.02868923917412758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.58195960521698, 0.005770227871835232, 0.021350521594285965, 0.004937298130244017, 0.2295757234096527, 0.1564067155122757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7233567833900452, 0.0016117390478029847, 0.015435848385095596, 0.013797404244542122, 0.02152739092707634, 0.10814462602138519, 0.11612622439861298, 0.0, 0.0, 0.0, 0.0, 0.0], [0.764048159122467, 0.007158809807151556, 0.029238520190119743, 0.050757743418216705, 0.013694913126528263, 0.0415508896112442, 0.019952986389398575, 0.0735979750752449, 0.0, 0.0, 0.0, 0.0], [0.8124006390571594, 0.002580837346613407, 0.0031250091269612312, 0.062498290091753006, 0.0024136893916875124, 0.011804914101958275, 0.004477393813431263, 0.06038243696093559, 0.04031679034233093, 0.0, 0.0, 0.0], [0.7954340577125549, 0.0012598322937265038, 0.0005548743647523224, 0.0323425754904747, 0.00147374183870852, 0.006659671198576689, 0.0039050811901688576, 0.056791361421346664, 0.07858122140169144, 0.02299756370484829, 0.0, 0.0], [0.8558189272880554, 0.00031048114760778844, 0.0009347301675006747, 0.0034630547743290663, 0.001486087217926979, 0.010470033623278141, 0.0012638373300433159, 0.007183428388088942, 0.053802285343408585, 0.031964972615242004, 0.033302150666713715, 0.0], [0.5606911778450012, 0.0026181505527347326, 0.008398705162107944, 0.02321617305278778, 0.001283177174627781, 0.002718803007155657, 0.003160267835482955, 0.03778328374028206, 0.07456651329994202, 0.03246864676475525, 0.1777493953704834, 0.07534568011760712]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9644703269004822, 0.03552963212132454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7528695464134216, 0.19286473095417023, 0.05426572635769844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7788475155830383, 0.1248021349310875, 0.05723518505692482, 0.03911523520946503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7946266531944275, 0.08212750405073166, 0.05126819387078285, 0.04783620685338974, 0.02414141222834587, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7422783374786377, 0.0853230208158493, 0.03206552192568779, 0.03932655602693558, 0.05803874135017395, 0.042967744171619415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6552797555923462, 0.11672412604093552, 0.0337221622467041, 0.031632836908102036, 0.0449555478990078, 0.08892031013965607, 0.028765281662344933, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3608851432800293, 0.0852198600769043, 0.04393673688173294, 0.08379776775836945, 0.17726555466651917, 0.10275989770889282, 0.10443105548620224, 0.04170392453670502, 0.0, 0.0, 0.0, 0.0], [0.38770633935928345, 0.15567108988761902, 0.08175283670425415, 0.08333868533372879, 0.035712096840143204, 0.04799732193350792, 0.03977351635694504, 0.1197882741689682, 0.0482599101960659, 0.0, 0.0, 0.0], [0.5098131895065308, 0.06338951736688614, 0.04915250465273857, 0.05149998888373375, 0.06213189288973808, 0.03186890110373497, 0.04803083464503288, 0.08709520101547241, 0.048196062445640564, 0.04882190749049187, 0.0, 0.0], [0.6212362051010132, 0.0223552193492651, 0.011310946196317673, 0.019597159698605537, 0.028136160224676132, 0.051669370383024216, 0.038193825632333755, 0.12336134165525436, 0.016494471579790115, 0.041280049830675125, 0.026365291327238083, 0.0], [0.5464949011802673, 0.018335502594709396, 0.010514362715184689, 0.014340645633637905, 0.07465578615665436, 0.026769915595650673, 0.03644287586212158, 0.05683872848749161, 0.035179898142814636, 0.04432374984025955, 0.10227642208337784, 0.03382721170783043]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8612442016601562, 0.13875578343868256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6703091263771057, 0.04511100798845291, 0.2845798432826996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3664025664329529, 0.02456653118133545, 0.051127467304468155, 0.5579034090042114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2949296832084656, 0.02059980109333992, 0.020737729966640472, 0.10881699621677399, 0.5549157857894897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3918759524822235, 0.031905025243759155, 0.013310380280017853, 0.006481973920017481, 0.08560909330844879, 0.47081759572029114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4153168797492981, 0.08099352568387985, 0.007670465856790543, 0.002534758998081088, 0.033681195229291916, 0.07703996449708939, 0.38276317715644836, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21986141800880432, 0.01428761426359415, 0.04459969326853752, 0.04046836867928505, 0.10855647921562195, 0.046856798231601715, 0.10545952618122101, 0.41991010308265686, 0.0, 0.0, 0.0, 0.0], [0.3468669056892395, 0.028023356571793556, 0.009275256656110287, 0.012249143794178963, 0.010792057029902935, 0.0029957559891045094, 0.0059724124148488045, 0.04908248782157898, 0.5347425937652588, 0.0, 0.0, 0.0], [0.3271314799785614, 0.011046454310417175, 0.011984176002442837, 0.02737758681178093, 0.010412841103971004, 0.0007079904316924512, 0.004021297674626112, 0.021373629570007324, 0.10685545951128006, 0.4790891706943512, 0.0, 0.0], [0.23487304151058197, 0.0044976407662034035, 0.00519044604152441, 0.025383394211530685, 0.010386220179498196, 0.002050821203738451, 0.0007845531799830496, 0.03214573487639427, 0.04117874801158905, 0.0739375501871109, 0.569571852684021, 0.0], [0.11304738372564316, 0.005671276710927486, 0.0009043028112500906, 0.0035631784703582525, 0.01839987188577652, 0.0024073966778814793, 0.009354323148727417, 0.021394705399870872, 0.017879577353596687, 0.017234308645129204, 0.1076001450419426, 0.6825434565544128]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9710803031921387, 0.028919678181409836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9340623021125793, 0.05638039484620094, 0.009557305835187435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9110105633735657, 0.04438447952270508, 0.013310583308339119, 0.031294405460357666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9443431496620178, 0.024629442021250725, 0.015872051939368248, 0.008853541687130928, 0.00630168104544282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9420199990272522, 0.021444272249937057, 0.006786629557609558, 0.00850259605795145, 0.008538993075489998, 0.012707522138953209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9033228158950806, 0.03156628459692001, 0.007138480897992849, 0.004885256756097078, 0.006696705706417561, 0.022197920829057693, 0.02419249899685383, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7946080565452576, 0.017074719071388245, 0.0096971420571208, 0.020763548091053963, 0.029830031096935272, 0.009004750289022923, 0.05513092130422592, 0.06389088183641434, 0.0, 0.0, 0.0, 0.0], [0.862190842628479, 0.01104274857789278, 0.006944428198039532, 0.02120966836810112, 0.012128102593123913, 0.009670631028711796, 0.021159781143069267, 0.039032068103551865, 0.016621900722384453, 0.0, 0.0, 0.0], [0.7834652066230774, 0.016882305964827538, 0.01036232989281416, 0.02627187967300415, 0.023592110723257065, 0.014777335338294506, 0.023655718192458153, 0.05452841520309448, 0.03845519572496414, 0.008009525947272778, 0.0, 0.0], [0.8591169118881226, 0.006282164249569178, 0.002322858665138483, 0.013590688817203045, 0.0234506968408823, 0.007349417544901371, 0.00890524685382843, 0.04949605464935303, 0.013291512615978718, 0.004631000105291605, 0.011563464999198914, 0.0], [0.8281149864196777, 0.022992007434368134, 0.009963789023458958, 0.005824141204357147, 0.022124001756310463, 0.014200293458998203, 0.0097507881000638, 0.026654958724975586, 0.009544597007334232, 0.0024143594782799482, 0.01332697831094265, 0.03508906438946724]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9692928194999695, 0.03070719540119171, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7655465006828308, 0.21792937815189362, 0.01652408391237259, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7284939289093018, 0.15181118249893188, 0.09248289465904236, 0.027212029322981834, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5448606610298157, 0.06959836184978485, 0.25840651988983154, 0.09127174317836761, 0.03586278483271599, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5348876118659973, 0.04674367606639862, 0.14455938339233398, 0.06993264704942703, 0.16392619907855988, 0.039950452744960785, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5550464987754822, 0.030172022059559822, 0.07502147555351257, 0.03189529478549957, 0.18326729536056519, 0.07982360571622849, 0.04477381333708763, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1594063937664032, 0.002581964945420623, 0.0777343213558197, 0.1711222529411316, 0.5128588676452637, 0.029179584234952927, 0.028125349432229996, 0.01899137534201145, 0.0, 0.0, 0.0, 0.0], [0.15407180786132812, 0.006567588075995445, 0.12404973804950714, 0.2672496438026428, 0.2486547976732254, 0.06178027763962746, 0.02384558692574501, 0.0858580619096756, 0.027922404929995537, 0.0, 0.0, 0.0], [0.22970886528491974, 0.022743329405784607, 0.031752776354551315, 0.37148919701576233, 0.088223397731781, 0.10058646649122238, 0.034146908670663834, 0.05395995453000069, 0.04676157236099243, 0.02062745951116085, 0.0, 0.0], [0.687197208404541, 0.014873194508254528, 0.008707037195563316, 0.0498763769865036, 0.010442696511745453, 0.015090162865817547, 0.004521558526903391, 0.04341590031981468, 0.07835012674331665, 0.04815789684653282, 0.03936781361699104, 0.0], [0.3763353228569031, 0.024939268827438354, 0.01832771487534046, 0.023585975170135498, 0.024585941806435585, 0.021567057818174362, 0.03019784390926361, 0.02173973061144352, 0.10250198841094971, 0.14403584599494934, 0.04545476660132408, 0.16672848165035248]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9793688058853149, 0.020631223917007446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9741688370704651, 0.005095220636576414, 0.02073593996465206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8945725560188293, 0.005275894422084093, 0.048020802438259125, 0.05213073641061783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9665771722793579, 0.002403691178187728, 0.008507889695465565, 0.0059300074353814125, 0.016581334173679352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.964103102684021, 0.004402808379381895, 0.0054499804973602295, 0.015799371525645256, 0.005501299165189266, 0.004743526689708233, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9598162770271301, 0.00462693814188242, 0.0009102479089051485, 0.004270290024578571, 0.013005238026380539, 0.006950489245355129, 0.010420604608952999, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9386754035949707, 0.0016522591467946768, 0.004007960204035044, 0.004536445718258619, 0.004350189119577408, 0.005812486633658409, 0.002812794176861644, 0.03815241530537605, 0.0, 0.0, 0.0, 0.0], [0.8777362704277039, 0.005650919396430254, 0.012193187139928341, 0.01830296590924263, 0.010574175044894218, 0.005623973440378904, 0.005084940232336521, 0.04917371645569801, 0.0156598798930645, 0.0, 0.0, 0.0], [0.8049421310424805, 0.00919889286160469, 0.01470598578453064, 0.025210320949554443, 0.030963551253080368, 0.0031984583474695683, 0.009254093281924725, 0.0614987388253212, 0.01234847865998745, 0.02867935039103031, 0.0, 0.0], [0.9097334742546082, 0.005716605111956596, 0.003986018244177103, 0.009318651631474495, 0.01592990756034851, 0.0036455837544053793, 0.0011503556743264198, 0.018542585894465446, 0.014746580272912979, 0.014323325827717781, 0.002906908979639411, 0.0], [0.8272758722305298, 0.005336360074579716, 0.005950741935521364, 0.00745109748095274, 0.031091151759028435, 0.008392727933824062, 0.01078002154827118, 0.04121207818388939, 0.014535414054989815, 0.028994562104344368, 0.007731772493571043, 0.011248236522078514]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9999358654022217, 6.408223998732865e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.1271306902926881e-05, 0.9999878406524658, 9.346393881060067e-07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [9.551270068186568e-07, 2.208091245847754e-06, 0.9999847412109375, 1.2078559848305304e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.318157837384206e-07, 6.395846074980227e-10, 0.0003079329908359796, 0.9996873140335083, 4.492219431995181e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.529226756247226e-05, 4.60093380061366e-10, 1.2847423125350588e-09, 1.3532701359508792e-07, 0.9961243271827698, 0.0038602736312896013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.375988724685364e-11, 7.600405547980582e-12, 2.3433084429473454e-16, 2.251364869407463e-15, 3.674826762334327e-11, 0.9999483823776245, 5.161577428225428e-05, 0.0, 0.0, 0.0, 0.0, 0.0], [1.914228997534906e-09, 7.021456749356503e-09, 1.4958284610955275e-11, 3.466919529238055e-10, 2.8170732093713013e-06, 7.847414053685497e-06, 0.9999892711639404, 1.9273308282663493e-08, 0.0, 0.0, 0.0, 0.0], [1.2000128890576889e-06, 3.479789373317388e-11, 3.5236431827900816e-11, 1.6091425414366256e-10, 4.3250875103595376e-11, 1.11177962669462e-10, 2.7608759722852483e-10, 0.9981616139411926, 0.0018371567130088806, 0.0, 0.0, 0.0], [1.2863683451769248e-08, 7.400758825215625e-12, 5.056328619690298e-13, 4.550329210650794e-11, 1.3393057279528775e-13, 8.047782817759142e-13, 6.0331158471815804e-12, 2.6751376935862936e-06, 0.9993687272071838, 0.0006285900017246604, 0.0, 0.0], [1.9763655600968377e-08, 1.2044798345560784e-14, 1.3666481141205722e-12, 7.651696165567046e-09, 8.353997181520523e-12, 3.1857776507972224e-14, 6.901226009929651e-14, 1.3980037572025594e-08, 3.1860656690696487e-07, 0.9999854564666748, 1.416238592355512e-05, 0.0], [3.728338818231869e-11, 1.5084694349412771e-15, 2.684730006870576e-15, 2.3487342713096604e-12, 1.3666697981640219e-11, 5.8086508693266925e-12, 1.508103537883887e-12, 1.3233705242754468e-09, 4.80286459492163e-08, 2.906275312852813e-06, 0.9999970197677612, 2.060134463022223e-08]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9862706661224365, 0.013729257509112358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9716172218322754, 0.002571107354015112, 0.025811707600951195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9379040002822876, 0.0019667146261781454, 0.0064268819987773895, 0.053702425211668015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9091672301292419, 0.0011215960839763284, 0.0020315544679760933, 0.014639242552220821, 0.07304032891988754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9425380229949951, 0.011753203347325325, 0.0041975355707108974, 0.006524919997900724, 0.007995775900781155, 0.026990512385964394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9514340758323669, 0.0010305801406502724, 0.006489698309451342, 0.004122532904148102, 0.013656609691679478, 0.015143630094826221, 0.008122840896248817, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9092976450920105, 0.001459592836908996, 0.0006829064805060625, 0.004830721765756607, 0.010654008015990257, 0.007802309934049845, 0.004879974760115147, 0.06039278954267502, 0.0, 0.0, 0.0, 0.0], [0.7039305567741394, 0.0016685185255482793, 0.007617327384650707, 0.008800373412668705, 0.09752170741558075, 0.03727404773235321, 0.018228542059659958, 0.10184107720851898, 0.02311783656477928, 0.0, 0.0, 0.0], [0.7594425678253174, 0.0007111155427992344, 0.0007354205008596182, 0.007879943586885929, 0.10203630477190018, 0.04283065348863602, 0.012358244508504868, 0.04115588963031769, 0.026891933754086494, 0.005957994144409895, 0.0, 0.0], [0.8984640836715698, 0.00012345483992248774, 8.73697645147331e-05, 0.00015437757247127593, 0.014848469756543636, 0.024966606870293617, 0.002587733091786504, 0.039919737726449966, 0.011541402898728848, 0.0032504915725439787, 0.004056274890899658, 0.0], [0.8991856575012207, 0.00021268715499900281, 0.0004574282211251557, 0.0007927641272544861, 0.0061491415835917, 0.02431800775229931, 0.006257375702261925, 0.03885315731167793, 0.007860830053687096, 0.0003921025199815631, 0.0027693696320056915, 0.012751461938023567]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9998134970664978, 0.00018652591097634286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9986003041267395, 4.416958472575061e-05, 0.0013555347686633468, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9986522793769836, 3.4657371088542277e-06, 1.3833662251272472e-06, 0.0013428699458017945, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9999693632125854, 3.381245733180549e-06, 7.225447973269183e-08, 7.359435585385654e-07, 2.6351270207669586e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9982061386108398, 8.270551188616082e-05, 5.881458653433924e-10, 8.389110739415173e-09, 9.454164029421008e-08, 0.0017110458575189114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9982237219810486, 3.3525848266435787e-06, 1.9430640207929173e-08, 1.4264405656660983e-08, 1.4104197987307998e-07, 0.00045336168841458857, 0.0013194628991186619, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9989645481109619, 9.898509597405791e-05, 1.1061962368330569e-07, 2.256051629956346e-06, 8.014218110474758e-06, 3.1764451705385e-05, 4.942969462717883e-05, 0.0008449301822111011, 0.0, 0.0, 0.0, 0.0], [0.9816675186157227, 0.00014934531645849347, 0.001053570187650621, 0.0029224297031760216, 0.002818659646436572, 4.640118277166039e-06, 1.2643449736060575e-05, 0.0002916442172136158, 0.011079631745815277, 0.0, 0.0, 0.0], [0.9872748851776123, 8.22190850158222e-05, 4.657024874177296e-06, 0.0006622541113756597, 0.00027068733470514417, 2.5436204964535136e-07, 1.0263497642881703e-06, 1.2193465408927295e-05, 0.0047157034277915955, 0.0069761574268341064, 0.0, 0.0], [0.9957754015922546, 3.744395507965237e-05, 5.1080704110972874e-08, 3.315742105769459e-06, 8.784259262029082e-05, 3.229534968340886e-06, 3.0643529953522375e-06, 2.2939926566323265e-05, 0.00047470591380260885, 0.00046423260937444866, 0.003127790056169033, 0.0], [0.9995877146720886, 1.9910971786885057e-06, 4.989660240539706e-08, 8.218784159907955e-07, 1.4829139445282635e-06, 2.530468009354081e-06, 6.854357934571453e-07, 2.3019671061774716e-05, 0.00010466221283422783, 3.0582941690227017e-06, 1.3229545174908708e-06, 0.00027282326482236385]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9394112229347229, 0.06058874353766441, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6134536862373352, 0.37280508875846863, 0.01374124363064766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.44353291392326355, 0.35981082916259766, 0.16465671360492706, 0.03199954703450203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7190516591072083, 0.06530801206827164, 0.12899617850780487, 0.0817997008562088, 0.004844412207603455, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7455986142158508, 0.03295586630702019, 0.03824028745293617, 0.013663670048117638, 0.14712034165859222, 0.02242117002606392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3663499057292938, 0.016634322702884674, 0.023124167695641518, 0.016737576574087143, 0.38864102959632874, 0.17457829415798187, 0.013934732414782047, 0.0, 0.0, 0.0, 0.0, 0.0], [0.31168121099472046, 0.03692607954144478, 0.12351755052804947, 0.09801720082759857, 0.3010963499546051, 0.08299373090267181, 0.021455299109220505, 0.024312613531947136, 0.0, 0.0, 0.0, 0.0], [0.5098214745521545, 0.016763800755143166, 0.03959371894598007, 0.07238148152828217, 0.0819115936756134, 0.09210085868835449, 0.017992377281188965, 0.13905495405197144, 0.030379734933376312, 0.0, 0.0, 0.0], [0.7082634568214417, 0.015725379809737206, 0.02583792246878147, 0.014098969288170338, 0.013666180893778801, 0.0439014807343483, 0.012719402089715004, 0.10304223746061325, 0.052939869463443756, 0.009805133566260338, 0.0, 0.0], [0.7208718061447144, 0.016825027763843536, 0.015787973999977112, 0.016605349257588387, 0.027327079325914383, 0.03527470678091049, 0.006776925176382065, 0.05936533585190773, 0.020780088379979134, 0.06847351044416428, 0.011912280693650246, 0.0], [0.6818859577178955, 0.008503641001880169, 0.04002416133880615, 0.02479736879467964, 0.017846496775746346, 0.0236582662910223, 0.005145726725459099, 0.05273667722940445, 0.03371243551373482, 0.04726302623748779, 0.052433889359235764, 0.01199236512184143]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9577831029891968, 0.04221692308783531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7955262064933777, 0.0944489985704422, 0.11002487689256668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5457058548927307, 0.1443907469511032, 0.265207976102829, 0.04469538480043411, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6612840890884399, 0.11533686518669128, 0.13719862699508667, 0.045346327126026154, 0.04083414748311043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5539434552192688, 0.13372567296028137, 0.1485862284898758, 0.04607323184609413, 0.10180695354938507, 0.015864543616771698, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4236028492450714, 0.08231806010007858, 0.1827048361301422, 0.09287583827972412, 0.15754671394824982, 0.039594247937202454, 0.021357478573918343, 0.0, 0.0, 0.0, 0.0, 0.0], [0.40825173258781433, 0.11351846903562546, 0.1686389446258545, 0.12367131561040878, 0.06848064064979553, 0.012393144890666008, 0.019350657239556313, 0.08569511771202087, 0.0, 0.0, 0.0, 0.0], [0.3281005918979645, 0.1108129471540451, 0.12185347825288773, 0.09897443652153015, 0.13213147222995758, 0.01940731145441532, 0.03158731386065483, 0.11574200540781021, 0.04139043018221855, 0.0, 0.0, 0.0], [0.3946619927883148, 0.10747633129358292, 0.07508139312267303, 0.037332095205783844, 0.12216528505086899, 0.019402308389544487, 0.029395347461104393, 0.13017287850379944, 0.06046897545456886, 0.02384336292743683, 0.0, 0.0], [0.2752320170402527, 0.049433302134275436, 0.07261069118976593, 0.044428884983062744, 0.11096470057964325, 0.01685745269060135, 0.009160774759948254, 0.16767147183418274, 0.14633408188819885, 0.05908677354454994, 0.048219889402389526, 0.0], [0.2692785859107971, 0.05322444438934326, 0.07644111663103104, 0.04614944010972977, 0.0853087455034256, 0.01849322021007538, 0.015561657957732677, 0.1956406533718109, 0.09888608753681183, 0.07391384243965149, 0.045148998498916626, 0.02195318415760994]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9734203815460205, 0.026579570025205612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9439903497695923, 0.034510403871536255, 0.021499179303646088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8633659482002258, 0.0704738050699234, 0.046857770532369614, 0.019302479922771454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7652139067649841, 0.012929015792906284, 0.03436727821826935, 0.04042338952422142, 0.1470663994550705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4670056700706482, 0.02600019797682762, 0.028311707079410553, 0.03391753137111664, 0.3768828809261322, 0.06788204610347748, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26549381017684937, 0.011877094395458698, 0.040608275681734085, 0.04199977591633797, 0.24433833360671997, 0.16405951976776123, 0.23162324726581573, 0.0, 0.0, 0.0, 0.0, 0.0], [0.27196449041366577, 0.019214527681469917, 0.07152102887630463, 0.12426760792732239, 0.19914549589157104, 0.1362442672252655, 0.1007230207324028, 0.07691953331232071, 0.0, 0.0, 0.0, 0.0], [0.36368700861930847, 0.008915852755308151, 0.013929765671491623, 0.00884312018752098, 0.33147764205932617, 0.0383300743997097, 0.011891312897205353, 0.20779713988304138, 0.01512807048857212, 0.0, 0.0, 0.0], [0.5279200673103333, 0.004776397254317999, 0.0027656282763928175, 0.0017110719345510006, 0.12267899513244629, 0.051862459629774094, 0.02956177480518818, 0.21372948586940765, 0.017448268830776215, 0.027545789256691933, 0.0, 0.0], [0.7016544342041016, 0.0049439589492976665, 0.003290503052994609, 0.0010138314682990313, 0.012862450443208218, 0.006715856958180666, 0.004264661110937595, 0.04951176047325134, 0.030953777953982353, 0.14248819649219513, 0.04230052977800369, 0.0], [0.31555190682411194, 0.0015690821455791593, 0.0019149610307067633, 0.0019230133621022105, 0.005184014327824116, 0.0047891149297356606, 0.008443376049399376, 0.01676780730485916, 0.013905501924455166, 0.042501192539930344, 0.5558950304985046, 0.031555019319057465]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9904971122741699, 0.009502969682216644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9366008043289185, 0.01968168467283249, 0.04371748864650726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9447512626647949, 0.011371291242539883, 0.0030632643029093742, 0.04081418365240097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9909945130348206, 0.008435769006609917, 3.700745219248347e-05, 0.00011384301615180448, 0.0004189149185549468, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9736932516098022, 0.022034427151083946, 5.277626041788608e-05, 9.809191396925598e-05, 0.00018176260346081108, 0.003939613699913025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9733288288116455, 0.0046349880285561085, 2.343687265238259e-05, 1.8517597709433176e-05, 1.0003212992160115e-05, 0.005523857194930315, 0.016460450366139412, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9065694808959961, 0.04573411867022514, 0.0007101416704244912, 0.0021210622508078814, 0.002939424477517605, 0.00061264936812222, 0.0039673359133303165, 0.03734581172466278, 0.0, 0.0, 0.0, 0.0], [0.4249071478843689, 0.01369367353618145, 0.049077585339546204, 0.05810462683439255, 0.42243674397468567, 0.0019153570756316185, 0.0033234180882573128, 0.019566532224416733, 0.006974922027438879, 0.0, 0.0, 0.0], [0.6180278658866882, 0.017358800396323204, 0.01378612034022808, 0.052532218396663666, 0.21863867342472076, 0.00043153009028173983, 0.0008946987800300121, 0.041161149740219116, 0.017416829243302345, 0.019752008840441704, 0.0, 0.0], [0.977985143661499, 0.008119921199977398, 1.3889560250390787e-05, 0.00012653386511374265, 0.0025188575964421034, 0.00023028053692542017, 0.0013656291412189603, 0.002523652045056224, 0.003271471941843629, 0.0021759141236543655, 0.0016686702147126198, 0.0], [0.9919856786727905, 0.0022183756809681654, 1.2143951607868075e-05, 0.00015453282685484737, 0.000165411111083813, 9.434905223315582e-05, 0.00037259230157360435, 0.0014993420336395502, 0.0031656306236982346, 4.481180076254532e-05, 2.1825824660481885e-05, 0.0002652851107995957]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9874216318130493, 0.012578319758176804, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9477344751358032, 0.04487646743655205, 0.007389013189822435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5323895812034607, 0.3086182475090027, 0.14735658466815948, 0.011635673232376575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8373852372169495, 0.04482816904783249, 0.05654862895607948, 0.050723955035209656, 0.010513956658542156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4876309633255005, 0.002764239441603422, 0.00018285488476976752, 0.0006610497366636992, 0.44187119603157043, 0.0668896734714508, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8244956135749817, 0.0023549539037048817, 0.0006299178930930793, 0.0007265323656611145, 0.06548205763101578, 0.06157571077346802, 0.04473522678017616, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9826860427856445, 0.000306863512378186, 0.0001772607211023569, 0.00021838382235728204, 0.0019031387055292726, 0.001173545839264989, 0.0055226716212928295, 0.008012108504772186, 0.0, 0.0, 0.0, 0.0], [0.909058690071106, 0.001997467363253236, 0.0057426802814006805, 0.0015973933041095734, 0.0026650044601410627, 0.00019282450375612825, 0.0005040098913013935, 0.07171326875686646, 0.006528748199343681, 0.0, 0.0, 0.0], [0.835741400718689, 0.005003038793802261, 0.004361034370958805, 0.0036734151653945446, 0.00404117489233613, 0.0018618288449943066, 0.0009302829275839031, 0.03666642680764198, 0.07917573302984238, 0.028545551002025604, 0.0, 0.0], [0.6481264233589172, 0.0030033679213374853, 0.0030457773245871067, 0.004705729894340038, 0.0010377856669947505, 0.0006557865999639034, 4.89650119561702e-05, 0.029910927638411522, 0.11923708766698837, 0.11201386153697968, 0.07821431756019592, 0.0], [0.6141602396965027, 0.0013756562257185578, 0.0010341659653931856, 0.0024410183541476727, 0.002007532399147749, 0.0005631093517877162, 0.0011992130894213915, 0.01761108636856079, 0.015335614793002605, 0.017420899122953415, 0.29714372754096985, 0.02970782108604908]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9644559025764465, 0.03554413467645645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9632844924926758, 0.016986733302474022, 0.019728826358914375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8904570937156677, 0.0433625690639019, 0.04854396730661392, 0.017636271193623543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.88223797082901, 0.013037574477493763, 0.015061426907777786, 0.009400513954460621, 0.08026251941919327, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4174163043498993, 0.010328792035579681, 0.01432518009096384, 0.017342006787657738, 0.49450716376304626, 0.04608049988746643, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6646803021430969, 0.008471735753118992, 0.022218530997633934, 0.02195555716753006, 0.15267346799373627, 0.026852412149310112, 0.10314802825450897, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5858094096183777, 0.006511001847684383, 0.015645433217287064, 0.11618630588054657, 0.07875917851924896, 0.040047887712717056, 0.12169376760721207, 0.035346973687410355, 0.0, 0.0, 0.0, 0.0], [0.4577379822731018, 0.010836576111614704, 0.03223777189850807, 0.08055289089679718, 0.14005021750926971, 0.020680928602814674, 0.020411523059010506, 0.2068876028060913, 0.03060445562005043, 0.0, 0.0, 0.0], [0.4610025882720947, 0.015955504029989243, 0.011323116719722748, 0.029141705483198166, 0.04178934171795845, 0.00560177443549037, 0.014971879310905933, 0.35453489422798157, 0.030559822916984558, 0.03511933982372284, 0.0, 0.0], [0.8622645735740662, 0.008207804523408413, 0.01005945261567831, 0.009113788604736328, 0.013288531452417374, 0.0076274871826171875, 0.0009260426741093397, 0.015220679342746735, 0.014986682683229446, 0.027755923569202423, 0.030549004673957825, 0.0], [0.8213890194892883, 0.006901752203702927, 0.00376480002887547, 0.002897704252973199, 0.005145258270204067, 0.00916153471916914, 0.005972843151539564, 0.04354910925030708, 0.009124930016696453, 0.01918819732964039, 0.05040751397609711, 0.022497378289699554]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.973280131816864, 0.026719845831394196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9528902173042297, 0.023664774373173714, 0.023445025086402893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9426316022872925, 0.0427081435918808, 0.00664528040215373, 0.008014939725399017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9741420745849609, 0.010101662017405033, 0.001205540494993329, 0.002529590157791972, 0.012021192349493504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.886735737323761, 0.05863676592707634, 0.005079470109194517, 0.003633498214185238, 0.004170882515609264, 0.0417436845600605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9018236994743347, 0.0213154423981905, 0.004266032017767429, 0.001162698958069086, 0.0007993063773028553, 0.03221867233514786, 0.03841414675116539, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8741706609725952, 0.043491046875715256, 0.002622503088787198, 0.004686746280640364, 0.023697560653090477, 0.006794138345867395, 0.0030228770337998867, 0.04151447117328644, 0.0, 0.0, 0.0, 0.0], [0.21342509984970093, 0.00647718133404851, 0.12119895964860916, 0.09657023102045059, 0.4766559898853302, 0.016438664868474007, 0.004659971687942743, 0.05631543695926666, 0.008258480578660965, 0.0, 0.0, 0.0], [0.6224207282066345, 0.01389407366514206, 0.02641911432147026, 0.01958695612847805, 0.10487104952335358, 0.016350148245692253, 0.007766847964376211, 0.1427789330482483, 0.028273455798625946, 0.01763877272605896, 0.0, 0.0], [0.9262216687202454, 0.008696381002664566, 0.0014172453666105866, 0.00029339970205910504, 0.005723617505282164, 0.004257006570696831, 0.0035338453017175198, 0.0179803017526865, 0.0172294769436121, 0.007746122311800718, 0.0069009000435471535, 0.0], [0.9369179606437683, 0.008559142239391804, 0.0006311919423751533, 0.00026972003979608417, 0.0031720728147774935, 0.0030700527131557465, 0.001963720889762044, 0.02849501557648182, 0.01085056271404028, 0.0015830306801944971, 0.0007787630311213434, 0.003708645235747099]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9915046095848083, 0.008495431393384933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9409522414207458, 0.0499345064163208, 0.009113309904932976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9142885208129883, 0.0549812950193882, 0.019179606810212135, 0.01155056618154049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9554874897003174, 0.02881288342177868, 0.007626700215041637, 0.004605319816619158, 0.0034675896167755127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9525143504142761, 0.02534453384578228, 0.004340874496847391, 0.003436743514612317, 0.006967354565858841, 0.007396155968308449, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9392948150634766, 0.04129568859934807, 0.005370023660361767, 0.002734455280005932, 0.002309748437255621, 0.0032576057128608227, 0.005737652536481619, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8601328134536743, 0.026189055293798447, 0.011161085218191147, 0.007517677266150713, 0.01405760832130909, 0.02056996338069439, 0.02995471842586994, 0.030417080968618393, 0.0, 0.0, 0.0, 0.0], [0.5845205187797546, 0.0480823777616024, 0.051847752183675766, 0.025795063003897667, 0.08516800403594971, 0.03815897926688194, 0.05323087051510811, 0.10147814452648163, 0.011718332767486572, 0.0, 0.0, 0.0], [0.6617465019226074, 0.04806642234325409, 0.020865686237812042, 0.014268106780946255, 0.0625927746295929, 0.028353389352560043, 0.04965650290250778, 0.08543705195188522, 0.014203905127942562, 0.014809704385697842, 0.0, 0.0], [0.8696556687355042, 0.009691283106803894, 0.006508284714072943, 0.0069804321974515915, 0.040021464228630066, 0.013052524998784065, 0.011227159760892391, 0.028489863499999046, 0.005432665813714266, 0.007121623493731022, 0.0018189360853284597, 0.0], [0.775040864944458, 0.02106986753642559, 0.008832802064716816, 0.004694243427366018, 0.0822470560669899, 0.01584630273282528, 0.0326775386929512, 0.031119750812649727, 0.011035834439098835, 0.007790881209075451, 0.005449997261166573, 0.004194918554276228]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8995503783226013, 0.10044961422681808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.237043559551239, 0.6913371682167053, 0.07161920517683029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38873451948165894, 0.444148987531662, 0.14558839797973633, 0.02152813784778118, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5962414145469666, 0.18892692029476166, 0.08036404103040695, 0.03322936221957207, 0.10123831033706665, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8481730818748474, 0.026788508519530296, 0.019832728430628777, 0.0167723186314106, 0.05951168015599251, 0.028921691700816154, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7422763705253601, 0.0046164183877408504, 0.009997491724789143, 0.006482747383415699, 0.037179477512836456, 0.008859082125127316, 0.19058842957019806, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26913347840309143, 0.04715494439005852, 0.03352709859609604, 0.01823338493704796, 0.4276607632637024, 0.01629267819225788, 0.13300469517707825, 0.0549929104745388, 0.0, 0.0, 0.0, 0.0], [0.09774062037467957, 0.41345977783203125, 0.12863418459892273, 0.02400706335902214, 0.22505474090576172, 0.01748388633131981, 0.01679830066859722, 0.051077112555503845, 0.025744246318936348, 0.0, 0.0, 0.0], [0.11320213228464127, 0.32132628560066223, 0.14067904651165009, 0.0289380494505167, 0.2111303061246872, 0.022402184084057808, 0.04000722989439964, 0.06274375319480896, 0.03204745799303055, 0.027523543685674667, 0.0, 0.0], [0.5320673584938049, 0.04583740234375, 0.053778164088726044, 0.01670394465327263, 0.0449758879840374, 0.021579822525382042, 0.03331758454442024, 0.19225037097930908, 0.017388833686709404, 0.032846324145793915, 0.009254307486116886, 0.0], [0.25792860984802246, 0.34489741921424866, 0.09964948147535324, 0.013341646641492844, 0.05762946605682373, 0.01382311712950468, 0.04296791926026344, 0.03731832653284073, 0.0605773963034153, 0.023143252357840538, 0.0019601956009864807, 0.04676319286227226]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9507707953453064, 0.049229174852371216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7864078879356384, 0.07888539135456085, 0.13470670580863953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6879477500915527, 0.010240526869893074, 0.025335127487778664, 0.27647665143013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8945687413215637, 0.057099565863609314, 0.013345658779144287, 0.011510740965604782, 0.023475324735045433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7848508358001709, 0.010637904517352581, 0.0034313530195504427, 0.0025766079779714346, 0.019310666248202324, 0.1791926771402359, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7923367619514465, 0.025505317375063896, 0.004114889539778233, 0.003897665999829769, 0.07199176400899887, 0.031312163919210434, 0.07084145396947861, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6457110643386841, 0.018790224567055702, 0.035470250993967056, 0.05502740293741226, 0.01880299113690853, 0.013497400097548962, 0.005999977234750986, 0.20670071244239807, 0.0, 0.0, 0.0, 0.0], [0.5905060172080994, 0.03963208198547363, 0.024203505367040634, 0.04088328778743744, 0.012723390944302082, 0.0035769471433013678, 0.001231091096997261, 0.07148633152246475, 0.21575726568698883, 0.0, 0.0, 0.0], [0.707156240940094, 0.007978491485118866, 0.011211619712412357, 0.011189951561391354, 0.008248573169112206, 0.004757784307003021, 0.0009708627476356924, 0.026502298191189766, 0.024211838841438293, 0.1977723240852356, 0.0, 0.0], [0.8721404671669006, 0.00953182764351368, 0.0030401074327528477, 0.017776504158973694, 0.00627356581389904, 0.0035301160532981157, 0.0005110941710881889, 0.013812095858156681, 0.011666199192404747, 0.013547264970839024, 0.04817076027393341, 0.0], [0.8702904582023621, 0.03567644953727722, 0.004048457369208336, 0.009014874696731567, 0.005986097734421492, 0.018628869205713272, 0.008256487548351288, 0.015406116843223572, 0.004659141879528761, 0.009540711529552937, 0.006559330504387617, 0.011932949535548687]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9446554183959961, 0.055344581604003906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7479656338691711, 0.23047728836536407, 0.021557068452239037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3424518406391144, 0.5532395243644714, 0.09769780188798904, 0.00661090761423111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15549062192440033, 0.40606486797332764, 0.2612753212451935, 0.16134820878505707, 0.015820926055312157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24518148601055145, 0.18329453468322754, 0.1221456527709961, 0.181937113404274, 0.24231937527656555, 0.025121791288256645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2515924572944641, 0.08682126551866531, 0.1243354007601738, 0.2891485095024109, 0.19110460579395294, 0.04647252708673477, 0.010525190271437168, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48698991537094116, 0.28010594844818115, 0.0852368101477623, 0.05953631550073624, 0.028436509892344475, 0.007522862404584885, 0.013074642047286034, 0.03909694403409958, 0.0, 0.0, 0.0, 0.0], [0.379254549741745, 0.3952479362487793, 0.05173702910542488, 0.01635282300412655, 0.03233957663178444, 0.023306116461753845, 0.011981950141489506, 0.0760851576924324, 0.013694917783141136, 0.0, 0.0, 0.0], [0.24557751417160034, 0.2692177891731262, 0.0351850725710392, 0.0046257381327450275, 0.021534278988838196, 0.013941536657512188, 0.030193015933036804, 0.22591881453990936, 0.12235473841428757, 0.031451448798179626, 0.0, 0.0], [0.108822301030159, 0.06060490012168884, 0.007597337476909161, 0.003547820495441556, 0.004093232564628124, 0.0026180902495980263, 0.016834944486618042, 0.18803146481513977, 0.22597068548202515, 0.3545601963996887, 0.027319002896547318, 0.0], [0.13016048073768616, 0.04032432660460472, 0.008466243743896484, 0.004922185093164444, 0.004558252636343241, 0.0028988774865865707, 0.013510270975530148, 0.09421252459287643, 0.39835572242736816, 0.2329159528017044, 0.0536779910326004, 0.01599724404513836]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9577032327651978, 0.04229680076241493, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9650375247001648, 0.026339063420891762, 0.008623423986136913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9608638882637024, 0.025751259177923203, 0.00740413460880518, 0.005980771034955978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9000632762908936, 0.04633566737174988, 0.002034288365393877, 0.0025073112919926643, 0.04905938357114792, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8073189854621887, 0.08565836399793625, 0.00047795535647310317, 0.0006753837806172669, 0.0901532918214798, 0.01571602001786232, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7747837901115417, 0.0388525053858757, 0.009821885265409946, 0.021200042217969894, 0.12607522308826447, 0.02046049013733864, 0.008806112222373486, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8333690762519836, 0.022144963964819908, 0.01070803590118885, 0.07443597167730331, 0.017036747187376022, 0.004109421279281378, 0.011249350383877754, 0.026946453377604485, 0.0, 0.0, 0.0, 0.0], [0.8405707478523254, 0.035723727196455, 0.016656946390867233, 0.019762061536312103, 0.03964391350746155, 0.003947921562939882, 0.004456433467566967, 0.026991836726665497, 0.01224637869745493, 0.0, 0.0, 0.0], [0.8770609498023987, 0.0421542152762413, 0.003459054743871093, 0.009395133703947067, 0.010794629342854023, 0.0023000447545200586, 0.0036356456112116575, 0.02237291820347309, 0.023817045614123344, 0.005010360386222601, 0.0, 0.0], [0.968289315700531, 0.008241969160735607, 0.00021475920220836997, 0.00011947475286433473, 0.0008863022085279226, 0.0005688124801963568, 0.00024964683689177036, 0.0030957446433603764, 0.005609787534922361, 0.004520471207797527, 0.008203759789466858, 0.0], [0.9408398270606995, 0.004278102423995733, 0.0004233094514347613, 0.0004223576106596738, 0.0008001092937774956, 0.000532265636138618, 0.001318478025496006, 0.01139399129897356, 0.0028685196302831173, 0.0031099349725991488, 0.009621615521609783, 0.02439144067466259]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9803985953330994, 0.01960144191980362, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8659805655479431, 0.05516272038221359, 0.07885679602622986, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8521763682365417, 0.03816651552915573, 0.06493030488491058, 0.04472685605287552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8277944922447205, 0.025318747386336327, 0.10786116868257523, 0.03406587243080139, 0.004959657322615385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9172587990760803, 0.01632121205329895, 0.027731608599424362, 0.0139353321865201, 0.0059932516887784, 0.018759749829769135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8161044716835022, 0.022697392851114273, 0.055231623351573944, 0.03310125321149826, 0.008885293267667294, 0.02327115461230278, 0.040708765387535095, 0.0, 0.0, 0.0, 0.0, 0.0], [0.764035701751709, 0.0252792090177536, 0.03564879298210144, 0.029560934752225876, 0.00765151996165514, 0.027923572808504105, 0.0628214180469513, 0.04707873985171318, 0.0, 0.0, 0.0, 0.0], [0.778481125831604, 0.017297448590397835, 0.036996059119701385, 0.025612007826566696, 0.010480724275112152, 0.014829779975116253, 0.06513643264770508, 0.03475701063871384, 0.016409514471888542, 0.0, 0.0, 0.0], [0.772700846195221, 0.020560912787914276, 0.03272576630115509, 0.029621567577123642, 0.012877260334789753, 0.012188892811536789, 0.04886302724480629, 0.03874390572309494, 0.01594681479036808, 0.01577099785208702, 0.0, 0.0], [0.7987111210823059, 0.01887398399412632, 0.037940602749586105, 0.022695021703839302, 0.008421014994382858, 0.0068824635818600655, 0.046285782009363174, 0.03652893379330635, 0.008031381294131279, 0.011542794294655323, 0.004087008535861969, 0.0], [0.5944671630859375, 0.02746618539094925, 0.08059348165988922, 0.030409028753638268, 0.022279607132077217, 0.013477973639965057, 0.09124625474214554, 0.08500867336988449, 0.019544651731848717, 0.023592693731188774, 0.004417104180902243, 0.007497156038880348]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9799265265464783, 0.020073525607585907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8610404133796692, 0.05613858625292778, 0.08282096683979034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8219932317733765, 0.07356089353561401, 0.0670946016907692, 0.03735121339559555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7266371250152588, 0.07286915183067322, 0.07893053442239761, 0.022344930097460747, 0.09921816736459732, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8460869789123535, 0.04218077287077904, 0.033469974994659424, 0.026103349402546883, 0.030550723895430565, 0.0216082651168108, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6139050722122192, 0.041648056358098984, 0.07263141125440598, 0.06678200513124466, 0.07589671015739441, 0.04271039739251137, 0.08642634749412537, 0.0, 0.0, 0.0, 0.0, 0.0], [0.607154130935669, 0.04182992875576019, 0.046687524765729904, 0.04753826931118965, 0.07352049648761749, 0.02177106775343418, 0.09787372499704361, 0.06362483650445938, 0.0, 0.0, 0.0, 0.0], [0.6364192962646484, 0.01865844614803791, 0.017789317294955254, 0.02263261005282402, 0.10995376110076904, 0.019181787967681885, 0.05224065110087395, 0.11280633509159088, 0.01031783688813448, 0.0, 0.0, 0.0], [0.605018675327301, 0.024244023486971855, 0.019397715106606483, 0.034090690314769745, 0.10085204243659973, 0.029756030067801476, 0.07247170060873032, 0.08419965207576752, 0.018952496349811554, 0.011016984470188618, 0.0, 0.0], [0.8317301869392395, 0.011798079125583172, 0.007432490587234497, 0.0059448895044624805, 0.05057826638221741, 0.015818944200873375, 0.020629137754440308, 0.03524010255932808, 0.008352801203727722, 0.007415813393890858, 0.005059233866631985, 0.0], [0.7001292109489441, 0.02934572845697403, 0.024056976661086082, 0.018217844888567924, 0.028565343469381332, 0.018943380564451218, 0.051360681653022766, 0.07191473245620728, 0.02120973914861679, 0.014685512520372868, 0.007220440544188023, 0.014350347220897675]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8318522572517395, 0.1681477427482605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.767510712146759, 0.12144988030195236, 0.11103947460651398, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3725925385951996, 0.21188440918922424, 0.3473057150840759, 0.06821729987859726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3995687961578369, 0.16443470120429993, 0.23080065846443176, 0.045440178364515305, 0.15975575149059296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4441279470920563, 0.14614234864711761, 0.11059850454330444, 0.0318414568901062, 0.23053249716758728, 0.03675718978047371, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7024731636047363, 0.04739227890968323, 0.0420801118016243, 0.026454152539372444, 0.03809567540884018, 0.013854065909981728, 0.12965050339698792, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4790492355823517, 0.04880538582801819, 0.08862525224685669, 0.04552998021245003, 0.10194271057844162, 0.012961296364665031, 0.05768927186727524, 0.16539689898490906, 0.0, 0.0, 0.0, 0.0], [0.48369544744491577, 0.08460567891597748, 0.03144480288028717, 0.01650407165288925, 0.10177762806415558, 0.012575758621096611, 0.09917514771223068, 0.15123796463012695, 0.01898355968296528, 0.0, 0.0, 0.0], [0.4243321120738983, 0.07450339198112488, 0.050864048302173615, 0.02012256719172001, 0.10565564036369324, 0.014595371671020985, 0.1239861398935318, 0.1431211531162262, 0.024285761639475822, 0.018533855676651, 0.0, 0.0], [0.17639833688735962, 0.06510629504919052, 0.05651375278830528, 0.028542988002300262, 0.15299667418003082, 0.011974243447184563, 0.16462257504463196, 0.26238682866096497, 0.026293352246284485, 0.023941989988088608, 0.031222952529788017, 0.0], [0.5262041091918945, 0.050975605845451355, 0.03519570082426071, 0.016006827354431152, 0.03480768948793411, 0.013387150131165981, 0.18432405591011047, 0.060379888862371445, 0.015382659621536732, 0.009803579188883305, 0.007097210269421339, 0.046435534954071045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9570327997207642, 0.04296719655394554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8363948464393616, 0.06108638644218445, 0.1025187075138092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8361901044845581, 0.06255879253149033, 0.05285844951868057, 0.0483926385641098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.804766833782196, 0.051299698650836945, 0.053352225571870804, 0.05726773664355278, 0.03331350162625313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7888414859771729, 0.028550347313284874, 0.02529098466038704, 0.03144735470414162, 0.024012187495827675, 0.10185772180557251, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8351284265518188, 0.017494194209575653, 0.03459516167640686, 0.03426530584692955, 0.009389511309564114, 0.0195010956376791, 0.04962625354528427, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7450125217437744, 0.025528300553560257, 0.03051944263279438, 0.046469856053590775, 0.018463747575879097, 0.027111997827887535, 0.03940323740243912, 0.06749080866575241, 0.0, 0.0, 0.0, 0.0], [0.5941357016563416, 0.03704528138041496, 0.02889583259820938, 0.03442893549799919, 0.03538563847541809, 0.016816481947898865, 0.03224068880081177, 0.10842178016901016, 0.11262974143028259, 0.0, 0.0, 0.0], [0.5953229665756226, 0.02842620573937893, 0.02466903254389763, 0.024136673659086227, 0.024097885936498642, 0.012411622330546379, 0.018294673413038254, 0.07634738087654114, 0.12679968774318695, 0.06949380040168762, 0.0, 0.0], [0.7280614972114563, 0.021994439885020256, 0.012627624906599522, 0.010426363907754421, 0.01270628347992897, 0.004515913315117359, 0.010398266837000847, 0.033980999141931534, 0.05903884395956993, 0.04948725178837776, 0.05676248297095299, 0.0], [0.7495908737182617, 0.008835379965603352, 0.009122515097260475, 0.008761754259467125, 0.006522269453853369, 0.005617587361484766, 0.010878782719373703, 0.03667033091187477, 0.034901972860097885, 0.03640781715512276, 0.03529167175292969, 0.05739904195070267]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.96860271692276, 0.031397245824337006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9360827207565308, 0.008639846928417683, 0.05527743697166443, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8980001211166382, 0.012206523679196835, 0.034089792519807816, 0.055703479796648026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.896878719329834, 0.016667179763317108, 0.021780354902148247, 0.008524850010871887, 0.05614888668060303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8720502257347107, 0.010710019618272781, 0.002429298358038068, 0.0027419112157076597, 0.02643023617565632, 0.0856383815407753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8993743062019348, 0.010029580444097519, 0.005654798820614815, 0.004039035178720951, 0.01950082555413246, 0.012802723795175552, 0.04859885200858116, 0.0, 0.0, 0.0, 0.0, 0.0], [0.820344090461731, 0.020803721621632576, 0.021939123049378395, 0.011058049276471138, 0.029559412971138954, 0.005067134741693735, 0.005806540604680777, 0.08542199432849884, 0.0, 0.0, 0.0, 0.0], [0.7480672001838684, 0.014153272844851017, 0.014122558757662773, 0.012806277722120285, 0.05253656581044197, 0.010928512550890446, 0.005899835377931595, 0.028114955872297287, 0.11337079852819443, 0.0, 0.0, 0.0], [0.7661000490188599, 0.009621068835258484, 0.018303075805306435, 0.014398807659745216, 0.02132515050470829, 0.002900755265727639, 0.0027680706698447466, 0.021270588040351868, 0.0466010607779026, 0.09671139717102051, 0.0, 0.0], [0.813506543636322, 0.010506854392588139, 0.012652642093598843, 0.011485083028674126, 0.012406030669808388, 0.0014587593032047153, 0.003203407395631075, 0.01248774491250515, 0.030224865302443504, 0.03229198604822159, 0.05977605655789375, 0.0], [0.9070471525192261, 0.009275322780013084, 0.00570889376103878, 0.003429487580433488, 0.011755089275538921, 0.002660647500306368, 0.0025515006855130196, 0.00759453559294343, 0.00793384201824665, 0.00844328012317419, 0.005955896805971861, 0.02764439955353737]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9058735966682434, 0.0941263735294342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8547714948654175, 0.10283026844263077, 0.04239817336201668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7967277765274048, 0.12795156240463257, 0.05165926739573479, 0.023661499843001366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6457647681236267, 0.08022861182689667, 0.05020105466246605, 0.029037142172455788, 0.194768488407135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.560978353023529, 0.05254652723670006, 0.02775329165160656, 0.01740254834294319, 0.21234504878520966, 0.12897421419620514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38919752836227417, 0.04149225726723671, 0.03528193011879921, 0.023619456216692924, 0.2269165962934494, 0.11501552909612656, 0.16847670078277588, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23794743418693542, 0.03389018028974533, 0.009277322329580784, 0.012151861563324928, 0.24107994139194489, 0.15855790674686432, 0.2711162567138672, 0.03597908467054367, 0.0, 0.0, 0.0, 0.0], [0.5048310160636902, 0.02442890591919422, 0.008012686856091022, 0.005297014955431223, 0.1841784417629242, 0.08057799935340881, 0.14032475650310516, 0.0428466759622097, 0.009502478875219822, 0.0, 0.0, 0.0], [0.44742658734321594, 0.04913056641817093, 0.010544228367507458, 0.004015755839645863, 0.187120720744133, 0.07861179113388062, 0.1442730873823166, 0.04497448354959488, 0.016636062413454056, 0.017266739159822464, 0.0, 0.0], [0.6047410368919373, 0.039380721747875214, 0.005877961870282888, 0.0019737023394554853, 0.11356792598962784, 0.059457216411828995, 0.11203350871801376, 0.019835161045193672, 0.013754557818174362, 0.019075680524110794, 0.010302487760782242, 0.0], [0.5637742280960083, 0.020610971376299858, 0.00566531578078866, 0.0029817663598805666, 0.09363611042499542, 0.04421856626868248, 0.11175379157066345, 0.027012577280402184, 0.014398572035133839, 0.014702673070132732, 0.013573974370956421, 0.08767139166593552]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9522379636764526, 0.04776206240057945, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5936537384986877, 0.3995467722415924, 0.0067994906567037106, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6453675031661987, 0.21716150641441345, 0.1117963045835495, 0.025674689561128616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7336032390594482, 0.0962720513343811, 0.04748236760497093, 0.11314821988344193, 0.009494119323790073, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4693325459957123, 0.01208086870610714, 0.03294246643781662, 0.062183599919080734, 0.40309181809425354, 0.020368635654449463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1923147737979889, 0.015042775310575962, 0.03273697942495346, 0.0869453102350235, 0.3865174949169159, 0.2627623975276947, 0.0236801877617836, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6587823629379272, 0.0515187531709671, 0.020230425521731377, 0.05390934646129608, 0.07937376946210861, 0.054251790046691895, 0.05635174363851547, 0.025581825524568558, 0.0, 0.0, 0.0, 0.0], [0.489083856344223, 0.04108835384249687, 0.011591138318181038, 0.013600858859717846, 0.026890359818935394, 0.01419010292738676, 0.021905334666371346, 0.28872644901275635, 0.0929235965013504, 0.0, 0.0, 0.0], [0.42224934697151184, 0.04459349438548088, 0.005609862972050905, 0.010576888918876648, 0.011492360383272171, 0.00930524431169033, 0.016726532950997353, 0.26202747225761414, 0.18754175305366516, 0.029877018183469772, 0.0, 0.0], [0.7074236273765564, 0.015703855082392693, 0.0026927778962999582, 0.0036161202006042004, 0.0009640600765123963, 0.001739089610055089, 0.003786074463278055, 0.10802596807479858, 0.04540855437517166, 0.09930279105901718, 0.011337077245116234, 0.0], [0.6983626484870911, 0.003270888002589345, 0.0016777702840045094, 0.0036678474862128496, 0.0014823648380115628, 0.0009634490124881268, 0.003358016489073634, 0.057375553995370865, 0.040708452463150024, 0.03935699537396431, 0.10114762932062149, 0.04862837865948677]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9990837574005127, 0.0009162150672636926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9925958514213562, 0.0035357850138098, 0.0038683414459228516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9924700260162354, 0.0005993121885694563, 0.0004625220026355237, 0.006468098144978285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.999372661113739, 0.00023436984338331968, 2.5177973839163315e-06, 9.814678378461394e-06, 0.00038068200228735805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9918655753135681, 0.0036173812113702297, 1.2744271771225613e-06, 1.176377395495365e-06, 1.5485717085539363e-05, 0.004499041009694338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9957262277603149, 0.0001274422975257039, 5.904584213567432e-06, 4.044217803311767e-06, 2.6174922822974622e-05, 0.0005240224418230355, 0.0035861937794834375, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9950109720230103, 0.002768290229141712, 6.361889973049983e-05, 0.00021656317403540015, 0.0006460605654865503, 1.2737649512928328e-06, 0.000241539761191234, 0.0010516408365219831, 0.0, 0.0, 0.0, 0.0], [0.9584470987319946, 0.003843692597001791, 0.002889609429985285, 0.005598521791398525, 0.014299626462161541, 3.163099790981505e-06, 0.0006549112149514258, 0.0012729908339679241, 0.012990383431315422, 0.0, 0.0, 0.0], [0.9762134552001953, 0.002213480416685343, 0.0004878939944319427, 0.0028625107370316982, 0.00515530863776803, 4.8823885379079e-08, 2.6122941562789492e-05, 0.0001577482180437073, 0.005212276242673397, 0.007671106141060591, 0.0, 0.0], [0.9971745014190674, 0.0005021642427891493, 1.2519487427198328e-05, 2.1440768250613473e-05, 0.0009439783752895892, 3.479599115507881e-07, 0.00013644427235703915, 8.216507558245212e-05, 7.411769911414012e-05, 0.00019481524941511452, 0.0008574238163419068, 0.0], [0.9991952776908875, 0.0001954089675564319, 3.1450472306460142e-06, 4.876108050666517e-06, 0.00010753116657724604, 4.037232486098219e-07, 3.9556281990371644e-05, 9.23228872125037e-05, 0.0001150129028246738, 9.215732461598236e-07, 5.398435405368218e-06, 0.0002400730736553669]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9745726585388184, 0.025427373126149178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9692674875259399, 0.011235627345740795, 0.019496776163578033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9604085683822632, 0.00925414077937603, 0.0156381968408823, 0.0146990567445755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9387809038162231, 0.005214521661400795, 0.00760044576600194, 0.006993259768933058, 0.04141088202595711, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502151012420654, 0.010272741317749023, 0.005210069473832846, 0.007572531700134277, 0.015759851783514023, 0.010969665832817554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9233594536781311, 0.006766364444047213, 0.011677166447043419, 0.00847257487475872, 0.030679821968078613, 0.006468135863542557, 0.012576556764543056, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9313353300094604, 0.002754763001576066, 0.0025127725675702095, 0.0032621044665575027, 0.019818350672721863, 0.004374953452497721, 0.01058138720691204, 0.02536037564277649, 0.0, 0.0, 0.0, 0.0], [0.9673178195953369, 0.0013951716246083379, 0.0009283863473683596, 0.0009483078611083329, 0.01288561336696148, 0.0014497442170977592, 0.006725535728037357, 0.004430845379829407, 0.003918737173080444, 0.0, 0.0, 0.0], [0.9674623608589172, 0.0009375839144922793, 0.0005083692376501858, 0.0008261919720098376, 0.008320074528455734, 0.0026120098773390055, 0.003716130508109927, 0.004396920558065176, 0.005490587092936039, 0.005729706957936287, 0.0, 0.0], [0.9494664072990417, 0.0017689316300675273, 0.0007035293965600431, 0.0011377884075045586, 0.010281615890562534, 0.005351317580789328, 0.007691557984799147, 0.004480541218072176, 0.003616730449721217, 0.005016958341002464, 0.010484556667506695, 0.0], [0.9297968149185181, 0.0014223272446542978, 0.0015771286562085152, 0.001283479854464531, 0.007793575990945101, 0.0022254742216318846, 0.007185652852058411, 0.0069367727264761925, 0.004442049656063318, 0.01038060337305069, 0.0036302318330854177, 0.023325912654399872]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9715749025344849, 0.028425127267837524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8468066453933716, 0.052453670650720596, 0.10073967278003693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7672279477119446, 0.07305256277322769, 0.12916472554206848, 0.03055480122566223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5925819873809814, 0.04784165695309639, 0.11125906556844711, 0.14642401039600372, 0.10189327597618103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5304303169250488, 0.07144026458263397, 0.03537323325872421, 0.07525135576725006, 0.23031878471374512, 0.05718603357672691, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5017051100730896, 0.01997850090265274, 0.07851913571357727, 0.27057716250419617, 0.04532363638281822, 0.032364413142204285, 0.05153201147913933, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7190533876419067, 0.017683245241642, 0.011026252061128616, 0.11592648178339005, 0.020324893295764923, 0.009024241007864475, 0.021099666133522987, 0.08586177229881287, 0.0, 0.0, 0.0, 0.0], [0.24035504460334778, 0.009865445084869862, 0.010073021985590458, 0.021925214678049088, 0.04742727428674698, 0.012429566122591496, 0.018996812403202057, 0.6164730787277222, 0.022454483434557915, 0.0, 0.0, 0.0], [0.2438616156578064, 0.01000683382153511, 0.006597400177270174, 0.013671644032001495, 0.021884236484766006, 0.01048413198441267, 0.01956259459257126, 0.6073765754699707, 0.036615923047065735, 0.02993897534906864, 0.0, 0.0], [0.40983107686042786, 0.004432422574609518, 0.0086220633238554, 0.01078603696078062, 0.014975713565945625, 0.006116189062595367, 0.006289609707891941, 0.3107253313064575, 0.06426945328712463, 0.11801248788833618, 0.0459396094083786, 0.0], [0.33516690135002136, 0.0022980899084359407, 0.002880312502384186, 0.0057466602884233, 0.003848528489470482, 0.0018249815329909325, 0.006251349579542875, 0.08869928121566772, 0.03667032718658447, 0.058688174933195114, 0.40825486183166504, 0.049670469015836716]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9604174494743347, 0.039582543075084686, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9302036166191101, 0.05278953164815903, 0.017006779089570045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7250233888626099, 0.12131921947002411, 0.146603524684906, 0.007053964305669069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8197346925735474, 0.017060810700058937, 0.08600418269634247, 0.053518906235694885, 0.023681428283452988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3075217306613922, 0.0031584729440510273, 0.04080018028616905, 0.024477044120430946, 0.5958470702171326, 0.02819548361003399, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6620691418647766, 0.007198153529316187, 0.04043550789356232, 0.034445010125637054, 0.12718011438846588, 0.11698365211486816, 0.011688381433486938, 0.0, 0.0, 0.0, 0.0, 0.0], [0.952570378780365, 0.003853980451822281, 0.002382048172876239, 0.001579070696607232, 0.003046038094907999, 0.005649921018630266, 0.009901386685669422, 0.02101728692650795, 0.0, 0.0, 0.0, 0.0], [0.71816486120224, 0.0016169676091521978, 0.002117960015311837, 0.0015178370522335172, 0.0028198075015097857, 0.0033842676784843206, 0.003451928263530135, 0.23453867435455322, 0.03238767385482788, 0.0, 0.0, 0.0], [0.6255384087562561, 0.0033618866000324488, 0.0012045861221849918, 0.00045203138142824173, 0.0015259988140314817, 0.0035484021063894033, 0.0032949568703770638, 0.17699697613716125, 0.1303788274526596, 0.05369787663221359, 0.0, 0.0], [0.48822644352912903, 0.002195131964981556, 0.001277025556191802, 0.0008118631667457521, 0.0005890650791116059, 0.0007458762265741825, 0.0004690751666203141, 0.033115170896053314, 0.04864155501127243, 0.3901495933532715, 0.03377922996878624, 0.0], [0.1206623986363411, 0.0001542099635116756, 0.0003788033500313759, 0.000553768128156662, 0.0007230282062664628, 0.0008555936510674655, 0.0004310756048653275, 0.02015961892902851, 0.014320526272058487, 0.11440885066986084, 0.7216766476631165, 0.005675381049513817]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9771901965141296, 0.022809775546193123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9622159004211426, 0.022914374247193336, 0.0148696294054389, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.967591404914856, 0.013713812455534935, 0.010110764764249325, 0.008584066294133663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9802769422531128, 0.003473642049357295, 0.0007596358773298562, 0.00022007881489116699, 0.01526976190507412, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9161737561225891, 0.004861997906118631, 0.0012346819275990129, 0.00025808284408412874, 0.005169196054339409, 0.0723021999001503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8860867619514465, 0.002550671575590968, 0.0005681288312189281, 0.00029398660990409553, 0.002057812875136733, 0.014958012849092484, 0.09348475188016891, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8239720463752747, 0.015059988014400005, 0.005366061348468065, 0.00464605214074254, 0.017714254558086395, 0.010538691654801369, 0.04676026850938797, 0.07594273239374161, 0.0, 0.0, 0.0, 0.0], [0.5921857357025146, 0.04718982055783272, 0.012900745496153831, 0.007604576647281647, 0.12969830632209778, 0.07985742390155792, 0.04215185344219208, 0.03065571002662182, 0.05775579437613487, 0.0, 0.0, 0.0], [0.7646058201789856, 0.055060554295778275, 0.012452681548893452, 0.004099702928215265, 0.044545404613018036, 0.015439503826200962, 0.025883737951517105, 0.03172194957733154, 0.038851089775562286, 0.0073395888321101665, 0.0, 0.0], [0.9083195328712463, 0.013242439366877079, 0.0016208419110625982, 0.001021038624458015, 0.013060810044407845, 0.014879059977829456, 0.019450563937425613, 0.004116518888622522, 0.009743223898112774, 0.004069522488862276, 0.010476340539753437, 0.0], [0.9742091298103333, 0.0015296511119231582, 0.00021405972074717283, 0.00010722944716690108, 0.0006045771879144013, 0.0004323552129790187, 0.0016197517979890108, 0.0009734275517985225, 0.0022490450646728277, 0.0007891341811046004, 0.0012036996195092797, 0.01606784574687481]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9985394477844238, 0.0014605555916205049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9986502528190613, 0.00042477637180127203, 0.0009250438888557255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9993321299552917, 0.00011975812230957672, 0.00012183502258267254, 0.00042616802966222167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9996675252914429, 5.2023748139617965e-05, 1.8827804524335079e-06, 5.037015398556832e-06, 0.00027354041230864823, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9977136850357056, 0.0006583358626812696, 2.4045000373007497e-06, 2.6401717150292825e-06, 5.37479245394934e-05, 0.0015691890148445964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9494306445121765, 5.8771180192707106e-05, 5.426961070043035e-05, 6.543424387928098e-05, 0.0013898499310016632, 0.005694125778973103, 0.04330698773264885, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9885339140892029, 0.001225082902237773, 3.447110429988243e-05, 0.0001938934001373127, 0.00045439822133630514, 0.00010234637738903984, 0.0032387515529990196, 0.006217148620635271, 0.0, 0.0, 0.0, 0.0], [0.9431915283203125, 0.0012713875621557236, 0.0028932110872119665, 0.006087747868150473, 0.008797180838882923, 1.582768527441658e-05, 0.0014113212237134576, 0.006033029872924089, 0.030298661440610886, 0.0, 0.0, 0.0], [0.9847925901412964, 0.0010301809525117278, 0.00022872057161293924, 0.0013880199985578656, 0.001352645456790924, 2.6707875804277137e-06, 0.00028452358674257994, 0.0007579481462016702, 0.007246790453791618, 0.00291593000292778, 0.0, 0.0], [0.990932822227478, 0.00046995203592814505, 9.296406642533839e-05, 0.00019844667986035347, 0.0011431952007114887, 8.318646905536298e-06, 0.0003570257395040244, 0.0001843328936956823, 0.0003031227388419211, 0.0006014984101057053, 0.005708289798349142, 0.0], [0.9959226846694946, 0.00010725126776378602, 1.3144716831448022e-05, 1.9676745068863966e-05, 5.866782521479763e-05, 1.1048859960283153e-05, 0.00010128822759725153, 0.00032831565476953983, 0.0005782492808066308, 5.311855420586653e-05, 6.372554344125092e-05, 0.0027427130844444036]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9504939317703247, 0.0495060496032238, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9076070189476013, 0.05649357661604881, 0.03589935973286629, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7804266214370728, 0.12540550529956818, 0.0644569844007492, 0.02971087023615837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.744705855846405, 0.12384400516748428, 0.044832341372966766, 0.03546601161360741, 0.051151715219020844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6510642766952515, 0.13040603697299957, 0.04918404668569565, 0.05394197255373001, 0.0906887948513031, 0.024714834988117218, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5602327585220337, 0.06747089326381683, 0.05746236443519592, 0.14063535630702972, 0.06863522529602051, 0.03525516018271446, 0.07030823826789856, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5776811838150024, 0.06861661374568939, 0.044061195105314255, 0.0667402371764183, 0.0446523018181324, 0.029427900910377502, 0.06246568262577057, 0.10635492950677872, 0.0, 0.0, 0.0, 0.0], [0.356006920337677, 0.02570418454706669, 0.03262970224022865, 0.03004833683371544, 0.09745220839977264, 0.0685359537601471, 0.10254757851362228, 0.241046741604805, 0.046028394252061844, 0.0, 0.0, 0.0], [0.3994085192680359, 0.0510125607252121, 0.02342045307159424, 0.020929578691720963, 0.05461261421442032, 0.028233902528882027, 0.07310803234577179, 0.23825223743915558, 0.07094515115022659, 0.04007689282298088, 0.0, 0.0], [0.5241561532020569, 0.03442823141813278, 0.01840989664196968, 0.016960419714450836, 0.04507363587617874, 0.03351109102368355, 0.10604362934827805, 0.0857718214392662, 0.0481056347489357, 0.05883481726050377, 0.028704635798931122, 0.0], [0.7208909392356873, 0.018267273902893066, 0.00672730291262269, 0.007251957431435585, 0.007097385358065367, 0.009229722432792187, 0.02613118477165699, 0.052498962730169296, 0.026851950213313103, 0.046916648745536804, 0.038941409438848495, 0.0391952320933342]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9898198246955872, 0.010180189274251461, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9774346351623535, 0.01544263120740652, 0.0071227229200303555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9617087841033936, 0.02107181027531624, 0.01241721399128437, 0.004802212584763765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.936856746673584, 0.019239841029047966, 0.022458020597696304, 0.012372147291898727, 0.0090732891112566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9600916504859924, 0.01206893753260374, 0.0042558833956718445, 0.0028612103778868914, 0.019162993878126144, 0.0015592952258884907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5761590600013733, 0.036050211638212204, 0.040854547172784805, 0.06731415539979935, 0.177379310131073, 0.07223252952098846, 0.030010230839252472, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6641125679016113, 0.04026361182332039, 0.07347430288791656, 0.10217506438493729, 0.031879931688308716, 0.015270295552909374, 0.040161389857530594, 0.03266283869743347, 0.0, 0.0, 0.0, 0.0], [0.9215661883354187, 0.005695777479559183, 0.007788368500769138, 0.00872710533440113, 0.006129023618996143, 0.003989039920270443, 0.006222695577889681, 0.026883268728852272, 0.012998390011489391, 0.0, 0.0, 0.0], [0.8710307478904724, 0.012615684419870377, 0.011766379699110985, 0.010187926702201366, 0.004241907969117165, 0.0051305266097188, 0.00918184407055378, 0.04382823780179024, 0.027211062610149384, 0.0048057762905955315, 0.0, 0.0], [0.913783848285675, 0.012204943224787712, 0.006063918583095074, 0.004754110239446163, 0.002799955429509282, 0.0030639171600341797, 0.005671921651810408, 0.019138837233185768, 0.012470102868974209, 0.018160274252295494, 0.0018880622228607535, 0.0], [0.6524505615234375, 0.013497716747224331, 0.014369409531354904, 0.018097467720508575, 0.004846205934882164, 0.004678158089518547, 0.0029393446166068316, 0.12171705812215805, 0.06515210121870041, 0.04838740825653076, 0.020545709878206253, 0.03331884369254112]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9621802568435669, 0.0378197580575943, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9406200051307678, 0.027340004220604897, 0.03203996270895004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8955293893814087, 0.020192153751850128, 0.05730083957314491, 0.02697763592004776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8267914056777954, 0.030658815056085587, 0.06731340289115906, 0.04709920287132263, 0.028137216344475746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6728754639625549, 0.0493679903447628, 0.07213177531957626, 0.04698864743113518, 0.08619087189435959, 0.07244517654180527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.739273726940155, 0.013345541432499886, 0.04270181432366371, 0.05516185611486435, 0.04005556181073189, 0.04779064282774925, 0.06167086586356163, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6110458374023438, 0.023230109363794327, 0.06672747433185577, 0.126177579164505, 0.0907445102930069, 0.020712684839963913, 0.034512437880039215, 0.026849374175071716, 0.0, 0.0, 0.0, 0.0], [0.6572778224945068, 0.015348791144788265, 0.05557309836149216, 0.11236949265003204, 0.05640522390604019, 0.01472702156752348, 0.022974686697125435, 0.057012807577848434, 0.008311024866998196, 0.0, 0.0, 0.0], [0.6482412815093994, 0.011458102613687515, 0.022950926795601845, 0.07537000626325607, 0.053174689412117004, 0.02772517316043377, 0.03274671733379364, 0.09314462542533875, 0.024601247161626816, 0.010587259195744991, 0.0, 0.0], [0.5539351105690002, 0.006383066531270742, 0.03138359263539314, 0.0315396711230278, 0.043113723397254944, 0.007673004176467657, 0.016137180849909782, 0.14625513553619385, 0.07468774914741516, 0.03697611391544342, 0.051915619522333145, 0.0], [0.7536489963531494, 0.013658531941473484, 0.01463752519339323, 0.014749199151992798, 0.008922536857426167, 0.003636894980445504, 0.01818663813173771, 0.05129677802324295, 0.0219875518232584, 0.031063897535204887, 0.032466839998960495, 0.03574451059103012]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9708282947540283, 0.029171686619520187, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9694469571113586, 0.022553356364369392, 0.007999653927981853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9244140386581421, 0.03660634160041809, 0.01408444344997406, 0.024895241484045982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9318094253540039, 0.03247929736971855, 0.004238942172378302, 0.011139127425849438, 0.020333148539066315, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9158239960670471, 0.03705563023686409, 0.00820318702608347, 0.011483320035040379, 0.022761277854442596, 0.004672633018344641, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8682935237884521, 0.018574591726064682, 0.008039206266403198, 0.022118056192994118, 0.026330817490816116, 0.003077436937019229, 0.05356632545590401, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8640682697296143, 0.030502628535032272, 0.015405193902552128, 0.02414499968290329, 0.019735969603061676, 0.0018500984879210591, 0.019079972058534622, 0.02521296590566635, 0.0, 0.0, 0.0, 0.0], [0.8024616241455078, 0.02182721719145775, 0.012636754661798477, 0.023598335683345795, 0.0454789362847805, 0.0035920317750424147, 0.035511795431375504, 0.037150558084249496, 0.01774275116622448, 0.0, 0.0, 0.0], [0.8342245221138, 0.024120647460222244, 0.007356510031968355, 0.011311501264572144, 0.02830822765827179, 0.002144640777260065, 0.03368804231286049, 0.0313975028693676, 0.013495922088623047, 0.013952430337667465, 0.0, 0.0], [0.8789562582969666, 0.02197320945560932, 0.0054310886189341545, 0.009763823822140694, 0.020371830090880394, 0.0026576449163258076, 0.016456399112939835, 0.014422464184463024, 0.008700119331479073, 0.012299634516239166, 0.008967657573521137, 0.0], [0.9182722568511963, 0.008179848082363605, 0.001989052165299654, 0.005627215839922428, 0.009249819442629814, 0.0018780091777443886, 0.015807252377271652, 0.013963600620627403, 0.00673707015812397, 0.005396303255110979, 0.006586019415408373, 0.006313462276011705]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954980611801147, 0.004501973278820515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9966865181922913, 0.0017453449545428157, 0.0015681401127949357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9958606362342834, 0.0010806269710883498, 0.000682331679854542, 0.002376382937654853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9878743290901184, 0.0018155878642573953, 0.0006917852442711592, 0.0012867357581853867, 0.00833150651305914, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9598414301872253, 0.00618495000526309, 0.0025381436571478844, 0.0031010606326162815, 0.009171919897198677, 0.019162476062774658, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9114314317703247, 0.002916567726060748, 0.0011327023385092616, 0.004016055725514889, 0.014627326279878616, 0.004234398249536753, 0.06164148822426796, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9391415119171143, 0.004776363261044025, 0.0026373909786343575, 0.005427191033959389, 0.011371796950697899, 0.0030510344076901674, 0.02206502854824066, 0.011529739946126938, 0.0, 0.0, 0.0, 0.0], [0.8454865217208862, 0.0029336444567888975, 0.0014568453188985586, 0.002557457657530904, 0.012474792078137398, 0.0060616242699325085, 0.08664198964834213, 0.01970699243247509, 0.02268027700483799, 0.0, 0.0, 0.0], [0.9175593256950378, 0.0013730874052271247, 0.000668866210617125, 0.0013883472420275211, 0.003526488086208701, 0.001990794437006116, 0.03566524758934975, 0.0071600042283535, 0.01712568663060665, 0.013542159460484982, 0.0, 0.0], [0.9802383184432983, 0.0005404870607890189, 0.00021263211965560913, 0.00042028730968013406, 0.001654899213463068, 0.0012096692807972431, 0.006987428292632103, 0.0006902841851115227, 0.003618576331064105, 0.002436508657410741, 0.0019909923430532217, 0.0], [0.9713584780693054, 0.0009448174387216568, 0.00022725843882653862, 0.00038950759335421026, 0.001187001820653677, 0.00038119233795441687, 0.0024588683154433966, 0.0018814575159922242, 0.005330737680196762, 0.003515726188197732, 0.0013166496064513922, 0.011008286848664284]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9714859127998352, 0.02851409837603569, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7714597582817078, 0.16485755145549774, 0.06368272751569748, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.43202266097068787, 0.3828534781932831, 0.16908831894397736, 0.016035521402955055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7320356965065002, 0.07854688167572021, 0.07717498391866684, 0.03506920859217644, 0.07717324793338776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5486602783203125, 0.1659742295742035, 0.04743988439440727, 0.023357348516583443, 0.19828234612941742, 0.016285870224237442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5462896227836609, 0.06450267136096954, 0.03986996412277222, 0.034310102462768555, 0.20795078575611115, 0.07331804931163788, 0.033758848905563354, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5813767313957214, 0.07858378440141678, 0.033334992825984955, 0.03861001133918762, 0.07638591527938843, 0.024287424981594086, 0.148990660905838, 0.018430490046739578, 0.0, 0.0, 0.0, 0.0], [0.30927038192749023, 0.1501440703868866, 0.06015915423631668, 0.0093722864985466, 0.051808591932058334, 0.01346260029822588, 0.09872028231620789, 0.2385048270225525, 0.06855779141187668, 0.0, 0.0, 0.0], [0.1855008751153946, 0.2092641144990921, 0.04240424931049347, 0.005843704100698233, 0.058655209839344025, 0.009105982258915901, 0.083842433989048, 0.12389946728944778, 0.22806136310100555, 0.05342263728380203, 0.0, 0.0], [0.11971992254257202, 0.09128701686859131, 0.015717461705207825, 0.00756356306374073, 0.0997047945857048, 0.008058776147663593, 0.10315125435590744, 0.038854099810123444, 0.2045592963695526, 0.2962348461151123, 0.015148972161114216, 0.0], [0.7949792146682739, 0.009946545585989952, 0.006263789255172014, 0.0017397315241396427, 0.018718477338552475, 0.0062827481888234615, 0.02008422650396824, 0.029990041628479958, 0.023665567860007286, 0.0272422693669796, 0.014818370342254639, 0.04626902565360069]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9870671629905701, 0.012932891957461834, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9198057055473328, 0.018702412024140358, 0.061491772532463074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7579038739204407, 0.055228304117918015, 0.1588001847267151, 0.02806766889989376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8816497325897217, 0.0175887793302536, 0.052515532821416855, 0.03854084759950638, 0.009705109521746635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8400552868843079, 0.034413453191518784, 0.03970469906926155, 0.05387880653142929, 0.02330288290977478, 0.008644955232739449, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7305130958557129, 0.044743768870830536, 0.08263319730758667, 0.07328136265277863, 0.03458539769053459, 0.022681565955281258, 0.011561652645468712, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6116317510604858, 0.0840664952993393, 0.12983722984790802, 0.07818383723497391, 0.01954570971429348, 0.007171822711825371, 0.009727444499731064, 0.059835683554410934, 0.0, 0.0, 0.0, 0.0], [0.2475050538778305, 0.0380217507481575, 0.0394602008163929, 0.03989731892943382, 0.03284277394413948, 0.016064301133155823, 0.03620562329888344, 0.47799092531204224, 0.07201205939054489, 0.0, 0.0, 0.0], [0.24498766660690308, 0.07575057446956635, 0.04206778481602669, 0.028315704315900803, 0.038078825920820236, 0.015312077477574348, 0.03579126298427582, 0.3479982018470764, 0.12932251393795013, 0.042375415563583374, 0.0, 0.0], [0.5384301543235779, 0.035014938563108444, 0.04010298103094101, 0.030297761783003807, 0.016980910673737526, 0.004667351488023996, 0.02412611059844494, 0.1397981345653534, 0.07583049684762955, 0.0633140578866005, 0.031437113881111145, 0.0], [0.5468704104423523, 0.022487999871373177, 0.02461211383342743, 0.015336631797254086, 0.013688808307051659, 0.007280681747943163, 0.012978747487068176, 0.07409809529781342, 0.09405390918254852, 0.10092570632696152, 0.05392022803425789, 0.033746641129255295]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.998475968837738, 0.0015240395441651344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9890474677085876, 0.0011630565859377384, 0.009789487347006798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9780538082122803, 0.00114974204916507, 0.0108914440497756, 0.009905070997774601, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9991136193275452, 0.00016781766316853464, 2.4993492843350396e-05, 4.5147869968786836e-05, 0.0006483724573627114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9740023612976074, 0.003699925495311618, 6.37277917121537e-05, 4.197729504085146e-05, 0.00028169419965706766, 0.02191031537950039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.972184419631958, 0.00031802969169802964, 0.0001716255210340023, 0.0001259989949176088, 0.0006395016680471599, 0.01291621383279562, 0.013644218444824219, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9384461641311646, 0.008473877795040607, 0.0015113122062757611, 0.002284188289195299, 0.0008860185625962913, 0.0004318671708460897, 0.004110751207917929, 0.04385572671890259, 0.0, 0.0, 0.0, 0.0], [0.9086377024650574, 0.006438672076910734, 0.013313245959579945, 0.00951423030346632, 0.020910777151584625, 0.0002960337733384222, 0.002096600830554962, 0.01380289625376463, 0.024989759549498558, 0.0, 0.0, 0.0], [0.9679588079452515, 0.0038848761469125748, 0.0020589090418070555, 0.0013070771237835288, 0.0012255168985575438, 3.7898360460530967e-05, 0.0004632784111890942, 0.009343606419861317, 0.011691833846271038, 0.0020281681790947914, 0.0, 0.0], [0.9945908784866333, 0.0018971646204590797, 0.0001239548291778192, 6.76795607432723e-05, 0.0012277030618861318, 1.7589940398465842e-05, 0.0002515133237466216, 0.0006825759191997349, 0.0002481763658579439, 0.0002158318820875138, 0.0006768791936337948, 0.0], [0.996960461139679, 0.0006087984656915069, 2.040335130004678e-05, 2.444620076857973e-05, 7.312429806916043e-05, 2.830147195709287e-06, 4.200232797302306e-05, 0.00010841114999493584, 0.00013490494166035205, 4.757601345772855e-06, 2.2736829123459756e-05, 0.001997155835852027]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9927821159362793, 0.007217843551188707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9867942929267883, 0.006222226656973362, 0.006983472499996424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9807507991790771, 0.006942576728761196, 0.004853390157222748, 0.007453189697116613, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.993164598941803, 0.00222657504491508, 0.0018698115600273013, 0.00124100293032825, 0.0014980491250753403, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9471458196640015, 0.03945197910070419, 0.005998448934406042, 0.0015308164292946458, 0.0024859642144292593, 0.003387050935998559, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9434520602226257, 0.007492748089134693, 0.007830358110368252, 0.009674347937107086, 0.004590350203216076, 0.0061397859826684, 0.020820287987589836, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9591125845909119, 0.0051765358075499535, 0.003453733865171671, 0.002607130678370595, 0.005264150910079479, 0.002685259561985731, 0.008658094331622124, 0.01304243691265583, 0.0, 0.0, 0.0, 0.0], [0.9122285842895508, 0.004526689648628235, 0.008149227127432823, 0.04828833043575287, 0.010022569447755814, 0.000761044560931623, 0.009623702615499496, 0.0027386071160435677, 0.0036611156538128853, 0.0, 0.0, 0.0], [0.8822763562202454, 0.0035955526400357485, 0.0032317922450602055, 0.06715413182973862, 0.011589575558900833, 0.0013740750728175044, 0.006143556442111731, 0.0027858156245201826, 0.004988240078091621, 0.016860835254192352, 0.0, 0.0], [0.9803778529167175, 0.0015811709454283118, 0.00047653718502260745, 0.0007894948939792812, 0.001486689900048077, 0.0014180223224684596, 0.004027754534035921, 0.00166410265956074, 0.0009349674801342189, 0.0016297998372465372, 0.0056136660277843475, 0.0], [0.9718797206878662, 0.0008234442211687565, 0.003486605593934655, 0.0017205806216225028, 0.0014565916499122977, 0.0013661220436915755, 0.00423714704811573, 0.008189242333173752, 0.0018339062808081508, 0.0016145295230671763, 0.0005657118163071573, 0.0028264191932976246]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9785429239273071, 0.021457064896821976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9299753308296204, 0.029141895473003387, 0.040882814675569534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9213064908981323, 0.01477731578052044, 0.03513401746749878, 0.028782153502106667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9211409091949463, 0.01596906967461109, 0.028594570234417915, 0.010056828148663044, 0.024238621816039085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9005007743835449, 0.013011328876018524, 0.025074420496821404, 0.00751542067155242, 0.017089715227484703, 0.03680823743343353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8979418873786926, 0.019481299445033073, 0.012876991182565689, 0.01026673149317503, 0.015676476061344147, 0.018259182572364807, 0.025497550144791603, 0.0, 0.0, 0.0, 0.0, 0.0], [0.646408200263977, 0.030610067769885063, 0.03615151718258858, 0.03396084904670715, 0.027119465172290802, 0.030329778790473938, 0.04156537353992462, 0.15385480225086212, 0.0, 0.0, 0.0, 0.0], [0.6649606823921204, 0.0157962366938591, 0.023659538477659225, 0.07570378482341766, 0.01390440110117197, 0.02083030715584755, 0.03877710551023483, 0.10322531312704086, 0.043142616748809814, 0.0, 0.0, 0.0], [0.6879334449768066, 0.014144712127745152, 0.03318830206990242, 0.038087327033281326, 0.011158139444887638, 0.032454513013362885, 0.03218429163098335, 0.08450433611869812, 0.026699692010879517, 0.03964526206254959, 0.0, 0.0], [0.7120550274848938, 0.008540334179997444, 0.03430625796318054, 0.034147027879953384, 0.004908512346446514, 0.012034259736537933, 0.019134439527988434, 0.02970195561647415, 0.011686685495078564, 0.034423407167196274, 0.09906210005283356, 0.0], [0.8991708755493164, 0.0255292821675539, 0.006440133322030306, 0.014237319119274616, 0.0042643132619559765, 0.007358605973422527, 0.01052772719413042, 0.011602901853621006, 0.002109092427417636, 0.004809824284166098, 0.002318130573257804, 0.01163184642791748]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9928945899009705, 0.007105353754013777, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9947165846824646, 0.0011583550367504358, 0.004125067498534918, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9893732666969299, 0.0017794754821807146, 0.0030499130953103304, 0.00579735217615962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9727770686149597, 0.002743737306445837, 0.0020542365964502096, 0.0031501161865890026, 0.019274864345788956, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9610493779182434, 0.014755818992853165, 0.0018420260166749358, 0.001140510430559516, 0.002411833731457591, 0.018800441175699234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9409165382385254, 0.0022103548981249332, 0.003270492423325777, 0.006869182456284761, 0.003678797045722604, 0.01210164837539196, 0.030952952802181244, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9517611861228943, 0.008474568836390972, 0.005591141059994698, 0.008169145323336124, 0.004486981779336929, 0.0008660368621349335, 0.0051094177179038525, 0.015541586093604565, 0.0, 0.0, 0.0, 0.0], [0.9406067132949829, 0.0054961615242064, 0.011726447381079197, 0.005837302654981613, 0.024391548708081245, 0.00041461552609689534, 0.0016759197460487485, 0.0038773114793002605, 0.0059740180149674416, 0.0, 0.0, 0.0], [0.9785146117210388, 0.0018249311251565814, 0.0010295179672539234, 0.0013972390443086624, 0.003634414402768016, 0.0002748271799646318, 0.0013667535968124866, 0.005658537615090609, 0.003659167792648077, 0.0026400587521493435, 0.0, 0.0], [0.9715367555618286, 0.000992300221696496, 0.0001720957807265222, 5.5647607950959355e-05, 0.003926153294742107, 0.000739394105039537, 0.0011099879629909992, 0.0017083350103348494, 0.0021713378373533487, 0.004881225526332855, 0.012706750072538853, 0.0], [0.9645591378211975, 0.0007455258164554834, 0.0011378362542018294, 0.0008240666356869042, 0.0007444082875736058, 0.0003435192920733243, 0.00045814213808625937, 0.01116601936519146, 0.0021155639551579952, 0.0014338258188217878, 0.0017822524532675743, 0.014689658768475056]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9833884239196777, 0.016611550003290176, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639711380004883, 0.007253073621541262, 0.02877575345337391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9331758618354797, 0.014997607097029686, 0.023817596957087517, 0.0280089620500803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9568120837211609, 0.009934096597135067, 0.01227604690939188, 0.016106655821204185, 0.004871029872447252, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9151563048362732, 0.032129593193531036, 0.014398870058357716, 0.01664332114160061, 0.00652395561337471, 0.015147935599088669, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7367724180221558, 0.021447639912366867, 0.0431126207113266, 0.08644845336675644, 0.009022037498652935, 0.022701524198055267, 0.08049540966749191, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7203764915466309, 0.022782059386372566, 0.026323888450860977, 0.06355153769254684, 0.005824857857078314, 0.0181754007935524, 0.09361011534929276, 0.04935562610626221, 0.0, 0.0, 0.0, 0.0], [0.7824261784553528, 0.010264112614095211, 0.014493719674646854, 0.1153576672077179, 0.004240964539349079, 0.014435202814638615, 0.03887070715427399, 0.010679810307919979, 0.009231519885361195, 0.0, 0.0, 0.0], [0.7738533020019531, 0.02246744930744171, 0.010654190555214882, 0.06270211189985275, 0.006174781825393438, 0.00859184842556715, 0.05814996734261513, 0.023065779358148575, 0.01593252457678318, 0.01840805821120739, 0.0, 0.0], [0.6581162810325623, 0.027781231328845024, 0.024232415482401848, 0.02947334013879299, 0.012357654049992561, 0.01235811598598957, 0.11365333944559097, 0.027893835678696632, 0.034340184181928635, 0.015996210277080536, 0.04379747435450554, 0.0], [0.8400260806083679, 0.0071548111736774445, 0.007588120177388191, 0.014718865044414997, 0.00522383488714695, 0.01866665668785572, 0.024050846695899963, 0.03681429475545883, 0.010391833260655403, 0.015045713633298874, 0.010403802618384361, 0.009915195405483246]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9864231944084167, 0.01357682328671217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9881282448768616, 0.00620835367590189, 0.005663427524268627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9853056073188782, 0.007153799291700125, 0.005347858183085918, 0.002192764077335596, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9797539114952087, 0.011222547851502895, 0.003685934003442526, 0.0010325682815164328, 0.004304980393499136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9682632684707642, 0.006303965579718351, 0.003172907279804349, 0.0008920436957851052, 0.003028686624020338, 0.01833914965391159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.809746265411377, 0.012085998430848122, 0.016362104564905167, 0.014269798062741756, 0.01812109351158142, 0.01189999096095562, 0.11751478910446167, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5867733359336853, 0.10182203352451324, 0.08833210915327072, 0.02607930265367031, 0.051862601190805435, 0.008960126899182796, 0.024406209588050842, 0.11176424473524094, 0.0, 0.0, 0.0, 0.0], [0.8303860425949097, 0.02753928303718567, 0.02247094176709652, 0.007505586836487055, 0.04472466930747032, 0.012498680502176285, 0.008127537555992603, 0.03845059871673584, 0.00829662848263979, 0.0, 0.0, 0.0], [0.8181108236312866, 0.06121882051229477, 0.021364646032452583, 0.005729142110794783, 0.01924677938222885, 0.012479309923946857, 0.005239915102720261, 0.041441041976213455, 0.009340645745396614, 0.005828950088471174, 0.0, 0.0], [0.894373893737793, 0.024768099188804626, 0.00910963211208582, 0.0011010831221938133, 0.004504394251853228, 0.002647736808285117, 0.0035257632844150066, 0.03519165515899658, 0.004848766140639782, 0.0026703577022999525, 0.01725870557129383, 0.0], [0.8591271042823792, 0.018691103905439377, 0.005416382569819689, 0.003999772015959024, 0.006000928580760956, 0.0027783813420683146, 0.01660899445414543, 0.04342593997716904, 0.005977710243314505, 0.00955294631421566, 0.008975571021437645, 0.019445113837718964]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9545314908027649, 0.045468490570783615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9539321064949036, 0.016750236973166466, 0.029317593201994896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7156912684440613, 0.06119076907634735, 0.19639764726161957, 0.0267203189432621, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9345943927764893, 0.0059928884729743, 0.014621291309595108, 0.026703910902142525, 0.018087441101670265, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7205244302749634, 0.015987621620297432, 0.013564241118729115, 0.041574105620384216, 0.19512180984020233, 0.013227792456746101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5296592116355896, 0.006609749980270863, 0.0326666496694088, 0.16582703590393066, 0.1074318140745163, 0.07241208851337433, 0.08539342135190964, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8330578804016113, 0.01420960295945406, 0.01148221641778946, 0.020361943170428276, 0.005557612981647253, 0.003711639204993844, 0.02928256429731846, 0.08233655989170074, 0.0, 0.0, 0.0, 0.0], [0.265632301568985, 0.010740576311945915, 0.008531873114407063, 0.0034803119488060474, 0.0020550708286464214, 0.0047296639531850815, 0.020066244527697563, 0.6132968664169312, 0.07146716117858887, 0.0, 0.0, 0.0], [0.23288007080554962, 0.01097123697400093, 0.01081137452274561, 0.007107522338628769, 0.003995854407548904, 0.006604264490306377, 0.02285950817167759, 0.48819786310195923, 0.17384830117225647, 0.042723994702100754, 0.0, 0.0], [0.5622583627700806, 0.004619690123945475, 0.0027602692134678364, 0.004669429734349251, 0.004548975266516209, 0.002609629649668932, 0.02168228290975094, 0.09853576123714447, 0.09816978871822357, 0.12446600198745728, 0.07567983120679855, 0.0], [0.5833532214164734, 0.0008391177398152649, 0.0025829472579061985, 0.001092777936719358, 0.004134505055844784, 0.0013618459925055504, 0.005708558950573206, 0.012731814756989479, 0.004722298122942448, 0.014649002812802792, 0.3487206995487213, 0.020103102549910545]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9599244594573975, 0.04007549583911896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9154186248779297, 0.0375947579741478, 0.04698658362030983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4553532004356384, 0.2755279839038849, 0.24509987235069275, 0.024018865078687668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2997068464756012, 0.11262737959623337, 0.2055726796388626, 0.3367130160331726, 0.04538007825613022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4944990575313568, 0.08293410390615463, 0.05914106220006943, 0.13579906523227692, 0.2080155909061432, 0.01961112953722477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2755037844181061, 0.0913882851600647, 0.11712999641895294, 0.29172468185424805, 0.13165025413036346, 0.04027045518159866, 0.0523325651884079, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5252630710601807, 0.1088501513004303, 0.123881496489048, 0.07995685189962387, 0.023757778108119965, 0.009376068599522114, 0.04533025994896889, 0.08358435332775116, 0.0, 0.0, 0.0, 0.0], [0.3491998314857483, 0.1478748470544815, 0.08351965248584747, 0.024971123784780502, 0.017809299752116203, 0.019113343209028244, 0.03939227759838104, 0.24330924451351166, 0.07481036335229874, 0.0, 0.0, 0.0], [0.18752792477607727, 0.1227913573384285, 0.055444661527872086, 0.03543190285563469, 0.02185727097094059, 0.012605033814907074, 0.03657497838139534, 0.26245415210723877, 0.22619396448135376, 0.039118800312280655, 0.0, 0.0], [0.23279622197151184, 0.18157947063446045, 0.07080518454313278, 0.01589854061603546, 0.01990116760134697, 0.010073570534586906, 0.03133144974708557, 0.19689248502254486, 0.13680054247379303, 0.09065612405538559, 0.013265252113342285, 0.0], [0.27414047718048096, 0.07588503509759903, 0.07546238601207733, 0.05332941189408302, 0.02539697289466858, 0.009483867324888706, 0.026718733832240105, 0.24128170311450958, 0.09875675290822983, 0.08951166272163391, 0.018554605543613434, 0.011478336527943611]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9641018509864807, 0.03589806705713272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9710012674331665, 0.010919103398919106, 0.018079550936818123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9170807003974915, 0.01946580782532692, 0.036353662610054016, 0.027099765837192535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9346190690994263, 0.01826765388250351, 0.007908038794994354, 0.01692192442715168, 0.022283250465989113, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9299283027648926, 0.03847460076212883, 0.003714577993378043, 0.005441469140350819, 0.010758050717413425, 0.011683084070682526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9585739374160767, 0.006481367163360119, 0.0027810425963252783, 0.007383889984339476, 0.006073185242712498, 0.00627536978572607, 0.012431229464709759, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8795750141143799, 0.029372479766607285, 0.01352210994809866, 0.01707076095044613, 0.008944825269281864, 0.017656443640589714, 0.013178383931517601, 0.020679980516433716, 0.0, 0.0, 0.0, 0.0], [0.7964720726013184, 0.009919540956616402, 0.007821394130587578, 0.010309536941349506, 0.01952836662530899, 0.03185391426086426, 0.012048512697219849, 0.08115827292203903, 0.03088843636214733, 0.0, 0.0, 0.0], [0.8709201812744141, 0.013776390813291073, 0.002330548595637083, 0.005582301411777735, 0.006572830490767956, 0.006913002114742994, 0.009338192641735077, 0.043526969850063324, 0.03524934872984886, 0.005790200550109148, 0.0, 0.0], [0.9654254913330078, 0.0028783672023564577, 0.0004976456402800977, 0.00031805047183297575, 0.0008818897185847163, 0.0030536502599716187, 0.0018710183212533593, 0.008011894300580025, 0.010718916542828083, 0.0036347401328384876, 0.0027082685846835375, 0.0], [0.9775938987731934, 0.0008234509150497615, 0.0006108650704845786, 0.0015767636941745877, 0.000458991969935596, 0.000868999573867768, 0.0008578190463595092, 0.005705096758902073, 0.0025221470277756453, 0.004016389604657888, 0.0029377362225204706, 0.002027790993452072]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9344093799591064, 0.06559056788682938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.913509726524353, 0.04982312023639679, 0.03666726127266884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.580337643623352, 0.23110836744308472, 0.1691732257604599, 0.019380683079361916, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7784227132797241, 0.010185912251472473, 0.06038329750299454, 0.06886337697505951, 0.08214464038610458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2429116815328598, 0.003962585236877203, 0.014769071713089943, 0.013769702054560184, 0.6829657554626465, 0.04162114858627319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6121495962142944, 0.011295163072645664, 0.024656226858496666, 0.02845974639058113, 0.15529166162014008, 0.1426578313112259, 0.025489801540970802, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9288339614868164, 0.011457913555204868, 0.004063802771270275, 0.001387398922815919, 0.0065957591868937016, 0.0019361911108717322, 0.005480026360601187, 0.04024500399827957, 0.0, 0.0, 0.0, 0.0], [0.7609086036682129, 0.00886379275470972, 0.003322234842926264, 0.0007435657316818833, 0.00332867749966681, 0.004057576879858971, 0.0034779254347085953, 0.18932844698429108, 0.025969181209802628, 0.0, 0.0, 0.0], [0.35696181654930115, 0.034133121371269226, 0.009367011487483978, 0.0016982831293717027, 0.0028338548727333546, 0.0019764259923249483, 0.004862355068325996, 0.16966365277767181, 0.34257078170776367, 0.07593267410993576, 0.0, 0.0], [0.19800199568271637, 0.025814514607191086, 0.006498319562524557, 0.0033900009002536535, 0.0005726346862502396, 0.000925157277379185, 0.0019261229317635298, 0.043951790779829025, 0.26470479369163513, 0.4272697865962982, 0.0269448421895504, 0.0], [0.4818594753742218, 0.0014963062712922692, 0.0009083296754397452, 0.00164273870177567, 0.0010060446802526712, 0.0008963003638200462, 0.001464467728510499, 0.03045981377363205, 0.06667499244213104, 0.10841517895460129, 0.30026134848594666, 0.004915055353194475]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9454092979431152, 0.05459076538681984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9455482363700867, 0.02960694581270218, 0.024844761937856674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7437288165092468, 0.10178420692682266, 0.10946114361286163, 0.045025791972875595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7934507131576538, 0.07622075080871582, 0.03350523114204407, 0.01983577013015747, 0.07698751986026764, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6495540738105774, 0.1336071491241455, 0.04275063797831535, 0.012052206322550774, 0.11466038972139359, 0.047375574707984924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7477673292160034, 0.03079468570649624, 0.011984283104538918, 0.013767553493380547, 0.038310546427965164, 0.01868578791618347, 0.1386898159980774, 0.0, 0.0, 0.0, 0.0, 0.0], [0.568367063999176, 0.1111936867237091, 0.030141862109303474, 0.024809127673506737, 0.017061498016119003, 0.012839935719966888, 0.1675516813993454, 0.06803508847951889, 0.0, 0.0, 0.0, 0.0], [0.463496595621109, 0.08066706359386444, 0.026486407965421677, 0.030326571315526962, 0.030952638015151024, 0.018361134454607964, 0.18911153078079224, 0.14585180580615997, 0.0147462859749794, 0.0, 0.0, 0.0], [0.2627663016319275, 0.10178075730800629, 0.023230552673339844, 0.013717129826545715, 0.06784432381391525, 0.029142923653125763, 0.3110882639884949, 0.14705567061901093, 0.022244855761528015, 0.02112925983965397, 0.0, 0.0], [0.2876681387424469, 0.09234239161014557, 0.02158532850444317, 0.0065733869560062885, 0.06516952067613602, 0.03599430248141289, 0.15227191150188446, 0.153856560587883, 0.06438586860895157, 0.09089946746826172, 0.029253121465444565, 0.0], [0.6103418469429016, 0.017316823825240135, 0.012136313132941723, 0.0071127088740468025, 0.014577041380107403, 0.005064326338469982, 0.08169989287853241, 0.0944306030869484, 0.014853653497993946, 0.022790325805544853, 0.021932747215032578, 0.09774366766214371]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9060660600662231, 0.09393386542797089, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9454213380813599, 0.029138846322894096, 0.025439882650971413, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9105293154716492, 0.031800832599401474, 0.019080108031630516, 0.03858979046344757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8922500014305115, 0.045301809906959534, 0.0068474686704576015, 0.007289707660675049, 0.04831111058592796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7288694381713867, 0.08836505562067032, 0.010482770390808582, 0.00817499402910471, 0.10777655988931656, 0.05633119121193886, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7346106171607971, 0.01621781848371029, 0.004986548330634832, 0.010838131420314312, 0.05239303037524223, 0.0216192826628685, 0.1593346744775772, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7248605489730835, 0.040237072855234146, 0.013126091100275517, 0.01843258924782276, 0.02800922468304634, 0.01425946969538927, 0.10032494366168976, 0.06075011193752289, 0.0, 0.0, 0.0, 0.0], [0.78936368227005, 0.030171556398272514, 0.006135299801826477, 0.016033673658967018, 0.026445290073752403, 0.009294361807405949, 0.060431599617004395, 0.042378980666399, 0.019745569676160812, 0.0, 0.0, 0.0], [0.727250337600708, 0.05443266034126282, 0.007885356433689594, 0.013156160712242126, 0.038205329328775406, 0.01150230597704649, 0.06820333003997803, 0.03225696086883545, 0.03033139929175377, 0.016776176169514656, 0.0, 0.0], [0.8617791533470154, 0.026595894247293472, 0.0027280678041279316, 0.002177381655201316, 0.033858299255371094, 0.010502438992261887, 0.037758685648441315, 0.01050114631652832, 0.006501533556729555, 0.005558961536735296, 0.002038508653640747, 0.0], [0.9008129239082336, 0.01155546959489584, 0.0014615850523114204, 0.0018201622879132628, 0.0068506342358887196, 0.0038335747085511684, 0.022935697808861732, 0.008171186782419682, 0.00401699636131525, 0.002819227520376444, 0.002851220779120922, 0.03287138789892197]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9711552858352661, 0.028844796121120453, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9400196075439453, 0.024724081158638, 0.035256341099739075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9291672110557556, 0.025177214294672012, 0.013872582465410233, 0.03178299963474274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9021143317222595, 0.04454120621085167, 0.004322773311287165, 0.009370722807943821, 0.039650920778512955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6811766624450684, 0.07174920290708542, 0.0035815262235701084, 0.0054411194287240505, 0.05967748537659645, 0.1783740520477295, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7336689829826355, 0.024708159267902374, 0.012755402363836765, 0.020908746868371964, 0.04402637109160423, 0.039308443665504456, 0.12462390214204788, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6342623829841614, 0.062019720673561096, 0.037523139268159866, 0.048569437116384506, 0.0436222068965435, 0.026331208646297455, 0.042912039905786514, 0.10475988686084747, 0.0, 0.0, 0.0, 0.0], [0.553666353225708, 0.0407608300447464, 0.038915377110242844, 0.035027530044317245, 0.11942419409751892, 0.028515340760350227, 0.02924085222184658, 0.08812159299850464, 0.06632786989212036, 0.0, 0.0, 0.0], [0.688938319683075, 0.05334436148405075, 0.01750391535460949, 0.014164717867970467, 0.0554332360625267, 0.01891702599823475, 0.026686951518058777, 0.05804955214262009, 0.03254781290888786, 0.03441402316093445, 0.0, 0.0], [0.7876285314559937, 0.039235543459653854, 0.006634343881160021, 0.005334618967026472, 0.014063711278140545, 0.012711395509541035, 0.03339666873216629, 0.029827270656824112, 0.022040268406271935, 0.015042371116578579, 0.03408533334732056, 0.0], [0.6718378663063049, 0.03505013883113861, 0.017613409087061882, 0.02034279704093933, 0.010089348070323467, 0.0067496150732040405, 0.02397306263446808, 0.07301551848649979, 0.04475362226366997, 0.031122377142310143, 0.018296103924512863, 0.04715625196695328]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9822845458984375, 0.017715459689497948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9900949001312256, 0.005737411789596081, 0.004167679697275162, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9839543104171753, 0.007681929040700197, 0.005227251909673214, 0.003136551473289728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9658727645874023, 0.017640473321080208, 0.003830207046121359, 0.00324417557567358, 0.009412380866706371, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8915450572967529, 0.051525432616472244, 0.006071369629353285, 0.0035812489222735167, 0.03287164866924286, 0.014405152760446072, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9218969941139221, 0.030399978160858154, 0.00790223479270935, 0.005038347095251083, 0.007177002262324095, 0.006294097751379013, 0.021291334182024002, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8118323683738708, 0.09103389829397202, 0.0319582037627697, 0.014074580743908882, 0.010015168227255344, 0.006937830708920956, 0.025586619973182678, 0.00856128241866827, 0.0, 0.0, 0.0, 0.0], [0.922038733959198, 0.02226976864039898, 0.018516307696700096, 0.012293116189539433, 0.005450281780213118, 0.0009698097710497677, 0.004299158696085215, 0.008207753300666809, 0.0059549519792199135, 0.0, 0.0, 0.0], [0.9112234115600586, 0.03702341765165329, 0.010586915537714958, 0.006676358170807362, 0.004719773773103952, 0.00241417670622468, 0.007074103225022554, 0.010462512262165546, 0.005340177100151777, 0.004479105118662119, 0.0, 0.0], [0.8715102672576904, 0.04508206620812416, 0.01316449511796236, 0.013952177949249744, 0.006064269691705704, 0.005258624441921711, 0.009051769971847534, 0.01715582236647606, 0.007955907844007015, 0.005622462369501591, 0.0051820361986756325, 0.0], [0.8859798908233643, 0.03400805592536926, 0.009713098406791687, 0.005235296208411455, 0.0022360130678862333, 0.0030401251278817654, 0.011765825562179089, 0.01835695281624794, 0.01226118952035904, 0.00515034468844533, 0.004004599992185831, 0.008248699828982353]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9083573222160339, 0.09164265543222427, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9380859732627869, 0.049213163554668427, 0.012700850144028664, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47146764397621155, 0.4680642783641815, 0.04094937443733215, 0.019518638029694557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8040472269058228, 0.12471917271614075, 0.008773540146648884, 0.009016537107527256, 0.05344346538186073, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7611523270606995, 0.15308216214179993, 0.006050650961697102, 0.00495164655148983, 0.04854501411318779, 0.026218228042125702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9379024505615234, 0.02704652212560177, 0.0018861687276512384, 0.0014925767900422215, 0.009963126853108406, 0.003181064734235406, 0.01852814108133316, 0.0, 0.0, 0.0, 0.0, 0.0], [0.795678436756134, 0.13805285096168518, 0.007810178212821484, 0.010206769220530987, 0.009115113876760006, 0.002196220913901925, 0.03175688534975052, 0.005183548666536808, 0.0, 0.0, 0.0, 0.0], [0.37061867117881775, 0.3573296368122101, 0.038041502237319946, 0.023555709049105644, 0.04372108727693558, 0.00560423918068409, 0.044380877166986465, 0.04077915474772453, 0.07596917450428009, 0.0, 0.0, 0.0], [0.2718384861946106, 0.5597829818725586, 0.013138820417225361, 0.01897973008453846, 0.021633068099617958, 0.0027037200052291155, 0.023393990471959114, 0.006207783706486225, 0.06513436883687973, 0.01718704216182232, 0.0, 0.0], [0.2894097566604614, 0.5015140771865845, 0.007352239917963743, 0.011384407058358192, 0.04323912784457207, 0.003372520674020052, 0.01933472976088524, 0.007036629132926464, 0.08157788962125778, 0.029770083725452423, 0.0060084969736635685, 0.0], [0.8378651738166809, 0.10098542273044586, 0.006563683971762657, 0.006245762575417757, 0.004992939997464418, 0.0006077795405872166, 0.01177496463060379, 0.004317070823162794, 0.007876033894717693, 0.0034802593290805817, 0.0041266800835728645, 0.011164212599396706]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9978633522987366, 0.002136687748134136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9936667084693909, 0.00111295934766531, 0.005220279563218355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9842586517333984, 0.0006363429129123688, 0.0019509481498971581, 0.013154077343642712, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9958784580230713, 0.0009725477430038154, 0.0003438743588048965, 0.0015586138470098376, 0.0012464041355997324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9910929799079895, 0.0017069658497348428, 0.0006144698709249496, 0.0011659094598144293, 0.0006327943992801011, 0.004786900244653225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9694110155105591, 0.0010654854122549295, 0.0028117725159972906, 0.013037307187914848, 0.00183580769225955, 0.004566478542983532, 0.007272124756127596, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9638490676879883, 0.002266687573865056, 0.0036652972921729088, 0.012074706144630909, 0.000817091902717948, 0.0032485960982739925, 0.0050157285295426846, 0.009062903933227062, 0.0, 0.0, 0.0, 0.0], [0.8921554684638977, 0.0011151746148243546, 0.04414283111691475, 0.05002937465906143, 0.0020383328665047884, 0.0008066057926043868, 0.002532707992941141, 0.003100438043475151, 0.00407894654199481, 0.0, 0.0, 0.0], [0.9638035893440247, 0.0015218525659292936, 0.002019214443862438, 0.014751988463103771, 0.0010176032083109021, 0.0006294742343015969, 0.003951151389628649, 0.004021752160042524, 0.0021218282636255026, 0.006161483936011791, 0.0, 0.0], [0.9906448125839233, 0.000569879834074527, 0.00015169235120993108, 0.00033012768835760653, 0.0005488651222549379, 0.0006114195566624403, 0.003713885322213173, 0.0002544716408010572, 0.00021875425591133535, 0.000473485590191558, 0.002482580253854394, 0.0], [0.9286215305328369, 0.00080977997276932, 0.0011899578385055065, 0.0037803007289767265, 0.0005017361254431307, 0.0011428705183789134, 0.0026029269210994244, 0.030657483264803886, 0.004102746024727821, 0.0031678960658609867, 0.015069478191435337, 0.008353290148079395]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8719334006309509, 0.12806659936904907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9812983274459839, 0.013914922252297401, 0.004786768462508917, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8051321506500244, 0.11729614436626434, 0.03648073598742485, 0.04109096899628639, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8084105849266052, 0.06693415343761444, 0.0032275309786200523, 0.012143696658313274, 0.10928407311439514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.774820864200592, 0.1098903939127922, 0.002049421425908804, 0.006369747221469879, 0.0663008987903595, 0.040568672120571136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8524223566055298, 0.02982816845178604, 0.0038805764634162188, 0.004693250171840191, 0.03846175596117973, 0.02717328630387783, 0.04354069009423256, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7842097878456116, 0.09376075118780136, 0.008352802135050297, 0.012617487460374832, 0.029124675318598747, 0.008978815749287605, 0.04339942708611488, 0.019556211307644844, 0.0, 0.0, 0.0, 0.0], [0.5212060809135437, 0.07670338451862335, 0.017001362517476082, 0.04129963368177414, 0.051241278648376465, 0.053196411579847336, 0.07913165539503098, 0.08908680826425552, 0.07113347202539444, 0.0, 0.0, 0.0], [0.3241563141345978, 0.17583559453487396, 0.019685788080096245, 0.05444914102554321, 0.06105230003595352, 0.023203469812870026, 0.09745614975690842, 0.05509212613105774, 0.09995582699775696, 0.08911324292421341, 0.0, 0.0], [0.4384008049964905, 0.19438093900680542, 0.01267324946820736, 0.028125805780291557, 0.06968530267477036, 0.015321681275963783, 0.07065600156784058, 0.02817959524691105, 0.07134325057268143, 0.050140898674726486, 0.021092411130666733, 0.0], [0.8714165091514587, 0.015842583030462265, 0.0023964911233633757, 0.0020063251722604036, 0.003373374929651618, 0.002911924384534359, 0.014991074800491333, 0.004303625784814358, 0.005433930084109306, 0.006405934691429138, 0.007156007457524538, 0.06376238167285919]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8568388223648071, 0.14316119253635406, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9193951487541199, 0.06870472431182861, 0.011900211684405804, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3993164002895355, 0.2779962420463562, 0.27255135774612427, 0.0501360185444355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.464661180973053, 0.05748578533530235, 0.168361634016037, 0.23778514564037323, 0.07170628756284714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2864910364151001, 0.02554885670542717, 0.11880244314670563, 0.13737311959266663, 0.3847564458847046, 0.04702811688184738, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5826156139373779, 0.04524430260062218, 0.08079396933317184, 0.11601204425096512, 0.06335814297199249, 0.08266730606555939, 0.02930864877998829, 0.0, 0.0, 0.0, 0.0, 0.0], [0.621957004070282, 0.03248189762234688, 0.09221664816141129, 0.023237692192196846, 0.006337635684758425, 0.004054639954119921, 0.1763744354248047, 0.04334011301398277, 0.0, 0.0, 0.0, 0.0], [0.21464376151561737, 0.08292514830827713, 0.06811226159334183, 0.04289105534553528, 0.009749014861881733, 0.006623787805438042, 0.15111719071865082, 0.34132760763168335, 0.08261016756296158, 0.0, 0.0, 0.0], [0.175147145986557, 0.05236845090985298, 0.023056963458657265, 0.03884847089648247, 0.015489337034523487, 0.014504949562251568, 0.1915929913520813, 0.1825890839099884, 0.23250852525234222, 0.07389406859874725, 0.0, 0.0], [0.27382057905197144, 0.03575701639056206, 0.0201625544577837, 0.016249284148216248, 0.007337996736168861, 0.004205544479191303, 0.16317179799079895, 0.08581888675689697, 0.14654229581356049, 0.21911534667015076, 0.02781870774924755, 0.0], [0.42977839708328247, 0.008929247036576271, 0.015465420670807362, 0.01951991394162178, 0.005024416837841272, 0.006255012471228838, 0.006588140968233347, 0.06622061878442764, 0.06647183746099472, 0.16748544573783875, 0.19983278214931488, 0.008428767323493958]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9911783337593079, 0.008821694180369377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9902048110961914, 0.005291587673127651, 0.0045036510564386845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9829759001731873, 0.007564705330878496, 0.0053575411438941956, 0.004101893398910761, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9868907332420349, 0.0013774647377431393, 0.001133812707848847, 0.0020211460068821907, 0.008576859720051289, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9826126098632812, 0.00105368928052485, 0.002848650561645627, 0.0017389522399753332, 0.008973107673227787, 0.002772937063127756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9663691520690918, 0.0049200826324522495, 0.0037204334512352943, 0.0021413606591522694, 0.004788492806255817, 0.002524899085983634, 0.015535526908934116, 0.0, 0.0, 0.0, 0.0, 0.0], [0.859234631061554, 0.030250336974859238, 0.022093260660767555, 0.021275794133543968, 0.00795057788491249, 0.0018679768545553088, 0.011464658193290234, 0.045862723141908646, 0.0, 0.0, 0.0, 0.0], [0.9502668380737305, 0.01678713597357273, 0.0040345704182982445, 0.0039037347305566072, 0.0017617355333641171, 0.002332107163965702, 0.0011692644329741597, 0.015074377879500389, 0.004670233931392431, 0.0, 0.0, 0.0], [0.9573739171028137, 0.023412372916936874, 0.0020914736669510603, 0.0022509703412652016, 0.0005541950813494623, 0.0009293589973822236, 0.0008928016759455204, 0.006361812818795443, 0.004439128562808037, 0.0016939443303272128, 0.0, 0.0], [0.9823766946792603, 0.012010788545012474, 0.001009209081530571, 0.000939368037506938, 0.00024436909006908536, 0.00026473094476386905, 0.0004882697539869696, 0.001088366494514048, 0.000725705991499126, 0.00028795996331609786, 0.0005645057535730302, 0.0], [0.9285996556282043, 0.008462183177471161, 0.0004960157093591988, 0.00287606636993587, 0.0005353108863346279, 0.0013386410428211093, 0.0015953679103404284, 0.009122012183070183, 0.003001635195687413, 0.0013374824775382876, 0.0018174820579588413, 0.04081810265779495]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9067530035972595, 0.09324691444635391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9309826493263245, 0.04504658654332161, 0.023970741778612137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9092339873313904, 0.04110626131296158, 0.013500199653208256, 0.03615960106253624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7281203866004944, 0.1462588757276535, 0.015973569825291634, 0.06325114518404007, 0.04639606177806854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7857105731964111, 0.08647371083498001, 0.005242717918008566, 0.018110303208231926, 0.044252924621105194, 0.06020974740386009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8466817736625671, 0.03605351969599724, 0.02203640714287758, 0.03463821858167648, 0.016753247007727623, 0.01494639553129673, 0.02889041043817997, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7086648941040039, 0.13225552439689636, 0.02823363058269024, 0.05346347764134407, 0.017867907881736755, 0.003718329593539238, 0.028196560218930244, 0.027599753811955452, 0.0, 0.0, 0.0, 0.0], [0.6521582007408142, 0.08188103139400482, 0.047886110842227936, 0.07303404062986374, 0.021332012489438057, 0.0027003581635653973, 0.0123111791908741, 0.03877689689397812, 0.06992021948099136, 0.0, 0.0, 0.0], [0.6676712036132812, 0.11749421060085297, 0.015705877915024757, 0.02451998181641102, 0.04135364294052124, 0.00487519009038806, 0.02778851054608822, 0.03070817142724991, 0.0445249117910862, 0.025358354672789574, 0.0, 0.0], [0.6953609585762024, 0.11288509517908096, 0.012430076487362385, 0.012638640590012074, 0.026928329840302467, 0.007906547747552395, 0.0275066327303648, 0.02850743755698204, 0.035835150629282, 0.01347059290856123, 0.026530414819717407, 0.0], [0.7998102903366089, 0.014226152561604977, 0.014012276194989681, 0.007453899830579758, 0.0027770567685365677, 0.0039000592660158873, 0.010952061973512173, 0.028943652287125587, 0.03049728274345398, 0.01820848509669304, 0.006345365196466446, 0.0628734827041626]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9880555272102356, 0.011944525875151157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9964759945869446, 0.0006061861640773714, 0.002917829668149352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9930844306945801, 0.001490541733801365, 0.003033445216715336, 0.0023915856145322323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9914074540138245, 0.00431450130417943, 0.0002573828387539834, 0.0010034998413175344, 0.0030171105172485113, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9740700125694275, 0.022409625351428986, 0.00018698530038818717, 0.00022328770137391984, 0.0017039516242220998, 0.0014062441186979413, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9795630574226379, 0.004615123383700848, 0.001092405873350799, 0.0007387528312392533, 0.00217645475640893, 0.0004575460043270141, 0.011356688104569912, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9588999152183533, 0.017958827316761017, 0.0009885195177048445, 0.001330448780208826, 0.003253326751291752, 0.0014409273862838745, 0.015131156891584396, 0.0009969016537070274, 0.0, 0.0, 0.0, 0.0], [0.9281482100486755, 0.004542018286883831, 0.01224128995090723, 0.007568803615868092, 0.026998482644557953, 0.0007288061315193772, 0.012489157728850842, 0.004019512329250574, 0.003263820894062519, 0.0, 0.0, 0.0], [0.9626762866973877, 0.01584896259009838, 0.0022223410196602345, 0.0014076919760555029, 0.008332472294569016, 0.00027121708262711763, 0.005194467026740313, 0.001047226251102984, 0.001561095705255866, 0.0014382838271558285, 0.0, 0.0], [0.9750708937644958, 0.010242724791169167, 0.0006991885602474213, 0.0002682592021301389, 0.0048714312724769115, 0.0002459019306115806, 0.0042515648528933525, 0.0006689619622193277, 0.00048618149594403803, 0.00047707444173283875, 0.0027177685406059027, 0.0], [0.9490756392478943, 0.009783460758626461, 0.001052114530466497, 0.0006409459165297449, 0.001741774845868349, 0.00030809841700829566, 0.008974538184702396, 0.007839265279471874, 0.0007630852051079273, 0.0005373173044063151, 0.000681112171150744, 0.018602663651108742]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9642065763473511, 0.03579336404800415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9825668931007385, 0.011166771873831749, 0.0062662530690431595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9261529445648193, 0.03268115967512131, 0.02121279388666153, 0.019953105598688126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.882043182849884, 0.06571570038795471, 0.011560861021280289, 0.019639695063233376, 0.02104063890874386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8616356253623962, 0.08733692020177841, 0.006380805745720863, 0.02222723513841629, 0.007759251166135073, 0.01466013677418232, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7066667675971985, 0.0514804869890213, 0.05588319152593613, 0.06063214689493179, 0.028761539608240128, 0.02982599101960659, 0.06674986332654953, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3600865602493286, 0.18019723892211914, 0.08328453451395035, 0.1453971415758133, 0.09309445321559906, 0.02139112539589405, 0.05785462260246277, 0.05869432911276817, 0.0, 0.0, 0.0, 0.0], [0.5882869958877563, 0.026431219652295113, 0.05022485926747322, 0.11200159788131714, 0.05610451102256775, 0.021694857627153397, 0.05218958482146263, 0.06584056466817856, 0.027225814759731293, 0.0, 0.0, 0.0], [0.6086090207099915, 0.07490181922912598, 0.015258785337209702, 0.0643756166100502, 0.05986897274851799, 0.018340755254030228, 0.06025011092424393, 0.042734187096357346, 0.030566731467843056, 0.025093980133533478, 0.0, 0.0], [0.8226079344749451, 0.06567767262458801, 0.0035248040221631527, 0.008106512017548084, 0.01875246688723564, 0.008733488619327545, 0.03077225387096405, 0.01780271902680397, 0.011922917328774929, 0.005528751295059919, 0.006570399273186922, 0.0], [0.6578125357627869, 0.0259100291877985, 0.03159765899181366, 0.03920446336269379, 0.020004183053970337, 0.013009735383093357, 0.04814480245113373, 0.057208605110645294, 0.031058456748723984, 0.036910030990839005, 0.024095509201288223, 0.015044021420180798]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9685986638069153, 0.03140135109424591, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9682132005691528, 0.025465765967965126, 0.006320956163108349, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9695953130722046, 0.024472501128911972, 0.0023836418986320496, 0.0035486400593072176, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9326664209365845, 0.048802752047777176, 0.006895655766129494, 0.004397108219563961, 0.007238076068460941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7093943357467651, 0.23864763975143433, 0.006291310302913189, 0.008692820556461811, 0.028195081278681755, 0.0087787676602602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.934461236000061, 0.01701609045267105, 0.0028434977866709232, 0.002343429485335946, 0.004842768423259258, 0.0026667683850973845, 0.03582623973488808, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8699634671211243, 0.03643565997481346, 0.005331853404641151, 0.006597083527594805, 0.010304955765604973, 0.007869871333241463, 0.0551215261220932, 0.008375569246709347, 0.0, 0.0, 0.0, 0.0], [0.9032288789749146, 0.03687676787376404, 0.007557252887636423, 0.00934548955410719, 0.005882586818188429, 0.0024084127508103848, 0.0151184918358922, 0.01426418125629425, 0.0053178658708930016, 0.0, 0.0, 0.0], [0.846503734588623, 0.06801556050777435, 0.004245505202561617, 0.007141346577554941, 0.007739260792732239, 0.003457383019849658, 0.0424082987010479, 0.01159694604575634, 0.0057854317128658295, 0.0031064837239682674, 0.0, 0.0], [0.8873735070228577, 0.06260819733142853, 0.002867533592507243, 0.0029608593322336674, 0.010398749262094498, 0.0030290542636066675, 0.01529025286436081, 0.006388470064848661, 0.0047644940204918385, 0.0028083587531000376, 0.0015105652855709195, 0.0], [0.8902162909507751, 0.03301147371530533, 0.00618304219096899, 0.013072359375655651, 0.006853392347693443, 0.004375688266009092, 0.016710350289940834, 0.007010468281805515, 0.0029177390970289707, 0.0060821776278316975, 0.004382497631013393, 0.009184484370052814]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.994461178779602, 0.005538840312510729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9986999034881592, 0.00046824259334243834, 0.0008318868349306285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9963828325271606, 0.0009283061954192817, 0.001546939485706389, 0.0011418551439419389, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9969876408576965, 0.001592902815900743, 0.0001457851758459583, 0.00023439523647539318, 0.0010392277035862207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9928420782089233, 0.004003777634352446, 0.00011105102021247149, 0.00011847006680909544, 0.0012210400309413671, 0.0017036066856235266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9916440844535828, 0.0023549096658825874, 0.00023676450655329973, 0.0002687149099074304, 0.001049158745445311, 0.000474019005196169, 0.003972328267991543, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9746821522712708, 0.011402412317693233, 0.0006524948985315859, 0.0007208043825812638, 0.0029426112305372953, 0.001003988436423242, 0.006663138046860695, 0.0019323431188240647, 0.0, 0.0, 0.0, 0.0], [0.9333696961402893, 0.003415751038119197, 0.007108181715011597, 0.006032825447618961, 0.03696012496948242, 0.0008536730892956257, 0.007170883473008871, 0.0030297436751425266, 0.002059126505628228, 0.0, 0.0, 0.0], [0.9707586765289307, 0.010710692033171654, 0.001793080591596663, 0.0013629375025629997, 0.007033053785562515, 0.00042083102744072676, 0.003225359134376049, 0.002422106685116887, 0.0013821051688864827, 0.0008911899058148265, 0.0, 0.0], [0.9874714016914368, 0.006124864798039198, 0.0004326601920183748, 0.00012500783486757427, 0.0013952761655673385, 0.0003261819074396044, 0.00240827864035964, 0.00030412079649977386, 8.481547411065549e-05, 0.00010053622827399522, 0.0012267890851944685, 0.0], [0.9751123785972595, 0.004809068515896797, 0.0004776626592501998, 0.000905886641703546, 0.002790424507111311, 0.0003015836118720472, 0.003393835388123989, 0.004881435073912144, 0.0005771630094386637, 0.00024913999368436635, 0.00045201211469247937, 0.006049461662769318]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9872762560844421, 0.012723677791655064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9769987463951111, 0.015089777298271656, 0.007911494001746178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7192603349685669, 0.07818789035081863, 0.16931496560573578, 0.03323685750365257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5110568404197693, 0.04792950674891472, 0.09090563654899597, 0.24487298727035522, 0.10523495823144913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6439914107322693, 0.024731473997235298, 0.05144745483994484, 0.10217438638210297, 0.1657983958721161, 0.011856816709041595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13008634746074677, 0.013661999255418777, 0.0674005076289177, 0.25941798090934753, 0.3698829114437103, 0.08887065947055817, 0.07067958265542984, 0.0, 0.0, 0.0, 0.0, 0.0], [0.914534330368042, 0.010548378340899944, 0.011680630035698414, 0.013536592945456505, 0.00821351446211338, 0.0033067918848246336, 0.009784086607396603, 0.028395747765898705, 0.0, 0.0, 0.0, 0.0], [0.5368485450744629, 0.008890808559954166, 0.01036416832357645, 0.004269768018275499, 0.0030370207969099283, 0.003252969356253743, 0.0023128686007112265, 0.3508163392543793, 0.08020754158496857, 0.0, 0.0, 0.0], [0.3706224262714386, 0.015943603590130806, 0.005344349890947342, 0.005884132813662291, 0.005949968472123146, 0.003633488668128848, 0.007731324527412653, 0.12839210033416748, 0.4056079387664795, 0.050890643149614334, 0.0, 0.0], [0.9035380482673645, 0.0065302373841404915, 0.0008655788260512054, 0.0007200771942734718, 0.001338525558821857, 0.0010542329400777817, 0.0022727001924067736, 0.011824101209640503, 0.03206139802932739, 0.03537563607096672, 0.004419403616338968, 0.0], [0.2813841700553894, 0.002389682689681649, 0.0037858278956264257, 0.005482211709022522, 0.0023430329747498035, 0.0015469792997464538, 0.006107555236667395, 0.025983456522226334, 0.021969415247440338, 0.08064486086368561, 0.5125088691711426, 0.055853988975286484]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9947903156280518, 0.005209686234593391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.99378502368927, 0.0010037294123321772, 0.005211161449551582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9909141063690186, 0.002276308834552765, 0.0036688533145934343, 0.0031407459173351526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9884196519851685, 0.005556918680667877, 0.0010611189063638449, 0.002036779886111617, 0.002925445791333914, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.984398603439331, 0.011248646304011345, 0.0005393119645304978, 0.000602163199800998, 0.0021721075754612684, 0.001039086258970201, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9890060424804688, 0.00273532560095191, 0.0012896652333438396, 0.0016003205673769116, 0.0010362454922869802, 0.0021972188260406256, 0.002135188551619649, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9843204021453857, 0.004532759543508291, 0.0009355939109809697, 0.0024651510175317526, 0.0008321055211126804, 0.002747180638834834, 0.0017029602313414216, 0.002463964046910405, 0.0, 0.0, 0.0, 0.0], [0.9581343531608582, 0.0015685476828366518, 0.01611737534403801, 0.020945990458130836, 0.001178841688670218, 0.00037148542469367385, 0.0004319259023759514, 0.000615814933553338, 0.0006355926743708551, 0.0, 0.0, 0.0], [0.9873623847961426, 0.003186252200976014, 0.0007715133251622319, 0.0018533384427428246, 0.0014256025897338986, 0.0007793690892867744, 0.0021085806656628847, 0.0007384673808701336, 0.0006160051561892033, 0.001158514991402626, 0.0, 0.0], [0.9896944761276245, 0.002258782973513007, 0.0009895296534523368, 0.0002367494598729536, 0.001121915178373456, 0.0011288117384538054, 0.0030758611392229795, 0.00037884447374381125, 0.00019132354645989835, 0.0005093953805044293, 0.0004142820544075221, 0.0], [0.9723635911941528, 0.004590950440615416, 0.0015852975193411112, 0.0019124726532027125, 0.0006244966061785817, 0.0018245531246066093, 0.0013179403031244874, 0.013607888482511044, 0.0011498673120513558, 0.0004161187098361552, 0.0004444616206455976, 0.00016226603474933654]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9543190002441406, 0.04568096995353699, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9760633707046509, 0.015798397362232208, 0.008138187229633331, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8876117467880249, 0.07582201808691025, 0.009464657865464687, 0.027101606130599976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9154512286186218, 0.059892505407333374, 0.0022952507715672255, 0.003513018134981394, 0.018848003819584846, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7615962624549866, 0.11863219738006592, 0.002697360934689641, 0.008166995830833912, 0.06705125421285629, 0.041855938732624054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8907578587532043, 0.054074306041002274, 0.0015865449095144868, 0.002978303236886859, 0.014226507395505905, 0.0028144505340605974, 0.033562105149030685, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6089158654212952, 0.2603952884674072, 0.005949078593403101, 0.010808133520185947, 0.015386083163321018, 0.006588610354810953, 0.07731135934591293, 0.014645609073340893, 0.0, 0.0, 0.0, 0.0], [0.7495609521865845, 0.11941171437501907, 0.009106636047363281, 0.012758154422044754, 0.045534104108810425, 0.0018989879172295332, 0.04524923861026764, 0.009814969263970852, 0.006665220018476248, 0.0, 0.0, 0.0], [0.6116514205932617, 0.21727371215820312, 0.009440603666007519, 0.016372358426451683, 0.04627518728375435, 0.00413135252892971, 0.05561627075076103, 0.011717678047716618, 0.014146260917186737, 0.0133750569075346, 0.0, 0.0], [0.6217761039733887, 0.17996561527252197, 0.0028440579771995544, 0.01633515954017639, 0.041642460972070694, 0.011440346017479897, 0.06433001905679703, 0.011231842450797558, 0.014393606223165989, 0.013460932299494743, 0.022579817101359367, 0.0], [0.881875216960907, 0.055054884403944016, 0.001358453999273479, 0.001920076902024448, 0.006598630920052528, 0.0006267299759201705, 0.028544064611196518, 0.005198860075324774, 0.0025411483366042376, 0.001579635078087449, 0.001354809501208365, 0.013347460888326168]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9937054514884949, 0.006294610444456339, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9919540286064148, 0.0010996103519573808, 0.006946371868252754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9888655543327332, 0.002075851894915104, 0.0029367092065513134, 0.006121953018009663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9879482984542847, 0.0020598366390913725, 0.00010376739373896271, 0.00025276330416090786, 0.009635331109166145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9824740290641785, 0.003924972377717495, 8.744622755330056e-05, 0.00014588590420316905, 0.008101114071905613, 0.0052665406838059425, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9825745224952698, 0.00250753085128963, 0.0004886615788564086, 0.0009825051529332995, 0.00509840389713645, 0.002528656041249633, 0.005819751415401697, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9469886422157288, 0.01957651786506176, 0.0016778658609837294, 0.0011432741302996874, 0.013198739849030972, 0.001042048679664731, 0.010323476046323776, 0.006049454677850008, 0.0, 0.0, 0.0, 0.0], [0.8860181570053101, 0.00405918387696147, 0.015827739611268044, 0.020663540810346603, 0.05516548082232475, 0.0028798722196370363, 0.00438960175961256, 0.003912420477718115, 0.007083941251039505, 0.0, 0.0, 0.0], [0.92234867811203, 0.01300344243645668, 0.0018250358989462256, 0.00270048133097589, 0.03910933807492256, 0.0011782440124079585, 0.008042137138545513, 0.004565051291137934, 0.0020416169427335262, 0.005186052061617374, 0.0, 0.0], [0.8858071565628052, 0.010795127600431442, 0.00041116730426438153, 0.0003892428649123758, 0.056896358728408813, 0.009440898895263672, 0.0246450025588274, 0.0012850179336965084, 0.0003783682477660477, 0.0018447452457621694, 0.00810687243938446, 0.0], [0.9220626354217529, 0.02045910619199276, 0.003360483795404434, 0.0008936765952967107, 0.020707320421934128, 0.0007258333498612046, 0.009807894937694073, 0.005775848403573036, 0.0004629535833373666, 0.0005500074476003647, 0.0013662015553563833, 0.013827912509441376]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9878359436988831, 0.012164026498794556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9773780703544617, 0.003073219209909439, 0.0195486880838871, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9784227013587952, 0.0022285515442490578, 0.00686977244913578, 0.012478945776820183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9840379953384399, 0.006100865546613932, 0.001037996611557901, 0.0013859396567568183, 0.007437133230268955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9622312784194946, 0.015207006596028805, 0.0005987289478071034, 0.0005947991739958525, 0.006585132796317339, 0.014783062972128391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9465333223342896, 0.011390892788767815, 0.007812505587935448, 0.010350317694246769, 0.010379744693636894, 0.003556235693395138, 0.009977029636502266, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9422536492347717, 0.026867052540183067, 0.003449484473094344, 0.006400480400770903, 0.004396482370793819, 0.0018576098373159766, 0.00811205804347992, 0.006663175765424967, 0.0, 0.0, 0.0, 0.0], [0.9078310132026672, 0.0038106224965304136, 0.02022111602127552, 0.03308572247624397, 0.02231615036725998, 0.0005308191175572574, 0.0037432925309985876, 0.006762630771845579, 0.001698701293207705, 0.0, 0.0, 0.0], [0.9332455396652222, 0.013164580799639225, 0.004455127753317356, 0.0155667494982481, 0.012901381589472294, 0.0008884693379513919, 0.0073503367602825165, 0.005716616753488779, 0.0027761582750827074, 0.003934997133910656, 0.0, 0.0], [0.9687536954879761, 0.013955953530967236, 0.0007245322922244668, 0.002059472259134054, 0.005090235732495785, 0.0006814255029894412, 0.004278966225683689, 0.0016897255554795265, 0.0009166120435111225, 0.0007309963111765683, 0.0011184706818312407, 0.0], [0.8652649521827698, 0.034600503742694855, 0.029179919511079788, 0.02033570408821106, 0.0056440760381519794, 0.0011487974552437663, 0.006637183949351311, 0.026794081553816795, 0.0022731153294444084, 0.0022174748592078686, 0.0014500448014587164, 0.004454222973436117]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.989349901676178, 0.010650074109435081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9975581169128418, 0.000920298567507416, 0.0015215041348710656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9668330550193787, 0.007319653406739235, 0.007216915022581816, 0.018630364909768105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9830961227416992, 0.009530679322779179, 0.0022947753313928843, 0.001652453327551484, 0.0034259390085935593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9379034042358398, 0.019590485841035843, 0.0038066664710640907, 0.0012193832080811262, 0.0071634878404438496, 0.030316578224301338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9186261892318726, 0.004355476703494787, 0.00383597519248724, 0.00829233881086111, 0.004790028557181358, 0.017112495377659798, 0.04298742488026619, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8980032205581665, 0.019281161949038506, 0.0036580744199454784, 0.0048960293643176556, 0.004814279731363058, 0.014283161610364914, 0.024821752682328224, 0.03024241328239441, 0.0, 0.0, 0.0, 0.0], [0.9254859089851379, 0.00489537138491869, 0.007237081881612539, 0.03221755474805832, 0.004543783608824015, 0.0013308172347024083, 0.0075867073610424995, 0.012697397731244564, 0.004005265887826681, 0.0, 0.0, 0.0], [0.9168170094490051, 0.00878805574029684, 0.00393130024895072, 0.01159159280359745, 0.007944073528051376, 0.0025698193348944187, 0.023726655170321465, 0.007430804893374443, 0.004958846606314182, 0.012241818010807037, 0.0, 0.0], [0.9346416592597961, 0.01002486515790224, 0.0012376689119264483, 0.0037315262015908957, 0.005642378237098455, 0.002060284372419119, 0.03379976376891136, 0.0033203980419784784, 0.0008341717766597867, 0.002464020624756813, 0.0022432326804846525, 0.0], [0.7767418622970581, 0.016890564933419228, 0.013018216006457806, 0.02099350467324257, 0.002914787968620658, 0.01539046410471201, 0.061619240790605545, 0.04035003110766411, 0.0161056499928236, 0.007125553674995899, 0.002865440445020795, 0.02598465047776699]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9571044445037842, 0.042895589023828506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8944013118743896, 0.002292641904205084, 0.10330596566200256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8432921171188354, 0.022459646686911583, 0.07121869176626205, 0.06302947551012039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9668034911155701, 0.01506803184747696, 0.0072390553541481495, 0.007142290007323027, 0.003747072070837021, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8088358640670776, 0.12081179022789001, 0.006581529043614864, 0.009613117203116417, 0.025724943727254868, 0.02843278832733631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9003180265426636, 0.010098268277943134, 0.04642542824149132, 0.022063136100769043, 0.005719350650906563, 0.009746170602738857, 0.005629665683954954, 0.0, 0.0, 0.0, 0.0, 0.0], [0.67498379945755, 0.07246924936771393, 0.05023349076509476, 0.04135661944746971, 0.013520489446818829, 0.01915188878774643, 0.06747189164161682, 0.060812562704086304, 0.0, 0.0, 0.0, 0.0], [0.23936080932617188, 0.017085157334804535, 0.46827107667922974, 0.08229976892471313, 0.014127971604466438, 0.00818091630935669, 0.030636059120297432, 0.10266854614019394, 0.03736968711018562, 0.0, 0.0, 0.0], [0.2580433785915375, 0.016452645882964134, 0.23415914177894592, 0.07607056945562363, 0.01965557411313057, 0.011278967373073101, 0.07139422744512558, 0.14227470755577087, 0.06406068056821823, 0.10661006718873978, 0.0, 0.0], [0.3067180812358856, 0.021915923804044724, 0.018273448571562767, 0.045844946056604385, 0.028904885053634644, 0.0272726621478796, 0.14117932319641113, 0.09534874558448792, 0.05842772498726845, 0.22343380749225616, 0.03268053010106087, 0.0], [0.7636430859565735, 0.016906265169382095, 0.0363956019282341, 0.034021392464637756, 0.0025655122008174658, 0.004900640342384577, 0.008197270333766937, 0.0928981602191925, 0.020430760458111763, 0.015800587832927704, 0.0025342702865600586, 0.0017063904087990522]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9551740884780884, 0.044825952500104904, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9905253052711487, 0.004271515645086765, 0.005203140899538994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8798518776893616, 0.027674585580825806, 0.061286989599466324, 0.031186547130346298, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7317190170288086, 0.0273505300283432, 0.11023665219545364, 0.10514233261346817, 0.025551535189151764, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9752873182296753, 0.00552776362746954, 0.005123698618263006, 0.0025828224606812, 0.007563234306871891, 0.003915078938007355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28885433077812195, 0.034611962735652924, 0.1357785165309906, 0.19892969727516174, 0.2229047417640686, 0.07223805785179138, 0.046682681888341904, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9209998250007629, 0.02510426752269268, 0.012525989674031734, 0.008671090006828308, 0.005500831175595522, 0.005292529705911875, 0.005513777956366539, 0.016391796991229057, 0.0, 0.0, 0.0, 0.0], [0.5864962935447693, 0.03225439041852951, 0.013528263196349144, 0.011383142322301865, 0.012345453724265099, 0.02692260406911373, 0.008988226763904095, 0.27134963870048523, 0.03673204407095909, 0.0, 0.0, 0.0], [0.7769827246665955, 0.019471561536192894, 0.010438368655741215, 0.01267638523131609, 0.012700083665549755, 0.015101400204002857, 0.007936738431453705, 0.07596447318792343, 0.04970310255885124, 0.019025161862373352, 0.0, 0.0], [0.982941210269928, 0.0024782021064311266, 0.0009888781933113933, 0.00043851794907823205, 0.0012743716361001134, 0.0010852544801309705, 0.0005498375394381583, 0.0029768033418804407, 0.002589152893051505, 0.004275110550224781, 0.0004027065297123045, 0.0], [0.19224433600902557, 0.028531476855278015, 0.07470971345901489, 0.06544695794582367, 0.03079812601208687, 0.026473935693502426, 0.040211599320173264, 0.16838237643241882, 0.03392409905791283, 0.21150092780590057, 0.0982368215918541, 0.02953965775668621]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9901427030563354, 0.009857220575213432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9956203103065491, 0.0029288853984326124, 0.001450871815904975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9871225357055664, 0.005410744342952967, 0.004569453652948141, 0.0028973049484193325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9829382300376892, 0.009795066900551319, 0.00048440656973980367, 0.0006644156528636813, 0.006117945536971092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9196339845657349, 0.02348903939127922, 0.0021676779724657536, 0.0033911550417542458, 0.026565710082650185, 0.024752343073487282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9727079272270203, 0.004823583643883467, 0.0014042456168681383, 0.0006801914423704147, 0.006205730605870485, 0.0008613868267275393, 0.013316919095814228, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8963191509246826, 0.021091049537062645, 0.002004441572353244, 0.0011293439893051982, 0.007470950484275818, 0.0013838437153026462, 0.06775467097759247, 0.002846572082489729, 0.0, 0.0, 0.0, 0.0], [0.9230843186378479, 0.006116335280239582, 0.006697113625705242, 0.0028287365566939116, 0.025952864438295364, 0.0009725744021125138, 0.026676738634705544, 0.005275245755910873, 0.002396050374954939, 0.0, 0.0, 0.0], [0.9133254885673523, 0.013240735046565533, 0.004918181337416172, 0.0033992212265729904, 0.018980111926794052, 0.0011770091950893402, 0.026475992053747177, 0.008248453959822655, 0.004303020425140858, 0.005931802559643984, 0.0, 0.0], [0.8944787979125977, 0.014529558829963207, 0.0029275636188685894, 0.0027920131105929613, 0.019198717549443245, 0.0009078379953280091, 0.03148907423019409, 0.0062180617824196815, 0.0031336485408246517, 0.004609462805092335, 0.019715191796422005, 0.0], [0.9134261608123779, 0.0210827998816967, 0.0016301231225952506, 0.0008393732132390141, 0.013307520188391209, 0.0008196980925276875, 0.03129567950963974, 0.003983227536082268, 0.0007332347449846566, 0.0009803202701732516, 0.0007437948952428997, 0.011158030480146408]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9320766925811768, 0.06792329996824265, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9994696974754333, 5.5106116633396596e-05, 0.00047510312288068235, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9155941009521484, 0.008786618709564209, 0.025426244363188744, 0.050193123519420624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9127328395843506, 0.012330151163041592, 0.004597332328557968, 0.0057458956725895405, 0.06459380686283112, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7912782430648804, 0.031697917729616165, 0.0024411873891949654, 0.006056800484657288, 0.08301986008882523, 0.0855059027671814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.85847407579422, 0.007273135706782341, 0.008292770013213158, 0.009355093352496624, 0.0641251653432846, 0.013391328044235706, 0.03908843174576759, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7664589285850525, 0.09918423742055893, 0.010723061859607697, 0.014917406253516674, 0.011985152028501034, 0.019865790382027626, 0.03505229949951172, 0.04181312769651413, 0.0, 0.0, 0.0, 0.0], [0.7487624883651733, 0.01648177020251751, 0.06857548654079437, 0.04779211804270744, 0.055606648325920105, 0.006192673929035664, 0.008028166368603706, 0.029528312385082245, 0.019032347947359085, 0.0, 0.0, 0.0], [0.6601600050926208, 0.0911974310874939, 0.028807159513235092, 0.030070796608924866, 0.05450253561139107, 0.009085063822567463, 0.02303803339600563, 0.027810310944914818, 0.028043802827596664, 0.047284796833992004, 0.0, 0.0], [0.7121720314025879, 0.03617529571056366, 0.006967268884181976, 0.01054224744439125, 0.012537460774183273, 0.007182869594544172, 0.015330012887716293, 0.01200556568801403, 0.008038940839469433, 0.02444978803396225, 0.15459847450256348, 0.0], [0.8188938498497009, 0.019866419956088066, 0.019717523828148842, 0.013817278668284416, 0.0056783463805913925, 0.003927802667021751, 0.00482503417879343, 0.03605891764163971, 0.021680818870663643, 0.020483946427702904, 0.00700108427554369, 0.028048906475305557]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9762955904006958, 0.02370440773665905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9964537620544434, 0.0009455238468945026, 0.0026007313281297684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9927160143852234, 0.0024936378467828035, 0.0016946479445323348, 0.0030957029666751623, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9921626448631287, 0.0010757981799542904, 0.0006514668348245323, 0.0006968984962441027, 0.005413220264017582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9647658467292786, 0.013677123934030533, 0.0026826574467122555, 0.0006928975344635546, 0.006404805928468704, 0.011776579543948174, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.985031008720398, 0.0022271042689681053, 0.00047792261466383934, 0.000882403168361634, 0.002198045840486884, 0.003196959150955081, 0.00598653219640255, 0.0, 0.0, 0.0, 0.0, 0.0], [0.970314085483551, 0.003203297732397914, 0.0017244055634364486, 0.0016517029143869877, 0.001970436656847596, 0.0072316317819058895, 0.003233143128454685, 0.010671241208910942, 0.0, 0.0, 0.0, 0.0], [0.96915203332901, 0.006529571954160929, 0.00971532054245472, 0.0029257272835820913, 0.004156057257205248, 0.0005843357648700476, 0.0011075569782406092, 0.002120258053764701, 0.003709207521751523, 0.0, 0.0, 0.0], [0.9632844924926758, 0.004359650891274214, 0.003775648307055235, 0.0025867570657283068, 0.006655755452811718, 0.0008473451598547399, 0.0023436436895281076, 0.005148527678102255, 0.0032596862874925137, 0.007738443557173014, 0.0, 0.0], [0.9750282764434814, 0.004368262365460396, 0.00177171197719872, 0.0005670529790222645, 0.006481274496763945, 0.0019257529638707638, 0.0026836402248591185, 0.001628961763344705, 0.0010947214905172586, 0.0020724881906062365, 0.0023779806215316057, 0.0], [0.953087568283081, 0.003069176571443677, 0.0003648049896582961, 0.001350541366264224, 0.0023573769722133875, 0.0015244920505210757, 0.0029395611491054296, 0.028329938650131226, 0.002083954168483615, 0.0013282963773235679, 0.0012913646642118692, 0.002273001242429018]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.959121584892273, 0.04087836295366287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9849779605865479, 0.010535422712564468, 0.004486729856580496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5740022659301758, 0.1436295360326767, 0.2543749213218689, 0.02799323759973049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7036653161048889, 0.042057104408741, 0.1522970199584961, 0.04739714786410332, 0.05458341911435127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9137982130050659, 0.011100233532488346, 0.016608767211437225, 0.00531514547765255, 0.0399610809981823, 0.013216590508818626, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3870084583759308, 0.03939186409115791, 0.031520236283540726, 0.01530604250729084, 0.2876778542995453, 0.17320218682289124, 0.06589341163635254, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3463834524154663, 0.0825919434428215, 0.16086871922016144, 0.14361286163330078, 0.10941073298454285, 0.017630290240049362, 0.017445171251893044, 0.12205677479505539, 0.0, 0.0, 0.0, 0.0], [0.7978115081787109, 0.03978286683559418, 0.029705794528126717, 0.01514966506510973, 0.012279201298952103, 0.017154635861516, 0.006976852659136057, 0.02428155019879341, 0.05685797333717346, 0.0, 0.0, 0.0], [0.595302164554596, 0.05352839455008507, 0.025338830426335335, 0.012450039386749268, 0.026830822229385376, 0.016402188688516617, 0.01801624707877636, 0.07190891355276108, 0.08011601120233536, 0.100106380879879, 0.0, 0.0], [0.3920123279094696, 0.012365564703941345, 0.007651458494365215, 0.00846302229911089, 0.018986454233527184, 0.011205814778804779, 0.0065323119051754475, 0.024048805236816406, 0.03920116275548935, 0.46189066767692566, 0.017642419785261154, 0.0], [0.4353075325489044, 0.0286267027258873, 0.010547078214585781, 0.005133218131959438, 0.03686060756444931, 0.020660843700170517, 0.11193608492612839, 0.03195985406637192, 0.029412351548671722, 0.06244899332523346, 0.07893283665180206, 0.14817386865615845]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.988471508026123, 0.01152846496552229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.997025191783905, 0.0010684425942599773, 0.0019063932122662663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.981872022151947, 0.006105675827711821, 0.002922933781519532, 0.009099304676055908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9868950843811035, 0.005766425747424364, 0.0009384111617691815, 0.002404608530923724, 0.003995539620518684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9269776344299316, 0.01321770902723074, 0.001213951501995325, 0.0015244808746501803, 0.011232753284275532, 0.045833390206098557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9824954271316528, 0.006927679292857647, 0.0010620810789987445, 0.0016974241007119417, 0.0037920065224170685, 0.0016877301968634129, 0.002337635960429907, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9633212089538574, 0.020637286826968193, 0.0020873781759291887, 0.0023505007848143578, 0.003917817026376724, 0.0010250085033476353, 0.004618571139872074, 0.002042324747890234, 0.0, 0.0, 0.0, 0.0], [0.9020165801048279, 0.010630141943693161, 0.008206064812839031, 0.02337317354977131, 0.02892986871302128, 0.0018138368614017963, 0.0055389138869941235, 0.012290393002331257, 0.007201052270829678, 0.0, 0.0, 0.0], [0.9389233589172363, 0.0250250156968832, 0.004534520674496889, 0.005400227848440409, 0.01658812165260315, 0.0005467529408633709, 0.0026901988312602043, 0.002411264693364501, 0.0020533695351332426, 0.0018271930748596787, 0.0, 0.0], [0.9356243014335632, 0.028681321069598198, 0.0019170657033100724, 0.0031472109258174896, 0.011792337521910667, 0.0014211094239726663, 0.006488801911473274, 0.0016711412463337183, 0.001289163832552731, 0.0016126185655593872, 0.006354885641485453, 0.0], [0.9441710710525513, 0.019937848672270775, 0.002020207466557622, 0.004517619032412767, 0.0046108015812933445, 0.0008675424032844603, 0.004476460162550211, 0.006584775634109974, 0.0028358646668493748, 0.0022599396761506796, 0.0017639427678659558, 0.005953977350145578]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9658048152923584, 0.03419516235589981, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9977747797966003, 0.0005930069019086659, 0.0016321236034855247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9685519337654114, 0.006309279706329107, 0.016255460679531097, 0.00888338964432478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6565639972686768, 0.001117004663683474, 0.0019855056889355183, 0.000657268741633743, 0.3396761417388916, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30962279438972473, 0.0007707318291068077, 0.0006253982428461313, 0.000492502877023071, 0.6372036337852478, 0.05128491297364235, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9483788013458252, 0.0004553207545541227, 0.0018248872365802526, 0.000591388379689306, 0.0380113422870636, 0.009637987241148949, 0.0011002890532836318, 0.0, 0.0, 0.0, 0.0, 0.0], [0.926287829875946, 0.02140622027218342, 0.004105891101062298, 0.004571663681417704, 0.010048470459878445, 0.0017752688145264983, 0.00917286891490221, 0.022631650790572166, 0.0, 0.0, 0.0, 0.0], [0.8606972098350525, 0.010049181990325451, 0.006400591228157282, 0.0031424753833562136, 0.06852227449417114, 0.0021534098777920008, 0.0029035040643066168, 0.03991401940584183, 0.0062174308113753796, 0.0, 0.0, 0.0], [0.7098503708839417, 0.04013543203473091, 0.00951176043599844, 0.00814045313745737, 0.01828039065003395, 0.0011987186735495925, 0.0018468399066478014, 0.029115771874785423, 0.13416148722171783, 0.04775881767272949, 0.0, 0.0], [0.5287734270095825, 0.05865494906902313, 0.007050220854580402, 0.02376426011323929, 0.030278267338871956, 0.0034825585316866636, 0.024989375844597816, 0.013268956914544106, 0.09461455047130585, 0.1843184530735016, 0.03080502711236477, 0.0], [0.9616473913192749, 0.0035129785537719727, 0.0022289776243269444, 0.0010204487480223179, 0.000796067004557699, 0.00025211370666511357, 0.0005352248554117978, 0.0038260589353740215, 0.009781530126929283, 0.011055126786231995, 0.002612531650811434, 0.002731556538492441]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7631521821022034, 0.23684778809547424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8140329718589783, 0.030582554638385773, 0.15538454055786133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3488771915435791, 0.3219284415245056, 0.046803493052721024, 0.2823909521102905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3251635432243347, 0.2207874059677124, 0.08190147578716278, 0.27009910345077515, 0.10204846411943436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4466016888618469, 0.211994469165802, 0.028147855773568153, 0.10988286882638931, 0.098207987844944, 0.10516513884067535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1506308615207672, 0.14232632517814636, 0.10152330994606018, 0.43014177680015564, 0.04141467437148094, 0.08830274641513824, 0.04566023498773575, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1392718404531479, 0.25003504753112793, 0.044028449803590775, 0.15102435648441315, 0.058656368404626846, 0.06717752665281296, 0.07385163009166718, 0.2159547060728073, 0.0, 0.0, 0.0, 0.0], [0.1720038503408432, 0.2029508799314499, 0.03353683650493622, 0.22065509855747223, 0.0381474569439888, 0.06568533182144165, 0.06879177689552307, 0.0990094542503357, 0.09921924769878387, 0.0, 0.0, 0.0], [0.15818248689174652, 0.1752844899892807, 0.04004589468240738, 0.1565445363521576, 0.03765828162431717, 0.05553760752081871, 0.05935663357377052, 0.09825722873210907, 0.08481957018375397, 0.13431322574615479, 0.0, 0.0], [0.14610618352890015, 0.18775931000709534, 0.025839736685156822, 0.14120346307754517, 0.032666079699993134, 0.04899565130472183, 0.03380166366696358, 0.08787865936756134, 0.07605503499507904, 0.17131344974040985, 0.048380762338638306, 0.0], [0.06531724333763123, 0.13758766651153564, 0.05666935071349144, 0.19564689695835114, 0.015892401337623596, 0.029399536550045013, 0.03075420670211315, 0.12380705773830414, 0.1491192728281021, 0.12851089239120483, 0.029563218355178833, 0.037732239812612534]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9908686876296997, 0.009131278842687607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.988663375377655, 0.0035724572371691465, 0.007764122914522886, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9552818536758423, 0.007662677671760321, 0.01920069195330143, 0.017854763194918633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9163065552711487, 0.022349607199430466, 0.01766827329993248, 0.029256893321871758, 0.014418699778616428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8694319128990173, 0.04450789466500282, 0.011516469530761242, 0.011525677517056465, 0.018964022397994995, 0.044054023921489716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9018348455429077, 0.013366020284593105, 0.020030176267027855, 0.03593089431524277, 0.010775761678814888, 0.008878192864358425, 0.009184080176055431, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7933292388916016, 0.03312031552195549, 0.012548861093819141, 0.050698939710855484, 0.019209016114473343, 0.02443666011095047, 0.012027139775454998, 0.05462976545095444, 0.0, 0.0, 0.0, 0.0], [0.9163272976875305, 0.004780156072229147, 0.013954690657556057, 0.018498217687010765, 0.01568741351366043, 0.002299074549227953, 0.0025913401041179895, 0.009906196966767311, 0.015955712646245956, 0.0, 0.0, 0.0], [0.7829375267028809, 0.01681559905409813, 0.010005817748606205, 0.03477846086025238, 0.02751985937356949, 0.0068509760312736034, 0.007235431112349033, 0.03473849222064018, 0.037409041076898575, 0.04170888662338257, 0.0, 0.0], [0.7540016770362854, 0.02415350265800953, 0.00832295510917902, 0.0363582968711853, 0.036991413682699203, 0.009868509136140347, 0.017644289880990982, 0.018432768061757088, 0.0177479051053524, 0.06257428973913193, 0.01390448771417141, 0.0], [0.7243934273719788, 0.02370026893913746, 0.022638244554400444, 0.056327395141124725, 0.016325432807207108, 0.003698812797665596, 0.0066945552825927734, 0.01840715855360031, 0.0460696667432785, 0.05286610499024391, 0.013935822993516922, 0.014942971989512444]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9717192053794861, 0.02828083001077175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9730884432792664, 0.007797665428370237, 0.019113847985863686, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9720280766487122, 0.016658803448081017, 0.005759866442531347, 0.005553212482482195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9517654180526733, 0.028112661093473434, 0.006765646860003471, 0.003212693613022566, 0.010143574327230453, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9334547519683838, 0.031418345868587494, 0.0031440751627087593, 0.0014471388421952724, 0.011637801304459572, 0.01889781653881073, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9432334303855896, 0.026043718680739403, 0.00664885388687253, 0.004433799535036087, 0.012353635393083096, 0.0013085742248222232, 0.0059779370203614235, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8821426630020142, 0.06355717778205872, 0.009088433347642422, 0.0044200546108186245, 0.007929269224405289, 0.005556000396609306, 0.011393939144909382, 0.015912454575300217, 0.0, 0.0, 0.0, 0.0], [0.8985357880592346, 0.0139147425070405, 0.016352234408259392, 0.014775230549275875, 0.03895158693194389, 0.0007104990072548389, 0.00434163399040699, 0.00693084578961134, 0.00548744248226285, 0.0, 0.0, 0.0], [0.8466895222663879, 0.05333248898386955, 0.00943991169333458, 0.010271945036947727, 0.0404677614569664, 0.0022221163380891085, 0.008404025807976723, 0.009704247117042542, 0.00691349059343338, 0.012554474174976349, 0.0, 0.0], [0.8651696443557739, 0.05246953293681145, 0.004206615500152111, 0.007429338991641998, 0.031007522717118263, 0.0038831511046737432, 0.0061407252214848995, 0.0067539638839662075, 0.004666269291192293, 0.012804986909031868, 0.005468238145112991, 0.0], [0.8807172179222107, 0.04973120242357254, 0.013478687033057213, 0.006705752573907375, 0.0121303154155612, 0.0007657504174858332, 0.008701514452695847, 0.013406415469944477, 0.0018641004571691155, 0.001981607172638178, 0.002217133529484272, 0.008300403133034706]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9312466979026794, 0.06875330209732056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9917951226234436, 0.004469121340662241, 0.0037357578985393047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7030307650566101, 0.19157391786575317, 0.023570094257593155, 0.08182527124881744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6711723208427429, 0.13334134221076965, 0.018017306923866272, 0.10341523587703705, 0.07405388355255127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4527410566806793, 0.06320580095052719, 0.01493831817060709, 0.09157432615756989, 0.23283158242702484, 0.14470890164375305, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8203462958335876, 0.0536433607339859, 0.012782424688339233, 0.027360081672668457, 0.043974507600069046, 0.02293824404478073, 0.01895497366786003, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3918808400630951, 0.17046700417995453, 0.005214526783674955, 0.03173243626952171, 0.02827325090765953, 0.029436856508255005, 0.19618608057498932, 0.14680902659893036, 0.0, 0.0, 0.0, 0.0], [0.4720349609851837, 0.07286762446165085, 0.019576475024223328, 0.05108219385147095, 0.04173942282795906, 0.00986722856760025, 0.21260416507720947, 0.06521333009004593, 0.05501453951001167, 0.0, 0.0, 0.0], [0.25927016139030457, 0.08728151023387909, 0.012478123418986797, 0.037995804101228714, 0.044688642024993896, 0.014521932229399681, 0.21757389605045319, 0.09061294794082642, 0.136713445186615, 0.09886349737644196, 0.0, 0.0], [0.2699909508228302, 0.09389787912368774, 0.007607426028698683, 0.024089427664875984, 0.030684567987918854, 0.008005748502910137, 0.11110535264015198, 0.041127122938632965, 0.07558272033929825, 0.2726375460624695, 0.06527116149663925, 0.0], [0.5813767313957214, 0.0267273411154747, 0.027138885110616684, 0.024491548538208008, 0.014174449257552624, 0.009728421457111835, 0.055766209959983826, 0.04522695392370224, 0.07563697546720505, 0.06725243479013443, 0.0553763210773468, 0.017103800550103188]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9433999061584473, 0.05660010501742363, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9803316593170166, 0.00934229139238596, 0.010326019488275051, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8844290375709534, 0.06136809661984444, 0.013398764654994011, 0.040804099291563034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9025850892066956, 0.026053162291646004, 0.007853358052670956, 0.02101930044591427, 0.042489033192396164, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7650192975997925, 0.04322713613510132, 0.005216097459197044, 0.024445662274956703, 0.11070672422647476, 0.0513850599527359, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8097188472747803, 0.02860715426504612, 0.023732800036668777, 0.0387372262775898, 0.04244602844119072, 0.03776996210217476, 0.018987987190485, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8284680843353271, 0.043347664177417755, 0.011651243083178997, 0.028351496905088425, 0.023011645302176476, 0.012510914355516434, 0.013447248376905918, 0.039211735129356384, 0.0, 0.0, 0.0, 0.0], [0.7604453563690186, 0.04346691444516182, 0.01948135532438755, 0.044830285012722015, 0.031897276639938354, 0.004119371995329857, 0.005922769196331501, 0.03711433336138725, 0.052722327411174774, 0.0, 0.0, 0.0], [0.6848206520080566, 0.043154019862413406, 0.011953242123126984, 0.045487433671951294, 0.049701590090990067, 0.0055799344554543495, 0.009332769550383091, 0.048327237367630005, 0.039242733269929886, 0.06240032613277435, 0.0, 0.0], [0.6196363568305969, 0.05217425525188446, 0.00932135246694088, 0.04323258250951767, 0.0663069561123848, 0.003830721601843834, 0.017899109050631523, 0.03607935458421707, 0.043535266071558, 0.08041983097791672, 0.02756427600979805, 0.0], [0.7398905754089355, 0.02465205267071724, 0.02178107760846615, 0.03567354008555412, 0.010522515513002872, 0.010300634428858757, 0.012830874882638454, 0.02410556748509407, 0.04927583038806915, 0.02837524004280567, 0.012745547108352184, 0.0298465508967638]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9880727529525757, 0.01192731224000454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.990120530128479, 0.003285810584202409, 0.0065937284380197525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9532556533813477, 0.005705546122044325, 0.01034686528146267, 0.03069189563393593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9839207530021667, 0.00531038036569953, 0.0029426945839077234, 0.004126533400267363, 0.00369967776350677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629443883895874, 0.014577193185687065, 0.002306272042915225, 0.0007732174708507955, 0.00539063848555088, 0.014008298516273499, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9635095596313477, 0.0018696795450523496, 0.004724960308521986, 0.013633298687636852, 0.005736196879297495, 0.004989220295101404, 0.005537118762731552, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9340261816978455, 0.009659226052463055, 0.00865731481462717, 0.007681199349462986, 0.006555176340043545, 0.005712056998163462, 0.007059326861053705, 0.020649448037147522, 0.0, 0.0, 0.0, 0.0], [0.7728214263916016, 0.006268407683819532, 0.04741418734192848, 0.07233688235282898, 0.021551214158535004, 0.017385372892022133, 0.010918731801211834, 0.028266143053770065, 0.02303769811987877, 0.0, 0.0, 0.0], [0.9355979561805725, 0.007211175747215748, 0.006232829764485359, 0.0070176939480006695, 0.016411010175943375, 0.003761001629754901, 0.004457825794816017, 0.010987616144120693, 0.004725192207843065, 0.0035976723302155733, 0.0, 0.0], [0.9544588327407837, 0.012585029006004333, 0.0026914251502603292, 0.001327384146861732, 0.00923770759254694, 0.003514742013067007, 0.004927559290081263, 0.004640736151486635, 0.0015775499632582068, 0.0011228359071537852, 0.003916231915354729, 0.0], [0.8606278896331787, 0.01438093651086092, 0.01507637556642294, 0.02235928177833557, 0.010862301103770733, 0.0030473419465124607, 0.012242461554706097, 0.03811972215771675, 0.004564095288515091, 0.005108581855893135, 0.0025323510635644197, 0.011078589595854282]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9893938302993774, 0.010606161318719387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9708994626998901, 0.015267358161509037, 0.013833197765052319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9856972694396973, 0.005904070567339659, 0.004182007629424334, 0.004216653294861317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9771594405174255, 0.0097661716863513, 0.002248637145385146, 0.0016573690809309483, 0.009168272837996483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9576557874679565, 0.0180364940315485, 0.001116042723879218, 0.0005862171528860927, 0.016994550824165344, 0.005610897671431303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9559290409088135, 0.00627050781622529, 0.004193597007542849, 0.005173348821699619, 0.020804375410079956, 0.0024536324199289083, 0.005175478290766478, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9417771697044373, 0.02214018628001213, 0.005322692915797234, 0.002287920331582427, 0.010924342088401318, 0.0025157046038657427, 0.004953461233526468, 0.010078522376716137, 0.0, 0.0, 0.0, 0.0], [0.935946524143219, 0.006674570497125387, 0.0067216865718364716, 0.005191418342292309, 0.03162883594632149, 0.0007190458127297461, 0.004577309358865023, 0.005430606659501791, 0.0031099042389541864, 0.0, 0.0, 0.0], [0.9244223833084106, 0.015658054500818253, 0.0032357966993004084, 0.004043035674840212, 0.03791172429919243, 0.0007248964393511415, 0.004218854010105133, 0.004599036183208227, 0.002256094478070736, 0.0029300812166184187, 0.0, 0.0], [0.9195272326469421, 0.017300326377153397, 0.002005603862926364, 0.0036873698700219393, 0.03553709387779236, 0.0007947604754008353, 0.005437491927295923, 0.00510157085955143, 0.00253883539699018, 0.003410973818972707, 0.004658759105950594, 0.0], [0.9104390740394592, 0.012203020043671131, 0.004971574991941452, 0.011680545285344124, 0.02282889373600483, 0.0020120511762797832, 0.0044261072762310505, 0.0133487768471241, 0.005050294566899538, 0.004083653911948204, 0.0022165365517139435, 0.006739581935107708]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9819416999816895, 0.018058300018310547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.986459493637085, 0.002706903498619795, 0.01083359494805336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9889689683914185, 0.0031762036960572004, 0.003816769225522876, 0.004038039594888687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9015501737594604, 0.02569587342441082, 0.021397452801465988, 0.036621492356061935, 0.014734948985278606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9302195310592651, 0.04133784398436546, 0.004020133055746555, 0.004583430010825396, 0.006555558647960424, 0.013283518142998219, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8343602418899536, 0.007369708735495806, 0.06178759038448334, 0.06174975633621216, 0.01642495207488537, 0.00400007888674736, 0.014307652600109577, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8021182417869568, 0.025954868644475937, 0.021860841661691666, 0.04549696668982506, 0.011668013408780098, 0.013084321282804012, 0.018992116674780846, 0.060824595391750336, 0.0, 0.0, 0.0, 0.0], [0.9705919623374939, 0.0035884291864931583, 0.006664678920060396, 0.006897459272295237, 0.0019949553534388542, 0.00025297931279055774, 0.0014679607702419162, 0.003545687533915043, 0.004995809402316809, 0.0, 0.0, 0.0], [0.9153609871864319, 0.0133579196408391, 0.009524354711174965, 0.011611844412982464, 0.005964069161564112, 0.002119312295690179, 0.005189538933336735, 0.016112202778458595, 0.011999635957181454, 0.008760157972574234, 0.0, 0.0], [0.9195884466171265, 0.02486583963036537, 0.004983294755220413, 0.007202292326837778, 0.0057625616900622845, 0.0020088041201233864, 0.012533283792436123, 0.008078020997345448, 0.005210998002439737, 0.006089264992624521, 0.0036772992461919785, 0.0], [0.6860412955284119, 0.01988273300230503, 0.047281112521886826, 0.06445646286010742, 0.008339229971170425, 0.002807067474350333, 0.02046331949532032, 0.04536151513457298, 0.036203112453222275, 0.026228686794638634, 0.005792134907096624, 0.037143390625715256]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0004216024244669825, 0.9995784163475037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [5.939265292909113e-07, 0.0008891902398318052, 0.9991101622581482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [7.578028453281149e-05, 0.23900216817855835, 0.39322996139526367, 0.36769208312034607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001941617694683373, 0.08571329712867737, 0.07652803510427475, 0.7278258800506592, 0.10799109190702438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0032237214036285877, 0.261954128742218, 0.01650141552090645, 0.1468925178050995, 0.30523502826690674, 0.2661932110786438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00010171601752517745, 0.07237115502357483, 0.04562731459736824, 0.504921555519104, 0.130992129445076, 0.013085815124213696, 0.23290027678012848, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0007685718010179698, 0.12037555873394012, 0.14970943331718445, 0.14721404016017914, 0.04442615807056427, 0.03857608139514923, 0.13860726356506348, 0.36032289266586304, 0.0, 0.0, 0.0, 0.0], [5.712137863156386e-05, 0.1551494598388672, 0.04787701740860939, 0.37209388613700867, 0.012721465900540352, 0.016788743436336517, 0.014196151867508888, 0.1767273098230362, 0.20438891649246216, 0.0, 0.0, 0.0], [0.0007459771586582065, 0.08834618330001831, 0.06937018036842346, 0.1192619651556015, 0.011684889905154705, 0.010517597198486328, 0.011106760241091251, 0.15228271484375, 0.3856702744960785, 0.15101340413093567, 0.0, 0.0], [0.0008670933311805129, 0.11802887171506882, 0.06807549297809601, 0.17348028719425201, 0.01228597667068243, 0.02329018898308277, 0.018128324300050735, 0.04735014587640762, 0.23612016439437866, 0.2777062654495239, 0.02466718479990959, 0.0], [2.4674644009792246e-05, 0.022967418655753136, 0.005188134964555502, 0.0419052429497242, 0.010617914609611034, 0.0009088711813092232, 0.8273705840110779, 0.01863281801342964, 0.011447254568338394, 0.012787116691470146, 0.001715334365144372, 0.046434614807367325]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954795837402344, 0.004520336631685495, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9754294157028198, 0.007082863245159388, 0.017487777397036552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.977192223072052, 0.0009520857711322606, 0.009156717918813229, 0.01269899308681488, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9903123378753662, 0.0018048776546493173, 0.0015391542110592127, 0.0010654889047145844, 0.005278115160763264, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9830564856529236, 0.004748734179884195, 0.0018226341344416142, 0.0009311830508522689, 0.006012840196490288, 0.0034280819818377495, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9832156300544739, 0.0019143893150612712, 0.004360282327979803, 0.004538257140666246, 0.002060305094346404, 0.0010143517283722758, 0.002896759659051895, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9605527520179749, 0.008186204358935356, 0.004707722924649715, 0.002240509958937764, 0.008306951262056828, 0.0015144703211262822, 0.0076276278123259544, 0.006863788235932589, 0.0, 0.0, 0.0, 0.0], [0.9187482595443726, 0.0011388043640181422, 0.008163578808307648, 0.017176859080791473, 0.022984186187386513, 0.0022011790424585342, 0.009803850203752518, 0.013505992479622364, 0.006277231965214014, 0.0, 0.0, 0.0], [0.9603893160820007, 0.0017607337795197964, 0.0023668804205954075, 0.006790065672248602, 0.012679792940616608, 0.0004223766445647925, 0.003303512930870056, 0.005148803815245628, 0.002339232014492154, 0.004799194633960724, 0.0, 0.0], [0.9678277373313904, 0.0028586050029844046, 0.0009684078977443278, 0.0027497189585119486, 0.007021124474704266, 0.0005153432139195502, 0.00657670060172677, 0.0027436797972768545, 0.0009197646286338568, 0.001676976215094328, 0.006142015103250742, 0.0], [0.9402444362640381, 0.007784521207213402, 0.005595549009740353, 0.004178989678621292, 0.009596201591193676, 0.000526236486621201, 0.004312952049076557, 0.015562310814857483, 0.002860755892470479, 0.0016276492970064282, 0.0008162400336004794, 0.006894135847687721]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9256590008735657, 0.07434093952178955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9915567636489868, 0.0011682562762871385, 0.00727497786283493, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6759268641471863, 0.1271727979183197, 0.11016874760389328, 0.08673159033060074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9256762862205505, 0.021833375096321106, 0.020789649337530136, 0.0039322334341704845, 0.027768341824412346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6604278683662415, 0.05409414693713188, 0.006903978995978832, 0.0022970647551119328, 0.2134467363357544, 0.06283026188611984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5504401326179504, 0.0077161723747849464, 0.04905043542385101, 0.01642593741416931, 0.27882447838783264, 0.04367481917142868, 0.05386805161833763, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6463593244552612, 0.07949896901845932, 0.0331137515604496, 0.013216925784945488, 0.01649431325495243, 0.013463841751217842, 0.016113486140966415, 0.18173940479755402, 0.0, 0.0, 0.0, 0.0], [0.5879252552986145, 0.07636091113090515, 0.037768639624118805, 0.037474583834409714, 0.034805551171302795, 0.004640959668904543, 0.01793419010937214, 0.08935602754354477, 0.11373382061719894, 0.0, 0.0, 0.0], [0.5903455018997192, 0.11167298257350922, 0.029884759336709976, 0.021299226209521294, 0.03282301872968674, 0.0051805488765239716, 0.019491389393806458, 0.06234202906489372, 0.05531122535467148, 0.07164929807186127, 0.0, 0.0], [0.3857288360595703, 0.08567506074905396, 0.011148869059979916, 0.01659177802503109, 0.024626536294817924, 0.0033786206040531397, 0.025753535330295563, 0.02555263787508011, 0.03990224748849869, 0.266242116689682, 0.11539971083402634, 0.0], [0.7153027057647705, 0.05001456290483475, 0.04816705733537674, 0.014798203483223915, 0.01342229638248682, 0.006257118657231331, 0.005401151720434427, 0.07443388551473618, 0.01671433635056019, 0.02688998356461525, 0.010476553812623024, 0.01812214031815529]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9609508514404297, 0.0390491858124733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9997989535331726, 0.00020110233162995428, 1.261105342332744e-14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8598716855049133, 0.0817338302731514, 5.9245110605843365e-05, 0.058335255831480026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8931370973587036, 0.011659977957606316, 3.307328006485477e-05, 0.014301215298473835, 0.08086859434843063, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6514581441879272, 0.012483704835176468, 0.00015726311539765447, 0.005182427819818258, 0.23798954486846924, 0.09272894263267517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7686364650726318, 0.009588642045855522, 4.322515394505899e-07, 0.01519712246954441, 0.12356163561344147, 0.049801040440797806, 0.03321462497115135, 0.0, 0.0, 0.0, 0.0, 0.0], [0.731519341468811, 0.048289164900779724, 0.0011978417169302702, 0.012012754566967487, 0.029558252543210983, 0.009174966253340244, 0.04024762660264969, 0.12799999117851257, 0.0, 0.0, 0.0, 0.0], [0.5644447803497314, 0.023072123527526855, 1.5924008039291948e-05, 0.04031866416335106, 0.06403933465480804, 0.009077566675841808, 0.13588236272335052, 0.06146431714296341, 0.10168498009443283, 0.0, 0.0, 0.0], [0.47045838832855225, 0.026133017614483833, 1.0419045111120795e-06, 0.024696240201592445, 0.06056400015950203, 0.020291516557335854, 0.11406996846199036, 0.04599842429161072, 0.14671114087104797, 0.0910763144493103, 0.0, 0.0], [0.46529993414878845, 0.015757448971271515, 1.8100247416441562e-06, 0.008106976747512817, 0.041194282472133636, 0.030415568500757217, 0.13378606736660004, 0.018252216279506683, 0.055582381784915924, 0.2015642374753952, 0.030038954690098763, 0.0], [0.5738769769668579, 0.025756703689694405, 1.928545998453046e-06, 0.01442476361989975, 0.02390991523861885, 0.009189783595502377, 0.05263228341937065, 0.16798675060272217, 0.024565694853663445, 0.029963573440909386, 0.03171833977103233, 0.04597330093383789]]]], "left_text": ["The", " animal", " didn", "'t", " cross", " the", " street", " because", " it", " was", " too", " tired"], "right_text": ["The", " animal", " didn", "'t", " cross", " the", " street", " because", " it", " was", " too", " tired"]}], "default_filter": "0", "display_mode": "dark", "root_div_id": "bertviz-6d5f6c190c0d473a97e9e93b0a3cd83d", "include_layers": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "include_heads": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "total_heads": 12}; // HACK: {"attention": [{"name": null, "attn": [[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8728697299957275, 0.12713032960891724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5175417065620422, 0.11140565574169159, 0.3710525929927826, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5058048367500305, 0.11489029228687286, 0.3174988925457001, 0.06180591136217117, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3867604434490204, 0.220637246966362, 0.1409534066915512, 0.096247099339962, 0.1554018259048462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.45108598470687866, 0.10241292417049408, 0.16359977424144745, 0.10497768968343735, 0.13571642339229584, 0.04220709949731827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.41864293813705444, 0.18242023885250092, 0.0589672289788723, 0.060029298067092896, 0.07290387153625488, 0.11826886236667633, 0.08876752853393555, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22391554713249207, 0.11939653754234314, 0.18790480494499207, 0.09788685292005539, 0.06084481254220009, 0.07281451672315598, 0.05366430804133415, 0.18357263505458832, 0.0, 0.0, 0.0, 0.0], [0.34604179859161377, 0.061977166682481766, 0.175901859998703, 0.07132770121097565, 0.09440907835960388, 0.044403597712516785, 0.053248897194862366, 0.08974883705377579, 0.06294109672307968, 0.0, 0.0, 0.0], [0.1479593962430954, 0.023442042991518974, 0.404382586479187, 0.03131354972720146, 0.025051061064004898, 0.0123198963701725, 0.03870793431997299, 0.05227704346179962, 0.04183676466345787, 0.22270964086055756, 0.0, 0.0], [0.21109521389007568, 0.12199978530406952, 0.09744543582201004, 0.05828961730003357, 0.08130468428134918, 0.04482971876859665, 0.046579837799072266, 0.09903564304113388, 0.05852463096380234, 0.12149063497781754, 0.059404801577329636, 0.0], [0.18719828128814697, 0.10780622065067291, 0.07435780763626099, 0.03466513380408287, 0.10310203582048416, 0.0584256649017334, 0.09578841924667358, 0.06054351478815079, 0.051962342113256454, 0.053742628544569016, 0.029733479022979736, 0.142674520611763]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [6.108285015216097e-05, 0.99993896484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00021011529315728694, 0.00099235400557518, 0.9987975358963013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0008750453707762063, 0.006663358770310879, 0.20087231695652008, 0.7915893197059631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003292917972430587, 0.0015007079346105456, 0.000736874935682863, 0.0002482582931406796, 0.9971848130226135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.064983069896698, 0.002671959577128291, 0.00010563300020294264, 0.00027576537104323506, 0.00063684320775792, 0.9313266277313232, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00012113703996874392, 0.003704436356201768, 0.0001494394091423601, 0.000288489944068715, 0.001142423483543098, 7.132500468287617e-05, 0.9945228099822998, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00021930693765170872, 0.0002042065025307238, 0.001590381609275937, 0.0001218211546074599, 3.5917539662477793e-06, 6.844720337539911e-05, 1.4673828445666004e-05, 0.9977776408195496, 0.0, 0.0, 0.0, 0.0], [0.008068851195275784, 0.004223143216222525, 0.0006737743970006704, 0.0005198557046242058, 9.300156671088189e-05, 0.017658421769738197, 1.895686364150606e-05, 0.0021341184619814157, 0.96660977602005, 0.0, 0.0, 0.0], [0.0033355734776705503, 0.0030058480333536863, 0.07951563596725464, 0.0015125838108360767, 0.00020864261023234576, 0.0010847202502191067, 4.4731208618031815e-05, 0.0004880736523773521, 0.0009698860230855644, 0.9098342657089233, 0.0, 0.0], [0.0003093187988270074, 0.0008345430251210928, 0.0035225353203713894, 0.0011243205517530441, 6.018464773660526e-05, 0.00027331075398251414, 6.361877603922039e-05, 0.007032756227999926, 0.0023551885969936848, 9.43261111387983e-05, 0.984329879283905, 0.0], [2.6519946914049797e-05, 0.00031819401192478836, 0.0008608945645391941, 0.00017407081031706184, 6.26821129117161e-05, 2.2431158868130296e-05, 0.00019366737978998572, 0.00196713930927217, 3.900060983141884e-05, 2.604008659545798e-05, 0.0005589863285422325, 0.9957504272460938]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8076128363609314, 0.19238720834255219, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7564974427223206, 0.13910256326198578, 0.10439995676279068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6285322308540344, 0.09183657914400101, 0.17913036048412323, 0.10050079226493835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6490501761436462, 0.13083405792713165, 0.09088309109210968, 0.07949371635913849, 0.04973892122507095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4437858462333679, 0.14629819989204407, 0.17920459806919098, 0.11442861706018448, 0.04963948577642441, 0.06664326786994934, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4212614893913269, 0.1986178457736969, 0.06085188314318657, 0.042993731796741486, 0.10431601852178574, 0.11300401389598846, 0.058954983949661255, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4052816927433014, 0.13476884365081787, 0.11341717839241028, 0.09819232672452927, 0.030604762956500053, 0.06306599080562592, 0.02787695825099945, 0.12679223716259003, 0.0, 0.0, 0.0, 0.0], [0.27947285771369934, 0.10286424309015274, 0.167910635471344, 0.0832909643650055, 0.02562699466943741, 0.05126956105232239, 0.039714645594358444, 0.19665390253067017, 0.05319620296359062, 0.0, 0.0, 0.0], [0.30790916085243225, 0.05029373615980148, 0.12240590900182724, 0.07851825654506683, 0.02606876753270626, 0.05091991275548935, 0.02441977709531784, 0.17152781784534454, 0.05295291170477867, 0.11498378217220306, 0.0, 0.0], [0.2208244502544403, 0.03443547710776329, 0.06055028364062309, 0.0898970440030098, 0.028440022841095924, 0.06481733918190002, 0.043153103440999985, 0.2896708548069, 0.05559169128537178, 0.06887681782245636, 0.043742936104536057, 0.0], [0.20316891372203827, 0.053842198103666306, 0.07006055116653442, 0.07429607212543488, 0.08070989698171616, 0.09928829222917557, 0.12549568712711334, 0.09593525528907776, 0.06501322984695435, 0.058736518025398254, 0.04788060486316681, 0.025572730228304863]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06895631551742554, 0.9310437440872192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.016044210642576218, 0.003348082536831498, 0.9806077480316162, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002597825601696968, 0.001177250756882131, 0.8540812134742737, 0.14214369654655457, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.003115046303719282, 0.0021718258503824472, 0.0005197911523282528, 0.0002590379153843969, 0.9939343333244324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06561258435249329, 0.030216623097658157, 0.03344912827014923, 0.01978001929819584, 0.2708275318145752, 0.5801140666007996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.003000790486112237, 0.0014867207501083612, 0.00024781335378065705, 0.00015431756037287414, 0.07085542380809784, 0.00043144385563209653, 0.9238234758377075, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009311525151133537, 0.001187278307043016, 0.016908884048461914, 0.003651350736618042, 0.0024482421576976776, 0.006121673621237278, 0.006837640423327684, 0.953533411026001, 0.0, 0.0, 0.0, 0.0], [0.007349053397774696, 0.00036790885496884584, 0.0009323050617240369, 0.0007442481582984328, 0.002626796020194888, 0.013326027430593967, 0.009429228492081165, 0.03102949447929859, 0.9341950416564941, 0.0, 0.0, 0.0], [0.009169088676571846, 0.0003112829872407019, 0.004256051033735275, 0.0006666898261755705, 0.0002504764124751091, 0.010006257332861423, 0.0019485640805214643, 0.04430656507611275, 0.13488054275512695, 0.7942044138908386, 0.0, 0.0], [0.0021743429824709892, 4.400549005367793e-05, 0.0011337270261719823, 0.0005299060139805079, 0.00038564420538023114, 0.0005951530183665454, 0.0004676577518694103, 0.002957117510959506, 0.029903478920459747, 0.017577586695551872, 0.9442313313484192, 0.0], [0.0003444154281169176, 4.677717697632033e-06, 3.269095759605989e-05, 3.8404403312597424e-05, 2.4318798750755377e-05, 2.835653504007496e-06, 0.00035965166171081364, 0.00018638838082551956, 8.838604117045179e-05, 0.00023993004288058728, 0.00679047591984272, 0.9918877482414246]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4607641398906708, 0.5392358303070068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4751085638999939, 0.11302156001329422, 0.4118698835372925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010727260261774063, 0.010956241749227047, 0.8002898693084717, 0.17802657186985016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06713563203811646, 0.11317005753517151, 0.014099083840847015, 0.0032748980447649956, 0.8023203015327454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19567757844924927, 0.07725901156663895, 0.19520670175552368, 0.0607624426484108, 0.34884950518608093, 0.12224484235048294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.023007115349173546, 0.03443444147706032, 0.002561410889029503, 0.0019284840673208237, 0.09173764288425446, 0.006991400849074125, 0.8393394947052002, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09177704900503159, 0.03550707921385765, 0.18329711258411407, 0.021718988195061684, 0.03723064064979553, 0.07004774361848831, 0.09536559879779816, 0.46505582332611084, 0.0, 0.0, 0.0, 0.0], [0.09844592958688736, 0.023700494319200516, 0.06559878587722778, 0.016079658642411232, 0.07972504198551178, 0.05119725316762924, 0.11837273091077805, 0.2941359281539917, 0.2527441084384918, 0.0, 0.0, 0.0], [0.12338249385356903, 0.020907437428832054, 0.06487789005041122, 0.017104074358940125, 0.02788626402616501, 0.03818007931113243, 0.08251627534627914, 0.15584000945091248, 0.150512233376503, 0.31879323720932007, 0.0, 0.0], [0.02439802512526512, 0.006507936865091324, 0.052472010254859924, 0.016575146466493607, 0.01601746492087841, 0.015596074052155018, 0.012763152830302715, 0.07024283707141876, 0.09645550698041916, 0.19563028216362, 0.4933415949344635, 0.0], [0.004754412453621626, 0.0012759313685819507, 0.0036332185845822096, 0.0007064667297527194, 0.001813474460504949, 0.0010349522344768047, 0.00175969407428056, 0.005657439585775137, 0.003501236904412508, 0.016981450840830803, 0.018394963815808296, 0.9404867887496948]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0350920744240284, 0.9649078845977783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.017149852588772774, 9.22839535633102e-05, 0.9827578663825989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.016695363447070122, 0.00017185522301588207, 0.4026584327220917, 0.5804743766784668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.024308882653713226, 7.026863750070333e-05, 7.09742880644626e-06, 1.3527117062039906e-06, 0.9756124019622803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.369503915309906, 0.08078742772340775, 0.04147397726774216, 0.08062081784009933, 0.048944707959890366, 0.3786691725254059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005368008278310299, 0.0011901067337021232, 1.1595958540056017e-06, 2.2413612441596342e-06, 0.00023564137518405914, 3.6005621950607747e-06, 0.9931991696357727, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010830705985426903, 0.0008522263378836215, 0.0017843025270849466, 0.0003264769911766052, 2.373226743657142e-05, 0.00010243060387438163, 1.073618568625534e-05, 0.9860694408416748, 0.0, 0.0, 0.0, 0.0], [0.08394403755664825, 0.011172972619533539, 0.03143863379955292, 0.014028087258338928, 0.00071554264286533, 0.012287710793316364, 0.0019427718361839652, 0.005782771855592728, 0.8386874198913574, 0.0, 0.0, 0.0], [0.03297869861125946, 0.0009347641025669873, 0.049889467656612396, 0.0013223604764789343, 0.0005436119390651584, 0.0016309588681906462, 0.00023146480089053512, 0.0007070330320857465, 0.0012526813661679626, 0.9105090498924255, 0.0, 0.0], [0.010116259567439556, 0.00015297441859729588, 0.0026823931839317083, 0.0005950400372967124, 6.250196020118892e-05, 1.939378671522718e-05, 9.410779966856353e-06, 0.0016189247835427523, 0.0001303377066506073, 1.8262175217387266e-05, 0.984594464302063, 0.0], [0.0013144222320988774, 5.157303894520737e-05, 0.0028327396139502525, 0.00014328223187476397, 1.7027043213602155e-05, 2.3940978621794784e-07, 9.704353033157531e-06, 5.872030305908993e-05, 8.171849117388774e-07, 2.892753343530785e-07, 0.0002102044818457216, 0.9953610301017761]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7840536236763, 0.21594637632369995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.46175748109817505, 0.2752997875213623, 0.26294267177581787, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.347594290971756, 0.19824321568012238, 0.3449990153312683, 0.10916347801685333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3028227388858795, 0.34311631321907043, 0.08300741761922836, 0.07853998988866806, 0.19251346588134766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1498660296201706, 0.30764830112457275, 0.19471493363380432, 0.10896730422973633, 0.23311977088451385, 0.0056836288422346115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24475958943367004, 0.31070372462272644, 0.03855603560805321, 0.03197957202792168, 0.16650812327861786, 0.028320904821157455, 0.1791720688343048, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12895876169204712, 0.25211194157600403, 0.1414985954761505, 0.058149829506874084, 0.18921132385730743, 0.03846234828233719, 0.13460887968540192, 0.056998275220394135, 0.0, 0.0, 0.0, 0.0], [0.1107507273554802, 0.20980755984783173, 0.13674712181091309, 0.0653587058186531, 0.1750716269016266, 0.008427752181887627, 0.22881995141506195, 0.05217902362346649, 0.01283758133649826, 0.0, 0.0, 0.0], [0.09499160200357437, 0.2059698849916458, 0.09614235162734985, 0.043118856847286224, 0.2156355381011963, 0.009005552157759666, 0.23591184616088867, 0.06384054571390152, 0.014153259806334972, 0.021230628713965416, 0.0, 0.0], [0.10118942707777023, 0.1541300266981125, 0.12498695403337479, 0.07628810405731201, 0.17016486823558807, 0.01900625042617321, 0.16963225603103638, 0.06684330850839615, 0.02166706509888172, 0.03310520946979523, 0.0629865825176239, 0.0], [0.09340905398130417, 0.156208336353302, 0.05762823671102524, 0.0384947769343853, 0.10130387544631958, 0.05469154566526413, 0.20964321494102478, 0.05482856184244156, 0.04473245516419411, 0.03423313423991203, 0.0662151500582695, 0.0886116698384285]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9780775308609009, 0.02192249335348606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6736375093460083, 0.11471686512231827, 0.21164564788341522, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4995661675930023, 0.05211051180958748, 0.303146094083786, 0.1451772302389145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33038750290870667, 0.13192273676395416, 0.31486958265304565, 0.1981905847787857, 0.024629658088088036, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16962504386901855, 0.05462287738919258, 0.13125978410243988, 0.13574470579624176, 0.17157454788684845, 0.33717310428619385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10292115807533264, 0.09207423031330109, 0.05589307099580765, 0.07956819236278534, 0.29818037152290344, 0.28000280261039734, 0.09136013686656952, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14299601316452026, 0.04475784674286842, 0.053097810596227646, 0.06785091012716293, 0.047260161489248276, 0.2249143272638321, 0.11457084864377975, 0.3045520782470703, 0.0, 0.0, 0.0, 0.0], [0.0789123997092247, 0.008341478183865547, 0.026632847264409065, 0.03329024836421013, 0.027116699144244194, 0.10302349925041199, 0.05209316313266754, 0.41128402948379517, 0.2593056559562683, 0.0, 0.0, 0.0], [0.04155365005135536, 0.005958626512438059, 0.01734502613544464, 0.021420443430542946, 0.008064493536949158, 0.062019798904657364, 0.04794998839497566, 0.28655534982681274, 0.25646114349365234, 0.2526715397834778, 0.0, 0.0], [0.07974296063184738, 0.005500698462128639, 0.023976197466254234, 0.049523379653692245, 0.009776486083865166, 0.05415314808487892, 0.011103822849690914, 0.13373155891895294, 0.19987474381923676, 0.28547707200050354, 0.14713986217975616, 0.0], [0.05216839909553528, 0.008333834819495678, 0.012947536073625088, 0.009802688844501972, 0.008580652996897697, 0.04168024659156799, 0.02927258238196373, 0.06936177611351013, 0.11549323797225952, 0.18446815013885498, 0.3898080587387085, 0.0780828669667244]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8899824619293213, 0.1100175753235817, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7910976409912109, 0.13273528218269348, 0.07616700977087021, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5870044231414795, 0.18726570904254913, 0.11894597113132477, 0.10678387433290482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6070454716682434, 0.13816702365875244, 0.08815921097993851, 0.06497915834188461, 0.10164909809827805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11922377347946167, 0.021004417911171913, 0.05086641013622284, 0.019459102302789688, 0.03123021312057972, 0.7582160234451294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4150432348251343, 0.08174128830432892, 0.038821496069431305, 0.04890870675444603, 0.06506572663784027, 0.1816372573375702, 0.1687822937965393, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18959496915340424, 0.037007544189691544, 0.11742503941059113, 0.055452268570661545, 0.037552639842033386, 0.38132354617118835, 0.0895354151725769, 0.09210861474275589, 0.0, 0.0, 0.0, 0.0], [0.1284029632806778, 0.015749910846352577, 0.050582192838191986, 0.017391901463270187, 0.02099684625864029, 0.38308435678482056, 0.03219332918524742, 0.05255797505378723, 0.2990405857563019, 0.0, 0.0, 0.0], [0.11030983179807663, 0.07149005681276321, 0.0278136245906353, 0.01897183246910572, 0.03508180007338524, 0.2551598846912384, 0.03378261625766754, 0.05971577763557434, 0.1384403258562088, 0.2492341697216034, 0.0, 0.0], [0.13853077590465546, 0.04226505756378174, 0.061700064688920975, 0.026694128289818764, 0.030155055224895477, 0.1933630406856537, 0.030162764713168144, 0.06526393443346024, 0.18384860455989838, 0.18518555164337158, 0.04283098876476288, 0.0], [0.18808332085609436, 0.08665277063846588, 0.06464262306690216, 0.05770152062177658, 0.07556959986686707, 0.07506731897592545, 0.12305204570293427, 0.09461119771003723, 0.07921434193849564, 0.050303906202316284, 0.06363522261381149, 0.04146607592701912]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9383811950683594, 0.06161879375576973, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7096535563468933, 0.12009122222661972, 0.17025528848171234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6245108246803284, 0.11435782164335251, 0.14431102573871613, 0.1168203055858612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5780115723609924, 0.14060483872890472, 0.12913811206817627, 0.117121122777462, 0.03512433543801308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4283130466938019, 0.10099487006664276, 0.12637944519519806, 0.10225489735603333, 0.07366866618394852, 0.16838905215263367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38447701930999756, 0.17786811292171478, 0.09393604844808578, 0.08223118633031845, 0.09574440866708755, 0.15477706491947174, 0.010966161265969276, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30859634280204773, 0.07401464879512787, 0.12571625411510468, 0.10065291076898575, 0.07448951154947281, 0.12167088687419891, 0.05235442891716957, 0.14250510931015015, 0.0, 0.0, 0.0, 0.0], [0.2724604904651642, 0.07329657673835754, 0.09941134601831436, 0.07452664524316788, 0.05941953510046005, 0.11507894098758698, 0.04539712518453598, 0.15814168751239777, 0.10226769000291824, 0.0, 0.0, 0.0], [0.21520473062992096, 0.0612168051302433, 0.0975215807557106, 0.06979615241289139, 0.05536844953894615, 0.10722650587558746, 0.048566557466983795, 0.14410267770290375, 0.10709703713655472, 0.09389952570199966, 0.0, 0.0], [0.22600826621055603, 0.05705907195806503, 0.081429623067379, 0.055139318108558655, 0.057059574872255325, 0.09463922679424286, 0.04760555550456047, 0.1230005994439125, 0.09137232601642609, 0.09042489528656006, 0.07626152783632278, 0.0], [0.20337553322315216, 0.06865828484296799, 0.07323914766311646, 0.07007752358913422, 0.0471903495490551, 0.07860611379146576, 0.07656153291463852, 0.1007312759757042, 0.07251296192407608, 0.07606791704893112, 0.09149494767189026, 0.041484393179416656]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7349589467048645, 0.26504114270210266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6350484490394592, 0.07779435068368912, 0.28715723752975464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4731510281562805, 0.0820721983909607, 0.1918269395828247, 0.2529498338699341, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5389819741249084, 0.09492167830467224, 0.05702498182654381, 0.053141962736845016, 0.25592944025993347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5582868456840515, 0.09634020924568176, 0.058614153414964676, 0.05053364485502243, 0.04481681436300278, 0.19140827655792236, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3542708456516266, 0.09667258709669113, 0.04984694719314575, 0.057038456201553345, 0.06632541120052338, 0.11566125601530075, 0.26018455624580383, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2819109559059143, 0.06420554965734482, 0.139084592461586, 0.07838134467601776, 0.02713886648416519, 0.09132640808820724, 0.02755514159798622, 0.29039716720581055, 0.0, 0.0, 0.0, 0.0], [0.25015780329704285, 0.07111040502786636, 0.07981448620557785, 0.0587257444858551, 0.03078356198966503, 0.1258556991815567, 0.024951079860329628, 0.09503911435604095, 0.2635622024536133, 0.0, 0.0, 0.0], [0.19839204847812653, 0.07056555896997452, 0.14053791761398315, 0.057803548872470856, 0.02710641920566559, 0.09296996146440506, 0.024104677140712738, 0.0847659781575203, 0.09305982291698456, 0.21069404482841492, 0.0, 0.0], [0.21326251327991486, 0.045818574726581573, 0.10826554149389267, 0.06558641791343689, 0.0304541178047657, 0.07516511529684067, 0.028256254270672798, 0.08140641450881958, 0.07022910565137863, 0.052346743643283844, 0.22920916974544525, 0.0], [0.14975866675376892, 0.045595683157444, 0.09922152012586594, 0.06701158732175827, 0.0272412970662117, 0.06311651319265366, 0.039232827723026276, 0.08968015760183334, 0.04797147586941719, 0.04765913635492325, 0.08759023994207382, 0.23592089116573334]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7732552289962769, 0.22674483060836792, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7912213802337646, 0.09266883879899979, 0.11610986292362213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5883223414421082, 0.0983569547533989, 0.1432572901248932, 0.17006340622901917, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48703742027282715, 0.1118689700961113, 0.1425783932209015, 0.12304051965475082, 0.13547472655773163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3959304988384247, 0.08376403898000717, 0.13426341116428375, 0.11199831962585449, 0.0767139121890068, 0.19732977449893951, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3574475646018982, 0.09473831206560135, 0.11103996634483337, 0.09657836705446243, 0.09167300164699554, 0.09265297651290894, 0.15586984157562256, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3627004623413086, 0.07596524059772491, 0.08705926686525345, 0.08842978626489639, 0.06313448399305344, 0.054963767528533936, 0.07222599536180496, 0.1955210268497467, 0.0, 0.0, 0.0, 0.0], [0.2852361798286438, 0.06665609031915665, 0.08555057644844055, 0.09368794411420822, 0.055589959025382996, 0.062020909041166306, 0.07374361157417297, 0.17022117972373962, 0.10729356110095978, 0.0, 0.0, 0.0], [0.2766849398612976, 0.0571669340133667, 0.10888494551181793, 0.07267199456691742, 0.058557722717523575, 0.0566844716668129, 0.05878359079360962, 0.14458636939525604, 0.06512584537267685, 0.10085316002368927, 0.0, 0.0], [0.22204309701919556, 0.05345913767814636, 0.08119961619377136, 0.10553348809480667, 0.0626453086733818, 0.0358818955719471, 0.0488056018948555, 0.182176411151886, 0.052070748060941696, 0.03290907293558121, 0.12327562272548676, 0.0], [0.17174239456653595, 0.043927084654569626, 0.09396396577358246, 0.07147853821516037, 0.03878185898065567, 0.032659366726875305, 0.03652893751859665, 0.18229392170906067, 0.073262058198452, 0.05900287628173828, 0.10664978623390198, 0.08970915526151657]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.979947030544281, 0.020053040236234665, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9064003229141235, 0.04455103352665901, 0.049048569053411484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2723913788795471, 0.01160728931427002, 0.696338415145874, 0.019662918522953987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14442968368530273, 0.04174760729074478, 0.5063616633415222, 0.2511478364467621, 0.05631326884031296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3186441659927368, 0.02709266170859337, 0.13278278708457947, 0.17500078678131104, 0.1905987560749054, 0.15588079392910004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1709321141242981, 0.035454489290714264, 0.03797627612948418, 0.05808206647634506, 0.3221748471260071, 0.2711712121963501, 0.10420902073383331, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1971396654844284, 0.03036244586110115, 0.18091575801372528, 0.17116889357566833, 0.059210654348134995, 0.13205456733703613, 0.045205872505903244, 0.18394222855567932, 0.0, 0.0, 0.0, 0.0], [0.13221381604671478, 0.019768381491303444, 0.06372471153736115, 0.06933099776506424, 0.06309947371482849, 0.09296020120382309, 0.030701227486133575, 0.3849904537200928, 0.14321079850196838, 0.0, 0.0, 0.0], [0.06980615854263306, 0.00614056084305048, 0.008247887715697289, 0.012550330720841885, 0.013560635037720203, 0.03587082028388977, 0.01228304859250784, 0.16868868470191956, 0.601778507232666, 0.07107339799404144, 0.0, 0.0], [0.0608648881316185, 0.0038375426083803177, 0.04279155656695366, 0.10038410127162933, 0.026612071320414543, 0.04333258792757988, 0.01065671257674694, 0.08342711627483368, 0.1718553900718689, 0.38817715644836426, 0.06806083768606186, 0.0], [0.014886012300848961, 0.0018363917479291558, 0.01209049392491579, 0.010846177116036415, 0.003093752544373274, 0.016056740656495094, 0.0017555614467710257, 0.020901760086417198, 0.030798381194472313, 0.23313745856285095, 0.5502512454986572, 0.10434607416391373]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9551265239715576, 0.04487345367670059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8799847960472107, 0.08408363908529282, 0.03593158349394798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8051981925964355, 0.08723460137844086, 0.051951903849840164, 0.055615298449993134, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5739929676055908, 0.04412629082798958, 0.1285352110862732, 0.14267240464687347, 0.11067305505275726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.44761157035827637, 0.05743060261011124, 0.08780208975076675, 0.10678548365831375, 0.16958056390285492, 0.1307896375656128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.41970279812812805, 0.024734431877732277, 0.08340764045715332, 0.07035268843173981, 0.08245468884706497, 0.23245976865291595, 0.08688792586326599, 0.0, 0.0, 0.0, 0.0, 0.0], [0.42025232315063477, 0.04738416522741318, 0.048687130212783813, 0.06443719565868378, 0.06721972674131393, 0.12863847613334656, 0.09138281643390656, 0.131998211145401, 0.0, 0.0, 0.0, 0.0], [0.22643543779850006, 0.02606033906340599, 0.02284727618098259, 0.029372673481702805, 0.05598103627562523, 0.08910596370697021, 0.08331067115068436, 0.34287595748901367, 0.12401062995195389, 0.0, 0.0, 0.0], [0.17026743292808533, 0.015088323503732681, 0.009379584342241287, 0.026578467339277267, 0.04159500077366829, 0.07079079747200012, 0.045527368783950806, 0.32141274213790894, 0.18145078420639038, 0.11790948361158371, 0.0, 0.0], [0.1350724846124649, 0.016675980761647224, 0.013373014517128468, 0.01955663040280342, 0.05289120227098465, 0.05380149930715561, 0.05511067435145378, 0.1726294755935669, 0.1553964465856552, 0.19214408099651337, 0.1333485245704651, 0.0], [0.07148509472608566, 0.00749995606020093, 0.009808235801756382, 0.016794810071587563, 0.022439315915107727, 0.047335729002952576, 0.031363796442747116, 0.20149238407611847, 0.1096927598118782, 0.2121623009443283, 0.17322920262813568, 0.09669644385576248]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9840556383132935, 0.015944339334964752, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8753244280815125, 0.029910502955317497, 0.09476509690284729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7785478830337524, 0.03376587852835655, 0.08547811210155487, 0.10220813751220703, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6078203916549683, 0.037794966250658035, 0.09666099399328232, 0.10369881242513657, 0.15402479469776154, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.43524253368377686, 0.03078000620007515, 0.06343892216682434, 0.07255823910236359, 0.10661431401968002, 0.29136598110198975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.37486594915390015, 0.028744203969836235, 0.06359034031629562, 0.06930729746818542, 0.095894955098629, 0.265307754278183, 0.10228946059942245, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28450676798820496, 0.025613868609070778, 0.05001720413565636, 0.055339615792036057, 0.07876990735530853, 0.19157899916172028, 0.08155117928981781, 0.2326224148273468, 0.0, 0.0, 0.0, 0.0], [0.22293546795845032, 0.0230584554374218, 0.038695383816957474, 0.04352843761444092, 0.06766421347856522, 0.1633482426404953, 0.06899033486843109, 0.19938266277313232, 0.17239682376384735, 0.0, 0.0, 0.0], [0.17135027050971985, 0.021583618596196175, 0.03695805370807648, 0.038456737995147705, 0.05774207413196564, 0.12768331170082092, 0.06149352714419365, 0.16471555829048157, 0.12349909543991089, 0.19651781022548676, 0.0, 0.0], [0.15861618518829346, 0.01985713094472885, 0.029123583808541298, 0.0323592871427536, 0.0519796721637249, 0.1182146742939949, 0.054614391177892685, 0.1511715203523636, 0.11388956755399704, 0.15963254868984222, 0.1105414479970932, 0.0], [0.12451773881912231, 0.024423295632004738, 0.0329747349023819, 0.03324553370475769, 0.04873286932706833, 0.10237710922956467, 0.0552690364420414, 0.13593509793281555, 0.10538710653781891, 0.13878096640110016, 0.09587018191814423, 0.10248634219169617]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6076840758323669, 0.39231589436531067, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5740119218826294, 0.22664694488048553, 0.1993410587310791, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5490851402282715, 0.16984055936336517, 0.14071883261203766, 0.1403554379940033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5604156851768494, 0.10848326981067657, 0.10910706967115402, 0.10159764438867569, 0.12039632350206375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5460138320922852, 0.08237838745117188, 0.0802190974354744, 0.07730230689048767, 0.09164692461490631, 0.1224394366145134, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5261914730072021, 0.0677928552031517, 0.06919460743665695, 0.06449531018733978, 0.07680013030767441, 0.09788374602794647, 0.09764182567596436, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5131775736808777, 0.05574741214513779, 0.051236238330602646, 0.049592673778533936, 0.06212632358074188, 0.07881930470466614, 0.07740150392055511, 0.11189895868301392, 0.0, 0.0, 0.0, 0.0], [0.526820719242096, 0.04039672017097473, 0.039656419306993484, 0.038343656808137894, 0.049558475613594055, 0.06518222391605377, 0.06257982552051544, 0.09358059614896774, 0.08388140052556992, 0.0, 0.0, 0.0], [0.529583215713501, 0.0338798463344574, 0.03318331390619278, 0.03119697980582714, 0.04034705460071564, 0.05263498052954674, 0.05049256235361099, 0.08114458620548248, 0.06535084545612335, 0.08218658715486526, 0.0, 0.0], [0.5429602861404419, 0.027156753465533257, 0.025428326800465584, 0.024930426850914955, 0.03425537422299385, 0.045957356691360474, 0.044235460460186005, 0.07109387218952179, 0.05870058387517929, 0.0700608491897583, 0.055220719426870346, 0.0], [0.5506265759468079, 0.022231576964259148, 0.021587437018752098, 0.020916588604450226, 0.02898082509636879, 0.03786320611834526, 0.03647599741816521, 0.059800852090120316, 0.050082769244909286, 0.06114479899406433, 0.050266657024621964, 0.060022708028554916]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8702662587165833, 0.12973372638225555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8472357392311096, 0.09009046107530594, 0.06267386674880981, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.782696545124054, 0.09648110717535019, 0.06159596145153046, 0.05922645330429077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7116276025772095, 0.0798899456858635, 0.05689363181591034, 0.049426473677158356, 0.10216224938631058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.601807177066803, 0.09040296077728271, 0.054780904203653336, 0.05295631289482117, 0.10202852636575699, 0.09802403301000595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6155767440795898, 0.06183405965566635, 0.042170554399490356, 0.04380261152982712, 0.07423573732376099, 0.0720072090625763, 0.09037303924560547, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5020349621772766, 0.0681060329079628, 0.04269871860742569, 0.039827678352594376, 0.08607848733663559, 0.0705646499991417, 0.09173229336738586, 0.0989571139216423, 0.0, 0.0, 0.0, 0.0], [0.45950639247894287, 0.059754569083452225, 0.0354926735162735, 0.03442965820431709, 0.07413391023874283, 0.06531193852424622, 0.08086792379617691, 0.09575708955526352, 0.09474585205316544, 0.0, 0.0, 0.0], [0.4242171049118042, 0.055863428860902786, 0.03791385889053345, 0.03367750346660614, 0.06485656648874283, 0.05714495852589607, 0.06973648816347122, 0.08327874541282654, 0.06851263344287872, 0.10479874163866043, 0.0, 0.0], [0.4251870810985565, 0.04572660103440285, 0.025779863819479942, 0.027172980830073357, 0.058162983506917953, 0.05110367760062218, 0.06293302029371262, 0.07560289651155472, 0.0698137879371643, 0.08496536314487457, 0.07355175912380219, 0.0], [0.3971458375453949, 0.04355883598327637, 0.02725597843527794, 0.023847106844186783, 0.046799108386039734, 0.044024109840393066, 0.0540614128112793, 0.06523529440164566, 0.058209050446748734, 0.07153372466564178, 0.06782133132219315, 0.10050824284553528]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9218317270278931, 0.07816830277442932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8978624939918518, 0.01866903342306614, 0.08346851915121078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7895718812942505, 0.014598335139453411, 0.028746197000145912, 0.16708362102508545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6952294111251831, 0.11708205193281174, 0.03865331411361694, 0.0947684720158577, 0.05426681786775589, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7230411171913147, 0.03417448699474335, 0.025864381343126297, 0.08141802996397018, 0.04185297340154648, 0.09364903718233109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5197308659553528, 0.09920022636651993, 0.01570837013423443, 0.024404294788837433, 0.07784514129161835, 0.0959615558385849, 0.16714954376220703, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4812636375427246, 0.026900850236415863, 0.02467389963567257, 0.06395314633846283, 0.013288728892803192, 0.0744088664650917, 0.0035473397001624107, 0.3119634687900543, 0.0, 0.0, 0.0, 0.0], [0.4392852485179901, 0.050179239362478256, 0.027471525594592094, 0.027292031794786453, 0.010739617049694061, 0.08148384094238281, 0.007755375932902098, 0.06490097939968109, 0.2908921539783478, 0.0, 0.0, 0.0], [0.4640936553478241, 0.016027523204684258, 0.095416359603405, 0.0877053514122963, 0.006881945766508579, 0.08289765566587448, 0.0044562420807778835, 0.1074572503566742, 0.040652040392160416, 0.0944119319319725, 0.0, 0.0], [0.36210981011390686, 0.017365405336022377, 0.020258352160453796, 0.2149999439716339, 0.01839406043291092, 0.07594575732946396, 0.003529764711856842, 0.15045060217380524, 0.04410959407687187, 0.03428013250231743, 0.05855671688914299, 0.0], [0.20636208355426788, 0.08818274736404419, 0.053778547793626785, 0.1101035624742508, 0.0295663233846426, 0.08989465236663818, 0.009544558823108673, 0.11749652028083801, 0.044293664395809174, 0.06098956614732742, 0.13603566586971283, 0.05375209078192711]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7630558013916016, 0.23694424331188202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8104128241539001, 0.16593848168849945, 0.02364877052605152, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8013966083526611, 0.05908661335706711, 0.08803565055131912, 0.051481135189533234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6542818546295166, 0.08055344223976135, 0.07231783121824265, 0.093824103474617, 0.09902267903089523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6464332342147827, 0.09665412455797195, 0.06958901137113571, 0.06390134245157242, 0.05082916095852852, 0.07259304821491241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5932385325431824, 0.07786495983600616, 0.04866541549563408, 0.026224525645375252, 0.0359153114259243, 0.06502923369407654, 0.15306195616722107, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4648856222629547, 0.08402153849601746, 0.1546728014945984, 0.14775429666042328, 0.02312767319381237, 0.06596957892179489, 0.02099539339542389, 0.03857317194342613, 0.0, 0.0, 0.0, 0.0], [0.4725463092327118, 0.28130587935447693, 0.031354039907455444, 0.045714255422353745, 0.035225410014390945, 0.05393796041607857, 0.020315248519182205, 0.043893616646528244, 0.015707310289144516, 0.0, 0.0, 0.0], [0.4615643620491028, 0.06369355320930481, 0.09353318810462952, 0.07067464292049408, 0.04094003885984421, 0.05316901579499245, 0.0341290645301342, 0.08444718271493912, 0.045428041368722916, 0.05242094025015831, 0.0, 0.0], [0.3452893793582916, 0.06941735744476318, 0.04384656995534897, 0.13930663466453552, 0.05961472913622856, 0.0849718376994133, 0.025742970407009125, 0.05666351690888405, 0.07251816987991333, 0.0729379802942276, 0.0296908151358366, 0.0], [0.4520183801651001, 0.03357364982366562, 0.05398453027009964, 0.035573069006204605, 0.0353504978120327, 0.07412748783826828, 0.05191688612103462, 0.04278302937746048, 0.06815274804830551, 0.050793930888175964, 0.0564747154712677, 0.04525120183825493]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.972019374370575, 0.027980569750070572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.766235888004303, 0.16074949502944946, 0.07301460951566696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7264899015426636, 0.09499865770339966, 0.09379664063453674, 0.08471482992172241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6163113117218018, 0.12251747399568558, 0.08084671199321747, 0.12036369740962982, 0.05996077135205269, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5449367165565491, 0.08718199282884598, 0.09454697370529175, 0.09300707280635834, 0.08671275526285172, 0.09361441433429718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.581535816192627, 0.13962030410766602, 0.04561341553926468, 0.07957355678081512, 0.031826429069042206, 0.06989289075136185, 0.05193748697638512, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4521293044090271, 0.06534257531166077, 0.06223730370402336, 0.08815828710794449, 0.073446124792099, 0.05534704774618149, 0.08042723685503006, 0.12291213124990463, 0.0, 0.0, 0.0, 0.0], [0.5168890953063965, 0.05495457723736763, 0.04661063477396965, 0.04772607609629631, 0.054437849670648575, 0.054699886590242386, 0.08823376893997192, 0.09498725831508636, 0.04146074503660202, 0.0, 0.0, 0.0], [0.45206570625305176, 0.07043009251356125, 0.03654498606920242, 0.046732399612665176, 0.062179744243621826, 0.04561404883861542, 0.07900413870811462, 0.12783221900463104, 0.04042116552591324, 0.03917555510997772, 0.0, 0.0], [0.4915085434913635, 0.046815503388643265, 0.0224485844373703, 0.054474372416734695, 0.06866524368524551, 0.043236277997493744, 0.058763518929481506, 0.08669673651456833, 0.0398537777364254, 0.04208027571439743, 0.045457061380147934, 0.0], [0.3946869373321533, 0.06832289695739746, 0.02945793606340885, 0.03097362071275711, 0.07954288274049759, 0.05126291513442993, 0.10539431124925613, 0.08275118470191956, 0.0355067178606987, 0.04803762957453728, 0.04208837449550629, 0.0319746732711792]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9688529372215271, 0.031147044152021408, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8570253849029541, 0.062306661158800125, 0.0806679055094719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8029581904411316, 0.04412786290049553, 0.05619947612285614, 0.09671443700790405, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7389590740203857, 0.040562234818935394, 0.07495889812707901, 0.09387699514627457, 0.0516427606344223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5993561744689941, 0.051472924649715424, 0.049909934401512146, 0.07945625483989716, 0.05817067623138428, 0.16163399815559387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5680141448974609, 0.036817993968725204, 0.05957496166229248, 0.08119294792413712, 0.05851511284708977, 0.1539078652858734, 0.04197695478796959, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48524320125579834, 0.043800197541713715, 0.05856427922844887, 0.07296425849199295, 0.05363108590245247, 0.13263963162899017, 0.04517727345228195, 0.10798004269599915, 0.0, 0.0, 0.0, 0.0], [0.4732201099395752, 0.02950545959174633, 0.04531603679060936, 0.06493087112903595, 0.037241507321596146, 0.12293045222759247, 0.030782992020249367, 0.10388990491628647, 0.09218274801969528, 0.0, 0.0, 0.0], [0.4268310070037842, 0.030795885249972343, 0.04433230683207512, 0.06361854076385498, 0.03509882465004921, 0.11961077153682709, 0.031144065782427788, 0.08233419060707092, 0.08275330811738968, 0.08348111063241959, 0.0, 0.0], [0.3738822042942047, 0.03556617721915245, 0.03818495571613312, 0.05299520492553711, 0.03664463385939598, 0.11148156225681305, 0.042840056121349335, 0.0729127749800682, 0.10209716111421585, 0.0776481181383133, 0.05574718862771988, 0.0], [0.3833029270172119, 0.026266418397426605, 0.046954914927482605, 0.04724700748920441, 0.03048652410507202, 0.0995405837893486, 0.029856879264116287, 0.08838009089231491, 0.0776040181517601, 0.09249158948659897, 0.052949681878089905, 0.024919403716921806]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9848400950431824, 0.015159910544753075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9608851671218872, 0.014262314885854721, 0.02485252544283867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9154780507087708, 0.009755338542163372, 0.022220587357878685, 0.052546028047800064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8640547394752502, 0.022632481530308723, 0.03669228032231331, 0.0643068477511406, 0.012313623912632465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7343919277191162, 0.01643194817006588, 0.03234057500958443, 0.05545142665505409, 0.02478897012770176, 0.13659514486789703, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7495422959327698, 0.02294948138296604, 0.0315098874270916, 0.06747899204492569, 0.016226673498749733, 0.08359023183584213, 0.028702476993203163, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6013484597206116, 0.02158457413315773, 0.037559930235147476, 0.061698153614997864, 0.02471565641462803, 0.1450560837984085, 0.0265207439661026, 0.08151637762784958, 0.0, 0.0, 0.0, 0.0], [0.5195896029472351, 0.013673385605216026, 0.02469421736896038, 0.05734190344810486, 0.023027218878269196, 0.1530330926179886, 0.017911896109580994, 0.08565989881753922, 0.10506880283355713, 0.0, 0.0, 0.0], [0.49409326910972595, 0.019376453012228012, 0.030479811131954193, 0.056374695152044296, 0.026226021349430084, 0.1314636766910553, 0.02314276620745659, 0.08470799773931503, 0.08495794236660004, 0.04917730763554573, 0.0, 0.0], [0.4547763466835022, 0.027728945016860962, 0.020163388922810555, 0.048880938440561295, 0.02525123581290245, 0.13165032863616943, 0.031582314521074295, 0.04620339721441269, 0.11664771288633347, 0.03626316413283348, 0.060852326452732086, 0.0], [0.4784049689769745, 0.023097189143300056, 0.035647861659526825, 0.04248501732945442, 0.0198901928961277, 0.1021590456366539, 0.03027435392141342, 0.06000401824712753, 0.09482362866401672, 0.04951559379696846, 0.04960549250245094, 0.01409261766821146]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00023010728182271123, 0.9997698664665222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00040440820157527924, 0.5760704278945923, 0.4235251545906067, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0006699447403661907, 0.4373238682746887, 0.29587674140930176, 0.2661294639110565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0007628405001014471, 0.2933131456375122, 0.21884940564632416, 0.19266118109226227, 0.2944134473800659, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0013304917374625802, 0.22389216721057892, 0.16386117041110992, 0.15429355204105377, 0.22289718687534332, 0.23372547328472137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0007811126415617764, 0.17180658876895905, 0.13584180176258087, 0.1269332766532898, 0.16967187821865082, 0.17110370099544525, 0.2238616943359375, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0011363985249772668, 0.1293402761220932, 0.09965262562036514, 0.0909978374838829, 0.13142170011997223, 0.13494838774204254, 0.17862166464328766, 0.23388105630874634, 0.0, 0.0, 0.0, 0.0], [0.0018828052561730146, 0.09856385737657547, 0.07240129262208939, 0.06875976920127869, 0.11190880089998245, 0.11563269048929214, 0.14963652193546295, 0.21002690494060516, 0.17118731141090393, 0.0, 0.0, 0.0], [0.0017585208406671882, 0.083652563393116, 0.06346957385540009, 0.05951950326561928, 0.09480505436658859, 0.09138507395982742, 0.12164143472909927, 0.17406365275382996, 0.13257531821727753, 0.1771293431520462, 0.0, 0.0], [0.0017982524586841464, 0.07162737101316452, 0.05087161809206009, 0.04886774346232414, 0.08079848438501358, 0.08193361014127731, 0.11045757681131363, 0.1608104556798935, 0.11930069327354431, 0.15031492710113525, 0.1232193112373352, 0.0], [0.0021407450549304485, 0.05850968137383461, 0.04200172796845436, 0.04180064797401428, 0.07000071555376053, 0.06888872385025024, 0.0903313085436821, 0.13403572142124176, 0.10497260093688965, 0.12410125881433487, 0.1082136482000351, 0.15500320494174957]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4703635573387146, 0.5296363830566406, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09644557535648346, 0.00046337247476913035, 0.9030910730361938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1662207841873169, 0.0012884103925898671, 0.029675235971808434, 0.8028154969215393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07393239438533783, 0.002491587307304144, 0.0011616703122854233, 0.0016388826770707965, 0.9207754731178284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39317989349365234, 0.0016690263291820884, 0.0022225764114409685, 0.00592972245067358, 0.0005455518257804215, 0.5964532494544983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09042319655418396, 0.007328287232667208, 0.0010630202013999224, 0.0016174084739759564, 0.003815952455624938, 0.0030302933882921934, 0.8927218317985535, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11827344447374344, 0.0023825904354453087, 0.006021094508469105, 0.00265209237113595, 0.0002592849195934832, 0.0024089738726615906, 0.0005828177672810853, 0.8674197196960449, 0.0, 0.0, 0.0, 0.0], [0.09650203585624695, 0.003430234966799617, 0.003914874047040939, 0.0025358637794852257, 0.0004887667600996792, 0.021757174283266068, 0.0018492029048502445, 0.0022155013866722584, 0.8673063516616821, 0.0, 0.0, 0.0], [0.07428883016109467, 0.001193568343296647, 0.04355399310588837, 0.011594090610742569, 0.00019835159764625132, 0.006407168693840504, 0.0007153560291044414, 0.006364504806697369, 0.016579916700720787, 0.8391042351722717, 0.0, 0.0], [0.03366722911596298, 0.00040106652886606753, 0.0025250082835555077, 0.004791175946593285, 0.00028016557916998863, 0.0007122975657694042, 0.000632882583886385, 0.008528497070074081, 0.0016292856307700276, 0.001422783941961825, 0.9454097151756287, 0.0], [0.055681731551885605, 0.0009613372385501862, 0.0024059081915766, 0.0019102273508906364, 0.0005529737682081759, 0.0019506729440763593, 0.0010463668731972575, 0.010695857927203178, 0.0003851715009659529, 0.0011233465047553182, 0.01299989316612482, 0.9102866053581238]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8781078457832336, 0.12189211696386337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6164735555648804, 0.19103063642978668, 0.19249577820301056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6650122404098511, 0.12475281208753586, 0.15640965104103088, 0.05382528528571129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5632638931274414, 0.09697529673576355, 0.11476663500070572, 0.0953347235918045, 0.12965941429138184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3389432430267334, 0.07471293210983276, 0.08159822225570679, 0.14115813374519348, 0.31209391355514526, 0.05149354785680771, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4212939143180847, 0.06571400910615921, 0.04560428485274315, 0.11548054218292236, 0.2409122735261917, 0.08692669123411179, 0.02406831830739975, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23083534836769104, 0.04499159753322601, 0.07112487405538559, 0.14779967069625854, 0.28178486227989197, 0.03674672544002533, 0.09242646396160126, 0.09429051727056503, 0.0, 0.0, 0.0, 0.0], [0.2732803225517273, 0.031674861907958984, 0.0702551081776619, 0.0804426521062851, 0.076832115650177, 0.027498045936226845, 0.05925281345844269, 0.2976400852203369, 0.08312404900789261, 0.0, 0.0, 0.0], [0.2441769689321518, 0.022628450766205788, 0.09720192104578018, 0.0733688548207283, 0.09339921921491623, 0.01914278417825699, 0.04514671117067337, 0.28711599111557007, 0.05994873866438866, 0.05787045508623123, 0.0, 0.0], [0.17422117292881012, 0.01404612883925438, 0.053765296936035156, 0.1819821149110794, 0.09341471642255783, 0.022614533081650734, 0.034402161836624146, 0.17333924770355225, 0.042598556727170944, 0.07513485103845596, 0.13448117673397064, 0.0], [0.26459449529647827, 0.018104610964655876, 0.04464280605316162, 0.07766652852296829, 0.06467735767364502, 0.026884667575359344, 0.04807349666953087, 0.14864404499530792, 0.04848519340157509, 0.03841494396328926, 0.12109217047691345, 0.09871960431337357]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9332111477851868, 0.06678880751132965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.823287844657898, 0.142595112323761, 0.03411710634827614, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7973390221595764, 0.15168169140815735, 0.032679665833711624, 0.018299609422683716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8104780912399292, 0.058685604482889175, 0.04641744866967201, 0.05605899915099144, 0.02835986204445362, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8055204749107361, 0.06521234661340714, 0.044814109802246094, 0.043998170644044876, 0.020638369023799896, 0.019816480576992035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7404448390007019, 0.033930033445358276, 0.011560495011508465, 0.042004719376564026, 0.07076054811477661, 0.03288567066192627, 0.06841365247964859, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8444225788116455, 0.07805097848176956, 0.009932824410498142, 0.011446095071732998, 0.012716361321508884, 0.010639584623277187, 0.016574356704950333, 0.01621723733842373, 0.0, 0.0, 0.0, 0.0], [0.7817609310150146, 0.0868074968457222, 0.019452117383480072, 0.017004400491714478, 0.013132546097040176, 0.013470613397657871, 0.040290169417858124, 0.015268928371369839, 0.01281281653791666, 0.0, 0.0, 0.0], [0.7691628336906433, 0.06588425487279892, 0.020462455227971077, 0.01662065088748932, 0.016864582896232605, 0.013528340496122837, 0.033799927681684494, 0.024586357176303864, 0.009423044510185719, 0.029667558148503304, 0.0, 0.0], [0.8633047342300415, 0.038117025047540665, 0.007750991731882095, 0.018767759203910828, 0.005141099449247122, 0.005130196921527386, 0.005833812523633242, 0.033420782536268234, 0.0038051342125982046, 0.013171013444662094, 0.005557485856115818, 0.0], [0.7065597772598267, 0.04700806364417076, 0.02238195389509201, 0.01639433391392231, 0.03139510750770569, 0.01754208654165268, 0.02735884301364422, 0.039188142865896225, 0.008682917803525925, 0.051923058927059174, 0.010514219291508198, 0.021051548421382904]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9379645586013794, 0.06203542649745941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7079555988311768, 0.24472026526927948, 0.047324102371931076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03715604543685913, 0.018268484622240067, 0.9409029483795166, 0.0036725325044244528, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22538582980632782, 0.005044805351644754, 0.6956296563148499, 0.0491027794778347, 0.024836944416165352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04836393520236015, 0.000954647024627775, 0.030809493735432625, 0.02715085633099079, 0.8526884913444519, 0.04003256559371948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02019609697163105, 0.0015556745929643512, 0.036298081278800964, 0.014755629003047943, 0.8196247220039368, 0.08057141304016113, 0.026998432353138924, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16762953996658325, 0.000587791670113802, 0.012316002510488033, 0.04345786198973656, 0.03088608756661415, 0.40523314476013184, 0.06328568607568741, 0.27660390734672546, 0.0, 0.0, 0.0, 0.0], [0.04366714134812355, 0.00022709081531502306, 0.00296855834312737, 0.002727970713749528, 0.011186012998223305, 0.06274452805519104, 0.08613348007202148, 0.7055478692054749, 0.08479733765125275, 0.0, 0.0, 0.0], [0.019737225025892258, 3.866089537041262e-05, 7.319901487790048e-05, 0.0008717062883079052, 0.0006596603197976947, 0.005625881254673004, 0.004456520080566406, 0.053980883210897446, 0.8280009627342224, 0.08655531704425812, 0.0, 0.0], [0.016411777585744858, 7.675777851545718e-06, 6.868194759590551e-05, 8.072177297435701e-05, 0.00040444653131999075, 0.0033121800515800714, 0.0008003880502656102, 0.012667200528085232, 0.43930771946907043, 0.4885188937187195, 0.03842031955718994, 0.0], [0.060993436723947525, 7.77978784753941e-05, 0.0004060136852785945, 0.00020667102944571525, 0.0006278342334553599, 0.0022513840813189745, 0.0010990225709974766, 0.012721770443022251, 0.07257095724344254, 0.28912219405174255, 0.4415247440338135, 0.11839824169874191]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9912973642349243, 0.008702684193849564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8648866415023804, 0.09594505280256271, 0.0391683392226696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8359034657478333, 0.11275361478328705, 0.024740837514400482, 0.026602065190672874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7440686821937561, 0.06632532924413681, 0.026145070791244507, 0.03581717982888222, 0.1276436746120453, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.31843435764312744, 0.013179956935346127, 0.0267920084297657, 0.043430984020233154, 0.4265963137149811, 0.17156633734703064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14179986715316772, 0.004731707274913788, 0.0218634195625782, 0.013440828770399094, 0.12196134775876999, 0.6652882695198059, 0.03091456927359104, 0.0, 0.0, 0.0, 0.0, 0.0], [0.221032977104187, 0.008466227911412716, 0.07445935904979706, 0.0396663174033165, 0.1500423103570938, 0.29216456413269043, 0.07217342406511307, 0.14199481904506683, 0.0, 0.0, 0.0, 0.0], [0.09185487776994705, 0.002501819981262088, 0.006770754233002663, 0.007829136215150356, 0.04507555440068245, 0.16715432703495026, 0.029191777110099792, 0.564083456993103, 0.0855383649468422, 0.0, 0.0, 0.0], [0.12729421257972717, 0.0010979826329275966, 0.004122352227568626, 0.00581877026706934, 0.02883356250822544, 0.11280454695224762, 0.020912544801831245, 0.4405272603034973, 0.21420541405677795, 0.04438337683677673, 0.0, 0.0], [0.057975538074970245, 0.0002739007177297026, 0.0013533690944314003, 0.0030177205335348845, 0.02802245505154133, 0.0673532485961914, 0.0065277134999632835, 0.4956493675708771, 0.23367507755756378, 0.07504769414663315, 0.03110402449965477, 0.0], [0.10563216358423233, 0.0009625510429032147, 0.0020227984059602022, 0.0014707164373248816, 0.019648760557174683, 0.0892830565571785, 0.010372592136263847, 0.07168444991111755, 0.32808971405029297, 0.03909464180469513, 0.14632777869701385, 0.18541084229946136]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.93415766954422, 0.06584230810403824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8781709671020508, 0.02717074751853943, 0.09465830773115158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7062388062477112, 0.03728875145316124, 0.24489866197109222, 0.01157375704497099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8126732707023621, 0.07964880019426346, 0.08058363944292068, 0.007200552150607109, 0.019893664866685867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4347286820411682, 0.017637748271226883, 0.02783961221575737, 0.10469397157430649, 0.29196664690971375, 0.12313339114189148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23461021482944489, 0.013393808156251907, 0.028551394119858742, 0.011351880617439747, 0.40035349130630493, 0.3019402325153351, 0.00979908648878336, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48288485407829285, 0.003346700919792056, 0.08956298977136612, 0.15661408007144928, 0.023249968886375427, 0.1860189288854599, 0.0016905857482925057, 0.05663188174366951, 0.0, 0.0, 0.0, 0.0], [0.39638856053352356, 0.007130577228963375, 0.016744393855333328, 0.013370837084949017, 0.006458314601331949, 0.06643921136856079, 0.012001914903521538, 0.4646482467651367, 0.016817938536405563, 0.0, 0.0, 0.0], [0.34083396196365356, 0.0006720253149978817, 0.006472840439528227, 0.014820754528045654, 0.005580693017691374, 0.03332879766821861, 0.0011610648361966014, 0.2690477669239044, 0.24367675185203552, 0.08440529555082321, 0.0, 0.0], [0.20390523970127106, 0.000153121814946644, 0.017549574375152588, 0.3541663885116577, 0.0009578660828992724, 0.027641739696264267, 8.7510677985847e-05, 0.07152913510799408, 0.05104142799973488, 0.2649223804473877, 0.008045703172683716, 0.0], [0.2707310616970062, 0.00037032441468909383, 0.006185768637806177, 0.007864363491535187, 0.0009796275990083814, 0.040453121066093445, 0.006254659499973059, 0.021984709426760674, 0.011691758409142494, 0.15498913824558258, 0.43243300914764404, 0.046062443405389786]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9590114951133728, 0.040988512337207794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5909857749938965, 0.29490604996681213, 0.11410824209451675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7424482107162476, 0.07264469563961029, 0.1396339237689972, 0.045273177325725555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5215661525726318, 0.05047183856368065, 0.18099644780158997, 0.19013552367687225, 0.05682998150587082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3786081373691559, 0.06172652542591095, 0.15618978440761566, 0.2035628855228424, 0.10177110135555267, 0.09814158082008362, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24982644617557526, 0.017605241388082504, 0.14165279269218445, 0.11939530819654465, 0.05709546059370041, 0.28054073452949524, 0.13388407230377197, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12028275430202484, 0.016026677563786507, 0.03646903485059738, 0.051655691117048264, 0.12589678168296814, 0.19984306395053864, 0.18626657128334045, 0.26355937123298645, 0.0, 0.0, 0.0, 0.0], [0.09398014098405838, 0.004285357892513275, 0.004469349980354309, 0.0072449506260454655, 0.006311268545687199, 0.04345414787530899, 0.059877097606658936, 0.6592835783958435, 0.12109418958425522, 0.0, 0.0, 0.0], [0.09061430394649506, 0.0013481259811669588, 0.0008830557344481349, 0.002764447359368205, 0.007852896116673946, 0.04917768016457558, 0.03815363347530365, 0.4651167690753937, 0.21454475820064545, 0.12954437732696533, 0.0, 0.0], [0.056537192314863205, 0.001229340210556984, 0.0018526456551626325, 0.0013827537186443806, 0.0038750397507101297, 0.02679772675037384, 0.018294908106327057, 0.2956799268722534, 0.21861840784549713, 0.34043604135513306, 0.03529601916670799, 0.0], [0.06780596077442169, 0.0018421338172629476, 0.002587857423350215, 0.0027698760386556387, 0.003574688220396638, 0.017963245511054993, 0.03389628976583481, 0.12340184301137924, 0.18436744809150696, 0.26132410764694214, 0.24650393426418304, 0.05396261066198349]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9736940860748291, 0.026305966079235077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9209668636322021, 0.030091503635048866, 0.04894161969423294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8649247288703918, 0.03205401077866554, 0.04293571412563324, 0.06008556857705116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8901171088218689, 0.013402741402387619, 0.02664656564593315, 0.03422350063920021, 0.03561001271009445, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8113662004470825, 0.011968432925641537, 0.03363415226340294, 0.037792086601257324, 0.032452091574668884, 0.07278704643249512, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8191323280334473, 0.010194334201514721, 0.028570281341671944, 0.03664132207632065, 0.020372258499264717, 0.061434555798769, 0.023654859513044357, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7478240728378296, 0.010737818665802479, 0.02998773753643036, 0.026183634996414185, 0.029361456632614136, 0.050624873489141464, 0.01880902796983719, 0.08647137135267258, 0.0, 0.0, 0.0, 0.0], [0.7198331952095032, 0.009542803280055523, 0.022422324866056442, 0.021884340792894363, 0.01618967205286026, 0.03239963948726654, 0.016953716054558754, 0.07919379323720932, 0.08158048987388611, 0.0, 0.0, 0.0], [0.7244294881820679, 0.006173714529722929, 0.014071539975702763, 0.016882434487342834, 0.014386113733053207, 0.026978041976690292, 0.014173042960464954, 0.07686232775449753, 0.06194736808538437, 0.04409588873386383, 0.0, 0.0], [0.5530064702033997, 0.00840730406343937, 0.017932601273059845, 0.0255845095962286, 0.02281295694410801, 0.0459609217941761, 0.013337604701519012, 0.11343717575073242, 0.0837775468826294, 0.03909250721335411, 0.07665035128593445, 0.0], [0.6812024712562561, 0.005081912502646446, 0.011029524728655815, 0.01381220668554306, 0.008537770248949528, 0.02969549410045147, 0.009517206810414791, 0.07080018520355225, 0.057230833917856216, 0.022029759362339973, 0.05572707578539848, 0.0353354811668396]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7174798250198364, 0.2825200855731964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5321810245513916, 0.148194819688797, 0.319624125957489, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49307969212532043, 0.10650792717933655, 0.21861152350902557, 0.18180087208747864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.40419021248817444, 0.07372257858514786, 0.13404139876365662, 0.11584916710853577, 0.2721967101097107, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2947199046611786, 0.04160097986459732, 0.10102204978466034, 0.08106310665607452, 0.20569005608558655, 0.2759039103984833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24270351231098175, 0.041247762739658356, 0.07888814806938171, 0.0601796992123127, 0.15548306703567505, 0.16124945878982544, 0.2602483034133911, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15333375334739685, 0.0247360672801733, 0.05465046688914299, 0.039793603122234344, 0.11203107237815857, 0.12232091277837753, 0.1755170226097107, 0.3176170885562897, 0.0, 0.0, 0.0, 0.0], [0.13877515494823456, 0.01843683235347271, 0.04112793132662773, 0.028804156929254532, 0.06946352124214172, 0.08256173133850098, 0.10517999529838562, 0.2463608831167221, 0.26928985118865967, 0.0, 0.0, 0.0], [0.1199040338397026, 0.01487675029784441, 0.031828153878450394, 0.022936033084988594, 0.054138340055942535, 0.06396904587745667, 0.07627788186073303, 0.1862259954214096, 0.19804146885871887, 0.23180240392684937, 0.0, 0.0], [0.12630590796470642, 0.015686102211475372, 0.028281889855861664, 0.02116563729941845, 0.05268620327115059, 0.059369105845689774, 0.068016417324543, 0.14470727741718292, 0.17204780876636505, 0.19920207560062408, 0.1125316247344017, 0.0], [0.14840739965438843, 0.01661476492881775, 0.025700176134705544, 0.01897691935300827, 0.04167233034968376, 0.047815874218940735, 0.06246653199195862, 0.12499336898326874, 0.1255367547273636, 0.14674519002437592, 0.08876433968544006, 0.15230637788772583]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9369667172431946, 0.06303325295448303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7100542187690735, 0.1202579215168953, 0.169687882065773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13701392710208893, 0.01138237863779068, 0.8253353834152222, 0.026268305256962776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3931029438972473, 0.010305263102054596, 0.37625741958618164, 0.08749116212129593, 0.13284313678741455, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12200688570737839, 0.008694413118064404, 0.0985313206911087, 0.0681489109992981, 0.6705349087715149, 0.03208352252840996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19365546107292175, 0.009477543644607067, 0.04317101463675499, 0.05981353670358658, 0.5394402742385864, 0.10020936280488968, 0.054232798516750336, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13985542953014374, 0.010571797378361225, 0.09438531845808029, 0.16049186885356903, 0.34686002135276794, 0.10269001126289368, 0.05728447437286377, 0.0878610759973526, 0.0, 0.0, 0.0, 0.0], [0.19576559960842133, 0.0031221627723425627, 0.053796298801898956, 0.03173494338989258, 0.1254771649837494, 0.08873596787452698, 0.0685482993721962, 0.3735332489013672, 0.05928633362054825, 0.0, 0.0, 0.0], [0.24252833425998688, 0.001316024106927216, 0.03527138754725456, 0.021554838865995407, 0.07222966849803925, 0.06741803139448166, 0.016597477719187737, 0.22136302292346954, 0.03975214436650276, 0.2819690704345703, 0.0, 0.0], [0.06255188584327698, 0.00034409918589517474, 0.0016009265091270208, 0.003741259453818202, 0.025818554684519768, 0.033243875950574875, 0.005965142976492643, 0.10557804256677628, 0.046737879514694214, 0.6736611723899841, 0.040757182985544205, 0.0], [0.1558845043182373, 0.0005898639792576432, 0.0017844068352133036, 0.004590149968862534, 0.009998415596783161, 0.02667095884680748, 0.003405632684007287, 0.037321269512176514, 0.05449407547712326, 0.4541740417480469, 0.13782721757888794, 0.1132594645023346]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7902512550354004, 0.2097487598657608, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16302646696567535, 0.8188311457633972, 0.018142392858862877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21496212482452393, 0.7441778779029846, 0.028864892199635506, 0.011995168402791023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2576298713684082, 0.5512527823448181, 0.06071155518293381, 0.03953644260764122, 0.09086935967206955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1743863970041275, 0.17801572382450104, 0.03725041449069977, 0.02944694086909294, 0.5401284098625183, 0.040772080421447754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1690203845500946, 0.08488938212394714, 0.04634728282690048, 0.03722001239657402, 0.29029759764671326, 0.17155815660953522, 0.20066718757152557, 0.0, 0.0, 0.0, 0.0, 0.0], [0.052473701536655426, 0.0397942028939724, 0.017460301518440247, 0.011766470968723297, 0.16572000086307526, 0.065220907330513, 0.6219889521598816, 0.025575540959835052, 0.0, 0.0, 0.0, 0.0], [0.09761087596416473, 0.02446305751800537, 0.009704435244202614, 0.008824140764772892, 0.08726299554109573, 0.04076192528009415, 0.35046592354774475, 0.09055014699697495, 0.29035651683807373, 0.0, 0.0, 0.0], [0.0891936719417572, 0.016631746664643288, 0.004590357653796673, 0.0025735972449183464, 0.060088884085416794, 0.018169047310948372, 0.2089393585920334, 0.07626218348741531, 0.45058995485305786, 0.07296118885278702, 0.0, 0.0], [0.06603395938873291, 0.007157415151596069, 0.004313490353524685, 0.003372266422957182, 0.0903496965765953, 0.01674657315015793, 0.0638502910733223, 0.06875314563512802, 0.41355639696121216, 0.16250348091125488, 0.10336321592330933, 0.0], [0.07272497564554214, 0.0305596012622118, 0.0023116839583963156, 0.0009517553844489157, 0.01897965744137764, 0.006342370994389057, 0.09928803890943527, 0.028887443244457245, 0.4353678226470947, 0.03425458446145058, 0.14930544793605804, 0.12102659791707993]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8651140928268433, 0.13488595187664032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7069637179374695, 0.13403700292110443, 0.1589992642402649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6579862236976624, 0.10959306359291077, 0.11564951390028, 0.11677118390798569, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5272865295410156, 0.08985163271427155, 0.08828489482402802, 0.0921960100531578, 0.2023809254169464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4782703220844269, 0.08764707297086716, 0.08274314552545547, 0.07801370322704315, 0.1678975224494934, 0.10542823374271393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3823850154876709, 0.06394025683403015, 0.059817925095558167, 0.05747097358107567, 0.14363016188144684, 0.08353633433580399, 0.2092193067073822, 0.0, 0.0, 0.0, 0.0, 0.0], [0.34527620673179626, 0.048548709601163864, 0.06020848825573921, 0.05394059419631958, 0.11346247792243958, 0.06415403634309769, 0.12884928286075592, 0.18556033074855804, 0.0, 0.0, 0.0, 0.0], [0.2516897916793823, 0.04683006927371025, 0.05075565353035927, 0.04517180100083351, 0.08541659265756607, 0.057843394577503204, 0.10375666618347168, 0.18626761436462402, 0.17226842045783997, 0.0, 0.0, 0.0], [0.23134355247020721, 0.03772030025720596, 0.04131350293755531, 0.04102708026766777, 0.08295273780822754, 0.04716109856963158, 0.07933615148067474, 0.15425843000411987, 0.13029208779335022, 0.1545950472354889, 0.0, 0.0], [0.18546068668365479, 0.03300140053033829, 0.03182963281869888, 0.033861931413412094, 0.06720530986785889, 0.044443778693675995, 0.07274916023015976, 0.12987762689590454, 0.12584485113620758, 0.1473376303911209, 0.1283879429101944, 0.0], [0.1627577245235443, 0.024810824543237686, 0.025144211947917938, 0.028414679691195488, 0.05169982090592384, 0.038615357130765915, 0.06539981067180634, 0.10991564393043518, 0.1000967025756836, 0.10192864388227463, 0.10087453573942184, 0.1903419941663742]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.932966411113739, 0.067033551633358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8618248105049133, 0.06330804526805878, 0.07486721128225327, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8436559438705444, 0.05087690055370331, 0.049410294741392136, 0.05605683848261833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8058751821517944, 0.040660977363586426, 0.03847081586718559, 0.03894269838929176, 0.07605024427175522, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7697471976280212, 0.035759881138801575, 0.035084210336208344, 0.032907795161008835, 0.05759386345744133, 0.06890703737735748, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7374449372291565, 0.02757931873202324, 0.029758905991911888, 0.028281286358833313, 0.036742087453603745, 0.056699249893426895, 0.0834941491484642, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6900503039360046, 0.025390060618519783, 0.02861245721578598, 0.025389475747942924, 0.03885332867503166, 0.03964974731206894, 0.07139166444540024, 0.08066301047801971, 0.0, 0.0, 0.0, 0.0], [0.6860940456390381, 0.016467422246932983, 0.017849231138825417, 0.01583140715956688, 0.02867799811065197, 0.028850499540567398, 0.046248577535152435, 0.07272989302873611, 0.08725086599588394, 0.0, 0.0, 0.0], [0.6597928404808044, 0.014094915241003036, 0.013287689536809921, 0.014738171361386776, 0.02721545472741127, 0.026123249903321266, 0.036225587129592896, 0.06444042921066284, 0.06819253414869308, 0.07588916271924973, 0.0, 0.0], [0.6366190910339355, 0.015182534232735634, 0.011112377047538757, 0.012331817299127579, 0.02593938633799553, 0.024615852162241936, 0.03139957785606384, 0.048720043152570724, 0.058031268417835236, 0.07494818419218063, 0.06109994277358055, 0.0], [0.6339138150215149, 0.010275943204760551, 0.009939133189618587, 0.009871595539152622, 0.019191591069102287, 0.019791187718510628, 0.028154002502560616, 0.049737464636564255, 0.0529216043651104, 0.04739025607705116, 0.05127650499343872, 0.06753694266080856]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9462036490440369, 0.05379636585712433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9859046339988708, 0.0010813204571604729, 0.013014083728194237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9427915811538696, 0.0010751297231763601, 0.017473088577389717, 0.03866022825241089, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9755438566207886, 0.00025894722784869373, 0.00010120125079993159, 0.00107152434065938, 0.023024415597319603, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9841082096099854, 0.005002154968678951, 0.0006323120323941112, 0.0002135200338670984, 3.7998248444637284e-05, 0.010005765594542027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9657293558120728, 0.00037993284058757126, 0.00010600060340948403, 0.00011835432087536901, 2.70258078671759e-05, 0.004543195944279432, 0.029096106067299843, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9533812403678894, 0.00031049156677909195, 0.0007170988828875124, 0.0010181042598560452, 1.0633904821588658e-05, 0.007014530245214701, 1.0494230991753284e-05, 0.03753732889890671, 0.0, 0.0, 0.0, 0.0], [0.8992465734481812, 0.02959398366510868, 0.000716496491804719, 0.0005821764934808016, 3.1428640795638785e-05, 0.010119626298546791, 0.00021349822054617107, 0.0005901998956687748, 0.05890617147088051, 0.0, 0.0, 0.0], [0.9564356207847595, 0.0017393501475453377, 0.004936703015118837, 0.0003567417443264276, 4.017451647087e-05, 0.0077008577063679695, 5.147924093762413e-05, 0.0004949343856424093, 0.002805377123877406, 0.025438854470849037, 0.0, 0.0], [0.9276311993598938, 0.0005981308058835566, 0.002999079879373312, 0.002071782946586609, 2.0481427782215178e-05, 0.00794089213013649, 4.463898221729323e-05, 0.0002562312292866409, 0.0007683400763198733, 0.00035894676693715155, 0.05731029808521271, 0.0], [0.8226107954978943, 0.0002930026385001838, 0.005195077508687973, 0.0036377517972141504, 8.86755806277506e-05, 0.017185620963573456, 0.0004103920073248446, 0.0003644432872533798, 0.00020085895084775984, 0.00509214261546731, 0.00046304764691740274, 0.14445827901363373]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.934181272983551, 0.06581870466470718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.946675717830658, 0.0348019003868103, 0.01852232962846756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8878981471061707, 0.046532485634088516, 0.026716630905866623, 0.03885267674922943, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7278714776039124, 0.03480902686715126, 0.07431179285049438, 0.12827998399734497, 0.034727707505226135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6610495448112488, 0.017683668062090874, 0.06894843280315399, 0.1414782702922821, 0.018861697986721992, 0.0919782891869545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6017756462097168, 0.019233180209994316, 0.10405001789331436, 0.1481013298034668, 0.0127408467233181, 0.0989888608455658, 0.015110144391655922, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6149452328681946, 0.009367543272674084, 0.06363137066364288, 0.12537536025047302, 0.03152664750814438, 0.035621579736471176, 0.005293888505548239, 0.11423838138580322, 0.0, 0.0, 0.0, 0.0], [0.5520167946815491, 0.009736021049320698, 0.031615182757377625, 0.02587008662521839, 0.00751398503780365, 0.07077289372682571, 0.016069753095507622, 0.23858679831027985, 0.047818493098020554, 0.0, 0.0, 0.0], [0.5053149461746216, 0.005238383077085018, 0.005833242554217577, 0.011323629878461361, 0.005753076169639826, 0.05265110731124878, 0.015692707151174545, 0.2686934471130371, 0.0671958401799202, 0.062303610146045685, 0.0, 0.0], [0.365776389837265, 0.00967499241232872, 0.013094278052449226, 0.0065970695577561855, 0.007869667373597622, 0.021574942395091057, 0.002572321565821767, 0.16977234184741974, 0.2834668755531311, 0.0911540687084198, 0.02844703011214733, 0.0], [0.43649736046791077, 0.002584250411018729, 0.008319142274558544, 0.009658015333116055, 0.006572813261300325, 0.01012723334133625, 0.00230936030857265, 0.1450168639421463, 0.25118187069892883, 0.08513636142015457, 0.01961982250213623, 0.02297699823975563]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8475502133369446, 0.15244972705841064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4268895387649536, 0.45276033878326416, 0.12035007774829865, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3262058198451996, 0.2552734911441803, 0.37729567289352417, 0.041224971413612366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10651334375143051, 0.03409997373819351, 0.4198276400566101, 0.22789905965328217, 0.21165995299816132, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04040772467851639, 0.0028312441427260637, 0.0426565557718277, 0.03170691803097725, 0.8076382875442505, 0.07475925236940384, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0806674212217331, 0.006583390757441521, 0.016676386818289757, 0.017218464985489845, 0.2559136748313904, 0.6083461046218872, 0.014594513922929764, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11650186777114868, 0.016504643484950066, 0.05102887377142906, 0.056034691631793976, 0.4465913772583008, 0.16767466068267822, 0.004269096069037914, 0.14139483869075775, 0.0, 0.0, 0.0, 0.0], [0.27949729561805725, 0.004680721089243889, 0.004073943477123976, 0.027938948944211006, 0.007497725542634726, 0.03518160805106163, 0.004174832254648209, 0.5182017683982849, 0.11875316500663757, 0.0, 0.0, 0.0], [0.1720791608095169, 0.0009431965299881995, 0.00011011768219759688, 0.002658707555383444, 0.0014664753107354045, 0.026084113866090775, 0.0023385584354400635, 0.4547872841358185, 0.29068970680236816, 0.048842739313840866, 0.0, 0.0], [0.2584436237812042, 0.0006138923345133662, 0.0002999557473231107, 0.0020178784616291523, 0.0032765865325927734, 0.008634144440293312, 0.000887721311300993, 0.08229543268680573, 0.1740889698266983, 0.43950697779655457, 0.0299347136169672, 0.0], [0.042027682065963745, 5.580105789704248e-05, 0.00014719620230607688, 0.0007677275571040809, 0.00029123813146725297, 0.0019376394338905811, 9.673462045611814e-05, 0.022064529359340668, 0.04079076275229454, 0.08050014823675156, 0.8002156615257263, 0.011104898527264595]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9778113961219788, 0.022188598290085793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8856669664382935, 0.10262762010097504, 0.011705362237989902, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5711549520492554, 0.020835628733038902, 0.34595465660095215, 0.06205468252301216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8675779104232788, 0.0029287526849657297, 0.023038797080516815, 0.008203993551433086, 0.09825059771537781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17063988745212555, 0.0038937849458307028, 0.010593392886221409, 0.00941922701895237, 0.7828241586685181, 0.02262955904006958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18013757467269897, 0.007006799802184105, 0.0061373659409582615, 0.009514699690043926, 0.7311381101608276, 0.062246814370155334, 0.0038186535239219666, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8846362233161926, 0.001095319981686771, 0.0009580539190210402, 0.004804512020200491, 0.010888881981372833, 0.01123096514493227, 0.001137223676778376, 0.08524884283542633, 0.0, 0.0, 0.0, 0.0], [0.43809089064598083, 0.00025616688071750104, 0.0025699555408209562, 0.0036727874539792538, 0.007057029753923416, 0.001999892061576247, 0.0016585997072979808, 0.5273739695549011, 0.017320577055215836, 0.0, 0.0, 0.0], [0.4491119384765625, 0.00023905841226223856, 0.0007206295849755406, 0.0007735940162092447, 0.0017132335342466831, 0.0015869137132540345, 0.000425097270635888, 0.22148583829402924, 0.21787409484386444, 0.10606969147920609, 0.0, 0.0], [0.227415531873703, 2.0085386495338753e-05, 0.0005525473388843238, 0.0070381760597229, 0.0017499318346381187, 0.0025515174493193626, 8.639314182801172e-05, 0.08004384487867355, 0.41885268688201904, 0.23752480745315552, 0.02416449971497059, 0.0], [0.8164998888969421, 0.00037504153442569077, 0.0005529123009182513, 0.00023904019326437265, 0.004184219986200333, 0.0007241279818117619, 0.0005008660373277962, 0.010204427875578403, 0.00695461593568325, 0.05242817848920822, 0.07450960576534271, 0.03282708674669266]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9973061084747314, 0.0026938924565911293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9833701848983765, 0.015229934826493263, 0.0013999724760651588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9694795608520508, 0.01317347027361393, 0.011636162176728249, 0.005710878409445286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9588772058486938, 0.023039821535348892, 0.011424114927649498, 0.006120182573795319, 0.0005386777338571846, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9631021022796631, 0.018249068409204483, 0.0030617681331932545, 0.0027161743491888046, 0.002595692640170455, 0.01027526892721653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9579458236694336, 0.018401505425572395, 0.002447160193696618, 0.006091397255659103, 0.00419163191691041, 0.010504133999347687, 0.0004182756529189646, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9676074385643005, 0.004889665637165308, 0.004013672471046448, 0.003380576381459832, 0.002149938140064478, 0.005730265751481056, 0.0037748857866972685, 0.00845354050397873, 0.0, 0.0, 0.0, 0.0], [0.9491918087005615, 0.01031600870192051, 0.007181162945926189, 0.0016818776493892074, 0.001220678212121129, 0.011364162899553776, 0.003179075662046671, 0.0069913859479129314, 0.00887387152761221, 0.0, 0.0, 0.0], [0.940422773361206, 0.010778812691569328, 0.004137901123613119, 0.0023584766313433647, 0.0018587415106594563, 0.010733678005635738, 0.003805146785452962, 0.006860995665192604, 0.010862688533961773, 0.008180786855518818, 0.0, 0.0], [0.956041693687439, 0.006083235610276461, 0.004101088736206293, 0.002086978405714035, 0.0027238531038165092, 0.007339073810726404, 0.0031898566521704197, 0.005603219848126173, 0.005675863474607468, 0.006961008068174124, 0.00019410581444390118, 0.0], [0.9106211066246033, 0.027075840160250664, 0.0018459341954439878, 0.003990750294178724, 0.013256181962788105, 0.015319242142140865, 0.009530570358037949, 0.0026450955774635077, 0.0034587529953569174, 0.004586934112012386, 0.0037229591980576515, 0.0039466251619160175]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9244049787521362, 0.07559501379728317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8413457274436951, 0.12676087021827698, 0.031893447041511536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7983846664428711, 0.11939822882413864, 0.06120268628001213, 0.021014433354139328, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.617295503616333, 0.26533445715904236, 0.09038365632295609, 0.006888146046549082, 0.020098160952329636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7058608531951904, 0.13515134155750275, 0.04769744724035263, 0.019744670018553734, 0.055128056555986404, 0.03641761094331741, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7561709880828857, 0.039225853979587555, 0.037663765251636505, 0.018122108653187752, 0.048967063426971436, 0.06614930182695389, 0.03370086848735809, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5732577443122864, 0.05628063902258873, 0.054805539548397064, 0.025832965970039368, 0.058667197823524475, 0.06369299441576004, 0.12524616718292236, 0.042216770350933075, 0.0, 0.0, 0.0, 0.0], [0.43667295575141907, 0.29494547843933105, 0.07805941998958588, 0.02462742105126381, 0.022452501580119133, 0.04949364438652992, 0.03455051779747009, 0.026887422427535057, 0.032310664653778076, 0.0, 0.0, 0.0], [0.46524375677108765, 0.1231870949268341, 0.13833998143672943, 0.05199529975652695, 0.03753659129142761, 0.04482344537973404, 0.05072709918022156, 0.02859198860824108, 0.02094249427318573, 0.03861227259039879, 0.0, 0.0], [0.6167864799499512, 0.08427728712558746, 0.06628365069627762, 0.02360384352505207, 0.016323436051607132, 0.03751397877931595, 0.02886350452899933, 0.03820088878273964, 0.05038609728217125, 0.03162790462374687, 0.0061329505406320095, 0.0], [0.32179737091064453, 0.1377774327993393, 0.06780994683504105, 0.0073193153366446495, 0.04428977146744728, 0.03387845680117607, 0.1433209776878357, 0.029624588787555695, 0.05317685008049011, 0.040202025324106216, 0.019032984972000122, 0.1017703041434288]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6848293542861938, 0.3151707053184509, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.405917227268219, 0.5538331866264343, 0.04024963825941086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5064757466316223, 0.2512700855731964, 0.1836850941181183, 0.058569081127643585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18066567182540894, 0.38471657037734985, 0.3004553020000458, 0.09840826690196991, 0.035754136741161346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.36469215154647827, 0.20566584169864655, 0.047985345125198364, 0.07275111973285675, 0.19145463407039642, 0.11745092272758484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30597013235092163, 0.08811716735363007, 0.0689636617898941, 0.08613263815641403, 0.07111217081546783, 0.2875581383705139, 0.09214616566896439, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15204812586307526, 0.3995952904224396, 0.033719711005687714, 0.0835268422961235, 0.03886743262410164, 0.12229737639427185, 0.10339587181806564, 0.06654931604862213, 0.0, 0.0, 0.0, 0.0], [0.3858775794506073, 0.054666146636009216, 0.016103658825159073, 0.021154530346393585, 0.008699505589902401, 0.012611358426511288, 0.042083490639925, 0.12304147332906723, 0.33576229214668274, 0.0, 0.0, 0.0], [0.2563883066177368, 0.006225172430276871, 0.0031406923662871122, 0.009269846603274345, 0.014981541782617569, 0.0029313445556908846, 0.0051747821271419525, 0.1047133207321167, 0.472259521484375, 0.12491544336080551, 0.0, 0.0], [0.22488461434841156, 0.006588257849216461, 0.0030539147555828094, 0.005225357599556446, 0.003515986492857337, 0.0018982253968715668, 0.003341678297147155, 0.05538947135210037, 0.41693413257598877, 0.1977931410074234, 0.08137518167495728, 0.0], [0.13688454031944275, 0.01187199167907238, 0.003071862505748868, 0.0035710057709366083, 0.002265369053930044, 0.0014930753968656063, 0.006160548888146877, 0.05137980356812477, 0.34265607595443726, 0.1919621378183365, 0.12970131635665894, 0.11898223310709]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9180123805999756, 0.08198761194944382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3949938416481018, 0.5839554667472839, 0.021050643175840378, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3877755403518677, 0.5568139553070068, 0.02831251733005047, 0.027097946032881737, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3355889320373535, 0.5343419313430786, 0.059505440294742584, 0.03628109395503998, 0.034282635897397995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3992776572704315, 0.1104472428560257, 0.019662750884890556, 0.016560113057494164, 0.33270061016082764, 0.12135162949562073, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1380329579114914, 0.06448762118816376, 0.18865221738815308, 0.13999728858470917, 0.12433087825775146, 0.25660088658332825, 0.08789817243814468, 0.0, 0.0, 0.0, 0.0, 0.0], [0.42494866251945496, 0.021410871297121048, 0.02127196453511715, 0.011130346916615963, 0.156648650765419, 0.07137682288885117, 0.22730380296707153, 0.06590884923934937, 0.0, 0.0, 0.0, 0.0], [0.4433141350746155, 0.005116062238812447, 0.0028766710311174393, 0.004301641136407852, 0.02347750775516033, 0.016305193305015564, 0.017478210851550102, 0.32150524854660034, 0.16562539339065552, 0.0, 0.0, 0.0], [0.40416419506073, 0.004320567008107901, 0.002450478496029973, 0.0015975813148543239, 0.024319665506482124, 0.011469296179711819, 0.011005894280970097, 0.10314484685659409, 0.3152018189430237, 0.12232562899589539, 0.0, 0.0], [0.40542247891426086, 0.0005554989329539239, 0.0021443134173750877, 0.00036240200279280543, 0.03350996598601341, 0.005181205924600363, 0.003017916576936841, 0.03355051204562187, 0.24882422387599945, 0.23244649171829224, 0.03498493507504463, 0.0], [0.5131354331970215, 0.005842556711286306, 0.0005023505073040724, 0.0001326373458141461, 0.017861418426036835, 0.005983739160001278, 0.010271260514855385, 0.016466530039906502, 0.1642664521932602, 0.08876743167638779, 0.03159203380346298, 0.14517810940742493]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9614267349243164, 0.03857320174574852, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6386594772338867, 0.08767986297607422, 0.2736606299877167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7111484408378601, 0.10255236178636551, 0.1398307830095291, 0.04646850377321243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.34518516063690186, 0.06946074962615967, 0.3812117874622345, 0.09928049147129059, 0.1048617959022522, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23724718391895294, 0.027826305478811264, 0.06943078339099884, 0.03797012194991112, 0.5739367604255676, 0.053588803857564926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16556432843208313, 0.00540589215233922, 0.05101452395319939, 0.039844341576099396, 0.6768275499343872, 0.051274027675390244, 0.010069329291582108, 0.0, 0.0, 0.0, 0.0, 0.0], [0.32803288102149963, 0.0047265635803341866, 0.04449059069156647, 0.05225992575287819, 0.2915233373641968, 0.029499247670173645, 0.05724119022488594, 0.19222630560398102, 0.0, 0.0, 0.0, 0.0], [0.13233308494091034, 0.0003766294976230711, 0.002018038649111986, 0.008727647364139557, 0.0022303592413663864, 0.00039485719753429294, 0.002645060420036316, 0.8244432806968689, 0.026830947026610374, 0.0, 0.0, 0.0], [0.24178439378738403, 0.0005728285759687424, 0.0016106656985357404, 0.00904602650552988, 0.008923226036131382, 0.0020288946107029915, 0.004533377010375261, 0.5979434847831726, 0.035811930894851685, 0.09774508327245712, 0.0, 0.0], [0.3394607901573181, 0.0003767766756936908, 0.0011075539514422417, 0.0007220124243758619, 0.003524676663801074, 0.0004385215579532087, 0.004461744800209999, 0.3389549255371094, 0.07574637979269028, 0.2162509560585022, 0.01895555853843689, 0.0], [0.410976380109787, 0.0007699335110373795, 0.002405997598543763, 0.001362967537716031, 0.029753968119621277, 0.0006104142521508038, 0.004897443577647209, 0.06702261418104172, 0.09554196894168854, 0.35167041420936584, 0.024177778512239456, 0.010810214094817638]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9539893865585327, 0.04601064324378967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9122704863548279, 0.06606083363294601, 0.021668674424290657, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5102095603942871, 0.04938278719782829, 0.40510088205337524, 0.03530684486031532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2631271779537201, 0.04231131821870804, 0.43990498781204224, 0.021146390587091446, 0.23351016640663147, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09409203380346298, 0.004995512776076794, 0.02625870332121849, 0.016330450773239136, 0.8483418822288513, 0.009981410577893257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1013886108994484, 0.001290985383093357, 0.024362735450267792, 0.025670234113931656, 0.8180003762245178, 0.019878540188074112, 0.009408443234860897, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2809317708015442, 0.009141347371041775, 0.36079326272010803, 0.17584992945194244, 0.08852569013834, 0.01629437319934368, 0.004121338948607445, 0.06434230506420135, 0.0, 0.0, 0.0, 0.0], [0.5295581221580505, 0.0033636451698839664, 0.02279689535498619, 0.0416388139128685, 0.02408556453883648, 0.00494986679404974, 0.0030154669657349586, 0.27245450019836426, 0.09813707321882248, 0.0, 0.0, 0.0], [0.5156345963478088, 0.007988205179572105, 0.0018925534095615149, 0.014108862727880478, 0.00636411365121603, 0.002628510119393468, 0.0013930859277024865, 0.2506510317325592, 0.053619176149368286, 0.14571987092494965, 0.0, 0.0], [0.25978711247444153, 0.0008039651438593864, 0.012721340171992779, 0.030898092314600945, 0.0009560766047798097, 0.001091594691388309, 0.002776163397356868, 0.08005262166261673, 0.13635307550430298, 0.4495926797389984, 0.024967320263385773, 0.0], [0.3658088445663452, 0.0019930177368223667, 0.011018095538020134, 0.02275533601641655, 0.008609471842646599, 0.005063106305897236, 0.003827338805422187, 0.022504078224301338, 0.0874822661280632, 0.3931390941143036, 0.034847453236579895, 0.04295191541314125]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9732004404067993, 0.02679956704378128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9314186573028564, 0.02178899012506008, 0.046792302280664444, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9197826385498047, 0.02465265803039074, 0.01591983623802662, 0.03964490070939064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.761898934841156, 0.01197794172912836, 0.014251968823373318, 0.025125592947006226, 0.1867455095052719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6390997171401978, 0.014319099485874176, 0.00945218838751316, 0.016696708276867867, 0.08699866384267807, 0.23343367874622345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7963140606880188, 0.01284806989133358, 0.009065455757081509, 0.010788903571665287, 0.021879462525248528, 0.026973184198141098, 0.12213091552257538, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7814188003540039, 0.004653508309274912, 0.010081375949084759, 0.042759984731674194, 0.033155471086502075, 0.02240855060517788, 0.016249017789959908, 0.08927334100008011, 0.0, 0.0, 0.0, 0.0], [0.706521213054657, 0.01534980721771717, 0.01677507720887661, 0.01400816347450018, 0.031778085976839066, 0.020790819078683853, 0.008240246213972569, 0.05242399126291275, 0.13411255180835724, 0.0, 0.0, 0.0], [0.6862983703613281, 0.009663430973887444, 0.018400929868221283, 0.01113735232502222, 0.031281713396310806, 0.02631983533501625, 0.0071691470220685005, 0.03737906739115715, 0.07027172297239304, 0.10207834839820862, 0.0, 0.0], [0.7075713276863098, 0.004273268859833479, 0.014776094816625118, 0.02503817342221737, 0.028900407254695892, 0.004385778214782476, 0.005319984629750252, 0.04460887983441353, 0.02679380215704441, 0.03303541988134384, 0.1052967980504036, 0.0], [0.6584796905517578, 0.0037740927655249834, 0.012275700457394123, 0.014592116698622704, 0.026735622435808182, 0.005563528276979923, 0.005506359972059727, 0.03028920479118824, 0.014621717855334282, 0.024132221937179565, 0.02886994369328022, 0.17515972256660461]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9183754920959473, 0.08162446320056915, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.613378643989563, 0.2889190912246704, 0.0977022796869278, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.616844654083252, 0.13575176894664764, 0.14538876712322235, 0.10201482474803925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2811371982097626, 0.05428781732916832, 0.4604642689228058, 0.14522957801818848, 0.058881163597106934, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24744997918605804, 0.015795528888702393, 0.052028827369213104, 0.1084214299917221, 0.5497640371322632, 0.02654021605849266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29004088044166565, 0.03145422041416168, 0.047215357422828674, 0.07205928862094879, 0.44222787022590637, 0.0394941009581089, 0.07750827819108963, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1786317080259323, 0.014454253017902374, 0.18418608605861664, 0.2883809804916382, 0.23233045637607574, 0.02106829360127449, 0.011311350390315056, 0.06963685154914856, 0.0, 0.0, 0.0, 0.0], [0.3969332277774811, 0.0118337357416749, 0.021994926035404205, 0.06408385932445526, 0.010975580662488937, 0.009779717773199081, 0.007898708805441856, 0.45046669244766235, 0.02603352628648281, 0.0, 0.0, 0.0], [0.33482107520103455, 0.02425556071102619, 0.020623283460736275, 0.020534556359052658, 0.008364737033843994, 0.009489926509559155, 0.01035021897405386, 0.48126745223999023, 0.028121022507548332, 0.06217218190431595, 0.0, 0.0], [0.3868201673030853, 0.0015774857020005584, 0.12976935505867004, 0.15973088145256042, 0.002939123660326004, 0.012929966673254967, 0.001030257437378168, 0.15698453783988953, 0.03071729838848114, 0.10136435925960541, 0.016136525198817253, 0.0], [0.4904960095882416, 0.0021130170207470655, 0.01666315086185932, 0.06591114401817322, 0.0136982137337327, 0.0051941336132586, 0.00356202176772058, 0.08004001528024673, 0.01657048426568508, 0.0762484073638916, 0.14045222103595734, 0.08905114233493805]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9916268587112427, 0.008373113349080086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9785059094429016, 0.011739584617316723, 0.009754432365298271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6891879439353943, 0.00971236638724804, 0.24130822718143463, 0.05979152023792267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6533017754554749, 0.003963386174291372, 0.035399891436100006, 0.04757675901055336, 0.25975820422172546, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.046492040157318115, 0.0021350018214434385, 0.011684387922286987, 0.005742967594414949, 0.9073606133460999, 0.026584923267364502, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1958036720752716, 0.0006484208279289305, 0.021577604115009308, 0.012443572282791138, 0.5194180607795715, 0.24188034236431122, 0.008228300139307976, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6254817247390747, 0.005490442737936974, 0.07993101328611374, 0.07406028360128403, 0.07481696456670761, 0.10413049906492233, 0.007200698833912611, 0.028888434171676636, 0.0, 0.0, 0.0, 0.0], [0.7911684513092041, 0.0008952774805948138, 0.016441810876131058, 0.0070267487317323685, 0.008342365734279156, 0.0026900889351963997, 0.0005639050505124032, 0.07687400281429291, 0.09599728882312775, 0.0, 0.0, 0.0], [0.8617139458656311, 0.0011017915094271302, 0.002620236948132515, 0.0016413311241194606, 0.002533764112740755, 0.0017492101760581136, 0.0006824972224421799, 0.030591562390327454, 0.036979787051677704, 0.06038583442568779, 0.0, 0.0], [0.38036587834358215, 0.0004438881005626172, 0.005241252947598696, 0.0016626837896183133, 0.0037095157895237207, 0.0019490160048007965, 3.2135510991793126e-05, 0.005728769116103649, 0.06048043444752693, 0.2509708106517792, 0.2894155979156494, 0.0], [0.18602709472179413, 0.00046482813195325434, 0.001229702029377222, 0.0019439341267570853, 0.0010734319221228361, 0.0011136098764836788, 0.00010452136484673247, 0.005022715777158737, 0.02124914340674877, 0.041930727660655975, 0.6888179183006287, 0.05102228373289108]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.971875011920929, 0.02812495082616806, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8864855766296387, 0.06220969185233116, 0.05130479484796524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.827964186668396, 0.027522273361682892, 0.10500240325927734, 0.03951115906238556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5721298456192017, 0.012099802494049072, 0.21772344410419464, 0.1826125532388687, 0.0154343843460083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4619048237800598, 0.024185946211218834, 0.13522516191005707, 0.24856916069984436, 0.056687574833631516, 0.07342737913131714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48561906814575195, 0.02038731426000595, 0.07830341160297394, 0.28824254870414734, 0.01786840334534645, 0.07493211328983307, 0.03464711084961891, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6177240014076233, 0.01831837370991707, 0.02144678123295307, 0.2346264272928238, 0.010465016588568687, 0.013433333486318588, 0.006588229443877935, 0.07739772647619247, 0.0, 0.0, 0.0, 0.0], [0.17064309120178223, 0.006338080391287804, 0.01714596152305603, 0.032212868332862854, 0.0028467108495533466, 0.0024092330131679773, 0.003001743694767356, 0.7317686676979065, 0.033633649349212646, 0.0, 0.0, 0.0], [0.07694181054830551, 0.005245609674602747, 0.0021100379526615143, 0.009623479098081589, 0.005406350828707218, 0.004624790512025356, 0.005937275476753712, 0.8445192575454712, 0.027675507590174675, 0.017915910109877586, 0.0, 0.0], [0.0698530524969101, 3.533143535605632e-05, 0.0012332746991887689, 0.01619541458785534, 0.0026448427233844995, 0.005400719121098518, 0.00024533135001547635, 0.8326626420021057, 0.023648645728826523, 0.04280540719628334, 0.00527527229860425, 0.0], [0.13982002437114716, 0.0008886256837286055, 0.004758882336318493, 0.03886324539780617, 0.003979677800089121, 0.005322924815118313, 0.0022382508032023907, 0.5608633756637573, 0.04811626300215721, 0.04522097110748291, 0.12907394766807556, 0.020853793248534203]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9745988249778748, 0.02540120854973793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9508765339851379, 0.032303523272275925, 0.01681988313794136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9217000603675842, 0.03772292286157608, 0.013875641860067844, 0.0267014242708683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9527413845062256, 0.026541007682681084, 0.009563452564179897, 0.010573301464319229, 0.0005808805581182241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.904239296913147, 0.02000918984413147, 0.016210485249757767, 0.029593635350465775, 0.009068161249160767, 0.020879333838820457, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9011470675468445, 0.020221704617142677, 0.018964337185025215, 0.018589358776807785, 0.009834060445427895, 0.017685649916529655, 0.01355784758925438, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8786922097206116, 0.016607340425252914, 0.011513836681842804, 0.0172122772783041, 0.006645049434155226, 0.014896126464009285, 0.031353238970041275, 0.02308001182973385, 0.0, 0.0, 0.0, 0.0], [0.795646071434021, 0.011176262982189655, 0.0206607598811388, 0.0238161850720644, 0.009468181058764458, 0.01179464627057314, 0.04857921227812767, 0.06296893954277039, 0.01588977500796318, 0.0, 0.0, 0.0], [0.7824582457542419, 0.012342039495706558, 0.020356297492980957, 0.020906617864966393, 0.007481393404304981, 0.012584630399942398, 0.05386853218078613, 0.04721526801586151, 0.01686694473028183, 0.0259200818836689, 0.0, 0.0], [0.7650517225265503, 0.0214928537607193, 0.02113945223391056, 0.035109832882881165, 0.008372221142053604, 0.012828447856009007, 0.047981925308704376, 0.05252368375658989, 0.009112877771258354, 0.009026141837239265, 0.01736074686050415, 0.0], [0.701083242893219, 0.034963514655828476, 0.02630223147571087, 0.020700929686427116, 0.010743987746536732, 0.01811748556792736, 0.057569947093725204, 0.0501299649477005, 0.025239212438464165, 0.02598019316792488, 0.01451875176280737, 0.014650581404566765]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8976650238037109, 0.10233494639396667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10251972079277039, 0.8853189945220947, 0.01216126512736082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1602431982755661, 0.7964072823524475, 0.036199018359184265, 0.007150525227189064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08985596895217896, 0.8738166093826294, 0.023656487464904785, 0.003785331267863512, 0.008885587565600872, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6170852184295654, 0.2821478843688965, 0.024813106283545494, 0.005818724166601896, 0.054924171417951584, 0.0152110131457448, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5874587893486023, 0.2845360338687897, 0.028187856078147888, 0.01620645634829998, 0.043142169713974, 0.0321548767387867, 0.008313828147947788, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12517106533050537, 0.20123349130153656, 0.07289708405733109, 0.017301220446825027, 0.4789695143699646, 0.055220186710357666, 0.01895202323794365, 0.030255381017923355, 0.0, 0.0, 0.0, 0.0], [0.08492691069841385, 0.8460666537284851, 0.018965408205986023, 0.003410051576793194, 0.01023133099079132, 0.005475788842886686, 0.0036752752494066954, 0.010594513267278671, 0.016654010862112045, 0.0, 0.0, 0.0], [0.15052379667758942, 0.5567432045936584, 0.007086496334522963, 0.001208747737109661, 0.014648091979324818, 0.004902316723018885, 0.011388173326849937, 0.0379825197160244, 0.16715434193611145, 0.04836226627230644, 0.0, 0.0], [0.28910037875175476, 0.16966500878334045, 0.007687435019761324, 0.002422981895506382, 0.020892659202218056, 0.010301738977432251, 0.01143357902765274, 0.07083204388618469, 0.2093251347541809, 0.15542054176330566, 0.05291854962706566, 0.0], [0.05920978635549545, 0.34150439500808716, 0.003558492986485362, 0.0006763122510164976, 0.004894494079053402, 0.001986775314435363, 0.006104869768023491, 0.006767371203750372, 0.47138598561286926, 0.059042539447546005, 0.011387810111045837, 0.03348105028271675]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9582101702690125, 0.041789811104536057, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.926227867603302, 0.06026637181639671, 0.013505754061043262, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.883263885974884, 0.010961456224322319, 0.05239122360944748, 0.05338342487812042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9666603803634644, 0.006177782081067562, 0.015207424759864807, 0.00967143103480339, 0.0022830236703157425, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9481725692749023, 0.00530355516821146, 0.004518948495388031, 0.006387536413967609, 0.012660246342420578, 0.02295706421136856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.864551842212677, 0.007598440628498793, 0.0017070939065888524, 0.0012534863781183958, 0.02837173268198967, 0.05752791836857796, 0.0389895923435688, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7952762246131897, 0.019959373399615288, 0.030558692291378975, 0.08939388394355774, 0.0074524907395243645, 0.01457296684384346, 0.0036466901656240225, 0.03913963958621025, 0.0, 0.0, 0.0, 0.0], [0.7641720771789551, 0.05240252614021301, 0.015671435743570328, 0.035485975444316864, 0.0014632222009822726, 0.01181564200669527, 0.0021043207962065935, 0.051919125020504, 0.06496578454971313, 0.0, 0.0, 0.0], [0.703173816204071, 0.027284841984510422, 0.012088809162378311, 0.04124690219759941, 0.0009836162207648158, 0.010432172566652298, 0.0020317097660154104, 0.043465469032526016, 0.09161631017923355, 0.06767626851797104, 0.0, 0.0], [0.745573103427887, 0.006055185571312904, 0.01108713448047638, 0.06693821400403976, 0.001724982401356101, 0.006130802445113659, 0.001709687989205122, 0.02814086712896824, 0.05905423313379288, 0.05313568189740181, 0.020450187847018242, 0.0], [0.4453561305999756, 0.008541356772184372, 0.006410693749785423, 0.017810961231589317, 0.0005199032020755112, 0.012691737152636051, 0.0015004739398136735, 0.017321180552244186, 0.028959030285477638, 0.05490780249238014, 0.3651728332042694, 0.0408078096807003]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9283452033996582, 0.07165483385324478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8323249816894531, 0.16098429262638092, 0.006690754555165768, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9476935267448425, 0.031869273632764816, 0.01847018487751484, 0.001967017538845539, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.821929931640625, 0.021780718117952347, 0.12256638705730438, 0.005033751949667931, 0.02868923917412758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.58195960521698, 0.005770227871835232, 0.021350521594285965, 0.004937298130244017, 0.2295757234096527, 0.1564067155122757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7233567833900452, 0.0016117390478029847, 0.015435848385095596, 0.013797404244542122, 0.02152739092707634, 0.10814462602138519, 0.11612622439861298, 0.0, 0.0, 0.0, 0.0, 0.0], [0.764048159122467, 0.007158809807151556, 0.029238520190119743, 0.050757743418216705, 0.013694913126528263, 0.0415508896112442, 0.019952986389398575, 0.0735979750752449, 0.0, 0.0, 0.0, 0.0], [0.8124006390571594, 0.002580837346613407, 0.0031250091269612312, 0.062498290091753006, 0.0024136893916875124, 0.011804914101958275, 0.004477393813431263, 0.06038243696093559, 0.04031679034233093, 0.0, 0.0, 0.0], [0.7954340577125549, 0.0012598322937265038, 0.0005548743647523224, 0.0323425754904747, 0.00147374183870852, 0.006659671198576689, 0.0039050811901688576, 0.056791361421346664, 0.07858122140169144, 0.02299756370484829, 0.0, 0.0], [0.8558189272880554, 0.00031048114760778844, 0.0009347301675006747, 0.0034630547743290663, 0.001486087217926979, 0.010470033623278141, 0.0012638373300433159, 0.007183428388088942, 0.053802285343408585, 0.031964972615242004, 0.033302150666713715, 0.0], [0.5606911778450012, 0.0026181505527347326, 0.008398705162107944, 0.02321617305278778, 0.001283177174627781, 0.002718803007155657, 0.003160267835482955, 0.03778328374028206, 0.07456651329994202, 0.03246864676475525, 0.1777493953704834, 0.07534568011760712]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9644703269004822, 0.03552963212132454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7528695464134216, 0.19286473095417023, 0.05426572635769844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7788475155830383, 0.1248021349310875, 0.05723518505692482, 0.03911523520946503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7946266531944275, 0.08212750405073166, 0.05126819387078285, 0.04783620685338974, 0.02414141222834587, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7422783374786377, 0.0853230208158493, 0.03206552192568779, 0.03932655602693558, 0.05803874135017395, 0.042967744171619415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6552797555923462, 0.11672412604093552, 0.0337221622467041, 0.031632836908102036, 0.0449555478990078, 0.08892031013965607, 0.028765281662344933, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3608851432800293, 0.0852198600769043, 0.04393673688173294, 0.08379776775836945, 0.17726555466651917, 0.10275989770889282, 0.10443105548620224, 0.04170392453670502, 0.0, 0.0, 0.0, 0.0], [0.38770633935928345, 0.15567108988761902, 0.08175283670425415, 0.08333868533372879, 0.035712096840143204, 0.04799732193350792, 0.03977351635694504, 0.1197882741689682, 0.0482599101960659, 0.0, 0.0, 0.0], [0.5098131895065308, 0.06338951736688614, 0.04915250465273857, 0.05149998888373375, 0.06213189288973808, 0.03186890110373497, 0.04803083464503288, 0.08709520101547241, 0.048196062445640564, 0.04882190749049187, 0.0, 0.0], [0.6212362051010132, 0.0223552193492651, 0.011310946196317673, 0.019597159698605537, 0.028136160224676132, 0.051669370383024216, 0.038193825632333755, 0.12336134165525436, 0.016494471579790115, 0.041280049830675125, 0.026365291327238083, 0.0], [0.5464949011802673, 0.018335502594709396, 0.010514362715184689, 0.014340645633637905, 0.07465578615665436, 0.026769915595650673, 0.03644287586212158, 0.05683872848749161, 0.035179898142814636, 0.04432374984025955, 0.10227642208337784, 0.03382721170783043]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8612442016601562, 0.13875578343868256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6703091263771057, 0.04511100798845291, 0.2845798432826996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3664025664329529, 0.02456653118133545, 0.051127467304468155, 0.5579034090042114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2949296832084656, 0.02059980109333992, 0.020737729966640472, 0.10881699621677399, 0.5549157857894897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3918759524822235, 0.031905025243759155, 0.013310380280017853, 0.006481973920017481, 0.08560909330844879, 0.47081759572029114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4153168797492981, 0.08099352568387985, 0.007670465856790543, 0.002534758998081088, 0.033681195229291916, 0.07703996449708939, 0.38276317715644836, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21986141800880432, 0.01428761426359415, 0.04459969326853752, 0.04046836867928505, 0.10855647921562195, 0.046856798231601715, 0.10545952618122101, 0.41991010308265686, 0.0, 0.0, 0.0, 0.0], [0.3468669056892395, 0.028023356571793556, 0.009275256656110287, 0.012249143794178963, 0.010792057029902935, 0.0029957559891045094, 0.0059724124148488045, 0.04908248782157898, 0.5347425937652588, 0.0, 0.0, 0.0], [0.3271314799785614, 0.011046454310417175, 0.011984176002442837, 0.02737758681178093, 0.010412841103971004, 0.0007079904316924512, 0.004021297674626112, 0.021373629570007324, 0.10685545951128006, 0.4790891706943512, 0.0, 0.0], [0.23487304151058197, 0.0044976407662034035, 0.00519044604152441, 0.025383394211530685, 0.010386220179498196, 0.002050821203738451, 0.0007845531799830496, 0.03214573487639427, 0.04117874801158905, 0.0739375501871109, 0.569571852684021, 0.0], [0.11304738372564316, 0.005671276710927486, 0.0009043028112500906, 0.0035631784703582525, 0.01839987188577652, 0.0024073966778814793, 0.009354323148727417, 0.021394705399870872, 0.017879577353596687, 0.017234308645129204, 0.1076001450419426, 0.6825434565544128]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9710803031921387, 0.028919678181409836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9340623021125793, 0.05638039484620094, 0.009557305835187435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9110105633735657, 0.04438447952270508, 0.013310583308339119, 0.031294405460357666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9443431496620178, 0.024629442021250725, 0.015872051939368248, 0.008853541687130928, 0.00630168104544282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9420199990272522, 0.021444272249937057, 0.006786629557609558, 0.00850259605795145, 0.008538993075489998, 0.012707522138953209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9033228158950806, 0.03156628459692001, 0.007138480897992849, 0.004885256756097078, 0.006696705706417561, 0.022197920829057693, 0.02419249899685383, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7946080565452576, 0.017074719071388245, 0.0096971420571208, 0.020763548091053963, 0.029830031096935272, 0.009004750289022923, 0.05513092130422592, 0.06389088183641434, 0.0, 0.0, 0.0, 0.0], [0.862190842628479, 0.01104274857789278, 0.006944428198039532, 0.02120966836810112, 0.012128102593123913, 0.009670631028711796, 0.021159781143069267, 0.039032068103551865, 0.016621900722384453, 0.0, 0.0, 0.0], [0.7834652066230774, 0.016882305964827538, 0.01036232989281416, 0.02627187967300415, 0.023592110723257065, 0.014777335338294506, 0.023655718192458153, 0.05452841520309448, 0.03845519572496414, 0.008009525947272778, 0.0, 0.0], [0.8591169118881226, 0.006282164249569178, 0.002322858665138483, 0.013590688817203045, 0.0234506968408823, 0.007349417544901371, 0.00890524685382843, 0.04949605464935303, 0.013291512615978718, 0.004631000105291605, 0.011563464999198914, 0.0], [0.8281149864196777, 0.022992007434368134, 0.009963789023458958, 0.005824141204357147, 0.022124001756310463, 0.014200293458998203, 0.0097507881000638, 0.026654958724975586, 0.009544597007334232, 0.0024143594782799482, 0.01332697831094265, 0.03508906438946724]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9692928194999695, 0.03070719540119171, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7655465006828308, 0.21792937815189362, 0.01652408391237259, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7284939289093018, 0.15181118249893188, 0.09248289465904236, 0.027212029322981834, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5448606610298157, 0.06959836184978485, 0.25840651988983154, 0.09127174317836761, 0.03586278483271599, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5348876118659973, 0.04674367606639862, 0.14455938339233398, 0.06993264704942703, 0.16392619907855988, 0.039950452744960785, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5550464987754822, 0.030172022059559822, 0.07502147555351257, 0.03189529478549957, 0.18326729536056519, 0.07982360571622849, 0.04477381333708763, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1594063937664032, 0.002581964945420623, 0.0777343213558197, 0.1711222529411316, 0.5128588676452637, 0.029179584234952927, 0.028125349432229996, 0.01899137534201145, 0.0, 0.0, 0.0, 0.0], [0.15407180786132812, 0.006567588075995445, 0.12404973804950714, 0.2672496438026428, 0.2486547976732254, 0.06178027763962746, 0.02384558692574501, 0.0858580619096756, 0.027922404929995537, 0.0, 0.0, 0.0], [0.22970886528491974, 0.022743329405784607, 0.031752776354551315, 0.37148919701576233, 0.088223397731781, 0.10058646649122238, 0.034146908670663834, 0.05395995453000069, 0.04676157236099243, 0.02062745951116085, 0.0, 0.0], [0.687197208404541, 0.014873194508254528, 0.008707037195563316, 0.0498763769865036, 0.010442696511745453, 0.015090162865817547, 0.004521558526903391, 0.04341590031981468, 0.07835012674331665, 0.04815789684653282, 0.03936781361699104, 0.0], [0.3763353228569031, 0.024939268827438354, 0.01832771487534046, 0.023585975170135498, 0.024585941806435585, 0.021567057818174362, 0.03019784390926361, 0.02173973061144352, 0.10250198841094971, 0.14403584599494934, 0.04545476660132408, 0.16672848165035248]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9793688058853149, 0.020631223917007446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9741688370704651, 0.005095220636576414, 0.02073593996465206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8945725560188293, 0.005275894422084093, 0.048020802438259125, 0.05213073641061783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9665771722793579, 0.002403691178187728, 0.008507889695465565, 0.0059300074353814125, 0.016581334173679352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.964103102684021, 0.004402808379381895, 0.0054499804973602295, 0.015799371525645256, 0.005501299165189266, 0.004743526689708233, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9598162770271301, 0.00462693814188242, 0.0009102479089051485, 0.004270290024578571, 0.013005238026380539, 0.006950489245355129, 0.010420604608952999, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9386754035949707, 0.0016522591467946768, 0.004007960204035044, 0.004536445718258619, 0.004350189119577408, 0.005812486633658409, 0.002812794176861644, 0.03815241530537605, 0.0, 0.0, 0.0, 0.0], [0.8777362704277039, 0.005650919396430254, 0.012193187139928341, 0.01830296590924263, 0.010574175044894218, 0.005623973440378904, 0.005084940232336521, 0.04917371645569801, 0.0156598798930645, 0.0, 0.0, 0.0], [0.8049421310424805, 0.00919889286160469, 0.01470598578453064, 0.025210320949554443, 0.030963551253080368, 0.0031984583474695683, 0.009254093281924725, 0.0614987388253212, 0.01234847865998745, 0.02867935039103031, 0.0, 0.0], [0.9097334742546082, 0.005716605111956596, 0.003986018244177103, 0.009318651631474495, 0.01592990756034851, 0.0036455837544053793, 0.0011503556743264198, 0.018542585894465446, 0.014746580272912979, 0.014323325827717781, 0.002906908979639411, 0.0], [0.8272758722305298, 0.005336360074579716, 0.005950741935521364, 0.00745109748095274, 0.031091151759028435, 0.008392727933824062, 0.01078002154827118, 0.04121207818388939, 0.014535414054989815, 0.028994562104344368, 0.007731772493571043, 0.011248236522078514]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9999358654022217, 6.408223998732865e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.1271306902926881e-05, 0.9999878406524658, 9.346393881060067e-07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [9.551270068186568e-07, 2.208091245847754e-06, 0.9999847412109375, 1.2078559848305304e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.318157837384206e-07, 6.395846074980227e-10, 0.0003079329908359796, 0.9996873140335083, 4.492219431995181e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.529226756247226e-05, 4.60093380061366e-10, 1.2847423125350588e-09, 1.3532701359508792e-07, 0.9961243271827698, 0.0038602736312896013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.375988724685364e-11, 7.600405547980582e-12, 2.3433084429473454e-16, 2.251364869407463e-15, 3.674826762334327e-11, 0.9999483823776245, 5.161577428225428e-05, 0.0, 0.0, 0.0, 0.0, 0.0], [1.914228997534906e-09, 7.021456749356503e-09, 1.4958284610955275e-11, 3.466919529238055e-10, 2.8170732093713013e-06, 7.847414053685497e-06, 0.9999892711639404, 1.9273308282663493e-08, 0.0, 0.0, 0.0, 0.0], [1.2000128890576889e-06, 3.479789373317388e-11, 3.5236431827900816e-11, 1.6091425414366256e-10, 4.3250875103595376e-11, 1.11177962669462e-10, 2.7608759722852483e-10, 0.9981616139411926, 0.0018371567130088806, 0.0, 0.0, 0.0], [1.2863683451769248e-08, 7.400758825215625e-12, 5.056328619690298e-13, 4.550329210650794e-11, 1.3393057279528775e-13, 8.047782817759142e-13, 6.0331158471815804e-12, 2.6751376935862936e-06, 0.9993687272071838, 0.0006285900017246604, 0.0, 0.0], [1.9763655600968377e-08, 1.2044798345560784e-14, 1.3666481141205722e-12, 7.651696165567046e-09, 8.353997181520523e-12, 3.1857776507972224e-14, 6.901226009929651e-14, 1.3980037572025594e-08, 3.1860656690696487e-07, 0.9999854564666748, 1.416238592355512e-05, 0.0], [3.728338818231869e-11, 1.5084694349412771e-15, 2.684730006870576e-15, 2.3487342713096604e-12, 1.3666697981640219e-11, 5.8086508693266925e-12, 1.508103537883887e-12, 1.3233705242754468e-09, 4.80286459492163e-08, 2.906275312852813e-06, 0.9999970197677612, 2.060134463022223e-08]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9862706661224365, 0.013729257509112358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9716172218322754, 0.002571107354015112, 0.025811707600951195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9379040002822876, 0.0019667146261781454, 0.0064268819987773895, 0.053702425211668015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9091672301292419, 0.0011215960839763284, 0.0020315544679760933, 0.014639242552220821, 0.07304032891988754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9425380229949951, 0.011753203347325325, 0.0041975355707108974, 0.006524919997900724, 0.007995775900781155, 0.026990512385964394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9514340758323669, 0.0010305801406502724, 0.006489698309451342, 0.004122532904148102, 0.013656609691679478, 0.015143630094826221, 0.008122840896248817, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9092976450920105, 0.001459592836908996, 0.0006829064805060625, 0.004830721765756607, 0.010654008015990257, 0.007802309934049845, 0.004879974760115147, 0.06039278954267502, 0.0, 0.0, 0.0, 0.0], [0.7039305567741394, 0.0016685185255482793, 0.007617327384650707, 0.008800373412668705, 0.09752170741558075, 0.03727404773235321, 0.018228542059659958, 0.10184107720851898, 0.02311783656477928, 0.0, 0.0, 0.0], [0.7594425678253174, 0.0007111155427992344, 0.0007354205008596182, 0.007879943586885929, 0.10203630477190018, 0.04283065348863602, 0.012358244508504868, 0.04115588963031769, 0.026891933754086494, 0.005957994144409895, 0.0, 0.0], [0.8984640836715698, 0.00012345483992248774, 8.73697645147331e-05, 0.00015437757247127593, 0.014848469756543636, 0.024966606870293617, 0.002587733091786504, 0.039919737726449966, 0.011541402898728848, 0.0032504915725439787, 0.004056274890899658, 0.0], [0.8991856575012207, 0.00021268715499900281, 0.0004574282211251557, 0.0007927641272544861, 0.0061491415835917, 0.02431800775229931, 0.006257375702261925, 0.03885315731167793, 0.007860830053687096, 0.0003921025199815631, 0.0027693696320056915, 0.012751461938023567]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9998134970664978, 0.00018652591097634286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9986003041267395, 4.416958472575061e-05, 0.0013555347686633468, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9986522793769836, 3.4657371088542277e-06, 1.3833662251272472e-06, 0.0013428699458017945, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9999693632125854, 3.381245733180549e-06, 7.225447973269183e-08, 7.359435585385654e-07, 2.6351270207669586e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9982061386108398, 8.270551188616082e-05, 5.881458653433924e-10, 8.389110739415173e-09, 9.454164029421008e-08, 0.0017110458575189114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9982237219810486, 3.3525848266435787e-06, 1.9430640207929173e-08, 1.4264405656660983e-08, 1.4104197987307998e-07, 0.00045336168841458857, 0.0013194628991186619, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9989645481109619, 9.898509597405791e-05, 1.1061962368330569e-07, 2.256051629956346e-06, 8.014218110474758e-06, 3.1764451705385e-05, 4.942969462717883e-05, 0.0008449301822111011, 0.0, 0.0, 0.0, 0.0], [0.9816675186157227, 0.00014934531645849347, 0.001053570187650621, 0.0029224297031760216, 0.002818659646436572, 4.640118277166039e-06, 1.2643449736060575e-05, 0.0002916442172136158, 0.011079631745815277, 0.0, 0.0, 0.0], [0.9872748851776123, 8.22190850158222e-05, 4.657024874177296e-06, 0.0006622541113756597, 0.00027068733470514417, 2.5436204964535136e-07, 1.0263497642881703e-06, 1.2193465408927295e-05, 0.0047157034277915955, 0.0069761574268341064, 0.0, 0.0], [0.9957754015922546, 3.744395507965237e-05, 5.1080704110972874e-08, 3.315742105769459e-06, 8.784259262029082e-05, 3.229534968340886e-06, 3.0643529953522375e-06, 2.2939926566323265e-05, 0.00047470591380260885, 0.00046423260937444866, 0.003127790056169033, 0.0], [0.9995877146720886, 1.9910971786885057e-06, 4.989660240539706e-08, 8.218784159907955e-07, 1.4829139445282635e-06, 2.530468009354081e-06, 6.854357934571453e-07, 2.3019671061774716e-05, 0.00010466221283422783, 3.0582941690227017e-06, 1.3229545174908708e-06, 0.00027282326482236385]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9394112229347229, 0.06058874353766441, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6134536862373352, 0.37280508875846863, 0.01374124363064766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.44353291392326355, 0.35981082916259766, 0.16465671360492706, 0.03199954703450203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7190516591072083, 0.06530801206827164, 0.12899617850780487, 0.0817997008562088, 0.004844412207603455, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7455986142158508, 0.03295586630702019, 0.03824028745293617, 0.013663670048117638, 0.14712034165859222, 0.02242117002606392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3663499057292938, 0.016634322702884674, 0.023124167695641518, 0.016737576574087143, 0.38864102959632874, 0.17457829415798187, 0.013934732414782047, 0.0, 0.0, 0.0, 0.0, 0.0], [0.31168121099472046, 0.03692607954144478, 0.12351755052804947, 0.09801720082759857, 0.3010963499546051, 0.08299373090267181, 0.021455299109220505, 0.024312613531947136, 0.0, 0.0, 0.0, 0.0], [0.5098214745521545, 0.016763800755143166, 0.03959371894598007, 0.07238148152828217, 0.0819115936756134, 0.09210085868835449, 0.017992377281188965, 0.13905495405197144, 0.030379734933376312, 0.0, 0.0, 0.0], [0.7082634568214417, 0.015725379809737206, 0.02583792246878147, 0.014098969288170338, 0.013666180893778801, 0.0439014807343483, 0.012719402089715004, 0.10304223746061325, 0.052939869463443756, 0.009805133566260338, 0.0, 0.0], [0.7208718061447144, 0.016825027763843536, 0.015787973999977112, 0.016605349257588387, 0.027327079325914383, 0.03527470678091049, 0.006776925176382065, 0.05936533585190773, 0.020780088379979134, 0.06847351044416428, 0.011912280693650246, 0.0], [0.6818859577178955, 0.008503641001880169, 0.04002416133880615, 0.02479736879467964, 0.017846496775746346, 0.0236582662910223, 0.005145726725459099, 0.05273667722940445, 0.03371243551373482, 0.04726302623748779, 0.052433889359235764, 0.01199236512184143]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9577831029891968, 0.04221692308783531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7955262064933777, 0.0944489985704422, 0.11002487689256668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5457058548927307, 0.1443907469511032, 0.265207976102829, 0.04469538480043411, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6612840890884399, 0.11533686518669128, 0.13719862699508667, 0.045346327126026154, 0.04083414748311043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5539434552192688, 0.13372567296028137, 0.1485862284898758, 0.04607323184609413, 0.10180695354938507, 0.015864543616771698, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4236028492450714, 0.08231806010007858, 0.1827048361301422, 0.09287583827972412, 0.15754671394824982, 0.039594247937202454, 0.021357478573918343, 0.0, 0.0, 0.0, 0.0, 0.0], [0.40825173258781433, 0.11351846903562546, 0.1686389446258545, 0.12367131561040878, 0.06848064064979553, 0.012393144890666008, 0.019350657239556313, 0.08569511771202087, 0.0, 0.0, 0.0, 0.0], [0.3281005918979645, 0.1108129471540451, 0.12185347825288773, 0.09897443652153015, 0.13213147222995758, 0.01940731145441532, 0.03158731386065483, 0.11574200540781021, 0.04139043018221855, 0.0, 0.0, 0.0], [0.3946619927883148, 0.10747633129358292, 0.07508139312267303, 0.037332095205783844, 0.12216528505086899, 0.019402308389544487, 0.029395347461104393, 0.13017287850379944, 0.06046897545456886, 0.02384336292743683, 0.0, 0.0], [0.2752320170402527, 0.049433302134275436, 0.07261069118976593, 0.044428884983062744, 0.11096470057964325, 0.01685745269060135, 0.009160774759948254, 0.16767147183418274, 0.14633408188819885, 0.05908677354454994, 0.048219889402389526, 0.0], [0.2692785859107971, 0.05322444438934326, 0.07644111663103104, 0.04614944010972977, 0.0853087455034256, 0.01849322021007538, 0.015561657957732677, 0.1956406533718109, 0.09888608753681183, 0.07391384243965149, 0.045148998498916626, 0.02195318415760994]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9734203815460205, 0.026579570025205612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9439903497695923, 0.034510403871536255, 0.021499179303646088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8633659482002258, 0.0704738050699234, 0.046857770532369614, 0.019302479922771454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7652139067649841, 0.012929015792906284, 0.03436727821826935, 0.04042338952422142, 0.1470663994550705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4670056700706482, 0.02600019797682762, 0.028311707079410553, 0.03391753137111664, 0.3768828809261322, 0.06788204610347748, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26549381017684937, 0.011877094395458698, 0.040608275681734085, 0.04199977591633797, 0.24433833360671997, 0.16405951976776123, 0.23162324726581573, 0.0, 0.0, 0.0, 0.0, 0.0], [0.27196449041366577, 0.019214527681469917, 0.07152102887630463, 0.12426760792732239, 0.19914549589157104, 0.1362442672252655, 0.1007230207324028, 0.07691953331232071, 0.0, 0.0, 0.0, 0.0], [0.36368700861930847, 0.008915852755308151, 0.013929765671491623, 0.00884312018752098, 0.33147764205932617, 0.0383300743997097, 0.011891312897205353, 0.20779713988304138, 0.01512807048857212, 0.0, 0.0, 0.0], [0.5279200673103333, 0.004776397254317999, 0.0027656282763928175, 0.0017110719345510006, 0.12267899513244629, 0.051862459629774094, 0.02956177480518818, 0.21372948586940765, 0.017448268830776215, 0.027545789256691933, 0.0, 0.0], [0.7016544342041016, 0.0049439589492976665, 0.003290503052994609, 0.0010138314682990313, 0.012862450443208218, 0.006715856958180666, 0.004264661110937595, 0.04951176047325134, 0.030953777953982353, 0.14248819649219513, 0.04230052977800369, 0.0], [0.31555190682411194, 0.0015690821455791593, 0.0019149610307067633, 0.0019230133621022105, 0.005184014327824116, 0.0047891149297356606, 0.008443376049399376, 0.01676780730485916, 0.013905501924455166, 0.042501192539930344, 0.5558950304985046, 0.031555019319057465]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9904971122741699, 0.009502969682216644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9366008043289185, 0.01968168467283249, 0.04371748864650726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9447512626647949, 0.011371291242539883, 0.0030632643029093742, 0.04081418365240097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9909945130348206, 0.008435769006609917, 3.700745219248347e-05, 0.00011384301615180448, 0.0004189149185549468, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9736932516098022, 0.022034427151083946, 5.277626041788608e-05, 9.809191396925598e-05, 0.00018176260346081108, 0.003939613699913025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9733288288116455, 0.0046349880285561085, 2.343687265238259e-05, 1.8517597709433176e-05, 1.0003212992160115e-05, 0.005523857194930315, 0.016460450366139412, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9065694808959961, 0.04573411867022514, 0.0007101416704244912, 0.0021210622508078814, 0.002939424477517605, 0.00061264936812222, 0.0039673359133303165, 0.03734581172466278, 0.0, 0.0, 0.0, 0.0], [0.4249071478843689, 0.01369367353618145, 0.049077585339546204, 0.05810462683439255, 0.42243674397468567, 0.0019153570756316185, 0.0033234180882573128, 0.019566532224416733, 0.006974922027438879, 0.0, 0.0, 0.0], [0.6180278658866882, 0.017358800396323204, 0.01378612034022808, 0.052532218396663666, 0.21863867342472076, 0.00043153009028173983, 0.0008946987800300121, 0.041161149740219116, 0.017416829243302345, 0.019752008840441704, 0.0, 0.0], [0.977985143661499, 0.008119921199977398, 1.3889560250390787e-05, 0.00012653386511374265, 0.0025188575964421034, 0.00023028053692542017, 0.0013656291412189603, 0.002523652045056224, 0.003271471941843629, 0.0021759141236543655, 0.0016686702147126198, 0.0], [0.9919856786727905, 0.0022183756809681654, 1.2143951607868075e-05, 0.00015453282685484737, 0.000165411111083813, 9.434905223315582e-05, 0.00037259230157360435, 0.0014993420336395502, 0.0031656306236982346, 4.481180076254532e-05, 2.1825824660481885e-05, 0.0002652851107995957]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9874216318130493, 0.012578319758176804, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9477344751358032, 0.04487646743655205, 0.007389013189822435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5323895812034607, 0.3086182475090027, 0.14735658466815948, 0.011635673232376575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8373852372169495, 0.04482816904783249, 0.05654862895607948, 0.050723955035209656, 0.010513956658542156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4876309633255005, 0.002764239441603422, 0.00018285488476976752, 0.0006610497366636992, 0.44187119603157043, 0.0668896734714508, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8244956135749817, 0.0023549539037048817, 0.0006299178930930793, 0.0007265323656611145, 0.06548205763101578, 0.06157571077346802, 0.04473522678017616, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9826860427856445, 0.000306863512378186, 0.0001772607211023569, 0.00021838382235728204, 0.0019031387055292726, 0.001173545839264989, 0.0055226716212928295, 0.008012108504772186, 0.0, 0.0, 0.0, 0.0], [0.909058690071106, 0.001997467363253236, 0.0057426802814006805, 0.0015973933041095734, 0.0026650044601410627, 0.00019282450375612825, 0.0005040098913013935, 0.07171326875686646, 0.006528748199343681, 0.0, 0.0, 0.0], [0.835741400718689, 0.005003038793802261, 0.004361034370958805, 0.0036734151653945446, 0.00404117489233613, 0.0018618288449943066, 0.0009302829275839031, 0.03666642680764198, 0.07917573302984238, 0.028545551002025604, 0.0, 0.0], [0.6481264233589172, 0.0030033679213374853, 0.0030457773245871067, 0.004705729894340038, 0.0010377856669947505, 0.0006557865999639034, 4.89650119561702e-05, 0.029910927638411522, 0.11923708766698837, 0.11201386153697968, 0.07821431756019592, 0.0], [0.6141602396965027, 0.0013756562257185578, 0.0010341659653931856, 0.0024410183541476727, 0.002007532399147749, 0.0005631093517877162, 0.0011992130894213915, 0.01761108636856079, 0.015335614793002605, 0.017420899122953415, 0.29714372754096985, 0.02970782108604908]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9644559025764465, 0.03554413467645645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9632844924926758, 0.016986733302474022, 0.019728826358914375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8904570937156677, 0.0433625690639019, 0.04854396730661392, 0.017636271193623543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.88223797082901, 0.013037574477493763, 0.015061426907777786, 0.009400513954460621, 0.08026251941919327, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4174163043498993, 0.010328792035579681, 0.01432518009096384, 0.017342006787657738, 0.49450716376304626, 0.04608049988746643, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6646803021430969, 0.008471735753118992, 0.022218530997633934, 0.02195555716753006, 0.15267346799373627, 0.026852412149310112, 0.10314802825450897, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5858094096183777, 0.006511001847684383, 0.015645433217287064, 0.11618630588054657, 0.07875917851924896, 0.040047887712717056, 0.12169376760721207, 0.035346973687410355, 0.0, 0.0, 0.0, 0.0], [0.4577379822731018, 0.010836576111614704, 0.03223777189850807, 0.08055289089679718, 0.14005021750926971, 0.020680928602814674, 0.020411523059010506, 0.2068876028060913, 0.03060445562005043, 0.0, 0.0, 0.0], [0.4610025882720947, 0.015955504029989243, 0.011323116719722748, 0.029141705483198166, 0.04178934171795845, 0.00560177443549037, 0.014971879310905933, 0.35453489422798157, 0.030559822916984558, 0.03511933982372284, 0.0, 0.0], [0.8622645735740662, 0.008207804523408413, 0.01005945261567831, 0.009113788604736328, 0.013288531452417374, 0.0076274871826171875, 0.0009260426741093397, 0.015220679342746735, 0.014986682683229446, 0.027755923569202423, 0.030549004673957825, 0.0], [0.8213890194892883, 0.006901752203702927, 0.00376480002887547, 0.002897704252973199, 0.005145258270204067, 0.00916153471916914, 0.005972843151539564, 0.04354910925030708, 0.009124930016696453, 0.01918819732964039, 0.05040751397609711, 0.022497378289699554]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.973280131816864, 0.026719845831394196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9528902173042297, 0.023664774373173714, 0.023445025086402893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9426316022872925, 0.0427081435918808, 0.00664528040215373, 0.008014939725399017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9741420745849609, 0.010101662017405033, 0.001205540494993329, 0.002529590157791972, 0.012021192349493504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.886735737323761, 0.05863676592707634, 0.005079470109194517, 0.003633498214185238, 0.004170882515609264, 0.0417436845600605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9018236994743347, 0.0213154423981905, 0.004266032017767429, 0.001162698958069086, 0.0007993063773028553, 0.03221867233514786, 0.03841414675116539, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8741706609725952, 0.043491046875715256, 0.002622503088787198, 0.004686746280640364, 0.023697560653090477, 0.006794138345867395, 0.0030228770337998867, 0.04151447117328644, 0.0, 0.0, 0.0, 0.0], [0.21342509984970093, 0.00647718133404851, 0.12119895964860916, 0.09657023102045059, 0.4766559898853302, 0.016438664868474007, 0.004659971687942743, 0.05631543695926666, 0.008258480578660965, 0.0, 0.0, 0.0], [0.6224207282066345, 0.01389407366514206, 0.02641911432147026, 0.01958695612847805, 0.10487104952335358, 0.016350148245692253, 0.007766847964376211, 0.1427789330482483, 0.028273455798625946, 0.01763877272605896, 0.0, 0.0], [0.9262216687202454, 0.008696381002664566, 0.0014172453666105866, 0.00029339970205910504, 0.005723617505282164, 0.004257006570696831, 0.0035338453017175198, 0.0179803017526865, 0.0172294769436121, 0.007746122311800718, 0.0069009000435471535, 0.0], [0.9369179606437683, 0.008559142239391804, 0.0006311919423751533, 0.00026972003979608417, 0.0031720728147774935, 0.0030700527131557465, 0.001963720889762044, 0.02849501557648182, 0.01085056271404028, 0.0015830306801944971, 0.0007787630311213434, 0.003708645235747099]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9915046095848083, 0.008495431393384933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9409522414207458, 0.0499345064163208, 0.009113309904932976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9142885208129883, 0.0549812950193882, 0.019179606810212135, 0.01155056618154049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9554874897003174, 0.02881288342177868, 0.007626700215041637, 0.004605319816619158, 0.0034675896167755127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9525143504142761, 0.02534453384578228, 0.004340874496847391, 0.003436743514612317, 0.006967354565858841, 0.007396155968308449, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9392948150634766, 0.04129568859934807, 0.005370023660361767, 0.002734455280005932, 0.002309748437255621, 0.0032576057128608227, 0.005737652536481619, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8601328134536743, 0.026189055293798447, 0.011161085218191147, 0.007517677266150713, 0.01405760832130909, 0.02056996338069439, 0.02995471842586994, 0.030417080968618393, 0.0, 0.0, 0.0, 0.0], [0.5845205187797546, 0.0480823777616024, 0.051847752183675766, 0.025795063003897667, 0.08516800403594971, 0.03815897926688194, 0.05323087051510811, 0.10147814452648163, 0.011718332767486572, 0.0, 0.0, 0.0], [0.6617465019226074, 0.04806642234325409, 0.020865686237812042, 0.014268106780946255, 0.0625927746295929, 0.028353389352560043, 0.04965650290250778, 0.08543705195188522, 0.014203905127942562, 0.014809704385697842, 0.0, 0.0], [0.8696556687355042, 0.009691283106803894, 0.006508284714072943, 0.0069804321974515915, 0.040021464228630066, 0.013052524998784065, 0.011227159760892391, 0.028489863499999046, 0.005432665813714266, 0.007121623493731022, 0.0018189360853284597, 0.0], [0.775040864944458, 0.02106986753642559, 0.008832802064716816, 0.004694243427366018, 0.0822470560669899, 0.01584630273282528, 0.0326775386929512, 0.031119750812649727, 0.011035834439098835, 0.007790881209075451, 0.005449997261166573, 0.004194918554276228]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8995503783226013, 0.10044961422681808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.237043559551239, 0.6913371682167053, 0.07161920517683029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38873451948165894, 0.444148987531662, 0.14558839797973633, 0.02152813784778118, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5962414145469666, 0.18892692029476166, 0.08036404103040695, 0.03322936221957207, 0.10123831033706665, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8481730818748474, 0.026788508519530296, 0.019832728430628777, 0.0167723186314106, 0.05951168015599251, 0.028921691700816154, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7422763705253601, 0.0046164183877408504, 0.009997491724789143, 0.006482747383415699, 0.037179477512836456, 0.008859082125127316, 0.19058842957019806, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26913347840309143, 0.04715494439005852, 0.03352709859609604, 0.01823338493704796, 0.4276607632637024, 0.01629267819225788, 0.13300469517707825, 0.0549929104745388, 0.0, 0.0, 0.0, 0.0], [0.09774062037467957, 0.41345977783203125, 0.12863418459892273, 0.02400706335902214, 0.22505474090576172, 0.01748388633131981, 0.01679830066859722, 0.051077112555503845, 0.025744246318936348, 0.0, 0.0, 0.0], [0.11320213228464127, 0.32132628560066223, 0.14067904651165009, 0.0289380494505167, 0.2111303061246872, 0.022402184084057808, 0.04000722989439964, 0.06274375319480896, 0.03204745799303055, 0.027523543685674667, 0.0, 0.0], [0.5320673584938049, 0.04583740234375, 0.053778164088726044, 0.01670394465327263, 0.0449758879840374, 0.021579822525382042, 0.03331758454442024, 0.19225037097930908, 0.017388833686709404, 0.032846324145793915, 0.009254307486116886, 0.0], [0.25792860984802246, 0.34489741921424866, 0.09964948147535324, 0.013341646641492844, 0.05762946605682373, 0.01382311712950468, 0.04296791926026344, 0.03731832653284073, 0.0605773963034153, 0.023143252357840538, 0.0019601956009864807, 0.04676319286227226]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9507707953453064, 0.049229174852371216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7864078879356384, 0.07888539135456085, 0.13470670580863953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6879477500915527, 0.010240526869893074, 0.025335127487778664, 0.27647665143013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8945687413215637, 0.057099565863609314, 0.013345658779144287, 0.011510740965604782, 0.023475324735045433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7848508358001709, 0.010637904517352581, 0.0034313530195504427, 0.0025766079779714346, 0.019310666248202324, 0.1791926771402359, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7923367619514465, 0.025505317375063896, 0.004114889539778233, 0.003897665999829769, 0.07199176400899887, 0.031312163919210434, 0.07084145396947861, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6457110643386841, 0.018790224567055702, 0.035470250993967056, 0.05502740293741226, 0.01880299113690853, 0.013497400097548962, 0.005999977234750986, 0.20670071244239807, 0.0, 0.0, 0.0, 0.0], [0.5905060172080994, 0.03963208198547363, 0.024203505367040634, 0.04088328778743744, 0.012723390944302082, 0.0035769471433013678, 0.001231091096997261, 0.07148633152246475, 0.21575726568698883, 0.0, 0.0, 0.0], [0.707156240940094, 0.007978491485118866, 0.011211619712412357, 0.011189951561391354, 0.008248573169112206, 0.004757784307003021, 0.0009708627476356924, 0.026502298191189766, 0.024211838841438293, 0.1977723240852356, 0.0, 0.0], [0.8721404671669006, 0.00953182764351368, 0.0030401074327528477, 0.017776504158973694, 0.00627356581389904, 0.0035301160532981157, 0.0005110941710881889, 0.013812095858156681, 0.011666199192404747, 0.013547264970839024, 0.04817076027393341, 0.0], [0.8702904582023621, 0.03567644953727722, 0.004048457369208336, 0.009014874696731567, 0.005986097734421492, 0.018628869205713272, 0.008256487548351288, 0.015406116843223572, 0.004659141879528761, 0.009540711529552937, 0.006559330504387617, 0.011932949535548687]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9446554183959961, 0.055344581604003906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7479656338691711, 0.23047728836536407, 0.021557068452239037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3424518406391144, 0.5532395243644714, 0.09769780188798904, 0.00661090761423111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15549062192440033, 0.40606486797332764, 0.2612753212451935, 0.16134820878505707, 0.015820926055312157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24518148601055145, 0.18329453468322754, 0.1221456527709961, 0.181937113404274, 0.24231937527656555, 0.025121791288256645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2515924572944641, 0.08682126551866531, 0.1243354007601738, 0.2891485095024109, 0.19110460579395294, 0.04647252708673477, 0.010525190271437168, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48698991537094116, 0.28010594844818115, 0.0852368101477623, 0.05953631550073624, 0.028436509892344475, 0.007522862404584885, 0.013074642047286034, 0.03909694403409958, 0.0, 0.0, 0.0, 0.0], [0.379254549741745, 0.3952479362487793, 0.05173702910542488, 0.01635282300412655, 0.03233957663178444, 0.023306116461753845, 0.011981950141489506, 0.0760851576924324, 0.013694917783141136, 0.0, 0.0, 0.0], [0.24557751417160034, 0.2692177891731262, 0.0351850725710392, 0.0046257381327450275, 0.021534278988838196, 0.013941536657512188, 0.030193015933036804, 0.22591881453990936, 0.12235473841428757, 0.031451448798179626, 0.0, 0.0], [0.108822301030159, 0.06060490012168884, 0.007597337476909161, 0.003547820495441556, 0.004093232564628124, 0.0026180902495980263, 0.016834944486618042, 0.18803146481513977, 0.22597068548202515, 0.3545601963996887, 0.027319002896547318, 0.0], [0.13016048073768616, 0.04032432660460472, 0.008466243743896484, 0.004922185093164444, 0.004558252636343241, 0.0028988774865865707, 0.013510270975530148, 0.09421252459287643, 0.39835572242736816, 0.2329159528017044, 0.0536779910326004, 0.01599724404513836]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9577032327651978, 0.04229680076241493, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9650375247001648, 0.026339063420891762, 0.008623423986136913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9608638882637024, 0.025751259177923203, 0.00740413460880518, 0.005980771034955978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9000632762908936, 0.04633566737174988, 0.002034288365393877, 0.0025073112919926643, 0.04905938357114792, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8073189854621887, 0.08565836399793625, 0.00047795535647310317, 0.0006753837806172669, 0.0901532918214798, 0.01571602001786232, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7747837901115417, 0.0388525053858757, 0.009821885265409946, 0.021200042217969894, 0.12607522308826447, 0.02046049013733864, 0.008806112222373486, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8333690762519836, 0.022144963964819908, 0.01070803590118885, 0.07443597167730331, 0.017036747187376022, 0.004109421279281378, 0.011249350383877754, 0.026946453377604485, 0.0, 0.0, 0.0, 0.0], [0.8405707478523254, 0.035723727196455, 0.016656946390867233, 0.019762061536312103, 0.03964391350746155, 0.003947921562939882, 0.004456433467566967, 0.026991836726665497, 0.01224637869745493, 0.0, 0.0, 0.0], [0.8770609498023987, 0.0421542152762413, 0.003459054743871093, 0.009395133703947067, 0.010794629342854023, 0.0023000447545200586, 0.0036356456112116575, 0.02237291820347309, 0.023817045614123344, 0.005010360386222601, 0.0, 0.0], [0.968289315700531, 0.008241969160735607, 0.00021475920220836997, 0.00011947475286433473, 0.0008863022085279226, 0.0005688124801963568, 0.00024964683689177036, 0.0030957446433603764, 0.005609787534922361, 0.004520471207797527, 0.008203759789466858, 0.0], [0.9408398270606995, 0.004278102423995733, 0.0004233094514347613, 0.0004223576106596738, 0.0008001092937774956, 0.000532265636138618, 0.001318478025496006, 0.01139399129897356, 0.0028685196302831173, 0.0031099349725991488, 0.009621615521609783, 0.02439144067466259]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9803985953330994, 0.01960144191980362, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8659805655479431, 0.05516272038221359, 0.07885679602622986, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8521763682365417, 0.03816651552915573, 0.06493030488491058, 0.04472685605287552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8277944922447205, 0.025318747386336327, 0.10786116868257523, 0.03406587243080139, 0.004959657322615385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9172587990760803, 0.01632121205329895, 0.027731608599424362, 0.0139353321865201, 0.0059932516887784, 0.018759749829769135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8161044716835022, 0.022697392851114273, 0.055231623351573944, 0.03310125321149826, 0.008885293267667294, 0.02327115461230278, 0.040708765387535095, 0.0, 0.0, 0.0, 0.0, 0.0], [0.764035701751709, 0.0252792090177536, 0.03564879298210144, 0.029560934752225876, 0.00765151996165514, 0.027923572808504105, 0.0628214180469513, 0.04707873985171318, 0.0, 0.0, 0.0, 0.0], [0.778481125831604, 0.017297448590397835, 0.036996059119701385, 0.025612007826566696, 0.010480724275112152, 0.014829779975116253, 0.06513643264770508, 0.03475701063871384, 0.016409514471888542, 0.0, 0.0, 0.0], [0.772700846195221, 0.020560912787914276, 0.03272576630115509, 0.029621567577123642, 0.012877260334789753, 0.012188892811536789, 0.04886302724480629, 0.03874390572309494, 0.01594681479036808, 0.01577099785208702, 0.0, 0.0], [0.7987111210823059, 0.01887398399412632, 0.037940602749586105, 0.022695021703839302, 0.008421014994382858, 0.0068824635818600655, 0.046285782009363174, 0.03652893379330635, 0.008031381294131279, 0.011542794294655323, 0.004087008535861969, 0.0], [0.5944671630859375, 0.02746618539094925, 0.08059348165988922, 0.030409028753638268, 0.022279607132077217, 0.013477973639965057, 0.09124625474214554, 0.08500867336988449, 0.019544651731848717, 0.023592693731188774, 0.004417104180902243, 0.007497156038880348]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9799265265464783, 0.020073525607585907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8610404133796692, 0.05613858625292778, 0.08282096683979034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8219932317733765, 0.07356089353561401, 0.0670946016907692, 0.03735121339559555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7266371250152588, 0.07286915183067322, 0.07893053442239761, 0.022344930097460747, 0.09921816736459732, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8460869789123535, 0.04218077287077904, 0.033469974994659424, 0.026103349402546883, 0.030550723895430565, 0.0216082651168108, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6139050722122192, 0.041648056358098984, 0.07263141125440598, 0.06678200513124466, 0.07589671015739441, 0.04271039739251137, 0.08642634749412537, 0.0, 0.0, 0.0, 0.0, 0.0], [0.607154130935669, 0.04182992875576019, 0.046687524765729904, 0.04753826931118965, 0.07352049648761749, 0.02177106775343418, 0.09787372499704361, 0.06362483650445938, 0.0, 0.0, 0.0, 0.0], [0.6364192962646484, 0.01865844614803791, 0.017789317294955254, 0.02263261005282402, 0.10995376110076904, 0.019181787967681885, 0.05224065110087395, 0.11280633509159088, 0.01031783688813448, 0.0, 0.0, 0.0], [0.605018675327301, 0.024244023486971855, 0.019397715106606483, 0.034090690314769745, 0.10085204243659973, 0.029756030067801476, 0.07247170060873032, 0.08419965207576752, 0.018952496349811554, 0.011016984470188618, 0.0, 0.0], [0.8317301869392395, 0.011798079125583172, 0.007432490587234497, 0.0059448895044624805, 0.05057826638221741, 0.015818944200873375, 0.020629137754440308, 0.03524010255932808, 0.008352801203727722, 0.007415813393890858, 0.005059233866631985, 0.0], [0.7001292109489441, 0.02934572845697403, 0.024056976661086082, 0.018217844888567924, 0.028565343469381332, 0.018943380564451218, 0.051360681653022766, 0.07191473245620728, 0.02120973914861679, 0.014685512520372868, 0.007220440544188023, 0.014350347220897675]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8318522572517395, 0.1681477427482605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.767510712146759, 0.12144988030195236, 0.11103947460651398, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3725925385951996, 0.21188440918922424, 0.3473057150840759, 0.06821729987859726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3995687961578369, 0.16443470120429993, 0.23080065846443176, 0.045440178364515305, 0.15975575149059296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4441279470920563, 0.14614234864711761, 0.11059850454330444, 0.0318414568901062, 0.23053249716758728, 0.03675718978047371, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7024731636047363, 0.04739227890968323, 0.0420801118016243, 0.026454152539372444, 0.03809567540884018, 0.013854065909981728, 0.12965050339698792, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4790492355823517, 0.04880538582801819, 0.08862525224685669, 0.04552998021245003, 0.10194271057844162, 0.012961296364665031, 0.05768927186727524, 0.16539689898490906, 0.0, 0.0, 0.0, 0.0], [0.48369544744491577, 0.08460567891597748, 0.03144480288028717, 0.01650407165288925, 0.10177762806415558, 0.012575758621096611, 0.09917514771223068, 0.15123796463012695, 0.01898355968296528, 0.0, 0.0, 0.0], [0.4243321120738983, 0.07450339198112488, 0.050864048302173615, 0.02012256719172001, 0.10565564036369324, 0.014595371671020985, 0.1239861398935318, 0.1431211531162262, 0.024285761639475822, 0.018533855676651, 0.0, 0.0], [0.17639833688735962, 0.06510629504919052, 0.05651375278830528, 0.028542988002300262, 0.15299667418003082, 0.011974243447184563, 0.16462257504463196, 0.26238682866096497, 0.026293352246284485, 0.023941989988088608, 0.031222952529788017, 0.0], [0.5262041091918945, 0.050975605845451355, 0.03519570082426071, 0.016006827354431152, 0.03480768948793411, 0.013387150131165981, 0.18432405591011047, 0.060379888862371445, 0.015382659621536732, 0.009803579188883305, 0.007097210269421339, 0.046435534954071045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9570327997207642, 0.04296719655394554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8363948464393616, 0.06108638644218445, 0.1025187075138092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8361901044845581, 0.06255879253149033, 0.05285844951868057, 0.0483926385641098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.804766833782196, 0.051299698650836945, 0.053352225571870804, 0.05726773664355278, 0.03331350162625313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7888414859771729, 0.028550347313284874, 0.02529098466038704, 0.03144735470414162, 0.024012187495827675, 0.10185772180557251, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8351284265518188, 0.017494194209575653, 0.03459516167640686, 0.03426530584692955, 0.009389511309564114, 0.0195010956376791, 0.04962625354528427, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7450125217437744, 0.025528300553560257, 0.03051944263279438, 0.046469856053590775, 0.018463747575879097, 0.027111997827887535, 0.03940323740243912, 0.06749080866575241, 0.0, 0.0, 0.0, 0.0], [0.5941357016563416, 0.03704528138041496, 0.02889583259820938, 0.03442893549799919, 0.03538563847541809, 0.016816481947898865, 0.03224068880081177, 0.10842178016901016, 0.11262974143028259, 0.0, 0.0, 0.0], [0.5953229665756226, 0.02842620573937893, 0.02466903254389763, 0.024136673659086227, 0.024097885936498642, 0.012411622330546379, 0.018294673413038254, 0.07634738087654114, 0.12679968774318695, 0.06949380040168762, 0.0, 0.0], [0.7280614972114563, 0.021994439885020256, 0.012627624906599522, 0.010426363907754421, 0.01270628347992897, 0.004515913315117359, 0.010398266837000847, 0.033980999141931534, 0.05903884395956993, 0.04948725178837776, 0.05676248297095299, 0.0], [0.7495908737182617, 0.008835379965603352, 0.009122515097260475, 0.008761754259467125, 0.006522269453853369, 0.005617587361484766, 0.010878782719373703, 0.03667033091187477, 0.034901972860097885, 0.03640781715512276, 0.03529167175292969, 0.05739904195070267]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.96860271692276, 0.031397245824337006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9360827207565308, 0.008639846928417683, 0.05527743697166443, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8980001211166382, 0.012206523679196835, 0.034089792519807816, 0.055703479796648026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.896878719329834, 0.016667179763317108, 0.021780354902148247, 0.008524850010871887, 0.05614888668060303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8720502257347107, 0.010710019618272781, 0.002429298358038068, 0.0027419112157076597, 0.02643023617565632, 0.0856383815407753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8993743062019348, 0.010029580444097519, 0.005654798820614815, 0.004039035178720951, 0.01950082555413246, 0.012802723795175552, 0.04859885200858116, 0.0, 0.0, 0.0, 0.0, 0.0], [0.820344090461731, 0.020803721621632576, 0.021939123049378395, 0.011058049276471138, 0.029559412971138954, 0.005067134741693735, 0.005806540604680777, 0.08542199432849884, 0.0, 0.0, 0.0, 0.0], [0.7480672001838684, 0.014153272844851017, 0.014122558757662773, 0.012806277722120285, 0.05253656581044197, 0.010928512550890446, 0.005899835377931595, 0.028114955872297287, 0.11337079852819443, 0.0, 0.0, 0.0], [0.7661000490188599, 0.009621068835258484, 0.018303075805306435, 0.014398807659745216, 0.02132515050470829, 0.002900755265727639, 0.0027680706698447466, 0.021270588040351868, 0.0466010607779026, 0.09671139717102051, 0.0, 0.0], [0.813506543636322, 0.010506854392588139, 0.012652642093598843, 0.011485083028674126, 0.012406030669808388, 0.0014587593032047153, 0.003203407395631075, 0.01248774491250515, 0.030224865302443504, 0.03229198604822159, 0.05977605655789375, 0.0], [0.9070471525192261, 0.009275322780013084, 0.00570889376103878, 0.003429487580433488, 0.011755089275538921, 0.002660647500306368, 0.0025515006855130196, 0.00759453559294343, 0.00793384201824665, 0.00844328012317419, 0.005955896805971861, 0.02764439955353737]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9058735966682434, 0.0941263735294342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8547714948654175, 0.10283026844263077, 0.04239817336201668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7967277765274048, 0.12795156240463257, 0.05165926739573479, 0.023661499843001366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6457647681236267, 0.08022861182689667, 0.05020105466246605, 0.029037142172455788, 0.194768488407135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.560978353023529, 0.05254652723670006, 0.02775329165160656, 0.01740254834294319, 0.21234504878520966, 0.12897421419620514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38919752836227417, 0.04149225726723671, 0.03528193011879921, 0.023619456216692924, 0.2269165962934494, 0.11501552909612656, 0.16847670078277588, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23794743418693542, 0.03389018028974533, 0.009277322329580784, 0.012151861563324928, 0.24107994139194489, 0.15855790674686432, 0.2711162567138672, 0.03597908467054367, 0.0, 0.0, 0.0, 0.0], [0.5048310160636902, 0.02442890591919422, 0.008012686856091022, 0.005297014955431223, 0.1841784417629242, 0.08057799935340881, 0.14032475650310516, 0.0428466759622097, 0.009502478875219822, 0.0, 0.0, 0.0], [0.44742658734321594, 0.04913056641817093, 0.010544228367507458, 0.004015755839645863, 0.187120720744133, 0.07861179113388062, 0.1442730873823166, 0.04497448354959488, 0.016636062413454056, 0.017266739159822464, 0.0, 0.0], [0.6047410368919373, 0.039380721747875214, 0.005877961870282888, 0.0019737023394554853, 0.11356792598962784, 0.059457216411828995, 0.11203350871801376, 0.019835161045193672, 0.013754557818174362, 0.019075680524110794, 0.010302487760782242, 0.0], [0.5637742280960083, 0.020610971376299858, 0.00566531578078866, 0.0029817663598805666, 0.09363611042499542, 0.04421856626868248, 0.11175379157066345, 0.027012577280402184, 0.014398572035133839, 0.014702673070132732, 0.013573974370956421, 0.08767139166593552]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9522379636764526, 0.04776206240057945, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5936537384986877, 0.3995467722415924, 0.0067994906567037106, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6453675031661987, 0.21716150641441345, 0.1117963045835495, 0.025674689561128616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7336032390594482, 0.0962720513343811, 0.04748236760497093, 0.11314821988344193, 0.009494119323790073, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4693325459957123, 0.01208086870610714, 0.03294246643781662, 0.062183599919080734, 0.40309181809425354, 0.020368635654449463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1923147737979889, 0.015042775310575962, 0.03273697942495346, 0.0869453102350235, 0.3865174949169159, 0.2627623975276947, 0.0236801877617836, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6587823629379272, 0.0515187531709671, 0.020230425521731377, 0.05390934646129608, 0.07937376946210861, 0.054251790046691895, 0.05635174363851547, 0.025581825524568558, 0.0, 0.0, 0.0, 0.0], [0.489083856344223, 0.04108835384249687, 0.011591138318181038, 0.013600858859717846, 0.026890359818935394, 0.01419010292738676, 0.021905334666371346, 0.28872644901275635, 0.0929235965013504, 0.0, 0.0, 0.0], [0.42224934697151184, 0.04459349438548088, 0.005609862972050905, 0.010576888918876648, 0.011492360383272171, 0.00930524431169033, 0.016726532950997353, 0.26202747225761414, 0.18754175305366516, 0.029877018183469772, 0.0, 0.0], [0.7074236273765564, 0.015703855082392693, 0.0026927778962999582, 0.0036161202006042004, 0.0009640600765123963, 0.001739089610055089, 0.003786074463278055, 0.10802596807479858, 0.04540855437517166, 0.09930279105901718, 0.011337077245116234, 0.0], [0.6983626484870911, 0.003270888002589345, 0.0016777702840045094, 0.0036678474862128496, 0.0014823648380115628, 0.0009634490124881268, 0.003358016489073634, 0.057375553995370865, 0.040708452463150024, 0.03935699537396431, 0.10114762932062149, 0.04862837865948677]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9990837574005127, 0.0009162150672636926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9925958514213562, 0.0035357850138098, 0.0038683414459228516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9924700260162354, 0.0005993121885694563, 0.0004625220026355237, 0.006468098144978285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.999372661113739, 0.00023436984338331968, 2.5177973839163315e-06, 9.814678378461394e-06, 0.00038068200228735805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9918655753135681, 0.0036173812113702297, 1.2744271771225613e-06, 1.176377395495365e-06, 1.5485717085539363e-05, 0.004499041009694338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9957262277603149, 0.0001274422975257039, 5.904584213567432e-06, 4.044217803311767e-06, 2.6174922822974622e-05, 0.0005240224418230355, 0.0035861937794834375, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9950109720230103, 0.002768290229141712, 6.361889973049983e-05, 0.00021656317403540015, 0.0006460605654865503, 1.2737649512928328e-06, 0.000241539761191234, 0.0010516408365219831, 0.0, 0.0, 0.0, 0.0], [0.9584470987319946, 0.003843692597001791, 0.002889609429985285, 0.005598521791398525, 0.014299626462161541, 3.163099790981505e-06, 0.0006549112149514258, 0.0012729908339679241, 0.012990383431315422, 0.0, 0.0, 0.0], [0.9762134552001953, 0.002213480416685343, 0.0004878939944319427, 0.0028625107370316982, 0.00515530863776803, 4.8823885379079e-08, 2.6122941562789492e-05, 0.0001577482180437073, 0.005212276242673397, 0.007671106141060591, 0.0, 0.0], [0.9971745014190674, 0.0005021642427891493, 1.2519487427198328e-05, 2.1440768250613473e-05, 0.0009439783752895892, 3.479599115507881e-07, 0.00013644427235703915, 8.216507558245212e-05, 7.411769911414012e-05, 0.00019481524941511452, 0.0008574238163419068, 0.0], [0.9991952776908875, 0.0001954089675564319, 3.1450472306460142e-06, 4.876108050666517e-06, 0.00010753116657724604, 4.037232486098219e-07, 3.9556281990371644e-05, 9.23228872125037e-05, 0.0001150129028246738, 9.215732461598236e-07, 5.398435405368218e-06, 0.0002400730736553669]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9745726585388184, 0.025427373126149178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9692674875259399, 0.011235627345740795, 0.019496776163578033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9604085683822632, 0.00925414077937603, 0.0156381968408823, 0.0146990567445755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9387809038162231, 0.005214521661400795, 0.00760044576600194, 0.006993259768933058, 0.04141088202595711, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502151012420654, 0.010272741317749023, 0.005210069473832846, 0.007572531700134277, 0.015759851783514023, 0.010969665832817554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9233594536781311, 0.006766364444047213, 0.011677166447043419, 0.00847257487475872, 0.030679821968078613, 0.006468135863542557, 0.012576556764543056, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9313353300094604, 0.002754763001576066, 0.0025127725675702095, 0.0032621044665575027, 0.019818350672721863, 0.004374953452497721, 0.01058138720691204, 0.02536037564277649, 0.0, 0.0, 0.0, 0.0], [0.9673178195953369, 0.0013951716246083379, 0.0009283863473683596, 0.0009483078611083329, 0.01288561336696148, 0.0014497442170977592, 0.006725535728037357, 0.004430845379829407, 0.003918737173080444, 0.0, 0.0, 0.0], [0.9674623608589172, 0.0009375839144922793, 0.0005083692376501858, 0.0008261919720098376, 0.008320074528455734, 0.0026120098773390055, 0.003716130508109927, 0.004396920558065176, 0.005490587092936039, 0.005729706957936287, 0.0, 0.0], [0.9494664072990417, 0.0017689316300675273, 0.0007035293965600431, 0.0011377884075045586, 0.010281615890562534, 0.005351317580789328, 0.007691557984799147, 0.004480541218072176, 0.003616730449721217, 0.005016958341002464, 0.010484556667506695, 0.0], [0.9297968149185181, 0.0014223272446542978, 0.0015771286562085152, 0.001283479854464531, 0.007793575990945101, 0.0022254742216318846, 0.007185652852058411, 0.0069367727264761925, 0.004442049656063318, 0.01038060337305069, 0.0036302318330854177, 0.023325912654399872]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9715749025344849, 0.028425127267837524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8468066453933716, 0.052453670650720596, 0.10073967278003693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7672279477119446, 0.07305256277322769, 0.12916472554206848, 0.03055480122566223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5925819873809814, 0.04784165695309639, 0.11125906556844711, 0.14642401039600372, 0.10189327597618103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5304303169250488, 0.07144026458263397, 0.03537323325872421, 0.07525135576725006, 0.23031878471374512, 0.05718603357672691, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5017051100730896, 0.01997850090265274, 0.07851913571357727, 0.27057716250419617, 0.04532363638281822, 0.032364413142204285, 0.05153201147913933, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7190533876419067, 0.017683245241642, 0.011026252061128616, 0.11592648178339005, 0.020324893295764923, 0.009024241007864475, 0.021099666133522987, 0.08586177229881287, 0.0, 0.0, 0.0, 0.0], [0.24035504460334778, 0.009865445084869862, 0.010073021985590458, 0.021925214678049088, 0.04742727428674698, 0.012429566122591496, 0.018996812403202057, 0.6164730787277222, 0.022454483434557915, 0.0, 0.0, 0.0], [0.2438616156578064, 0.01000683382153511, 0.006597400177270174, 0.013671644032001495, 0.021884236484766006, 0.01048413198441267, 0.01956259459257126, 0.6073765754699707, 0.036615923047065735, 0.02993897534906864, 0.0, 0.0], [0.40983107686042786, 0.004432422574609518, 0.0086220633238554, 0.01078603696078062, 0.014975713565945625, 0.006116189062595367, 0.006289609707891941, 0.3107253313064575, 0.06426945328712463, 0.11801248788833618, 0.0459396094083786, 0.0], [0.33516690135002136, 0.0022980899084359407, 0.002880312502384186, 0.0057466602884233, 0.003848528489470482, 0.0018249815329909325, 0.006251349579542875, 0.08869928121566772, 0.03667032718658447, 0.058688174933195114, 0.40825486183166504, 0.049670469015836716]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9604174494743347, 0.039582543075084686, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9302036166191101, 0.05278953164815903, 0.017006779089570045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7250233888626099, 0.12131921947002411, 0.146603524684906, 0.007053964305669069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8197346925735474, 0.017060810700058937, 0.08600418269634247, 0.053518906235694885, 0.023681428283452988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3075217306613922, 0.0031584729440510273, 0.04080018028616905, 0.024477044120430946, 0.5958470702171326, 0.02819548361003399, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6620691418647766, 0.007198153529316187, 0.04043550789356232, 0.034445010125637054, 0.12718011438846588, 0.11698365211486816, 0.011688381433486938, 0.0, 0.0, 0.0, 0.0, 0.0], [0.952570378780365, 0.003853980451822281, 0.002382048172876239, 0.001579070696607232, 0.003046038094907999, 0.005649921018630266, 0.009901386685669422, 0.02101728692650795, 0.0, 0.0, 0.0, 0.0], [0.71816486120224, 0.0016169676091521978, 0.002117960015311837, 0.0015178370522335172, 0.0028198075015097857, 0.0033842676784843206, 0.003451928263530135, 0.23453867435455322, 0.03238767385482788, 0.0, 0.0, 0.0], [0.6255384087562561, 0.0033618866000324488, 0.0012045861221849918, 0.00045203138142824173, 0.0015259988140314817, 0.0035484021063894033, 0.0032949568703770638, 0.17699697613716125, 0.1303788274526596, 0.05369787663221359, 0.0, 0.0], [0.48822644352912903, 0.002195131964981556, 0.001277025556191802, 0.0008118631667457521, 0.0005890650791116059, 0.0007458762265741825, 0.0004690751666203141, 0.033115170896053314, 0.04864155501127243, 0.3901495933532715, 0.03377922996878624, 0.0], [0.1206623986363411, 0.0001542099635116756, 0.0003788033500313759, 0.000553768128156662, 0.0007230282062664628, 0.0008555936510674655, 0.0004310756048653275, 0.02015961892902851, 0.014320526272058487, 0.11440885066986084, 0.7216766476631165, 0.005675381049513817]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9771901965141296, 0.022809775546193123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9622159004211426, 0.022914374247193336, 0.0148696294054389, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.967591404914856, 0.013713812455534935, 0.010110764764249325, 0.008584066294133663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9802769422531128, 0.003473642049357295, 0.0007596358773298562, 0.00022007881489116699, 0.01526976190507412, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9161737561225891, 0.004861997906118631, 0.0012346819275990129, 0.00025808284408412874, 0.005169196054339409, 0.0723021999001503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8860867619514465, 0.002550671575590968, 0.0005681288312189281, 0.00029398660990409553, 0.002057812875136733, 0.014958012849092484, 0.09348475188016891, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8239720463752747, 0.015059988014400005, 0.005366061348468065, 0.00464605214074254, 0.017714254558086395, 0.010538691654801369, 0.04676026850938797, 0.07594273239374161, 0.0, 0.0, 0.0, 0.0], [0.5921857357025146, 0.04718982055783272, 0.012900745496153831, 0.007604576647281647, 0.12969830632209778, 0.07985742390155792, 0.04215185344219208, 0.03065571002662182, 0.05775579437613487, 0.0, 0.0, 0.0], [0.7646058201789856, 0.055060554295778275, 0.012452681548893452, 0.004099702928215265, 0.044545404613018036, 0.015439503826200962, 0.025883737951517105, 0.03172194957733154, 0.038851089775562286, 0.0073395888321101665, 0.0, 0.0], [0.9083195328712463, 0.013242439366877079, 0.0016208419110625982, 0.001021038624458015, 0.013060810044407845, 0.014879059977829456, 0.019450563937425613, 0.004116518888622522, 0.009743223898112774, 0.004069522488862276, 0.010476340539753437, 0.0], [0.9742091298103333, 0.0015296511119231582, 0.00021405972074717283, 0.00010722944716690108, 0.0006045771879144013, 0.0004323552129790187, 0.0016197517979890108, 0.0009734275517985225, 0.0022490450646728277, 0.0007891341811046004, 0.0012036996195092797, 0.01606784574687481]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9985394477844238, 0.0014605555916205049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9986502528190613, 0.00042477637180127203, 0.0009250438888557255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9993321299552917, 0.00011975812230957672, 0.00012183502258267254, 0.00042616802966222167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9996675252914429, 5.2023748139617965e-05, 1.8827804524335079e-06, 5.037015398556832e-06, 0.00027354041230864823, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9977136850357056, 0.0006583358626812696, 2.4045000373007497e-06, 2.6401717150292825e-06, 5.37479245394934e-05, 0.0015691890148445964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9494306445121765, 5.8771180192707106e-05, 5.426961070043035e-05, 6.543424387928098e-05, 0.0013898499310016632, 0.005694125778973103, 0.04330698773264885, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9885339140892029, 0.001225082902237773, 3.447110429988243e-05, 0.0001938934001373127, 0.00045439822133630514, 0.00010234637738903984, 0.0032387515529990196, 0.006217148620635271, 0.0, 0.0, 0.0, 0.0], [0.9431915283203125, 0.0012713875621557236, 0.0028932110872119665, 0.006087747868150473, 0.008797180838882923, 1.582768527441658e-05, 0.0014113212237134576, 0.006033029872924089, 0.030298661440610886, 0.0, 0.0, 0.0], [0.9847925901412964, 0.0010301809525117278, 0.00022872057161293924, 0.0013880199985578656, 0.001352645456790924, 2.6707875804277137e-06, 0.00028452358674257994, 0.0007579481462016702, 0.007246790453791618, 0.00291593000292778, 0.0, 0.0], [0.990932822227478, 0.00046995203592814505, 9.296406642533839e-05, 0.00019844667986035347, 0.0011431952007114887, 8.318646905536298e-06, 0.0003570257395040244, 0.0001843328936956823, 0.0003031227388419211, 0.0006014984101057053, 0.005708289798349142, 0.0], [0.9959226846694946, 0.00010725126776378602, 1.3144716831448022e-05, 1.9676745068863966e-05, 5.866782521479763e-05, 1.1048859960283153e-05, 0.00010128822759725153, 0.00032831565476953983, 0.0005782492808066308, 5.311855420586653e-05, 6.372554344125092e-05, 0.0027427130844444036]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9504939317703247, 0.0495060496032238, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9076070189476013, 0.05649357661604881, 0.03589935973286629, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7804266214370728, 0.12540550529956818, 0.0644569844007492, 0.02971087023615837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.744705855846405, 0.12384400516748428, 0.044832341372966766, 0.03546601161360741, 0.051151715219020844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6510642766952515, 0.13040603697299957, 0.04918404668569565, 0.05394197255373001, 0.0906887948513031, 0.024714834988117218, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5602327585220337, 0.06747089326381683, 0.05746236443519592, 0.14063535630702972, 0.06863522529602051, 0.03525516018271446, 0.07030823826789856, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5776811838150024, 0.06861661374568939, 0.044061195105314255, 0.0667402371764183, 0.0446523018181324, 0.029427900910377502, 0.06246568262577057, 0.10635492950677872, 0.0, 0.0, 0.0, 0.0], [0.356006920337677, 0.02570418454706669, 0.03262970224022865, 0.03004833683371544, 0.09745220839977264, 0.0685359537601471, 0.10254757851362228, 0.241046741604805, 0.046028394252061844, 0.0, 0.0, 0.0], [0.3994085192680359, 0.0510125607252121, 0.02342045307159424, 0.020929578691720963, 0.05461261421442032, 0.028233902528882027, 0.07310803234577179, 0.23825223743915558, 0.07094515115022659, 0.04007689282298088, 0.0, 0.0], [0.5241561532020569, 0.03442823141813278, 0.01840989664196968, 0.016960419714450836, 0.04507363587617874, 0.03351109102368355, 0.10604362934827805, 0.0857718214392662, 0.0481056347489357, 0.05883481726050377, 0.028704635798931122, 0.0], [0.7208909392356873, 0.018267273902893066, 0.00672730291262269, 0.007251957431435585, 0.007097385358065367, 0.009229722432792187, 0.02613118477165699, 0.052498962730169296, 0.026851950213313103, 0.046916648745536804, 0.038941409438848495, 0.0391952320933342]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9898198246955872, 0.010180189274251461, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9774346351623535, 0.01544263120740652, 0.0071227229200303555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9617087841033936, 0.02107181027531624, 0.01241721399128437, 0.004802212584763765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.936856746673584, 0.019239841029047966, 0.022458020597696304, 0.012372147291898727, 0.0090732891112566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9600916504859924, 0.01206893753260374, 0.0042558833956718445, 0.0028612103778868914, 0.019162993878126144, 0.0015592952258884907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5761590600013733, 0.036050211638212204, 0.040854547172784805, 0.06731415539979935, 0.177379310131073, 0.07223252952098846, 0.030010230839252472, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6641125679016113, 0.04026361182332039, 0.07347430288791656, 0.10217506438493729, 0.031879931688308716, 0.015270295552909374, 0.040161389857530594, 0.03266283869743347, 0.0, 0.0, 0.0, 0.0], [0.9215661883354187, 0.005695777479559183, 0.007788368500769138, 0.00872710533440113, 0.006129023618996143, 0.003989039920270443, 0.006222695577889681, 0.026883268728852272, 0.012998390011489391, 0.0, 0.0, 0.0], [0.8710307478904724, 0.012615684419870377, 0.011766379699110985, 0.010187926702201366, 0.004241907969117165, 0.0051305266097188, 0.00918184407055378, 0.04382823780179024, 0.027211062610149384, 0.0048057762905955315, 0.0, 0.0], [0.913783848285675, 0.012204943224787712, 0.006063918583095074, 0.004754110239446163, 0.002799955429509282, 0.0030639171600341797, 0.005671921651810408, 0.019138837233185768, 0.012470102868974209, 0.018160274252295494, 0.0018880622228607535, 0.0], [0.6524505615234375, 0.013497716747224331, 0.014369409531354904, 0.018097467720508575, 0.004846205934882164, 0.004678158089518547, 0.0029393446166068316, 0.12171705812215805, 0.06515210121870041, 0.04838740825653076, 0.020545709878206253, 0.03331884369254112]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9621802568435669, 0.0378197580575943, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9406200051307678, 0.027340004220604897, 0.03203996270895004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8955293893814087, 0.020192153751850128, 0.05730083957314491, 0.02697763592004776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8267914056777954, 0.030658815056085587, 0.06731340289115906, 0.04709920287132263, 0.028137216344475746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6728754639625549, 0.0493679903447628, 0.07213177531957626, 0.04698864743113518, 0.08619087189435959, 0.07244517654180527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.739273726940155, 0.013345541432499886, 0.04270181432366371, 0.05516185611486435, 0.04005556181073189, 0.04779064282774925, 0.06167086586356163, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6110458374023438, 0.023230109363794327, 0.06672747433185577, 0.126177579164505, 0.0907445102930069, 0.020712684839963913, 0.034512437880039215, 0.026849374175071716, 0.0, 0.0, 0.0, 0.0], [0.6572778224945068, 0.015348791144788265, 0.05557309836149216, 0.11236949265003204, 0.05640522390604019, 0.01472702156752348, 0.022974686697125435, 0.057012807577848434, 0.008311024866998196, 0.0, 0.0, 0.0], [0.6482412815093994, 0.011458102613687515, 0.022950926795601845, 0.07537000626325607, 0.053174689412117004, 0.02772517316043377, 0.03274671733379364, 0.09314462542533875, 0.024601247161626816, 0.010587259195744991, 0.0, 0.0], [0.5539351105690002, 0.006383066531270742, 0.03138359263539314, 0.0315396711230278, 0.043113723397254944, 0.007673004176467657, 0.016137180849909782, 0.14625513553619385, 0.07468774914741516, 0.03697611391544342, 0.051915619522333145, 0.0], [0.7536489963531494, 0.013658531941473484, 0.01463752519339323, 0.014749199151992798, 0.008922536857426167, 0.003636894980445504, 0.01818663813173771, 0.05129677802324295, 0.0219875518232584, 0.031063897535204887, 0.032466839998960495, 0.03574451059103012]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9708282947540283, 0.029171686619520187, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9694469571113586, 0.022553356364369392, 0.007999653927981853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9244140386581421, 0.03660634160041809, 0.01408444344997406, 0.024895241484045982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9318094253540039, 0.03247929736971855, 0.004238942172378302, 0.011139127425849438, 0.020333148539066315, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9158239960670471, 0.03705563023686409, 0.00820318702608347, 0.011483320035040379, 0.022761277854442596, 0.004672633018344641, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8682935237884521, 0.018574591726064682, 0.008039206266403198, 0.022118056192994118, 0.026330817490816116, 0.003077436937019229, 0.05356632545590401, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8640682697296143, 0.030502628535032272, 0.015405193902552128, 0.02414499968290329, 0.019735969603061676, 0.0018500984879210591, 0.019079972058534622, 0.02521296590566635, 0.0, 0.0, 0.0, 0.0], [0.8024616241455078, 0.02182721719145775, 0.012636754661798477, 0.023598335683345795, 0.0454789362847805, 0.0035920317750424147, 0.035511795431375504, 0.037150558084249496, 0.01774275116622448, 0.0, 0.0, 0.0], [0.8342245221138, 0.024120647460222244, 0.007356510031968355, 0.011311501264572144, 0.02830822765827179, 0.002144640777260065, 0.03368804231286049, 0.0313975028693676, 0.013495922088623047, 0.013952430337667465, 0.0, 0.0], [0.8789562582969666, 0.02197320945560932, 0.0054310886189341545, 0.009763823822140694, 0.020371830090880394, 0.0026576449163258076, 0.016456399112939835, 0.014422464184463024, 0.008700119331479073, 0.012299634516239166, 0.008967657573521137, 0.0], [0.9182722568511963, 0.008179848082363605, 0.001989052165299654, 0.005627215839922428, 0.009249819442629814, 0.0018780091777443886, 0.015807252377271652, 0.013963600620627403, 0.00673707015812397, 0.005396303255110979, 0.006586019415408373, 0.006313462276011705]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954980611801147, 0.004501973278820515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9966865181922913, 0.0017453449545428157, 0.0015681401127949357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9958606362342834, 0.0010806269710883498, 0.000682331679854542, 0.002376382937654853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9878743290901184, 0.0018155878642573953, 0.0006917852442711592, 0.0012867357581853867, 0.00833150651305914, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9598414301872253, 0.00618495000526309, 0.0025381436571478844, 0.0031010606326162815, 0.009171919897198677, 0.019162476062774658, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9114314317703247, 0.002916567726060748, 0.0011327023385092616, 0.004016055725514889, 0.014627326279878616, 0.004234398249536753, 0.06164148822426796, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9391415119171143, 0.004776363261044025, 0.0026373909786343575, 0.005427191033959389, 0.011371796950697899, 0.0030510344076901674, 0.02206502854824066, 0.011529739946126938, 0.0, 0.0, 0.0, 0.0], [0.8454865217208862, 0.0029336444567888975, 0.0014568453188985586, 0.002557457657530904, 0.012474792078137398, 0.0060616242699325085, 0.08664198964834213, 0.01970699243247509, 0.02268027700483799, 0.0, 0.0, 0.0], [0.9175593256950378, 0.0013730874052271247, 0.000668866210617125, 0.0013883472420275211, 0.003526488086208701, 0.001990794437006116, 0.03566524758934975, 0.0071600042283535, 0.01712568663060665, 0.013542159460484982, 0.0, 0.0], [0.9802383184432983, 0.0005404870607890189, 0.00021263211965560913, 0.00042028730968013406, 0.001654899213463068, 0.0012096692807972431, 0.006987428292632103, 0.0006902841851115227, 0.003618576331064105, 0.002436508657410741, 0.0019909923430532217, 0.0], [0.9713584780693054, 0.0009448174387216568, 0.00022725843882653862, 0.00038950759335421026, 0.001187001820653677, 0.00038119233795441687, 0.0024588683154433966, 0.0018814575159922242, 0.005330737680196762, 0.003515726188197732, 0.0013166496064513922, 0.011008286848664284]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9714859127998352, 0.02851409837603569, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7714597582817078, 0.16485755145549774, 0.06368272751569748, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.43202266097068787, 0.3828534781932831, 0.16908831894397736, 0.016035521402955055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7320356965065002, 0.07854688167572021, 0.07717498391866684, 0.03506920859217644, 0.07717324793338776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5486602783203125, 0.1659742295742035, 0.04743988439440727, 0.023357348516583443, 0.19828234612941742, 0.016285870224237442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5462896227836609, 0.06450267136096954, 0.03986996412277222, 0.034310102462768555, 0.20795078575611115, 0.07331804931163788, 0.033758848905563354, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5813767313957214, 0.07858378440141678, 0.033334992825984955, 0.03861001133918762, 0.07638591527938843, 0.024287424981594086, 0.148990660905838, 0.018430490046739578, 0.0, 0.0, 0.0, 0.0], [0.30927038192749023, 0.1501440703868866, 0.06015915423631668, 0.0093722864985466, 0.051808591932058334, 0.01346260029822588, 0.09872028231620789, 0.2385048270225525, 0.06855779141187668, 0.0, 0.0, 0.0], [0.1855008751153946, 0.2092641144990921, 0.04240424931049347, 0.005843704100698233, 0.058655209839344025, 0.009105982258915901, 0.083842433989048, 0.12389946728944778, 0.22806136310100555, 0.05342263728380203, 0.0, 0.0], [0.11971992254257202, 0.09128701686859131, 0.015717461705207825, 0.00756356306374073, 0.0997047945857048, 0.008058776147663593, 0.10315125435590744, 0.038854099810123444, 0.2045592963695526, 0.2962348461151123, 0.015148972161114216, 0.0], [0.7949792146682739, 0.009946545585989952, 0.006263789255172014, 0.0017397315241396427, 0.018718477338552475, 0.0062827481888234615, 0.02008422650396824, 0.029990041628479958, 0.023665567860007286, 0.0272422693669796, 0.014818370342254639, 0.04626902565360069]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9870671629905701, 0.012932891957461834, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9198057055473328, 0.018702412024140358, 0.061491772532463074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7579038739204407, 0.055228304117918015, 0.1588001847267151, 0.02806766889989376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8816497325897217, 0.0175887793302536, 0.052515532821416855, 0.03854084759950638, 0.009705109521746635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8400552868843079, 0.034413453191518784, 0.03970469906926155, 0.05387880653142929, 0.02330288290977478, 0.008644955232739449, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7305130958557129, 0.044743768870830536, 0.08263319730758667, 0.07328136265277863, 0.03458539769053459, 0.022681565955281258, 0.011561652645468712, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6116317510604858, 0.0840664952993393, 0.12983722984790802, 0.07818383723497391, 0.01954570971429348, 0.007171822711825371, 0.009727444499731064, 0.059835683554410934, 0.0, 0.0, 0.0, 0.0], [0.2475050538778305, 0.0380217507481575, 0.0394602008163929, 0.03989731892943382, 0.03284277394413948, 0.016064301133155823, 0.03620562329888344, 0.47799092531204224, 0.07201205939054489, 0.0, 0.0, 0.0], [0.24498766660690308, 0.07575057446956635, 0.04206778481602669, 0.028315704315900803, 0.038078825920820236, 0.015312077477574348, 0.03579126298427582, 0.3479982018470764, 0.12932251393795013, 0.042375415563583374, 0.0, 0.0], [0.5384301543235779, 0.035014938563108444, 0.04010298103094101, 0.030297761783003807, 0.016980910673737526, 0.004667351488023996, 0.02412611059844494, 0.1397981345653534, 0.07583049684762955, 0.0633140578866005, 0.031437113881111145, 0.0], [0.5468704104423523, 0.022487999871373177, 0.02461211383342743, 0.015336631797254086, 0.013688808307051659, 0.007280681747943163, 0.012978747487068176, 0.07409809529781342, 0.09405390918254852, 0.10092570632696152, 0.05392022803425789, 0.033746641129255295]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.998475968837738, 0.0015240395441651344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9890474677085876, 0.0011630565859377384, 0.009789487347006798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9780538082122803, 0.00114974204916507, 0.0108914440497756, 0.009905070997774601, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9991136193275452, 0.00016781766316853464, 2.4993492843350396e-05, 4.5147869968786836e-05, 0.0006483724573627114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9740023612976074, 0.003699925495311618, 6.37277917121537e-05, 4.197729504085146e-05, 0.00028169419965706766, 0.02191031537950039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.972184419631958, 0.00031802969169802964, 0.0001716255210340023, 0.0001259989949176088, 0.0006395016680471599, 0.01291621383279562, 0.013644218444824219, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9384461641311646, 0.008473877795040607, 0.0015113122062757611, 0.002284188289195299, 0.0008860185625962913, 0.0004318671708460897, 0.004110751207917929, 0.04385572671890259, 0.0, 0.0, 0.0, 0.0], [0.9086377024650574, 0.006438672076910734, 0.013313245959579945, 0.00951423030346632, 0.020910777151584625, 0.0002960337733384222, 0.002096600830554962, 0.01380289625376463, 0.024989759549498558, 0.0, 0.0, 0.0], [0.9679588079452515, 0.0038848761469125748, 0.0020589090418070555, 0.0013070771237835288, 0.0012255168985575438, 3.7898360460530967e-05, 0.0004632784111890942, 0.009343606419861317, 0.011691833846271038, 0.0020281681790947914, 0.0, 0.0], [0.9945908784866333, 0.0018971646204590797, 0.0001239548291778192, 6.76795607432723e-05, 0.0012277030618861318, 1.7589940398465842e-05, 0.0002515133237466216, 0.0006825759191997349, 0.0002481763658579439, 0.0002158318820875138, 0.0006768791936337948, 0.0], [0.996960461139679, 0.0006087984656915069, 2.040335130004678e-05, 2.444620076857973e-05, 7.312429806916043e-05, 2.830147195709287e-06, 4.200232797302306e-05, 0.00010841114999493584, 0.00013490494166035205, 4.757601345772855e-06, 2.2736829123459756e-05, 0.001997155835852027]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9927821159362793, 0.007217843551188707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9867942929267883, 0.006222226656973362, 0.006983472499996424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9807507991790771, 0.006942576728761196, 0.004853390157222748, 0.007453189697116613, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.993164598941803, 0.00222657504491508, 0.0018698115600273013, 0.00124100293032825, 0.0014980491250753403, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9471458196640015, 0.03945197910070419, 0.005998448934406042, 0.0015308164292946458, 0.0024859642144292593, 0.003387050935998559, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9434520602226257, 0.007492748089134693, 0.007830358110368252, 0.009674347937107086, 0.004590350203216076, 0.0061397859826684, 0.020820287987589836, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9591125845909119, 0.0051765358075499535, 0.003453733865171671, 0.002607130678370595, 0.005264150910079479, 0.002685259561985731, 0.008658094331622124, 0.01304243691265583, 0.0, 0.0, 0.0, 0.0], [0.9122285842895508, 0.004526689648628235, 0.008149227127432823, 0.04828833043575287, 0.010022569447755814, 0.000761044560931623, 0.009623702615499496, 0.0027386071160435677, 0.0036611156538128853, 0.0, 0.0, 0.0], [0.8822763562202454, 0.0035955526400357485, 0.0032317922450602055, 0.06715413182973862, 0.011589575558900833, 0.0013740750728175044, 0.006143556442111731, 0.0027858156245201826, 0.004988240078091621, 0.016860835254192352, 0.0, 0.0], [0.9803778529167175, 0.0015811709454283118, 0.00047653718502260745, 0.0007894948939792812, 0.001486689900048077, 0.0014180223224684596, 0.004027754534035921, 0.00166410265956074, 0.0009349674801342189, 0.0016297998372465372, 0.0056136660277843475, 0.0], [0.9718797206878662, 0.0008234442211687565, 0.003486605593934655, 0.0017205806216225028, 0.0014565916499122977, 0.0013661220436915755, 0.00423714704811573, 0.008189242333173752, 0.0018339062808081508, 0.0016145295230671763, 0.0005657118163071573, 0.0028264191932976246]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9785429239273071, 0.021457064896821976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9299753308296204, 0.029141895473003387, 0.040882814675569534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9213064908981323, 0.01477731578052044, 0.03513401746749878, 0.028782153502106667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9211409091949463, 0.01596906967461109, 0.028594570234417915, 0.010056828148663044, 0.024238621816039085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9005007743835449, 0.013011328876018524, 0.025074420496821404, 0.00751542067155242, 0.017089715227484703, 0.03680823743343353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8979418873786926, 0.019481299445033073, 0.012876991182565689, 0.01026673149317503, 0.015676476061344147, 0.018259182572364807, 0.025497550144791603, 0.0, 0.0, 0.0, 0.0, 0.0], [0.646408200263977, 0.030610067769885063, 0.03615151718258858, 0.03396084904670715, 0.027119465172290802, 0.030329778790473938, 0.04156537353992462, 0.15385480225086212, 0.0, 0.0, 0.0, 0.0], [0.6649606823921204, 0.0157962366938591, 0.023659538477659225, 0.07570378482341766, 0.01390440110117197, 0.02083030715584755, 0.03877710551023483, 0.10322531312704086, 0.043142616748809814, 0.0, 0.0, 0.0], [0.6879334449768066, 0.014144712127745152, 0.03318830206990242, 0.038087327033281326, 0.011158139444887638, 0.032454513013362885, 0.03218429163098335, 0.08450433611869812, 0.026699692010879517, 0.03964526206254959, 0.0, 0.0], [0.7120550274848938, 0.008540334179997444, 0.03430625796318054, 0.034147027879953384, 0.004908512346446514, 0.012034259736537933, 0.019134439527988434, 0.02970195561647415, 0.011686685495078564, 0.034423407167196274, 0.09906210005283356, 0.0], [0.8991708755493164, 0.0255292821675539, 0.006440133322030306, 0.014237319119274616, 0.0042643132619559765, 0.007358605973422527, 0.01052772719413042, 0.011602901853621006, 0.002109092427417636, 0.004809824284166098, 0.002318130573257804, 0.01163184642791748]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9928945899009705, 0.007105353754013777, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9947165846824646, 0.0011583550367504358, 0.004125067498534918, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9893732666969299, 0.0017794754821807146, 0.0030499130953103304, 0.00579735217615962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9727770686149597, 0.002743737306445837, 0.0020542365964502096, 0.0031501161865890026, 0.019274864345788956, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9610493779182434, 0.014755818992853165, 0.0018420260166749358, 0.001140510430559516, 0.002411833731457591, 0.018800441175699234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9409165382385254, 0.0022103548981249332, 0.003270492423325777, 0.006869182456284761, 0.003678797045722604, 0.01210164837539196, 0.030952952802181244, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9517611861228943, 0.008474568836390972, 0.005591141059994698, 0.008169145323336124, 0.004486981779336929, 0.0008660368621349335, 0.0051094177179038525, 0.015541586093604565, 0.0, 0.0, 0.0, 0.0], [0.9406067132949829, 0.0054961615242064, 0.011726447381079197, 0.005837302654981613, 0.024391548708081245, 0.00041461552609689534, 0.0016759197460487485, 0.0038773114793002605, 0.0059740180149674416, 0.0, 0.0, 0.0], [0.9785146117210388, 0.0018249311251565814, 0.0010295179672539234, 0.0013972390443086624, 0.003634414402768016, 0.0002748271799646318, 0.0013667535968124866, 0.005658537615090609, 0.003659167792648077, 0.0026400587521493435, 0.0, 0.0], [0.9715367555618286, 0.000992300221696496, 0.0001720957807265222, 5.5647607950959355e-05, 0.003926153294742107, 0.000739394105039537, 0.0011099879629909992, 0.0017083350103348494, 0.0021713378373533487, 0.004881225526332855, 0.012706750072538853, 0.0], [0.9645591378211975, 0.0007455258164554834, 0.0011378362542018294, 0.0008240666356869042, 0.0007444082875736058, 0.0003435192920733243, 0.00045814213808625937, 0.01116601936519146, 0.0021155639551579952, 0.0014338258188217878, 0.0017822524532675743, 0.014689658768475056]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9833884239196777, 0.016611550003290176, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639711380004883, 0.007253073621541262, 0.02877575345337391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9331758618354797, 0.014997607097029686, 0.023817596957087517, 0.0280089620500803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9568120837211609, 0.009934096597135067, 0.01227604690939188, 0.016106655821204185, 0.004871029872447252, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9151563048362732, 0.032129593193531036, 0.014398870058357716, 0.01664332114160061, 0.00652395561337471, 0.015147935599088669, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7367724180221558, 0.021447639912366867, 0.0431126207113266, 0.08644845336675644, 0.009022037498652935, 0.022701524198055267, 0.08049540966749191, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7203764915466309, 0.022782059386372566, 0.026323888450860977, 0.06355153769254684, 0.005824857857078314, 0.0181754007935524, 0.09361011534929276, 0.04935562610626221, 0.0, 0.0, 0.0, 0.0], [0.7824261784553528, 0.010264112614095211, 0.014493719674646854, 0.1153576672077179, 0.004240964539349079, 0.014435202814638615, 0.03887070715427399, 0.010679810307919979, 0.009231519885361195, 0.0, 0.0, 0.0], [0.7738533020019531, 0.02246744930744171, 0.010654190555214882, 0.06270211189985275, 0.006174781825393438, 0.00859184842556715, 0.05814996734261513, 0.023065779358148575, 0.01593252457678318, 0.01840805821120739, 0.0, 0.0], [0.6581162810325623, 0.027781231328845024, 0.024232415482401848, 0.02947334013879299, 0.012357654049992561, 0.01235811598598957, 0.11365333944559097, 0.027893835678696632, 0.034340184181928635, 0.015996210277080536, 0.04379747435450554, 0.0], [0.8400260806083679, 0.0071548111736774445, 0.007588120177388191, 0.014718865044414997, 0.00522383488714695, 0.01866665668785572, 0.024050846695899963, 0.03681429475545883, 0.010391833260655403, 0.015045713633298874, 0.010403802618384361, 0.009915195405483246]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9864231944084167, 0.01357682328671217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9881282448768616, 0.00620835367590189, 0.005663427524268627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9853056073188782, 0.007153799291700125, 0.005347858183085918, 0.002192764077335596, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9797539114952087, 0.011222547851502895, 0.003685934003442526, 0.0010325682815164328, 0.004304980393499136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9682632684707642, 0.006303965579718351, 0.003172907279804349, 0.0008920436957851052, 0.003028686624020338, 0.01833914965391159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.809746265411377, 0.012085998430848122, 0.016362104564905167, 0.014269798062741756, 0.01812109351158142, 0.01189999096095562, 0.11751478910446167, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5867733359336853, 0.10182203352451324, 0.08833210915327072, 0.02607930265367031, 0.051862601190805435, 0.008960126899182796, 0.024406209588050842, 0.11176424473524094, 0.0, 0.0, 0.0, 0.0], [0.8303860425949097, 0.02753928303718567, 0.02247094176709652, 0.007505586836487055, 0.04472466930747032, 0.012498680502176285, 0.008127537555992603, 0.03845059871673584, 0.00829662848263979, 0.0, 0.0, 0.0], [0.8181108236312866, 0.06121882051229477, 0.021364646032452583, 0.005729142110794783, 0.01924677938222885, 0.012479309923946857, 0.005239915102720261, 0.041441041976213455, 0.009340645745396614, 0.005828950088471174, 0.0, 0.0], [0.894373893737793, 0.024768099188804626, 0.00910963211208582, 0.0011010831221938133, 0.004504394251853228, 0.002647736808285117, 0.0035257632844150066, 0.03519165515899658, 0.004848766140639782, 0.0026703577022999525, 0.01725870557129383, 0.0], [0.8591271042823792, 0.018691103905439377, 0.005416382569819689, 0.003999772015959024, 0.006000928580760956, 0.0027783813420683146, 0.01660899445414543, 0.04342593997716904, 0.005977710243314505, 0.00955294631421566, 0.008975571021437645, 0.019445113837718964]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9545314908027649, 0.045468490570783615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9539321064949036, 0.016750236973166466, 0.029317593201994896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7156912684440613, 0.06119076907634735, 0.19639764726161957, 0.0267203189432621, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9345943927764893, 0.0059928884729743, 0.014621291309595108, 0.026703910902142525, 0.018087441101670265, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7205244302749634, 0.015987621620297432, 0.013564241118729115, 0.041574105620384216, 0.19512180984020233, 0.013227792456746101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5296592116355896, 0.006609749980270863, 0.0326666496694088, 0.16582703590393066, 0.1074318140745163, 0.07241208851337433, 0.08539342135190964, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8330578804016113, 0.01420960295945406, 0.01148221641778946, 0.020361943170428276, 0.005557612981647253, 0.003711639204993844, 0.02928256429731846, 0.08233655989170074, 0.0, 0.0, 0.0, 0.0], [0.265632301568985, 0.010740576311945915, 0.008531873114407063, 0.0034803119488060474, 0.0020550708286464214, 0.0047296639531850815, 0.020066244527697563, 0.6132968664169312, 0.07146716117858887, 0.0, 0.0, 0.0], [0.23288007080554962, 0.01097123697400093, 0.01081137452274561, 0.007107522338628769, 0.003995854407548904, 0.006604264490306377, 0.02285950817167759, 0.48819786310195923, 0.17384830117225647, 0.042723994702100754, 0.0, 0.0], [0.5622583627700806, 0.004619690123945475, 0.0027602692134678364, 0.004669429734349251, 0.004548975266516209, 0.002609629649668932, 0.02168228290975094, 0.09853576123714447, 0.09816978871822357, 0.12446600198745728, 0.07567983120679855, 0.0], [0.5833532214164734, 0.0008391177398152649, 0.0025829472579061985, 0.001092777936719358, 0.004134505055844784, 0.0013618459925055504, 0.005708558950573206, 0.012731814756989479, 0.004722298122942448, 0.014649002812802792, 0.3487206995487213, 0.020103102549910545]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9599244594573975, 0.04007549583911896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9154186248779297, 0.0375947579741478, 0.04698658362030983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4553532004356384, 0.2755279839038849, 0.24509987235069275, 0.024018865078687668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2997068464756012, 0.11262737959623337, 0.2055726796388626, 0.3367130160331726, 0.04538007825613022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4944990575313568, 0.08293410390615463, 0.05914106220006943, 0.13579906523227692, 0.2080155909061432, 0.01961112953722477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2755037844181061, 0.0913882851600647, 0.11712999641895294, 0.29172468185424805, 0.13165025413036346, 0.04027045518159866, 0.0523325651884079, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5252630710601807, 0.1088501513004303, 0.123881496489048, 0.07995685189962387, 0.023757778108119965, 0.009376068599522114, 0.04533025994896889, 0.08358435332775116, 0.0, 0.0, 0.0, 0.0], [0.3491998314857483, 0.1478748470544815, 0.08351965248584747, 0.024971123784780502, 0.017809299752116203, 0.019113343209028244, 0.03939227759838104, 0.24330924451351166, 0.07481036335229874, 0.0, 0.0, 0.0], [0.18752792477607727, 0.1227913573384285, 0.055444661527872086, 0.03543190285563469, 0.02185727097094059, 0.012605033814907074, 0.03657497838139534, 0.26245415210723877, 0.22619396448135376, 0.039118800312280655, 0.0, 0.0], [0.23279622197151184, 0.18157947063446045, 0.07080518454313278, 0.01589854061603546, 0.01990116760134697, 0.010073570534586906, 0.03133144974708557, 0.19689248502254486, 0.13680054247379303, 0.09065612405538559, 0.013265252113342285, 0.0], [0.27414047718048096, 0.07588503509759903, 0.07546238601207733, 0.05332941189408302, 0.02539697289466858, 0.009483867324888706, 0.026718733832240105, 0.24128170311450958, 0.09875675290822983, 0.08951166272163391, 0.018554605543613434, 0.011478336527943611]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9641018509864807, 0.03589806705713272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9710012674331665, 0.010919103398919106, 0.018079550936818123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9170807003974915, 0.01946580782532692, 0.036353662610054016, 0.027099765837192535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9346190690994263, 0.01826765388250351, 0.007908038794994354, 0.01692192442715168, 0.022283250465989113, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9299283027648926, 0.03847460076212883, 0.003714577993378043, 0.005441469140350819, 0.010758050717413425, 0.011683084070682526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9585739374160767, 0.006481367163360119, 0.0027810425963252783, 0.007383889984339476, 0.006073185242712498, 0.00627536978572607, 0.012431229464709759, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8795750141143799, 0.029372479766607285, 0.01352210994809866, 0.01707076095044613, 0.008944825269281864, 0.017656443640589714, 0.013178383931517601, 0.020679980516433716, 0.0, 0.0, 0.0, 0.0], [0.7964720726013184, 0.009919540956616402, 0.007821394130587578, 0.010309536941349506, 0.01952836662530899, 0.03185391426086426, 0.012048512697219849, 0.08115827292203903, 0.03088843636214733, 0.0, 0.0, 0.0], [0.8709201812744141, 0.013776390813291073, 0.002330548595637083, 0.005582301411777735, 0.006572830490767956, 0.006913002114742994, 0.009338192641735077, 0.043526969850063324, 0.03524934872984886, 0.005790200550109148, 0.0, 0.0], [0.9654254913330078, 0.0028783672023564577, 0.0004976456402800977, 0.00031805047183297575, 0.0008818897185847163, 0.0030536502599716187, 0.0018710183212533593, 0.008011894300580025, 0.010718916542828083, 0.0036347401328384876, 0.0027082685846835375, 0.0], [0.9775938987731934, 0.0008234509150497615, 0.0006108650704845786, 0.0015767636941745877, 0.000458991969935596, 0.000868999573867768, 0.0008578190463595092, 0.005705096758902073, 0.0025221470277756453, 0.004016389604657888, 0.0029377362225204706, 0.002027790993452072]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9344093799591064, 0.06559056788682938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.913509726524353, 0.04982312023639679, 0.03666726127266884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.580337643623352, 0.23110836744308472, 0.1691732257604599, 0.019380683079361916, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7784227132797241, 0.010185912251472473, 0.06038329750299454, 0.06886337697505951, 0.08214464038610458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2429116815328598, 0.003962585236877203, 0.014769071713089943, 0.013769702054560184, 0.6829657554626465, 0.04162114858627319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6121495962142944, 0.011295163072645664, 0.024656226858496666, 0.02845974639058113, 0.15529166162014008, 0.1426578313112259, 0.025489801540970802, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9288339614868164, 0.011457913555204868, 0.004063802771270275, 0.001387398922815919, 0.0065957591868937016, 0.0019361911108717322, 0.005480026360601187, 0.04024500399827957, 0.0, 0.0, 0.0, 0.0], [0.7609086036682129, 0.00886379275470972, 0.003322234842926264, 0.0007435657316818833, 0.00332867749966681, 0.004057576879858971, 0.0034779254347085953, 0.18932844698429108, 0.025969181209802628, 0.0, 0.0, 0.0], [0.35696181654930115, 0.034133121371269226, 0.009367011487483978, 0.0016982831293717027, 0.0028338548727333546, 0.0019764259923249483, 0.004862355068325996, 0.16966365277767181, 0.34257078170776367, 0.07593267410993576, 0.0, 0.0], [0.19800199568271637, 0.025814514607191086, 0.006498319562524557, 0.0033900009002536535, 0.0005726346862502396, 0.000925157277379185, 0.0019261229317635298, 0.043951790779829025, 0.26470479369163513, 0.4272697865962982, 0.0269448421895504, 0.0], [0.4818594753742218, 0.0014963062712922692, 0.0009083296754397452, 0.00164273870177567, 0.0010060446802526712, 0.0008963003638200462, 0.001464467728510499, 0.03045981377363205, 0.06667499244213104, 0.10841517895460129, 0.30026134848594666, 0.004915055353194475]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9454092979431152, 0.05459076538681984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9455482363700867, 0.02960694581270218, 0.024844761937856674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7437288165092468, 0.10178420692682266, 0.10946114361286163, 0.045025791972875595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7934507131576538, 0.07622075080871582, 0.03350523114204407, 0.01983577013015747, 0.07698751986026764, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6495540738105774, 0.1336071491241455, 0.04275063797831535, 0.012052206322550774, 0.11466038972139359, 0.047375574707984924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7477673292160034, 0.03079468570649624, 0.011984283104538918, 0.013767553493380547, 0.038310546427965164, 0.01868578791618347, 0.1386898159980774, 0.0, 0.0, 0.0, 0.0, 0.0], [0.568367063999176, 0.1111936867237091, 0.030141862109303474, 0.024809127673506737, 0.017061498016119003, 0.012839935719966888, 0.1675516813993454, 0.06803508847951889, 0.0, 0.0, 0.0, 0.0], [0.463496595621109, 0.08066706359386444, 0.026486407965421677, 0.030326571315526962, 0.030952638015151024, 0.018361134454607964, 0.18911153078079224, 0.14585180580615997, 0.0147462859749794, 0.0, 0.0, 0.0], [0.2627663016319275, 0.10178075730800629, 0.023230552673339844, 0.013717129826545715, 0.06784432381391525, 0.029142923653125763, 0.3110882639884949, 0.14705567061901093, 0.022244855761528015, 0.02112925983965397, 0.0, 0.0], [0.2876681387424469, 0.09234239161014557, 0.02158532850444317, 0.0065733869560062885, 0.06516952067613602, 0.03599430248141289, 0.15227191150188446, 0.153856560587883, 0.06438586860895157, 0.09089946746826172, 0.029253121465444565, 0.0], [0.6103418469429016, 0.017316823825240135, 0.012136313132941723, 0.0071127088740468025, 0.014577041380107403, 0.005064326338469982, 0.08169989287853241, 0.0944306030869484, 0.014853653497993946, 0.022790325805544853, 0.021932747215032578, 0.09774366766214371]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9060660600662231, 0.09393386542797089, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9454213380813599, 0.029138846322894096, 0.025439882650971413, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9105293154716492, 0.031800832599401474, 0.019080108031630516, 0.03858979046344757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8922500014305115, 0.045301809906959534, 0.0068474686704576015, 0.007289707660675049, 0.04831111058592796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7288694381713867, 0.08836505562067032, 0.010482770390808582, 0.00817499402910471, 0.10777655988931656, 0.05633119121193886, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7346106171607971, 0.01621781848371029, 0.004986548330634832, 0.010838131420314312, 0.05239303037524223, 0.0216192826628685, 0.1593346744775772, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7248605489730835, 0.040237072855234146, 0.013126091100275517, 0.01843258924782276, 0.02800922468304634, 0.01425946969538927, 0.10032494366168976, 0.06075011193752289, 0.0, 0.0, 0.0, 0.0], [0.78936368227005, 0.030171556398272514, 0.006135299801826477, 0.016033673658967018, 0.026445290073752403, 0.009294361807405949, 0.060431599617004395, 0.042378980666399, 0.019745569676160812, 0.0, 0.0, 0.0], [0.727250337600708, 0.05443266034126282, 0.007885356433689594, 0.013156160712242126, 0.038205329328775406, 0.01150230597704649, 0.06820333003997803, 0.03225696086883545, 0.03033139929175377, 0.016776176169514656, 0.0, 0.0], [0.8617791533470154, 0.026595894247293472, 0.0027280678041279316, 0.002177381655201316, 0.033858299255371094, 0.010502438992261887, 0.037758685648441315, 0.01050114631652832, 0.006501533556729555, 0.005558961536735296, 0.002038508653640747, 0.0], [0.9008129239082336, 0.01155546959489584, 0.0014615850523114204, 0.0018201622879132628, 0.0068506342358887196, 0.0038335747085511684, 0.022935697808861732, 0.008171186782419682, 0.00401699636131525, 0.002819227520376444, 0.002851220779120922, 0.03287138789892197]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9711552858352661, 0.028844796121120453, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9400196075439453, 0.024724081158638, 0.035256341099739075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9291672110557556, 0.025177214294672012, 0.013872582465410233, 0.03178299963474274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9021143317222595, 0.04454120621085167, 0.004322773311287165, 0.009370722807943821, 0.039650920778512955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6811766624450684, 0.07174920290708542, 0.0035815262235701084, 0.0054411194287240505, 0.05967748537659645, 0.1783740520477295, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7336689829826355, 0.024708159267902374, 0.012755402363836765, 0.020908746868371964, 0.04402637109160423, 0.039308443665504456, 0.12462390214204788, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6342623829841614, 0.062019720673561096, 0.037523139268159866, 0.048569437116384506, 0.0436222068965435, 0.026331208646297455, 0.042912039905786514, 0.10475988686084747, 0.0, 0.0, 0.0, 0.0], [0.553666353225708, 0.0407608300447464, 0.038915377110242844, 0.035027530044317245, 0.11942419409751892, 0.028515340760350227, 0.02924085222184658, 0.08812159299850464, 0.06632786989212036, 0.0, 0.0, 0.0], [0.688938319683075, 0.05334436148405075, 0.01750391535460949, 0.014164717867970467, 0.0554332360625267, 0.01891702599823475, 0.026686951518058777, 0.05804955214262009, 0.03254781290888786, 0.03441402316093445, 0.0, 0.0], [0.7876285314559937, 0.039235543459653854, 0.006634343881160021, 0.005334618967026472, 0.014063711278140545, 0.012711395509541035, 0.03339666873216629, 0.029827270656824112, 0.022040268406271935, 0.015042371116578579, 0.03408533334732056, 0.0], [0.6718378663063049, 0.03505013883113861, 0.017613409087061882, 0.02034279704093933, 0.010089348070323467, 0.0067496150732040405, 0.02397306263446808, 0.07301551848649979, 0.04475362226366997, 0.031122377142310143, 0.018296103924512863, 0.04715625196695328]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9822845458984375, 0.017715459689497948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9900949001312256, 0.005737411789596081, 0.004167679697275162, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9839543104171753, 0.007681929040700197, 0.005227251909673214, 0.003136551473289728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9658727645874023, 0.017640473321080208, 0.003830207046121359, 0.00324417557567358, 0.009412380866706371, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8915450572967529, 0.051525432616472244, 0.006071369629353285, 0.0035812489222735167, 0.03287164866924286, 0.014405152760446072, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9218969941139221, 0.030399978160858154, 0.00790223479270935, 0.005038347095251083, 0.007177002262324095, 0.006294097751379013, 0.021291334182024002, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8118323683738708, 0.09103389829397202, 0.0319582037627697, 0.014074580743908882, 0.010015168227255344, 0.006937830708920956, 0.025586619973182678, 0.00856128241866827, 0.0, 0.0, 0.0, 0.0], [0.922038733959198, 0.02226976864039898, 0.018516307696700096, 0.012293116189539433, 0.005450281780213118, 0.0009698097710497677, 0.004299158696085215, 0.008207753300666809, 0.0059549519792199135, 0.0, 0.0, 0.0], [0.9112234115600586, 0.03702341765165329, 0.010586915537714958, 0.006676358170807362, 0.004719773773103952, 0.00241417670622468, 0.007074103225022554, 0.010462512262165546, 0.005340177100151777, 0.004479105118662119, 0.0, 0.0], [0.8715102672576904, 0.04508206620812416, 0.01316449511796236, 0.013952177949249744, 0.006064269691705704, 0.005258624441921711, 0.009051769971847534, 0.01715582236647606, 0.007955907844007015, 0.005622462369501591, 0.0051820361986756325, 0.0], [0.8859798908233643, 0.03400805592536926, 0.009713098406791687, 0.005235296208411455, 0.0022360130678862333, 0.0030401251278817654, 0.011765825562179089, 0.01835695281624794, 0.01226118952035904, 0.00515034468844533, 0.004004599992185831, 0.008248699828982353]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9083573222160339, 0.09164265543222427, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9380859732627869, 0.049213163554668427, 0.012700850144028664, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47146764397621155, 0.4680642783641815, 0.04094937443733215, 0.019518638029694557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8040472269058228, 0.12471917271614075, 0.008773540146648884, 0.009016537107527256, 0.05344346538186073, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7611523270606995, 0.15308216214179993, 0.006050650961697102, 0.00495164655148983, 0.04854501411318779, 0.026218228042125702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9379024505615234, 0.02704652212560177, 0.0018861687276512384, 0.0014925767900422215, 0.009963126853108406, 0.003181064734235406, 0.01852814108133316, 0.0, 0.0, 0.0, 0.0, 0.0], [0.795678436756134, 0.13805285096168518, 0.007810178212821484, 0.010206769220530987, 0.009115113876760006, 0.002196220913901925, 0.03175688534975052, 0.005183548666536808, 0.0, 0.0, 0.0, 0.0], [0.37061867117881775, 0.3573296368122101, 0.038041502237319946, 0.023555709049105644, 0.04372108727693558, 0.00560423918068409, 0.044380877166986465, 0.04077915474772453, 0.07596917450428009, 0.0, 0.0, 0.0], [0.2718384861946106, 0.5597829818725586, 0.013138820417225361, 0.01897973008453846, 0.021633068099617958, 0.0027037200052291155, 0.023393990471959114, 0.006207783706486225, 0.06513436883687973, 0.01718704216182232, 0.0, 0.0], [0.2894097566604614, 0.5015140771865845, 0.007352239917963743, 0.011384407058358192, 0.04323912784457207, 0.003372520674020052, 0.01933472976088524, 0.007036629132926464, 0.08157788962125778, 0.029770083725452423, 0.0060084969736635685, 0.0], [0.8378651738166809, 0.10098542273044586, 0.006563683971762657, 0.006245762575417757, 0.004992939997464418, 0.0006077795405872166, 0.01177496463060379, 0.004317070823162794, 0.007876033894717693, 0.0034802593290805817, 0.0041266800835728645, 0.011164212599396706]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9978633522987366, 0.002136687748134136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9936667084693909, 0.00111295934766531, 0.005220279563218355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9842586517333984, 0.0006363429129123688, 0.0019509481498971581, 0.013154077343642712, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9958784580230713, 0.0009725477430038154, 0.0003438743588048965, 0.0015586138470098376, 0.0012464041355997324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9910929799079895, 0.0017069658497348428, 0.0006144698709249496, 0.0011659094598144293, 0.0006327943992801011, 0.004786900244653225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9694110155105591, 0.0010654854122549295, 0.0028117725159972906, 0.013037307187914848, 0.00183580769225955, 0.004566478542983532, 0.007272124756127596, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9638490676879883, 0.002266687573865056, 0.0036652972921729088, 0.012074706144630909, 0.000817091902717948, 0.0032485960982739925, 0.0050157285295426846, 0.009062903933227062, 0.0, 0.0, 0.0, 0.0], [0.8921554684638977, 0.0011151746148243546, 0.04414283111691475, 0.05002937465906143, 0.0020383328665047884, 0.0008066057926043868, 0.002532707992941141, 0.003100438043475151, 0.00407894654199481, 0.0, 0.0, 0.0], [0.9638035893440247, 0.0015218525659292936, 0.002019214443862438, 0.014751988463103771, 0.0010176032083109021, 0.0006294742343015969, 0.003951151389628649, 0.004021752160042524, 0.0021218282636255026, 0.006161483936011791, 0.0, 0.0], [0.9906448125839233, 0.000569879834074527, 0.00015169235120993108, 0.00033012768835760653, 0.0005488651222549379, 0.0006114195566624403, 0.003713885322213173, 0.0002544716408010572, 0.00021875425591133535, 0.000473485590191558, 0.002482580253854394, 0.0], [0.9286215305328369, 0.00080977997276932, 0.0011899578385055065, 0.0037803007289767265, 0.0005017361254431307, 0.0011428705183789134, 0.0026029269210994244, 0.030657483264803886, 0.004102746024727821, 0.0031678960658609867, 0.015069478191435337, 0.008353290148079395]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8719334006309509, 0.12806659936904907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9812983274459839, 0.013914922252297401, 0.004786768462508917, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8051321506500244, 0.11729614436626434, 0.03648073598742485, 0.04109096899628639, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8084105849266052, 0.06693415343761444, 0.0032275309786200523, 0.012143696658313274, 0.10928407311439514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.774820864200592, 0.1098903939127922, 0.002049421425908804, 0.006369747221469879, 0.0663008987903595, 0.040568672120571136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8524223566055298, 0.02982816845178604, 0.0038805764634162188, 0.004693250171840191, 0.03846175596117973, 0.02717328630387783, 0.04354069009423256, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7842097878456116, 0.09376075118780136, 0.008352802135050297, 0.012617487460374832, 0.029124675318598747, 0.008978815749287605, 0.04339942708611488, 0.019556211307644844, 0.0, 0.0, 0.0, 0.0], [0.5212060809135437, 0.07670338451862335, 0.017001362517476082, 0.04129963368177414, 0.051241278648376465, 0.053196411579847336, 0.07913165539503098, 0.08908680826425552, 0.07113347202539444, 0.0, 0.0, 0.0], [0.3241563141345978, 0.17583559453487396, 0.019685788080096245, 0.05444914102554321, 0.06105230003595352, 0.023203469812870026, 0.09745614975690842, 0.05509212613105774, 0.09995582699775696, 0.08911324292421341, 0.0, 0.0], [0.4384008049964905, 0.19438093900680542, 0.01267324946820736, 0.028125805780291557, 0.06968530267477036, 0.015321681275963783, 0.07065600156784058, 0.02817959524691105, 0.07134325057268143, 0.050140898674726486, 0.021092411130666733, 0.0], [0.8714165091514587, 0.015842583030462265, 0.0023964911233633757, 0.0020063251722604036, 0.003373374929651618, 0.002911924384534359, 0.014991074800491333, 0.004303625784814358, 0.005433930084109306, 0.006405934691429138, 0.007156007457524538, 0.06376238167285919]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8568388223648071, 0.14316119253635406, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9193951487541199, 0.06870472431182861, 0.011900211684405804, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3993164002895355, 0.2779962420463562, 0.27255135774612427, 0.0501360185444355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.464661180973053, 0.05748578533530235, 0.168361634016037, 0.23778514564037323, 0.07170628756284714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2864910364151001, 0.02554885670542717, 0.11880244314670563, 0.13737311959266663, 0.3847564458847046, 0.04702811688184738, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5826156139373779, 0.04524430260062218, 0.08079396933317184, 0.11601204425096512, 0.06335814297199249, 0.08266730606555939, 0.02930864877998829, 0.0, 0.0, 0.0, 0.0, 0.0], [0.621957004070282, 0.03248189762234688, 0.09221664816141129, 0.023237692192196846, 0.006337635684758425, 0.004054639954119921, 0.1763744354248047, 0.04334011301398277, 0.0, 0.0, 0.0, 0.0], [0.21464376151561737, 0.08292514830827713, 0.06811226159334183, 0.04289105534553528, 0.009749014861881733, 0.006623787805438042, 0.15111719071865082, 0.34132760763168335, 0.08261016756296158, 0.0, 0.0, 0.0], [0.175147145986557, 0.05236845090985298, 0.023056963458657265, 0.03884847089648247, 0.015489337034523487, 0.014504949562251568, 0.1915929913520813, 0.1825890839099884, 0.23250852525234222, 0.07389406859874725, 0.0, 0.0], [0.27382057905197144, 0.03575701639056206, 0.0201625544577837, 0.016249284148216248, 0.007337996736168861, 0.004205544479191303, 0.16317179799079895, 0.08581888675689697, 0.14654229581356049, 0.21911534667015076, 0.02781870774924755, 0.0], [0.42977839708328247, 0.008929247036576271, 0.015465420670807362, 0.01951991394162178, 0.005024416837841272, 0.006255012471228838, 0.006588140968233347, 0.06622061878442764, 0.06647183746099472, 0.16748544573783875, 0.19983278214931488, 0.008428767323493958]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9911783337593079, 0.008821694180369377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9902048110961914, 0.005291587673127651, 0.0045036510564386845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9829759001731873, 0.007564705330878496, 0.0053575411438941956, 0.004101893398910761, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9868907332420349, 0.0013774647377431393, 0.001133812707848847, 0.0020211460068821907, 0.008576859720051289, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9826126098632812, 0.00105368928052485, 0.002848650561645627, 0.0017389522399753332, 0.008973107673227787, 0.002772937063127756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9663691520690918, 0.0049200826324522495, 0.0037204334512352943, 0.0021413606591522694, 0.004788492806255817, 0.002524899085983634, 0.015535526908934116, 0.0, 0.0, 0.0, 0.0, 0.0], [0.859234631061554, 0.030250336974859238, 0.022093260660767555, 0.021275794133543968, 0.00795057788491249, 0.0018679768545553088, 0.011464658193290234, 0.045862723141908646, 0.0, 0.0, 0.0, 0.0], [0.9502668380737305, 0.01678713597357273, 0.0040345704182982445, 0.0039037347305566072, 0.0017617355333641171, 0.002332107163965702, 0.0011692644329741597, 0.015074377879500389, 0.004670233931392431, 0.0, 0.0, 0.0], [0.9573739171028137, 0.023412372916936874, 0.0020914736669510603, 0.0022509703412652016, 0.0005541950813494623, 0.0009293589973822236, 0.0008928016759455204, 0.006361812818795443, 0.004439128562808037, 0.0016939443303272128, 0.0, 0.0], [0.9823766946792603, 0.012010788545012474, 0.001009209081530571, 0.000939368037506938, 0.00024436909006908536, 0.00026473094476386905, 0.0004882697539869696, 0.001088366494514048, 0.000725705991499126, 0.00028795996331609786, 0.0005645057535730302, 0.0], [0.9285996556282043, 0.008462183177471161, 0.0004960157093591988, 0.00287606636993587, 0.0005353108863346279, 0.0013386410428211093, 0.0015953679103404284, 0.009122012183070183, 0.003001635195687413, 0.0013374824775382876, 0.0018174820579588413, 0.04081810265779495]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9067530035972595, 0.09324691444635391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9309826493263245, 0.04504658654332161, 0.023970741778612137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9092339873313904, 0.04110626131296158, 0.013500199653208256, 0.03615960106253624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7281203866004944, 0.1462588757276535, 0.015973569825291634, 0.06325114518404007, 0.04639606177806854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7857105731964111, 0.08647371083498001, 0.005242717918008566, 0.018110303208231926, 0.044252924621105194, 0.06020974740386009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8466817736625671, 0.03605351969599724, 0.02203640714287758, 0.03463821858167648, 0.016753247007727623, 0.01494639553129673, 0.02889041043817997, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7086648941040039, 0.13225552439689636, 0.02823363058269024, 0.05346347764134407, 0.017867907881736755, 0.003718329593539238, 0.028196560218930244, 0.027599753811955452, 0.0, 0.0, 0.0, 0.0], [0.6521582007408142, 0.08188103139400482, 0.047886110842227936, 0.07303404062986374, 0.021332012489438057, 0.0027003581635653973, 0.0123111791908741, 0.03877689689397812, 0.06992021948099136, 0.0, 0.0, 0.0], [0.6676712036132812, 0.11749421060085297, 0.015705877915024757, 0.02451998181641102, 0.04135364294052124, 0.00487519009038806, 0.02778851054608822, 0.03070817142724991, 0.0445249117910862, 0.025358354672789574, 0.0, 0.0], [0.6953609585762024, 0.11288509517908096, 0.012430076487362385, 0.012638640590012074, 0.026928329840302467, 0.007906547747552395, 0.0275066327303648, 0.02850743755698204, 0.035835150629282, 0.01347059290856123, 0.026530414819717407, 0.0], [0.7998102903366089, 0.014226152561604977, 0.014012276194989681, 0.007453899830579758, 0.0027770567685365677, 0.0039000592660158873, 0.010952061973512173, 0.028943652287125587, 0.03049728274345398, 0.01820848509669304, 0.006345365196466446, 0.0628734827041626]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9880555272102356, 0.011944525875151157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9964759945869446, 0.0006061861640773714, 0.002917829668149352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9930844306945801, 0.001490541733801365, 0.003033445216715336, 0.0023915856145322323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9914074540138245, 0.00431450130417943, 0.0002573828387539834, 0.0010034998413175344, 0.0030171105172485113, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9740700125694275, 0.022409625351428986, 0.00018698530038818717, 0.00022328770137391984, 0.0017039516242220998, 0.0014062441186979413, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9795630574226379, 0.004615123383700848, 0.001092405873350799, 0.0007387528312392533, 0.00217645475640893, 0.0004575460043270141, 0.011356688104569912, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9588999152183533, 0.017958827316761017, 0.0009885195177048445, 0.001330448780208826, 0.003253326751291752, 0.0014409273862838745, 0.015131156891584396, 0.0009969016537070274, 0.0, 0.0, 0.0, 0.0], [0.9281482100486755, 0.004542018286883831, 0.01224128995090723, 0.007568803615868092, 0.026998482644557953, 0.0007288061315193772, 0.012489157728850842, 0.004019512329250574, 0.003263820894062519, 0.0, 0.0, 0.0], [0.9626762866973877, 0.01584896259009838, 0.0022223410196602345, 0.0014076919760555029, 0.008332472294569016, 0.00027121708262711763, 0.005194467026740313, 0.001047226251102984, 0.001561095705255866, 0.0014382838271558285, 0.0, 0.0], [0.9750708937644958, 0.010242724791169167, 0.0006991885602474213, 0.0002682592021301389, 0.0048714312724769115, 0.0002459019306115806, 0.0042515648528933525, 0.0006689619622193277, 0.00048618149594403803, 0.00047707444173283875, 0.0027177685406059027, 0.0], [0.9490756392478943, 0.009783460758626461, 0.001052114530466497, 0.0006409459165297449, 0.001741774845868349, 0.00030809841700829566, 0.008974538184702396, 0.007839265279471874, 0.0007630852051079273, 0.0005373173044063151, 0.000681112171150744, 0.018602663651108742]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9642065763473511, 0.03579336404800415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9825668931007385, 0.011166771873831749, 0.0062662530690431595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9261529445648193, 0.03268115967512131, 0.02121279388666153, 0.019953105598688126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.882043182849884, 0.06571570038795471, 0.011560861021280289, 0.019639695063233376, 0.02104063890874386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8616356253623962, 0.08733692020177841, 0.006380805745720863, 0.02222723513841629, 0.007759251166135073, 0.01466013677418232, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7066667675971985, 0.0514804869890213, 0.05588319152593613, 0.06063214689493179, 0.028761539608240128, 0.02982599101960659, 0.06674986332654953, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3600865602493286, 0.18019723892211914, 0.08328453451395035, 0.1453971415758133, 0.09309445321559906, 0.02139112539589405, 0.05785462260246277, 0.05869432911276817, 0.0, 0.0, 0.0, 0.0], [0.5882869958877563, 0.026431219652295113, 0.05022485926747322, 0.11200159788131714, 0.05610451102256775, 0.021694857627153397, 0.05218958482146263, 0.06584056466817856, 0.027225814759731293, 0.0, 0.0, 0.0], [0.6086090207099915, 0.07490181922912598, 0.015258785337209702, 0.0643756166100502, 0.05986897274851799, 0.018340755254030228, 0.06025011092424393, 0.042734187096357346, 0.030566731467843056, 0.025093980133533478, 0.0, 0.0], [0.8226079344749451, 0.06567767262458801, 0.0035248040221631527, 0.008106512017548084, 0.01875246688723564, 0.008733488619327545, 0.03077225387096405, 0.01780271902680397, 0.011922917328774929, 0.005528751295059919, 0.006570399273186922, 0.0], [0.6578125357627869, 0.0259100291877985, 0.03159765899181366, 0.03920446336269379, 0.020004183053970337, 0.013009735383093357, 0.04814480245113373, 0.057208605110645294, 0.031058456748723984, 0.036910030990839005, 0.024095509201288223, 0.015044021420180798]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9685986638069153, 0.03140135109424591, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9682132005691528, 0.025465765967965126, 0.006320956163108349, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9695953130722046, 0.024472501128911972, 0.0023836418986320496, 0.0035486400593072176, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9326664209365845, 0.048802752047777176, 0.006895655766129494, 0.004397108219563961, 0.007238076068460941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7093943357467651, 0.23864763975143433, 0.006291310302913189, 0.008692820556461811, 0.028195081278681755, 0.0087787676602602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.934461236000061, 0.01701609045267105, 0.0028434977866709232, 0.002343429485335946, 0.004842768423259258, 0.0026667683850973845, 0.03582623973488808, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8699634671211243, 0.03643565997481346, 0.005331853404641151, 0.006597083527594805, 0.010304955765604973, 0.007869871333241463, 0.0551215261220932, 0.008375569246709347, 0.0, 0.0, 0.0, 0.0], [0.9032288789749146, 0.03687676787376404, 0.007557252887636423, 0.00934548955410719, 0.005882586818188429, 0.0024084127508103848, 0.0151184918358922, 0.01426418125629425, 0.0053178658708930016, 0.0, 0.0, 0.0], [0.846503734588623, 0.06801556050777435, 0.004245505202561617, 0.007141346577554941, 0.007739260792732239, 0.003457383019849658, 0.0424082987010479, 0.01159694604575634, 0.0057854317128658295, 0.0031064837239682674, 0.0, 0.0], [0.8873735070228577, 0.06260819733142853, 0.002867533592507243, 0.0029608593322336674, 0.010398749262094498, 0.0030290542636066675, 0.01529025286436081, 0.006388470064848661, 0.0047644940204918385, 0.0028083587531000376, 0.0015105652855709195, 0.0], [0.8902162909507751, 0.03301147371530533, 0.00618304219096899, 0.013072359375655651, 0.006853392347693443, 0.004375688266009092, 0.016710350289940834, 0.007010468281805515, 0.0029177390970289707, 0.0060821776278316975, 0.004382497631013393, 0.009184484370052814]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.994461178779602, 0.005538840312510729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9986999034881592, 0.00046824259334243834, 0.0008318868349306285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9963828325271606, 0.0009283061954192817, 0.001546939485706389, 0.0011418551439419389, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9969876408576965, 0.001592902815900743, 0.0001457851758459583, 0.00023439523647539318, 0.0010392277035862207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9928420782089233, 0.004003777634352446, 0.00011105102021247149, 0.00011847006680909544, 0.0012210400309413671, 0.0017036066856235266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9916440844535828, 0.0023549096658825874, 0.00023676450655329973, 0.0002687149099074304, 0.001049158745445311, 0.000474019005196169, 0.003972328267991543, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9746821522712708, 0.011402412317693233, 0.0006524948985315859, 0.0007208043825812638, 0.0029426112305372953, 0.001003988436423242, 0.006663138046860695, 0.0019323431188240647, 0.0, 0.0, 0.0, 0.0], [0.9333696961402893, 0.003415751038119197, 0.007108181715011597, 0.006032825447618961, 0.03696012496948242, 0.0008536730892956257, 0.007170883473008871, 0.0030297436751425266, 0.002059126505628228, 0.0, 0.0, 0.0], [0.9707586765289307, 0.010710692033171654, 0.001793080591596663, 0.0013629375025629997, 0.007033053785562515, 0.00042083102744072676, 0.003225359134376049, 0.002422106685116887, 0.0013821051688864827, 0.0008911899058148265, 0.0, 0.0], [0.9874714016914368, 0.006124864798039198, 0.0004326601920183748, 0.00012500783486757427, 0.0013952761655673385, 0.0003261819074396044, 0.00240827864035964, 0.00030412079649977386, 8.481547411065549e-05, 0.00010053622827399522, 0.0012267890851944685, 0.0], [0.9751123785972595, 0.004809068515896797, 0.0004776626592501998, 0.000905886641703546, 0.002790424507111311, 0.0003015836118720472, 0.003393835388123989, 0.004881435073912144, 0.0005771630094386637, 0.00024913999368436635, 0.00045201211469247937, 0.006049461662769318]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9872762560844421, 0.012723677791655064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9769987463951111, 0.015089777298271656, 0.007911494001746178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7192603349685669, 0.07818789035081863, 0.16931496560573578, 0.03323685750365257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5110568404197693, 0.04792950674891472, 0.09090563654899597, 0.24487298727035522, 0.10523495823144913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6439914107322693, 0.024731473997235298, 0.05144745483994484, 0.10217438638210297, 0.1657983958721161, 0.011856816709041595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13008634746074677, 0.013661999255418777, 0.0674005076289177, 0.25941798090934753, 0.3698829114437103, 0.08887065947055817, 0.07067958265542984, 0.0, 0.0, 0.0, 0.0, 0.0], [0.914534330368042, 0.010548378340899944, 0.011680630035698414, 0.013536592945456505, 0.00821351446211338, 0.0033067918848246336, 0.009784086607396603, 0.028395747765898705, 0.0, 0.0, 0.0, 0.0], [0.5368485450744629, 0.008890808559954166, 0.01036416832357645, 0.004269768018275499, 0.0030370207969099283, 0.003252969356253743, 0.0023128686007112265, 0.3508163392543793, 0.08020754158496857, 0.0, 0.0, 0.0], [0.3706224262714386, 0.015943603590130806, 0.005344349890947342, 0.005884132813662291, 0.005949968472123146, 0.003633488668128848, 0.007731324527412653, 0.12839210033416748, 0.4056079387664795, 0.050890643149614334, 0.0, 0.0], [0.9035380482673645, 0.0065302373841404915, 0.0008655788260512054, 0.0007200771942734718, 0.001338525558821857, 0.0010542329400777817, 0.0022727001924067736, 0.011824101209640503, 0.03206139802932739, 0.03537563607096672, 0.004419403616338968, 0.0], [0.2813841700553894, 0.002389682689681649, 0.0037858278956264257, 0.005482211709022522, 0.0023430329747498035, 0.0015469792997464538, 0.006107555236667395, 0.025983456522226334, 0.021969415247440338, 0.08064486086368561, 0.5125088691711426, 0.055853988975286484]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9947903156280518, 0.005209686234593391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.99378502368927, 0.0010037294123321772, 0.005211161449551582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9909141063690186, 0.002276308834552765, 0.0036688533145934343, 0.0031407459173351526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9884196519851685, 0.005556918680667877, 0.0010611189063638449, 0.002036779886111617, 0.002925445791333914, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.984398603439331, 0.011248646304011345, 0.0005393119645304978, 0.000602163199800998, 0.0021721075754612684, 0.001039086258970201, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9890060424804688, 0.00273532560095191, 0.0012896652333438396, 0.0016003205673769116, 0.0010362454922869802, 0.0021972188260406256, 0.002135188551619649, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9843204021453857, 0.004532759543508291, 0.0009355939109809697, 0.0024651510175317526, 0.0008321055211126804, 0.002747180638834834, 0.0017029602313414216, 0.002463964046910405, 0.0, 0.0, 0.0, 0.0], [0.9581343531608582, 0.0015685476828366518, 0.01611737534403801, 0.020945990458130836, 0.001178841688670218, 0.00037148542469367385, 0.0004319259023759514, 0.000615814933553338, 0.0006355926743708551, 0.0, 0.0, 0.0], [0.9873623847961426, 0.003186252200976014, 0.0007715133251622319, 0.0018533384427428246, 0.0014256025897338986, 0.0007793690892867744, 0.0021085806656628847, 0.0007384673808701336, 0.0006160051561892033, 0.001158514991402626, 0.0, 0.0], [0.9896944761276245, 0.002258782973513007, 0.0009895296534523368, 0.0002367494598729536, 0.001121915178373456, 0.0011288117384538054, 0.0030758611392229795, 0.00037884447374381125, 0.00019132354645989835, 0.0005093953805044293, 0.0004142820544075221, 0.0], [0.9723635911941528, 0.004590950440615416, 0.0015852975193411112, 0.0019124726532027125, 0.0006244966061785817, 0.0018245531246066093, 0.0013179403031244874, 0.013607888482511044, 0.0011498673120513558, 0.0004161187098361552, 0.0004444616206455976, 0.00016226603474933654]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9543190002441406, 0.04568096995353699, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9760633707046509, 0.015798397362232208, 0.008138187229633331, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8876117467880249, 0.07582201808691025, 0.009464657865464687, 0.027101606130599976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9154512286186218, 0.059892505407333374, 0.0022952507715672255, 0.003513018134981394, 0.018848003819584846, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7615962624549866, 0.11863219738006592, 0.002697360934689641, 0.008166995830833912, 0.06705125421285629, 0.041855938732624054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8907578587532043, 0.054074306041002274, 0.0015865449095144868, 0.002978303236886859, 0.014226507395505905, 0.0028144505340605974, 0.033562105149030685, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6089158654212952, 0.2603952884674072, 0.005949078593403101, 0.010808133520185947, 0.015386083163321018, 0.006588610354810953, 0.07731135934591293, 0.014645609073340893, 0.0, 0.0, 0.0, 0.0], [0.7495609521865845, 0.11941171437501907, 0.009106636047363281, 0.012758154422044754, 0.045534104108810425, 0.0018989879172295332, 0.04524923861026764, 0.009814969263970852, 0.006665220018476248, 0.0, 0.0, 0.0], [0.6116514205932617, 0.21727371215820312, 0.009440603666007519, 0.016372358426451683, 0.04627518728375435, 0.00413135252892971, 0.05561627075076103, 0.011717678047716618, 0.014146260917186737, 0.0133750569075346, 0.0, 0.0], [0.6217761039733887, 0.17996561527252197, 0.0028440579771995544, 0.01633515954017639, 0.041642460972070694, 0.011440346017479897, 0.06433001905679703, 0.011231842450797558, 0.014393606223165989, 0.013460932299494743, 0.022579817101359367, 0.0], [0.881875216960907, 0.055054884403944016, 0.001358453999273479, 0.001920076902024448, 0.006598630920052528, 0.0006267299759201705, 0.028544064611196518, 0.005198860075324774, 0.0025411483366042376, 0.001579635078087449, 0.001354809501208365, 0.013347460888326168]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9937054514884949, 0.006294610444456339, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9919540286064148, 0.0010996103519573808, 0.006946371868252754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9888655543327332, 0.002075851894915104, 0.0029367092065513134, 0.006121953018009663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9879482984542847, 0.0020598366390913725, 0.00010376739373896271, 0.00025276330416090786, 0.009635331109166145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9824740290641785, 0.003924972377717495, 8.744622755330056e-05, 0.00014588590420316905, 0.008101114071905613, 0.0052665406838059425, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9825745224952698, 0.00250753085128963, 0.0004886615788564086, 0.0009825051529332995, 0.00509840389713645, 0.002528656041249633, 0.005819751415401697, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9469886422157288, 0.01957651786506176, 0.0016778658609837294, 0.0011432741302996874, 0.013198739849030972, 0.001042048679664731, 0.010323476046323776, 0.006049454677850008, 0.0, 0.0, 0.0, 0.0], [0.8860181570053101, 0.00405918387696147, 0.015827739611268044, 0.020663540810346603, 0.05516548082232475, 0.0028798722196370363, 0.00438960175961256, 0.003912420477718115, 0.007083941251039505, 0.0, 0.0, 0.0], [0.92234867811203, 0.01300344243645668, 0.0018250358989462256, 0.00270048133097589, 0.03910933807492256, 0.0011782440124079585, 0.008042137138545513, 0.004565051291137934, 0.0020416169427335262, 0.005186052061617374, 0.0, 0.0], [0.8858071565628052, 0.010795127600431442, 0.00041116730426438153, 0.0003892428649123758, 0.056896358728408813, 0.009440898895263672, 0.0246450025588274, 0.0012850179336965084, 0.0003783682477660477, 0.0018447452457621694, 0.00810687243938446, 0.0], [0.9220626354217529, 0.02045910619199276, 0.003360483795404434, 0.0008936765952967107, 0.020707320421934128, 0.0007258333498612046, 0.009807894937694073, 0.005775848403573036, 0.0004629535833373666, 0.0005500074476003647, 0.0013662015553563833, 0.013827912509441376]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9878359436988831, 0.012164026498794556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9773780703544617, 0.003073219209909439, 0.0195486880838871, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9784227013587952, 0.0022285515442490578, 0.00686977244913578, 0.012478945776820183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9840379953384399, 0.006100865546613932, 0.001037996611557901, 0.0013859396567568183, 0.007437133230268955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9622312784194946, 0.015207006596028805, 0.0005987289478071034, 0.0005947991739958525, 0.006585132796317339, 0.014783062972128391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9465333223342896, 0.011390892788767815, 0.007812505587935448, 0.010350317694246769, 0.010379744693636894, 0.003556235693395138, 0.009977029636502266, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9422536492347717, 0.026867052540183067, 0.003449484473094344, 0.006400480400770903, 0.004396482370793819, 0.0018576098373159766, 0.00811205804347992, 0.006663175765424967, 0.0, 0.0, 0.0, 0.0], [0.9078310132026672, 0.0038106224965304136, 0.02022111602127552, 0.03308572247624397, 0.02231615036725998, 0.0005308191175572574, 0.0037432925309985876, 0.006762630771845579, 0.001698701293207705, 0.0, 0.0, 0.0], [0.9332455396652222, 0.013164580799639225, 0.004455127753317356, 0.0155667494982481, 0.012901381589472294, 0.0008884693379513919, 0.0073503367602825165, 0.005716616753488779, 0.0027761582750827074, 0.003934997133910656, 0.0, 0.0], [0.9687536954879761, 0.013955953530967236, 0.0007245322922244668, 0.002059472259134054, 0.005090235732495785, 0.0006814255029894412, 0.004278966225683689, 0.0016897255554795265, 0.0009166120435111225, 0.0007309963111765683, 0.0011184706818312407, 0.0], [0.8652649521827698, 0.034600503742694855, 0.029179919511079788, 0.02033570408821106, 0.0056440760381519794, 0.0011487974552437663, 0.006637183949351311, 0.026794081553816795, 0.0022731153294444084, 0.0022174748592078686, 0.0014500448014587164, 0.004454222973436117]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.989349901676178, 0.010650074109435081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9975581169128418, 0.000920298567507416, 0.0015215041348710656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9668330550193787, 0.007319653406739235, 0.007216915022581816, 0.018630364909768105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9830961227416992, 0.009530679322779179, 0.0022947753313928843, 0.001652453327551484, 0.0034259390085935593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9379034042358398, 0.019590485841035843, 0.0038066664710640907, 0.0012193832080811262, 0.0071634878404438496, 0.030316578224301338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9186261892318726, 0.004355476703494787, 0.00383597519248724, 0.00829233881086111, 0.004790028557181358, 0.017112495377659798, 0.04298742488026619, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8980032205581665, 0.019281161949038506, 0.0036580744199454784, 0.0048960293643176556, 0.004814279731363058, 0.014283161610364914, 0.024821752682328224, 0.03024241328239441, 0.0, 0.0, 0.0, 0.0], [0.9254859089851379, 0.00489537138491869, 0.007237081881612539, 0.03221755474805832, 0.004543783608824015, 0.0013308172347024083, 0.0075867073610424995, 0.012697397731244564, 0.004005265887826681, 0.0, 0.0, 0.0], [0.9168170094490051, 0.00878805574029684, 0.00393130024895072, 0.01159159280359745, 0.007944073528051376, 0.0025698193348944187, 0.023726655170321465, 0.007430804893374443, 0.004958846606314182, 0.012241818010807037, 0.0, 0.0], [0.9346416592597961, 0.01002486515790224, 0.0012376689119264483, 0.0037315262015908957, 0.005642378237098455, 0.002060284372419119, 0.03379976376891136, 0.0033203980419784784, 0.0008341717766597867, 0.002464020624756813, 0.0022432326804846525, 0.0], [0.7767418622970581, 0.016890564933419228, 0.013018216006457806, 0.02099350467324257, 0.002914787968620658, 0.01539046410471201, 0.061619240790605545, 0.04035003110766411, 0.0161056499928236, 0.007125553674995899, 0.002865440445020795, 0.02598465047776699]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9571044445037842, 0.042895589023828506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8944013118743896, 0.002292641904205084, 0.10330596566200256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8432921171188354, 0.022459646686911583, 0.07121869176626205, 0.06302947551012039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9668034911155701, 0.01506803184747696, 0.0072390553541481495, 0.007142290007323027, 0.003747072070837021, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8088358640670776, 0.12081179022789001, 0.006581529043614864, 0.009613117203116417, 0.025724943727254868, 0.02843278832733631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9003180265426636, 0.010098268277943134, 0.04642542824149132, 0.022063136100769043, 0.005719350650906563, 0.009746170602738857, 0.005629665683954954, 0.0, 0.0, 0.0, 0.0, 0.0], [0.67498379945755, 0.07246924936771393, 0.05023349076509476, 0.04135661944746971, 0.013520489446818829, 0.01915188878774643, 0.06747189164161682, 0.060812562704086304, 0.0, 0.0, 0.0, 0.0], [0.23936080932617188, 0.017085157334804535, 0.46827107667922974, 0.08229976892471313, 0.014127971604466438, 0.00818091630935669, 0.030636059120297432, 0.10266854614019394, 0.03736968711018562, 0.0, 0.0, 0.0], [0.2580433785915375, 0.016452645882964134, 0.23415914177894592, 0.07607056945562363, 0.01965557411313057, 0.011278967373073101, 0.07139422744512558, 0.14227470755577087, 0.06406068056821823, 0.10661006718873978, 0.0, 0.0], [0.3067180812358856, 0.021915923804044724, 0.018273448571562767, 0.045844946056604385, 0.028904885053634644, 0.0272726621478796, 0.14117932319641113, 0.09534874558448792, 0.05842772498726845, 0.22343380749225616, 0.03268053010106087, 0.0], [0.7636430859565735, 0.016906265169382095, 0.0363956019282341, 0.034021392464637756, 0.0025655122008174658, 0.004900640342384577, 0.008197270333766937, 0.0928981602191925, 0.020430760458111763, 0.015800587832927704, 0.0025342702865600586, 0.0017063904087990522]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9551740884780884, 0.044825952500104904, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9905253052711487, 0.004271515645086765, 0.005203140899538994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8798518776893616, 0.027674585580825806, 0.061286989599466324, 0.031186547130346298, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7317190170288086, 0.0273505300283432, 0.11023665219545364, 0.10514233261346817, 0.025551535189151764, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9752873182296753, 0.00552776362746954, 0.005123698618263006, 0.0025828224606812, 0.007563234306871891, 0.003915078938007355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28885433077812195, 0.034611962735652924, 0.1357785165309906, 0.19892969727516174, 0.2229047417640686, 0.07223805785179138, 0.046682681888341904, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9209998250007629, 0.02510426752269268, 0.012525989674031734, 0.008671090006828308, 0.005500831175595522, 0.005292529705911875, 0.005513777956366539, 0.016391796991229057, 0.0, 0.0, 0.0, 0.0], [0.5864962935447693, 0.03225439041852951, 0.013528263196349144, 0.011383142322301865, 0.012345453724265099, 0.02692260406911373, 0.008988226763904095, 0.27134963870048523, 0.03673204407095909, 0.0, 0.0, 0.0], [0.7769827246665955, 0.019471561536192894, 0.010438368655741215, 0.01267638523131609, 0.012700083665549755, 0.015101400204002857, 0.007936738431453705, 0.07596447318792343, 0.04970310255885124, 0.019025161862373352, 0.0, 0.0], [0.982941210269928, 0.0024782021064311266, 0.0009888781933113933, 0.00043851794907823205, 0.0012743716361001134, 0.0010852544801309705, 0.0005498375394381583, 0.0029768033418804407, 0.002589152893051505, 0.004275110550224781, 0.0004027065297123045, 0.0], [0.19224433600902557, 0.028531476855278015, 0.07470971345901489, 0.06544695794582367, 0.03079812601208687, 0.026473935693502426, 0.040211599320173264, 0.16838237643241882, 0.03392409905791283, 0.21150092780590057, 0.0982368215918541, 0.02953965775668621]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9901427030563354, 0.009857220575213432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9956203103065491, 0.0029288853984326124, 0.001450871815904975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9871225357055664, 0.005410744342952967, 0.004569453652948141, 0.0028973049484193325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9829382300376892, 0.009795066900551319, 0.00048440656973980367, 0.0006644156528636813, 0.006117945536971092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9196339845657349, 0.02348903939127922, 0.0021676779724657536, 0.0033911550417542458, 0.026565710082650185, 0.024752343073487282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9727079272270203, 0.004823583643883467, 0.0014042456168681383, 0.0006801914423704147, 0.006205730605870485, 0.0008613868267275393, 0.013316919095814228, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8963191509246826, 0.021091049537062645, 0.002004441572353244, 0.0011293439893051982, 0.007470950484275818, 0.0013838437153026462, 0.06775467097759247, 0.002846572082489729, 0.0, 0.0, 0.0, 0.0], [0.9230843186378479, 0.006116335280239582, 0.006697113625705242, 0.0028287365566939116, 0.025952864438295364, 0.0009725744021125138, 0.026676738634705544, 0.005275245755910873, 0.002396050374954939, 0.0, 0.0, 0.0], [0.9133254885673523, 0.013240735046565533, 0.004918181337416172, 0.0033992212265729904, 0.018980111926794052, 0.0011770091950893402, 0.026475992053747177, 0.008248453959822655, 0.004303020425140858, 0.005931802559643984, 0.0, 0.0], [0.8944787979125977, 0.014529558829963207, 0.0029275636188685894, 0.0027920131105929613, 0.019198717549443245, 0.0009078379953280091, 0.03148907423019409, 0.0062180617824196815, 0.0031336485408246517, 0.004609462805092335, 0.019715191796422005, 0.0], [0.9134261608123779, 0.0210827998816967, 0.0016301231225952506, 0.0008393732132390141, 0.013307520188391209, 0.0008196980925276875, 0.03129567950963974, 0.003983227536082268, 0.0007332347449846566, 0.0009803202701732516, 0.0007437948952428997, 0.011158030480146408]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9320766925811768, 0.06792329996824265, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9994696974754333, 5.5106116633396596e-05, 0.00047510312288068235, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9155941009521484, 0.008786618709564209, 0.025426244363188744, 0.050193123519420624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9127328395843506, 0.012330151163041592, 0.004597332328557968, 0.0057458956725895405, 0.06459380686283112, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7912782430648804, 0.031697917729616165, 0.0024411873891949654, 0.006056800484657288, 0.08301986008882523, 0.0855059027671814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.85847407579422, 0.007273135706782341, 0.008292770013213158, 0.009355093352496624, 0.0641251653432846, 0.013391328044235706, 0.03908843174576759, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7664589285850525, 0.09918423742055893, 0.010723061859607697, 0.014917406253516674, 0.011985152028501034, 0.019865790382027626, 0.03505229949951172, 0.04181312769651413, 0.0, 0.0, 0.0, 0.0], [0.7487624883651733, 0.01648177020251751, 0.06857548654079437, 0.04779211804270744, 0.055606648325920105, 0.006192673929035664, 0.008028166368603706, 0.029528312385082245, 0.019032347947359085, 0.0, 0.0, 0.0], [0.6601600050926208, 0.0911974310874939, 0.028807159513235092, 0.030070796608924866, 0.05450253561139107, 0.009085063822567463, 0.02303803339600563, 0.027810310944914818, 0.028043802827596664, 0.047284796833992004, 0.0, 0.0], [0.7121720314025879, 0.03617529571056366, 0.006967268884181976, 0.01054224744439125, 0.012537460774183273, 0.007182869594544172, 0.015330012887716293, 0.01200556568801403, 0.008038940839469433, 0.02444978803396225, 0.15459847450256348, 0.0], [0.8188938498497009, 0.019866419956088066, 0.019717523828148842, 0.013817278668284416, 0.0056783463805913925, 0.003927802667021751, 0.00482503417879343, 0.03605891764163971, 0.021680818870663643, 0.020483946427702904, 0.00700108427554369, 0.028048906475305557]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9762955904006958, 0.02370440773665905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9964537620544434, 0.0009455238468945026, 0.0026007313281297684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9927160143852234, 0.0024936378467828035, 0.0016946479445323348, 0.0030957029666751623, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9921626448631287, 0.0010757981799542904, 0.0006514668348245323, 0.0006968984962441027, 0.005413220264017582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9647658467292786, 0.013677123934030533, 0.0026826574467122555, 0.0006928975344635546, 0.006404805928468704, 0.011776579543948174, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.985031008720398, 0.0022271042689681053, 0.00047792261466383934, 0.000882403168361634, 0.002198045840486884, 0.003196959150955081, 0.00598653219640255, 0.0, 0.0, 0.0, 0.0, 0.0], [0.970314085483551, 0.003203297732397914, 0.0017244055634364486, 0.0016517029143869877, 0.001970436656847596, 0.0072316317819058895, 0.003233143128454685, 0.010671241208910942, 0.0, 0.0, 0.0, 0.0], [0.96915203332901, 0.006529571954160929, 0.00971532054245472, 0.0029257272835820913, 0.004156057257205248, 0.0005843357648700476, 0.0011075569782406092, 0.002120258053764701, 0.003709207521751523, 0.0, 0.0, 0.0], [0.9632844924926758, 0.004359650891274214, 0.003775648307055235, 0.0025867570657283068, 0.006655755452811718, 0.0008473451598547399, 0.0023436436895281076, 0.005148527678102255, 0.0032596862874925137, 0.007738443557173014, 0.0, 0.0], [0.9750282764434814, 0.004368262365460396, 0.00177171197719872, 0.0005670529790222645, 0.006481274496763945, 0.0019257529638707638, 0.0026836402248591185, 0.001628961763344705, 0.0010947214905172586, 0.0020724881906062365, 0.0023779806215316057, 0.0], [0.953087568283081, 0.003069176571443677, 0.0003648049896582961, 0.001350541366264224, 0.0023573769722133875, 0.0015244920505210757, 0.0029395611491054296, 0.028329938650131226, 0.002083954168483615, 0.0013282963773235679, 0.0012913646642118692, 0.002273001242429018]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.959121584892273, 0.04087836295366287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9849779605865479, 0.010535422712564468, 0.004486729856580496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5740022659301758, 0.1436295360326767, 0.2543749213218689, 0.02799323759973049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7036653161048889, 0.042057104408741, 0.1522970199584961, 0.04739714786410332, 0.05458341911435127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9137982130050659, 0.011100233532488346, 0.016608767211437225, 0.00531514547765255, 0.0399610809981823, 0.013216590508818626, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3870084583759308, 0.03939186409115791, 0.031520236283540726, 0.01530604250729084, 0.2876778542995453, 0.17320218682289124, 0.06589341163635254, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3463834524154663, 0.0825919434428215, 0.16086871922016144, 0.14361286163330078, 0.10941073298454285, 0.017630290240049362, 0.017445171251893044, 0.12205677479505539, 0.0, 0.0, 0.0, 0.0], [0.7978115081787109, 0.03978286683559418, 0.029705794528126717, 0.01514966506510973, 0.012279201298952103, 0.017154635861516, 0.006976852659136057, 0.02428155019879341, 0.05685797333717346, 0.0, 0.0, 0.0], [0.595302164554596, 0.05352839455008507, 0.025338830426335335, 0.012450039386749268, 0.026830822229385376, 0.016402188688516617, 0.01801624707877636, 0.07190891355276108, 0.08011601120233536, 0.100106380879879, 0.0, 0.0], [0.3920123279094696, 0.012365564703941345, 0.007651458494365215, 0.00846302229911089, 0.018986454233527184, 0.011205814778804779, 0.0065323119051754475, 0.024048805236816406, 0.03920116275548935, 0.46189066767692566, 0.017642419785261154, 0.0], [0.4353075325489044, 0.0286267027258873, 0.010547078214585781, 0.005133218131959438, 0.03686060756444931, 0.020660843700170517, 0.11193608492612839, 0.03195985406637192, 0.029412351548671722, 0.06244899332523346, 0.07893283665180206, 0.14817386865615845]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.988471508026123, 0.01152846496552229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.997025191783905, 0.0010684425942599773, 0.0019063932122662663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.981872022151947, 0.006105675827711821, 0.002922933781519532, 0.009099304676055908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9868950843811035, 0.005766425747424364, 0.0009384111617691815, 0.002404608530923724, 0.003995539620518684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9269776344299316, 0.01321770902723074, 0.001213951501995325, 0.0015244808746501803, 0.011232753284275532, 0.045833390206098557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9824954271316528, 0.006927679292857647, 0.0010620810789987445, 0.0016974241007119417, 0.0037920065224170685, 0.0016877301968634129, 0.002337635960429907, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9633212089538574, 0.020637286826968193, 0.0020873781759291887, 0.0023505007848143578, 0.003917817026376724, 0.0010250085033476353, 0.004618571139872074, 0.002042324747890234, 0.0, 0.0, 0.0, 0.0], [0.9020165801048279, 0.010630141943693161, 0.008206064812839031, 0.02337317354977131, 0.02892986871302128, 0.0018138368614017963, 0.0055389138869941235, 0.012290393002331257, 0.007201052270829678, 0.0, 0.0, 0.0], [0.9389233589172363, 0.0250250156968832, 0.004534520674496889, 0.005400227848440409, 0.01658812165260315, 0.0005467529408633709, 0.0026901988312602043, 0.002411264693364501, 0.0020533695351332426, 0.0018271930748596787, 0.0, 0.0], [0.9356243014335632, 0.028681321069598198, 0.0019170657033100724, 0.0031472109258174896, 0.011792337521910667, 0.0014211094239726663, 0.006488801911473274, 0.0016711412463337183, 0.001289163832552731, 0.0016126185655593872, 0.006354885641485453, 0.0], [0.9441710710525513, 0.019937848672270775, 0.002020207466557622, 0.004517619032412767, 0.0046108015812933445, 0.0008675424032844603, 0.004476460162550211, 0.006584775634109974, 0.0028358646668493748, 0.0022599396761506796, 0.0017639427678659558, 0.005953977350145578]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9658048152923584, 0.03419516235589981, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9977747797966003, 0.0005930069019086659, 0.0016321236034855247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9685519337654114, 0.006309279706329107, 0.016255460679531097, 0.00888338964432478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6565639972686768, 0.001117004663683474, 0.0019855056889355183, 0.000657268741633743, 0.3396761417388916, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30962279438972473, 0.0007707318291068077, 0.0006253982428461313, 0.000492502877023071, 0.6372036337852478, 0.05128491297364235, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9483788013458252, 0.0004553207545541227, 0.0018248872365802526, 0.000591388379689306, 0.0380113422870636, 0.009637987241148949, 0.0011002890532836318, 0.0, 0.0, 0.0, 0.0, 0.0], [0.926287829875946, 0.02140622027218342, 0.004105891101062298, 0.004571663681417704, 0.010048470459878445, 0.0017752688145264983, 0.00917286891490221, 0.022631650790572166, 0.0, 0.0, 0.0, 0.0], [0.8606972098350525, 0.010049181990325451, 0.006400591228157282, 0.0031424753833562136, 0.06852227449417114, 0.0021534098777920008, 0.0029035040643066168, 0.03991401940584183, 0.0062174308113753796, 0.0, 0.0, 0.0], [0.7098503708839417, 0.04013543203473091, 0.00951176043599844, 0.00814045313745737, 0.01828039065003395, 0.0011987186735495925, 0.0018468399066478014, 0.029115771874785423, 0.13416148722171783, 0.04775881767272949, 0.0, 0.0], [0.5287734270095825, 0.05865494906902313, 0.007050220854580402, 0.02376426011323929, 0.030278267338871956, 0.0034825585316866636, 0.024989375844597816, 0.013268956914544106, 0.09461455047130585, 0.1843184530735016, 0.03080502711236477, 0.0], [0.9616473913192749, 0.0035129785537719727, 0.0022289776243269444, 0.0010204487480223179, 0.000796067004557699, 0.00025211370666511357, 0.0005352248554117978, 0.0038260589353740215, 0.009781530126929283, 0.011055126786231995, 0.002612531650811434, 0.002731556538492441]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7631521821022034, 0.23684778809547424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8140329718589783, 0.030582554638385773, 0.15538454055786133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3488771915435791, 0.3219284415245056, 0.046803493052721024, 0.2823909521102905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3251635432243347, 0.2207874059677124, 0.08190147578716278, 0.27009910345077515, 0.10204846411943436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4466016888618469, 0.211994469165802, 0.028147855773568153, 0.10988286882638931, 0.098207987844944, 0.10516513884067535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1506308615207672, 0.14232632517814636, 0.10152330994606018, 0.43014177680015564, 0.04141467437148094, 0.08830274641513824, 0.04566023498773575, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1392718404531479, 0.25003504753112793, 0.044028449803590775, 0.15102435648441315, 0.058656368404626846, 0.06717752665281296, 0.07385163009166718, 0.2159547060728073, 0.0, 0.0, 0.0, 0.0], [0.1720038503408432, 0.2029508799314499, 0.03353683650493622, 0.22065509855747223, 0.0381474569439888, 0.06568533182144165, 0.06879177689552307, 0.0990094542503357, 0.09921924769878387, 0.0, 0.0, 0.0], [0.15818248689174652, 0.1752844899892807, 0.04004589468240738, 0.1565445363521576, 0.03765828162431717, 0.05553760752081871, 0.05935663357377052, 0.09825722873210907, 0.08481957018375397, 0.13431322574615479, 0.0, 0.0], [0.14610618352890015, 0.18775931000709534, 0.025839736685156822, 0.14120346307754517, 0.032666079699993134, 0.04899565130472183, 0.03380166366696358, 0.08787865936756134, 0.07605503499507904, 0.17131344974040985, 0.048380762338638306, 0.0], [0.06531724333763123, 0.13758766651153564, 0.05666935071349144, 0.19564689695835114, 0.015892401337623596, 0.029399536550045013, 0.03075420670211315, 0.12380705773830414, 0.1491192728281021, 0.12851089239120483, 0.029563218355178833, 0.037732239812612534]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9908686876296997, 0.009131278842687607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.988663375377655, 0.0035724572371691465, 0.007764122914522886, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9552818536758423, 0.007662677671760321, 0.01920069195330143, 0.017854763194918633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9163065552711487, 0.022349607199430466, 0.01766827329993248, 0.029256893321871758, 0.014418699778616428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8694319128990173, 0.04450789466500282, 0.011516469530761242, 0.011525677517056465, 0.018964022397994995, 0.044054023921489716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9018348455429077, 0.013366020284593105, 0.020030176267027855, 0.03593089431524277, 0.010775761678814888, 0.008878192864358425, 0.009184080176055431, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7933292388916016, 0.03312031552195549, 0.012548861093819141, 0.050698939710855484, 0.019209016114473343, 0.02443666011095047, 0.012027139775454998, 0.05462976545095444, 0.0, 0.0, 0.0, 0.0], [0.9163272976875305, 0.004780156072229147, 0.013954690657556057, 0.018498217687010765, 0.01568741351366043, 0.002299074549227953, 0.0025913401041179895, 0.009906196966767311, 0.015955712646245956, 0.0, 0.0, 0.0], [0.7829375267028809, 0.01681559905409813, 0.010005817748606205, 0.03477846086025238, 0.02751985937356949, 0.0068509760312736034, 0.007235431112349033, 0.03473849222064018, 0.037409041076898575, 0.04170888662338257, 0.0, 0.0], [0.7540016770362854, 0.02415350265800953, 0.00832295510917902, 0.0363582968711853, 0.036991413682699203, 0.009868509136140347, 0.017644289880990982, 0.018432768061757088, 0.0177479051053524, 0.06257428973913193, 0.01390448771417141, 0.0], [0.7243934273719788, 0.02370026893913746, 0.022638244554400444, 0.056327395141124725, 0.016325432807207108, 0.003698812797665596, 0.0066945552825927734, 0.01840715855360031, 0.0460696667432785, 0.05286610499024391, 0.013935822993516922, 0.014942971989512444]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9717192053794861, 0.02828083001077175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9730884432792664, 0.007797665428370237, 0.019113847985863686, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9720280766487122, 0.016658803448081017, 0.005759866442531347, 0.005553212482482195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9517654180526733, 0.028112661093473434, 0.006765646860003471, 0.003212693613022566, 0.010143574327230453, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9334547519683838, 0.031418345868587494, 0.0031440751627087593, 0.0014471388421952724, 0.011637801304459572, 0.01889781653881073, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9432334303855896, 0.026043718680739403, 0.00664885388687253, 0.004433799535036087, 0.012353635393083096, 0.0013085742248222232, 0.0059779370203614235, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8821426630020142, 0.06355717778205872, 0.009088433347642422, 0.0044200546108186245, 0.007929269224405289, 0.005556000396609306, 0.011393939144909382, 0.015912454575300217, 0.0, 0.0, 0.0, 0.0], [0.8985357880592346, 0.0139147425070405, 0.016352234408259392, 0.014775230549275875, 0.03895158693194389, 0.0007104990072548389, 0.00434163399040699, 0.00693084578961134, 0.00548744248226285, 0.0, 0.0, 0.0], [0.8466895222663879, 0.05333248898386955, 0.00943991169333458, 0.010271945036947727, 0.0404677614569664, 0.0022221163380891085, 0.008404025807976723, 0.009704247117042542, 0.00691349059343338, 0.012554474174976349, 0.0, 0.0], [0.8651696443557739, 0.05246953293681145, 0.004206615500152111, 0.007429338991641998, 0.031007522717118263, 0.0038831511046737432, 0.0061407252214848995, 0.0067539638839662075, 0.004666269291192293, 0.012804986909031868, 0.005468238145112991, 0.0], [0.8807172179222107, 0.04973120242357254, 0.013478687033057213, 0.006705752573907375, 0.0121303154155612, 0.0007657504174858332, 0.008701514452695847, 0.013406415469944477, 0.0018641004571691155, 0.001981607172638178, 0.002217133529484272, 0.008300403133034706]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9312466979026794, 0.06875330209732056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9917951226234436, 0.004469121340662241, 0.0037357578985393047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7030307650566101, 0.19157391786575317, 0.023570094257593155, 0.08182527124881744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6711723208427429, 0.13334134221076965, 0.018017306923866272, 0.10341523587703705, 0.07405388355255127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4527410566806793, 0.06320580095052719, 0.01493831817060709, 0.09157432615756989, 0.23283158242702484, 0.14470890164375305, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8203462958335876, 0.0536433607339859, 0.012782424688339233, 0.027360081672668457, 0.043974507600069046, 0.02293824404478073, 0.01895497366786003, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3918808400630951, 0.17046700417995453, 0.005214526783674955, 0.03173243626952171, 0.02827325090765953, 0.029436856508255005, 0.19618608057498932, 0.14680902659893036, 0.0, 0.0, 0.0, 0.0], [0.4720349609851837, 0.07286762446165085, 0.019576475024223328, 0.05108219385147095, 0.04173942282795906, 0.00986722856760025, 0.21260416507720947, 0.06521333009004593, 0.05501453951001167, 0.0, 0.0, 0.0], [0.25927016139030457, 0.08728151023387909, 0.012478123418986797, 0.037995804101228714, 0.044688642024993896, 0.014521932229399681, 0.21757389605045319, 0.09061294794082642, 0.136713445186615, 0.09886349737644196, 0.0, 0.0], [0.2699909508228302, 0.09389787912368774, 0.007607426028698683, 0.024089427664875984, 0.030684567987918854, 0.008005748502910137, 0.11110535264015198, 0.041127122938632965, 0.07558272033929825, 0.2726375460624695, 0.06527116149663925, 0.0], [0.5813767313957214, 0.0267273411154747, 0.027138885110616684, 0.024491548538208008, 0.014174449257552624, 0.009728421457111835, 0.055766209959983826, 0.04522695392370224, 0.07563697546720505, 0.06725243479013443, 0.0553763210773468, 0.017103800550103188]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9433999061584473, 0.05660010501742363, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9803316593170166, 0.00934229139238596, 0.010326019488275051, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8844290375709534, 0.06136809661984444, 0.013398764654994011, 0.040804099291563034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9025850892066956, 0.026053162291646004, 0.007853358052670956, 0.02101930044591427, 0.042489033192396164, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7650192975997925, 0.04322713613510132, 0.005216097459197044, 0.024445662274956703, 0.11070672422647476, 0.0513850599527359, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8097188472747803, 0.02860715426504612, 0.023732800036668777, 0.0387372262775898, 0.04244602844119072, 0.03776996210217476, 0.018987987190485, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8284680843353271, 0.043347664177417755, 0.011651243083178997, 0.028351496905088425, 0.023011645302176476, 0.012510914355516434, 0.013447248376905918, 0.039211735129356384, 0.0, 0.0, 0.0, 0.0], [0.7604453563690186, 0.04346691444516182, 0.01948135532438755, 0.044830285012722015, 0.031897276639938354, 0.004119371995329857, 0.005922769196331501, 0.03711433336138725, 0.052722327411174774, 0.0, 0.0, 0.0], [0.6848206520080566, 0.043154019862413406, 0.011953242123126984, 0.045487433671951294, 0.049701590090990067, 0.0055799344554543495, 0.009332769550383091, 0.048327237367630005, 0.039242733269929886, 0.06240032613277435, 0.0, 0.0], [0.6196363568305969, 0.05217425525188446, 0.00932135246694088, 0.04323258250951767, 0.0663069561123848, 0.003830721601843834, 0.017899109050631523, 0.03607935458421707, 0.043535266071558, 0.08041983097791672, 0.02756427600979805, 0.0], [0.7398905754089355, 0.02465205267071724, 0.02178107760846615, 0.03567354008555412, 0.010522515513002872, 0.010300634428858757, 0.012830874882638454, 0.02410556748509407, 0.04927583038806915, 0.02837524004280567, 0.012745547108352184, 0.0298465508967638]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9880727529525757, 0.01192731224000454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.990120530128479, 0.003285810584202409, 0.0065937284380197525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9532556533813477, 0.005705546122044325, 0.01034686528146267, 0.03069189563393593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9839207530021667, 0.00531038036569953, 0.0029426945839077234, 0.004126533400267363, 0.00369967776350677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629443883895874, 0.014577193185687065, 0.002306272042915225, 0.0007732174708507955, 0.00539063848555088, 0.014008298516273499, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9635095596313477, 0.0018696795450523496, 0.004724960308521986, 0.013633298687636852, 0.005736196879297495, 0.004989220295101404, 0.005537118762731552, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9340261816978455, 0.009659226052463055, 0.00865731481462717, 0.007681199349462986, 0.006555176340043545, 0.005712056998163462, 0.007059326861053705, 0.020649448037147522, 0.0, 0.0, 0.0, 0.0], [0.7728214263916016, 0.006268407683819532, 0.04741418734192848, 0.07233688235282898, 0.021551214158535004, 0.017385372892022133, 0.010918731801211834, 0.028266143053770065, 0.02303769811987877, 0.0, 0.0, 0.0], [0.9355979561805725, 0.007211175747215748, 0.006232829764485359, 0.0070176939480006695, 0.016411010175943375, 0.003761001629754901, 0.004457825794816017, 0.010987616144120693, 0.004725192207843065, 0.0035976723302155733, 0.0, 0.0], [0.9544588327407837, 0.012585029006004333, 0.0026914251502603292, 0.001327384146861732, 0.00923770759254694, 0.003514742013067007, 0.004927559290081263, 0.004640736151486635, 0.0015775499632582068, 0.0011228359071537852, 0.003916231915354729, 0.0], [0.8606278896331787, 0.01438093651086092, 0.01507637556642294, 0.02235928177833557, 0.010862301103770733, 0.0030473419465124607, 0.012242461554706097, 0.03811972215771675, 0.004564095288515091, 0.005108581855893135, 0.0025323510635644197, 0.011078589595854282]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9893938302993774, 0.010606161318719387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9708994626998901, 0.015267358161509037, 0.013833197765052319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9856972694396973, 0.005904070567339659, 0.004182007629424334, 0.004216653294861317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9771594405174255, 0.0097661716863513, 0.002248637145385146, 0.0016573690809309483, 0.009168272837996483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9576557874679565, 0.0180364940315485, 0.001116042723879218, 0.0005862171528860927, 0.016994550824165344, 0.005610897671431303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9559290409088135, 0.00627050781622529, 0.004193597007542849, 0.005173348821699619, 0.020804375410079956, 0.0024536324199289083, 0.005175478290766478, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9417771697044373, 0.02214018628001213, 0.005322692915797234, 0.002287920331582427, 0.010924342088401318, 0.0025157046038657427, 0.004953461233526468, 0.010078522376716137, 0.0, 0.0, 0.0, 0.0], [0.935946524143219, 0.006674570497125387, 0.0067216865718364716, 0.005191418342292309, 0.03162883594632149, 0.0007190458127297461, 0.004577309358865023, 0.005430606659501791, 0.0031099042389541864, 0.0, 0.0, 0.0], [0.9244223833084106, 0.015658054500818253, 0.0032357966993004084, 0.004043035674840212, 0.03791172429919243, 0.0007248964393511415, 0.004218854010105133, 0.004599036183208227, 0.002256094478070736, 0.0029300812166184187, 0.0, 0.0], [0.9195272326469421, 0.017300326377153397, 0.002005603862926364, 0.0036873698700219393, 0.03553709387779236, 0.0007947604754008353, 0.005437491927295923, 0.00510157085955143, 0.00253883539699018, 0.003410973818972707, 0.004658759105950594, 0.0], [0.9104390740394592, 0.012203020043671131, 0.004971574991941452, 0.011680545285344124, 0.02282889373600483, 0.0020120511762797832, 0.0044261072762310505, 0.0133487768471241, 0.005050294566899538, 0.004083653911948204, 0.0022165365517139435, 0.006739581935107708]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9819416999816895, 0.018058300018310547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.986459493637085, 0.002706903498619795, 0.01083359494805336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9889689683914185, 0.0031762036960572004, 0.003816769225522876, 0.004038039594888687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9015501737594604, 0.02569587342441082, 0.021397452801465988, 0.036621492356061935, 0.014734948985278606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9302195310592651, 0.04133784398436546, 0.004020133055746555, 0.004583430010825396, 0.006555558647960424, 0.013283518142998219, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8343602418899536, 0.007369708735495806, 0.06178759038448334, 0.06174975633621216, 0.01642495207488537, 0.00400007888674736, 0.014307652600109577, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8021182417869568, 0.025954868644475937, 0.021860841661691666, 0.04549696668982506, 0.011668013408780098, 0.013084321282804012, 0.018992116674780846, 0.060824595391750336, 0.0, 0.0, 0.0, 0.0], [0.9705919623374939, 0.0035884291864931583, 0.006664678920060396, 0.006897459272295237, 0.0019949553534388542, 0.00025297931279055774, 0.0014679607702419162, 0.003545687533915043, 0.004995809402316809, 0.0, 0.0, 0.0], [0.9153609871864319, 0.0133579196408391, 0.009524354711174965, 0.011611844412982464, 0.005964069161564112, 0.002119312295690179, 0.005189538933336735, 0.016112202778458595, 0.011999635957181454, 0.008760157972574234, 0.0, 0.0], [0.9195884466171265, 0.02486583963036537, 0.004983294755220413, 0.007202292326837778, 0.0057625616900622845, 0.0020088041201233864, 0.012533283792436123, 0.008078020997345448, 0.005210998002439737, 0.006089264992624521, 0.0036772992461919785, 0.0], [0.6860412955284119, 0.01988273300230503, 0.047281112521886826, 0.06445646286010742, 0.008339229971170425, 0.002807067474350333, 0.02046331949532032, 0.04536151513457298, 0.036203112453222275, 0.026228686794638634, 0.005792134907096624, 0.037143390625715256]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0004216024244669825, 0.9995784163475037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [5.939265292909113e-07, 0.0008891902398318052, 0.9991101622581482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [7.578028453281149e-05, 0.23900216817855835, 0.39322996139526367, 0.36769208312034607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001941617694683373, 0.08571329712867737, 0.07652803510427475, 0.7278258800506592, 0.10799109190702438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0032237214036285877, 0.261954128742218, 0.01650141552090645, 0.1468925178050995, 0.30523502826690674, 0.2661932110786438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00010171601752517745, 0.07237115502357483, 0.04562731459736824, 0.504921555519104, 0.130992129445076, 0.013085815124213696, 0.23290027678012848, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0007685718010179698, 0.12037555873394012, 0.14970943331718445, 0.14721404016017914, 0.04442615807056427, 0.03857608139514923, 0.13860726356506348, 0.36032289266586304, 0.0, 0.0, 0.0, 0.0], [5.712137863156386e-05, 0.1551494598388672, 0.04787701740860939, 0.37209388613700867, 0.012721465900540352, 0.016788743436336517, 0.014196151867508888, 0.1767273098230362, 0.20438891649246216, 0.0, 0.0, 0.0], [0.0007459771586582065, 0.08834618330001831, 0.06937018036842346, 0.1192619651556015, 0.011684889905154705, 0.010517597198486328, 0.011106760241091251, 0.15228271484375, 0.3856702744960785, 0.15101340413093567, 0.0, 0.0], [0.0008670933311805129, 0.11802887171506882, 0.06807549297809601, 0.17348028719425201, 0.01228597667068243, 0.02329018898308277, 0.018128324300050735, 0.04735014587640762, 0.23612016439437866, 0.2777062654495239, 0.02466718479990959, 0.0], [2.4674644009792246e-05, 0.022967418655753136, 0.005188134964555502, 0.0419052429497242, 0.010617914609611034, 0.0009088711813092232, 0.8273705840110779, 0.01863281801342964, 0.011447254568338394, 0.012787116691470146, 0.001715334365144372, 0.046434614807367325]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954795837402344, 0.004520336631685495, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9754294157028198, 0.007082863245159388, 0.017487777397036552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.977192223072052, 0.0009520857711322606, 0.009156717918813229, 0.01269899308681488, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9903123378753662, 0.0018048776546493173, 0.0015391542110592127, 0.0010654889047145844, 0.005278115160763264, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9830564856529236, 0.004748734179884195, 0.0018226341344416142, 0.0009311830508522689, 0.006012840196490288, 0.0034280819818377495, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9832156300544739, 0.0019143893150612712, 0.004360282327979803, 0.004538257140666246, 0.002060305094346404, 0.0010143517283722758, 0.002896759659051895, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9605527520179749, 0.008186204358935356, 0.004707722924649715, 0.002240509958937764, 0.008306951262056828, 0.0015144703211262822, 0.0076276278123259544, 0.006863788235932589, 0.0, 0.0, 0.0, 0.0], [0.9187482595443726, 0.0011388043640181422, 0.008163578808307648, 0.017176859080791473, 0.022984186187386513, 0.0022011790424585342, 0.009803850203752518, 0.013505992479622364, 0.006277231965214014, 0.0, 0.0, 0.0], [0.9603893160820007, 0.0017607337795197964, 0.0023668804205954075, 0.006790065672248602, 0.012679792940616608, 0.0004223766445647925, 0.003303512930870056, 0.005148803815245628, 0.002339232014492154, 0.004799194633960724, 0.0, 0.0], [0.9678277373313904, 0.0028586050029844046, 0.0009684078977443278, 0.0027497189585119486, 0.007021124474704266, 0.0005153432139195502, 0.00657670060172677, 0.0027436797972768545, 0.0009197646286338568, 0.001676976215094328, 0.006142015103250742, 0.0], [0.9402444362640381, 0.007784521207213402, 0.005595549009740353, 0.004178989678621292, 0.009596201591193676, 0.000526236486621201, 0.004312952049076557, 0.015562310814857483, 0.002860755892470479, 0.0016276492970064282, 0.0008162400336004794, 0.006894135847687721]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9256590008735657, 0.07434093952178955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9915567636489868, 0.0011682562762871385, 0.00727497786283493, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6759268641471863, 0.1271727979183197, 0.11016874760389328, 0.08673159033060074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9256762862205505, 0.021833375096321106, 0.020789649337530136, 0.0039322334341704845, 0.027768341824412346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6604278683662415, 0.05409414693713188, 0.006903978995978832, 0.0022970647551119328, 0.2134467363357544, 0.06283026188611984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5504401326179504, 0.0077161723747849464, 0.04905043542385101, 0.01642593741416931, 0.27882447838783264, 0.04367481917142868, 0.05386805161833763, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6463593244552612, 0.07949896901845932, 0.0331137515604496, 0.013216925784945488, 0.01649431325495243, 0.013463841751217842, 0.016113486140966415, 0.18173940479755402, 0.0, 0.0, 0.0, 0.0], [0.5879252552986145, 0.07636091113090515, 0.037768639624118805, 0.037474583834409714, 0.034805551171302795, 0.004640959668904543, 0.01793419010937214, 0.08935602754354477, 0.11373382061719894, 0.0, 0.0, 0.0], [0.5903455018997192, 0.11167298257350922, 0.029884759336709976, 0.021299226209521294, 0.03282301872968674, 0.0051805488765239716, 0.019491389393806458, 0.06234202906489372, 0.05531122535467148, 0.07164929807186127, 0.0, 0.0], [0.3857288360595703, 0.08567506074905396, 0.011148869059979916, 0.01659177802503109, 0.024626536294817924, 0.0033786206040531397, 0.025753535330295563, 0.02555263787508011, 0.03990224748849869, 0.266242116689682, 0.11539971083402634, 0.0], [0.7153027057647705, 0.05001456290483475, 0.04816705733537674, 0.014798203483223915, 0.01342229638248682, 0.006257118657231331, 0.005401151720434427, 0.07443388551473618, 0.01671433635056019, 0.02688998356461525, 0.010476553812623024, 0.01812214031815529]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9609508514404297, 0.0390491858124733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9997989535331726, 0.00020110233162995428, 1.261105342332744e-14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8598716855049133, 0.0817338302731514, 5.9245110605843365e-05, 0.058335255831480026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8931370973587036, 0.011659977957606316, 3.307328006485477e-05, 0.014301215298473835, 0.08086859434843063, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6514581441879272, 0.012483704835176468, 0.00015726311539765447, 0.005182427819818258, 0.23798954486846924, 0.09272894263267517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7686364650726318, 0.009588642045855522, 4.322515394505899e-07, 0.01519712246954441, 0.12356163561344147, 0.049801040440797806, 0.03321462497115135, 0.0, 0.0, 0.0, 0.0, 0.0], [0.731519341468811, 0.048289164900779724, 0.0011978417169302702, 0.012012754566967487, 0.029558252543210983, 0.009174966253340244, 0.04024762660264969, 0.12799999117851257, 0.0, 0.0, 0.0, 0.0], [0.5644447803497314, 0.023072123527526855, 1.5924008039291948e-05, 0.04031866416335106, 0.06403933465480804, 0.009077566675841808, 0.13588236272335052, 0.06146431714296341, 0.10168498009443283, 0.0, 0.0, 0.0], [0.47045838832855225, 0.026133017614483833, 1.0419045111120795e-06, 0.024696240201592445, 0.06056400015950203, 0.020291516557335854, 0.11406996846199036, 0.04599842429161072, 0.14671114087104797, 0.0910763144493103, 0.0, 0.0], [0.46529993414878845, 0.015757448971271515, 1.8100247416441562e-06, 0.008106976747512817, 0.041194282472133636, 0.030415568500757217, 0.13378606736660004, 0.018252216279506683, 0.055582381784915924, 0.2015642374753952, 0.030038954690098763, 0.0], [0.5738769769668579, 0.025756703689694405, 1.928545998453046e-06, 0.01442476361989975, 0.02390991523861885, 0.009189783595502377, 0.05263228341937065, 0.16798675060272217, 0.024565694853663445, 0.029963573440909386, 0.03171833977103233, 0.04597330093383789]]]], "left_text": ["The", " animal", " didn", "'t", " cross", " the", " street", " because", " it", " was", " too", " tired"], "right_text": ["The", " animal", " didn", "'t", " cross", " the", " street", " because", " it", " was", " too", " tired"]}], "default_filter": "0", "display_mode": "dark", "root_div_id": "bertviz-6d5f6c190c0d473a97e9e93b0a3cd83d", "include_layers": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "include_heads": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "total_heads": 12} is a template marker that is replaced by actual params.
        const config = {};

        const MIN_X = 0;
        const MIN_Y = 0;
        const DIV_WIDTH = 970;
        const THUMBNAIL_PADDING = 5;
        const DETAIL_WIDTH = 300;
        const DETAIL_ATTENTION_WIDTH = 140;
        const DETAIL_BOX_WIDTH = 80;
        const DETAIL_BOX_HEIGHT = 18;
        const DETAIL_PADDING = 15;
        const ATTN_PADDING = 0;
        const DETAIL_HEADING_HEIGHT = 25;
        const HEADING_TEXT_SIZE = 15;
        const HEADING_PADDING = 5;
        const TEXT_SIZE = 13;
        const TEXT_PADDING = 5;
        const LAYER_COLORS = d3.schemeCategory10;
        const PALETTE = {
            'light': {
                'text': 'black',
                'background': 'white',
                'highlight': '#F5F5F5'
            },
            'dark': {
                'text': '#ccc',
                'background': 'black',
                'highlight': '#222'
            }
        }

        function render() {

            // Set global state variables

            var attData = config.attention[config.filter];
            config.leftText = attData.left_text;
            config.rightText = attData.right_text;
            config.attn = attData.attn;
            config.numLayers = config.attn.length;
            config.numHeads = config.attn[0].length;
            config.thumbnailBoxHeight = 7 * (12 / config.totalHeads);
            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;
            config.thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;
            config.thumbnailWidth = (DIV_WIDTH - axisSize) / config.totalHeads;
            config.detailHeight = Math.max(config.leftText.length, config.rightText.length) * DETAIL_BOX_HEIGHT + 2 * DETAIL_PADDING + DETAIL_HEADING_HEIGHT;
            config.divHeight = Math.max(config.numLayers * config.thumbnailHeight + axisSize, config.detailHeight);

            const vis = $(`#${config.rootDivId} #vis`)
            vis.empty();
            vis.attr("height", config.divHeight);
            config.svg = d3.select(`#${config.rootDivId} #vis`)
                .append('svg')
                .attr("width", DIV_WIDTH)
                .attr("height", config.divHeight)
                .attr("fill", getBackgroundColor());

            renderAxisLabels();

            var i;
            var j;
            for (i = 0; i < config.numLayers; i++) {
                for (j = 0; j < config.numHeads; j++) {
                    renderThumbnail(i, j);
                }
            }
        }

        function renderAxisLabels() {
            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;
            const tableWidth = config.thumbnailWidth * config.heads.length;
            config.svg.append("text")
                .text("Heads")
                .attr("fill", "black")
                .attr("font-weight", "bold")
                .attr("font-size", HEADING_TEXT_SIZE + "px")
                .attr("x", axisSize + tableWidth / 2)
                .attr("text-anchor", "middle")
                .attr("y", 0)
                .attr("dy", HEADING_TEXT_SIZE);
            for (let i = 0; i < config.numHeads; i++) {
                config.svg.append("text")
                    .text(config.heads[i])
                    .attr("fill", "black")
                    .attr("font-size", TEXT_SIZE + "px")
                    .attr("x", axisSize + (i + .5) * config.thumbnailWidth)
                    .attr("text-anchor", "middle")
                    .attr("y", HEADING_TEXT_SIZE + HEADING_PADDING)
                    .attr("dy", TEXT_SIZE);
            }
            let x = 0;
            let y = axisSize + config.thumbnailHeight * config.layers.length / 2;
            console.log("x", x, y)
            config.svg.append("text")
                .text("Layers")
                .attr("fill", "black")
                .attr("font-weight", "bold")
                .attr("transform", "rotate(270, " + x  + ", " + y + ")")
                .attr("font-size", HEADING_TEXT_SIZE + "px")
                .attr("x", x)
                .attr("text-anchor", "middle")
                .attr("y", y)
                .attr("dy", HEADING_TEXT_SIZE);
            for (let i = 0; i < config.numLayers; i++) {
                x = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE; // HACK
                y = axisSize + (i + .5) * config.thumbnailHeight;
                config.svg.append("text")
                    .text(config.layers[i])
                    .attr("fill", "black")
                    .attr("font-size", TEXT_SIZE + "px")
                    .attr("x", x)
                    .attr("text-anchor", "end")
                    .attr("y", y)
                    .attr("dy", TEXT_SIZE / 2);
            }
        }


        function renderThumbnail(layerIndex, headIndex) {
            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING
            const x = headIndex * config.thumbnailWidth + axisSize;
            const y = layerIndex * config.thumbnailHeight + axisSize;
            renderThumbnailAttn(x, y, config.attn[layerIndex][headIndex], layerIndex, headIndex);
        }

        function renderDetail(att, layerIndex, headIndex) {
            const axisSize = TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;
            var xOffset = .8 * config.thumbnailWidth;
            var maxX = DIV_WIDTH;
            var maxY = config.divHeight - 3;
            var leftPos = axisSize + headIndex * config.thumbnailWidth;
            var x = leftPos + THUMBNAIL_PADDING + xOffset;
            if (x < MIN_X) {
                x = MIN_X;
            } else if (x + DETAIL_WIDTH > maxX) {
                x = leftPos + THUMBNAIL_PADDING - DETAIL_WIDTH + 8;
            }
            var posLeftText = x;
            var posAttention = posLeftText + DETAIL_BOX_WIDTH;
            var posRightText = posAttention + DETAIL_ATTENTION_WIDTH;
            var thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;
            var yOffset = 20;
            var y = layerIndex * thumbnailHeight + THUMBNAIL_PADDING + yOffset;
            if (y < MIN_Y) {
                y = MIN_Y;
            } else if (y + config.detailHeight > maxY) {
                y = maxY - config.detailHeight;
            }
            renderDetailFrame(x, y, layerIndex);
            y = y + DETAIL_PADDING;
            renderDetailHeading(x, y, layerIndex, headIndex);
            y = y + DETAIL_HEADING_HEIGHT;
            renderDetailText(config.leftText, "leftText", posLeftText, y , layerIndex);
            renderDetailAttn(posAttention, y, att, layerIndex, headIndex);
            renderDetailText(config.rightText, "rightText", posRightText, y, layerIndex);
        }

        function renderDetailHeading(x, y, layerIndex, headIndex) {
            var fillColor = getTextColor();
            config.svg.append("text")
                .classed("detail", true)
                .text('Layer ' + config.layers[layerIndex] + ", Head " + config.heads[headIndex])
                .attr("font-size", TEXT_SIZE + "px")
                .attr("font-weight", "bold")
                .style("cursor", "default")
                .style("-webkit-user-select", "none")
                .attr("fill", fillColor)
                .attr("x", x + DETAIL_WIDTH / 2)
                .attr("text-anchor", "middle")
                .attr("y", y)
                .attr("height", DETAIL_HEADING_HEIGHT)
                .attr("width", DETAIL_WIDTH)
                .attr("dy", HEADING_TEXT_SIZE);
        }

        function renderDetailText(text, id, x, y, layerIndex) {
            var tokenContainer = config.svg.append("svg:g")
                .classed("detail", true)
                .selectAll("g")
                .data(text)
                .enter()
                .append("g");

            var fillColor = getTextColor();

            tokenContainer.append("rect")
                .classed("highlight", true)
                .attr("fill", fillColor)
                .style("opacity", 0.0)
                .attr("height", DETAIL_BOX_HEIGHT)
                .attr("width", DETAIL_BOX_WIDTH)
                .attr("x", x)
                .attr("y", function (d, i) {
                    return y + i * DETAIL_BOX_HEIGHT;
                });

            var textContainer = tokenContainer.append("text")
                .classed("token", true)
                .text(function (d) {
                    return d;
                })
                .attr("font-size", TEXT_SIZE + "px")
                .style("cursor", "default")
                .style("-webkit-user-select", "none")
                .attr("fill", fillColor)
                .attr("x", x)
                .attr("y", function (d, i) {
                    return i * DETAIL_BOX_HEIGHT + y;
                })
                .attr("height", DETAIL_BOX_HEIGHT)
                .attr("width", DETAIL_BOX_WIDTH)
                .attr("dy", TEXT_SIZE);

            if (id == "leftText") {
                textContainer.style("text-anchor", "end")
                    .attr("dx", DETAIL_BOX_WIDTH - 2);
                tokenContainer.on("mouseover", function (d, index) {
                    highlightSelection(index);
                });
                tokenContainer.on("mouseleave", function () {
                    unhighlightSelection();
                });
            }
        }

        function highlightSelection(index) {
            config.svg.select("#leftText")
                .selectAll(".highlight")
                .style("opacity", function (d, i) {
                    return i == index ? 1.0 : 0.0;
                });
            config.svg.selectAll(".attn-line-group")
                .style("opacity", function (d, i) {
                    return i == index ? 1.0 : 0.0;
                });
        }

        function unhighlightSelection() {
            config.svg.select("#leftText")
                .selectAll(".highlight")
                .style("opacity", 0.0);
            config.svg.selectAll(".attn-line-group")
                .style("opacity", 1);
        }

        function renderThumbnailAttn(x, y, att, layerIndex, headIndex) {

            var attnContainer = config.svg.append("svg:g");

            var attnBackground = attnContainer.append("rect")
                .attr("id", 'attn_background_' + layerIndex + "_" + headIndex)
                .classed("attn_background", true)
                .attr("x", x)
                .attr("y", y)
                .attr("height", config.thumbnailHeight)
                .attr("width", config.thumbnailWidth)
                .attr("stroke-width", 2)
                .attr("stroke", getLayerColor(layerIndex))
                .attr("stroke-opacity", 0)
                .attr("fill", getBackgroundColor());
            var x1 = x + THUMBNAIL_PADDING;
            var x2 = x1 + config.thumbnailWidth - 14;
            var y1 = y + THUMBNAIL_PADDING;

            attnContainer.selectAll("g")
                .data(att)
                .enter()
                .append("g") // Add group for each source token
                .attr("source-index", function (d, i) { // Save index of source token
                    return i;
                })
                .selectAll("line")
                .data(function (d) { // Loop over all target tokens
                    return d;
                })
                .enter() // When entering
                .append("line")
                .attr("x1", x1)
                .attr("y1", function (d) {
                    var sourceIndex = +this.parentNode.getAttribute("source-index");
                    return y1 + (sourceIndex + .5) * config.thumbnailBoxHeight;
                })
                .attr("x2", x2)
                .attr("y2", function (d, targetIndex) {
                    return y1 + (targetIndex + .5) * config.thumbnailBoxHeight;
                })
                .attr("stroke-width", 2.2)
                .attr("stroke", getLayerColor(layerIndex))
                .attr("stroke-opacity", function (d) {
                    return d;
                });

            var clickRegion = attnContainer.append("rect")
                .attr("x", x)
                .attr("y", y)
                .attr("height", config.thumbnailHeight)
                .attr("width", config.thumbnailWidth)
                .style("opacity", 0);

            clickRegion.on("click", function (d, index) {
                var attnBackgroundOther = config.svg.selectAll(".attn_background");
                attnBackgroundOther.attr("fill", getBackgroundColor());
                attnBackgroundOther.attr("stroke-opacity", 0);

                config.svg.selectAll(".detail").remove();
                if (config.detail_layer != layerIndex || config.detail_head != headIndex) {
                    renderDetail(att, layerIndex, headIndex);
                    config.detail_layer = layerIndex;
                    config.detail_head = headIndex;
                    attnBackground.attr("fill", getHighlightColor());
                    attnBackground.attr("stroke-opacity", .8);
                } else {
                    config.detail_layer = null;
                    config.detail_head = null;
                    attnBackground.attr("fill", getBackgroundColor());
                    attnBackground.attr("stroke-opacity", 0);
                }
            });

            clickRegion.on("mouseover", function (d) {
                d3.select(this).style("cursor", "pointer");
            });
        }

        function renderDetailFrame(x, y, layerIndex) {
            var detailFrame = config.svg.append("rect")
                .classed("detail", true)
                .attr("x", x)
                .attr("y", y)
                .attr("height", config.detailHeight)
                .attr("width", DETAIL_WIDTH)
                .style("opacity", 1)
                .attr("stroke-width", 1.5)
                .attr("stroke-opacity", 0.7)
                .attr("stroke", getLayerColor(layerIndex));
        }

        function renderDetailAttn(x, y, att, layerIndex) {
            var attnContainer = config.svg.append("svg:g")
                .classed("detail", true)
                .attr("pointer-events", "none");
            attnContainer.selectAll("g")
                .data(att)
                .enter()
                .append("g") // Add group for each source token
                .classed('attn-line-group', true)
                .attr("source-index", function (d, i) { // Save index of source token
                    return i;
                })
                .selectAll("line")
                .data(function (d) { // Loop over all target tokens
                    return d;
                })
                .enter()
                .append("line")
                .attr("x1", x + ATTN_PADDING)
                .attr("y1", function (d) {
                    var sourceIndex = +this.parentNode.getAttribute("source-index");
                    return y + (sourceIndex + .5) * DETAIL_BOX_HEIGHT;
                })
                .attr("x2", x + DETAIL_ATTENTION_WIDTH - ATTN_PADDING)
                .attr("y2", function (d, targetIndex) {
                    return y + (targetIndex + .5) * DETAIL_BOX_HEIGHT;
                })
                .attr("stroke-width", 2.2)
                .attr("stroke", getLayerColor(layerIndex))
                .attr("stroke-opacity", function (d) {
                    return d;
                });
        }

        function getLayerColor(layer) {
          return LAYER_COLORS[config.layers[layer] % 10];
        }

        function getTextColor() {
            return PALETTE[config.mode]['text']
        }

        function getBackgroundColor() {
           return PALETTE[config.mode]['background']
        }

        function getHighlightColor() {
           return PALETTE[config.mode]['highlight']
        }

        function initialize() {
            config.attention = params['attention'];
            config.filter = params['default_filter'];
            config.mode = params['display_mode'];
            config.layers = params['include_layers']
            config.heads = params['include_heads']
            config.totalHeads = params['total_heads']
            config.rootDivId = params['root_div_id'];
            $(`#${config.rootDivId} #filter`).on('change', function (e) {
                config.filter = e.currentTarget.value;
                render();
            });
        }

        initialize();
        render();

    });
</script>
</div>
</div>
</section>
</section>
<section id="positional-encoding" class="level2">
<h2 class="anchored" data-anchor-id="positional-encoding">Positional encoding</h2>
<p>We just discussed attention mechanisms which account for context between words. Another question we should ask is how do we account for the order of words in an input sentence</p>
<p>Consider the following two sentences to see why this is important:</p>
<p><code>The man ate the sandwich.</code></p>
<p><code>The sandwich ate the man.</code></p>
<p>Clearly, these are two vastly different situations even though they have the same words. The Transformer can</p>
<p>Transformers differentiate between these situations by adding a <strong>Positional encoding</strong> vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word.</p>
<div id="fig-positional-encoding" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-positional-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/positional_encoding.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;7: Positional encoding"><img src="images/positional_encoding.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-positional-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Positional encoding
</figcaption>
</figure>
</div>
<p>Image credit: <a href="https://medium.com/@xuer.chen.human/llm-study-notes-positional-encoding-0639a1002ec0" class="uri">https://medium.com/@xuer.chen.human/llm-study-notes-positional-encoding-0639a1002ec0</a></p>
<p>We set up positional encoding similarly as token embedding using the <code>nn.Embedding</code> tool. We use a simple embedding here but there are more complex positional encodings used such as sinusoidal.</p>
<p>For an explanation of different positional encodings, refer to this post: https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/</p>
<div id="46f45a52" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>vocab_size <span class="op">=</span> <span class="dv">65</span></span>
<span id="cb19-2"><a href="#cb19-2"></a>n_embd <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb19-3"><a href="#cb19-3"></a></span>
<span id="cb19-4"><a href="#cb19-4"></a>token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb19-5"><a href="#cb19-5"></a>block_size <span class="op">=</span> <span class="dv">32</span>  <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb19-6"><a href="#cb19-6"></a>position_embedding_table <span class="op">=</span> nn.Embedding(block_size, n_embd)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>You will notice the positional encoding size is <code>(block_size, n_embed)</code> because it encodes for the postion of a token within the sequence of size <code>block_size</code></p>
<p>Then, the position embedding used is simply added to the token embedding to apply positional embedding.</p>
<p>Let’s look at token embedding alone:</p>
<div id="b2e780f4" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>x <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">15</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">9</span>])</span>
<span id="cb20-2"><a href="#cb20-2"></a>x <span class="op">=</span> token_embedding_table(x)</span>
<span id="cb20-3"><a href="#cb20-3"></a><span class="bu">print</span>(x[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7221</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.9629</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.0578</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.9740</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7434</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.1139</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6926</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0296</span>,
         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6405</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6464</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4935</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7485</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9238</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4940</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4814</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3859</span>,
        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3094</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.1066</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2891</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1891</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.0440</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.7945</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4331</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3007</span>,
         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.4317</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2881</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4343</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4280</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.2469</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.4047</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3404</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.2190</span>,
         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4893</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0398</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2717</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.2400</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0029</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4251</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7330</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3551</span>,
         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1472</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.1895</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8407</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3134</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6709</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8176</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6929</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6374</span>,
         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3174</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4837</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0073</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.5924</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.8606</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.2910</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1594</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3111</span>,
        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1536</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3414</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0170</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1633</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2794</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6755</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7066</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6665</span><span style="font-weight: bold">]</span>,
       <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">SelectBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div>
</div>
<p>And token + positional embeddings:</p>
<div id="f119d570" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>x <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">15</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">9</span>])</span>
<span id="cb21-2"><a href="#cb21-2"></a>x <span class="op">=</span> position_embedding_table(x) <span class="op">+</span> token_embedding_table(x)</span>
<span id="cb21-3"><a href="#cb21-3"></a><span class="bu">print</span>(x[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4326</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6287</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8684</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3.0704</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3646</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.9826</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7582</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1918</span>,
         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0491</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.2562</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4931</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.7808</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.7206</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.0297</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.0798</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.3427</span>,
        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.7896</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1746</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0926</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0543</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.3831</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6208</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3902</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1097</span>,
         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0455</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4557</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3402</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.6717</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.8380</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.2628</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4831</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-4.6023</span>,
         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6959</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0347</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5903</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.7541</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4682</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3895</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.1526</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6272</span>,
        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8558</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8434</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1311</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.0272</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.0580</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0584</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3442</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3464</span>,
        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3444</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.3134</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.1142</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4629</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3.3503</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.0594</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.4105</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4558</span>,
        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.3366</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.9283</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.5187</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3906</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.1448</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8422</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.2692</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.7949</span><span style="font-weight: bold">]</span>,
       <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">SelectBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div>
</div>
<p>You can see a clear offset between these two embeddings.</p>
<p>During the training process, these embeddings will be learned to best encode the token and positional embeddings of the sequences.</p>
</section>
<section id="output-layers" class="level2">
<h2 class="anchored" data-anchor-id="output-layers">Output layers</h2>
<p>At the end of our Transformer model, we are left with a vector, so how do we turn this into a word?</p>
<p><img src="images/transformer-decoder-intro.png" alt="Drawing" style="width: 400px;"></p>
<p>Using a final Linear layer and a Softmax Layer. The Linear layer projects the vector produced by the stack of decoders, into a larger vector called a logits vector.</p>
<p>If our model knows 10,000 unique English words learned from its training dataset the logits vector is 10,000 cells wide – each cell corresponds to the score of a unique word.</p>
<p>The softmax layer turns those scores into probabilities. The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.</p>
<div id="fig-transformer-decoder-output-softmax" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-decoder-output-softmax-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/transformer_decoder_output_softmax.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;8: Transformer decoder output softmax"><img src="images/transformer_decoder_output_softmax.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-decoder-output-softmax-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Transformer decoder output softmax
</figcaption>
</figure>
</div>
<p>Image credit: https://jalammar.github.io/illustrated-transformer/</p>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<p>How does an LLM improve over time? We want to compare the probabilitiy distribution for each token generated by our model to the ground truths. Our model produces a probability distribution for each token. We want to compare these probability distributions to the ground truths. For example, when translating the sentence: “je suis étudiant” into “i am a student” as can be seen in the example:</p>
<div id="fig-output-target-probability-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-output-target-probability-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/output_target_probability_distributions.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;9: Output target probability distributions"><img src="images/output_target_probability_distributions.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-output-target-probability-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Output target probability distributions
</figcaption>
</figure>
</div>
<p>Image credit: https://jalammar.github.io/illustrated-transformer/</p>
<p>The model can calculate the loss between the vector it generates and the ground truth vector seen in this example. A commonly used loss function is cross entropy loss:</p>
<p><span class="math display">CE = -\sum_{x \in X} p(x) log q(x)</span></p>
<p>where p(x) represents the true distribution and q(x) represents the predicted distribution.</p>
<div id="d7ab867e" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb22-2"><a href="#cb22-2"></a></span>
<span id="cb22-3"><a href="#cb22-3"></a>logits <span class="op">=</span> torch.tensor([<span class="fl">0.5</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>])</span>
<span id="cb22-4"><a href="#cb22-4"></a>targets <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>])</span>
<span id="cb22-5"><a href="#cb22-5"></a>loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb22-6"><a href="#cb22-6"></a><span class="bu">print</span>(loss)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9119</span><span style="font-weight: bold">)</span>
</pre>
</div>
</div>
<p>Another important metric commonly used in LLMs is <strong>perplexity</strong>.</p>
<p>Intuitively, perplexity means to be surprised. We measure how much the model is surprised by seeing new data. The lower the perplexity, the better the training is.</p>
<p>Mathematically, perplexity is just the exponent of the negative cross entropy loss:</p>
<p><span class="math display">\text{perplexity} = exp(\text{CE})</span></p>
<div id="42b539a7" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>perplexity <span class="op">=</span> torch.exp(loss)</span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="bu">print</span>(perplexity)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.4891</span><span style="font-weight: bold">)</span>
</pre>
</div>
</div>
</section>
<section id="lets-train-a-mini-llm-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="lets-train-a-mini-llm-from-scratch">Let’s train a mini-LLM from scratch</h2>
<section id="set-up-hyperparameters" class="level3">
<h3 class="anchored" data-anchor-id="set-up-hyperparameters">Set up hyperparameters:</h3>
<div id="ef7a3b84" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="co"># hyperparameters</span></span>
<span id="cb24-2"><a href="#cb24-2"></a>batch_size <span class="op">=</span> <span class="dv">4</span>  <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb24-3"><a href="#cb24-3"></a>block_size <span class="op">=</span> <span class="dv">32</span>  <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb24-4"><a href="#cb24-4"></a>max_iters <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb24-5"><a href="#cb24-5"></a>eval_interval <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb24-6"><a href="#cb24-6"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb24-7"><a href="#cb24-7"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb24-8"><a href="#cb24-8"></a>eval_iters <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb24-9"><a href="#cb24-9"></a>n_embd <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb24-10"><a href="#cb24-10"></a>n_head <span class="op">=</span> <span class="dv">4</span>  <span class="co">## so head_size = 16</span></span>
<span id="cb24-11"><a href="#cb24-11"></a>n_layer <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb24-12"><a href="#cb24-12"></a>dropout <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb24-13"><a href="#cb24-13"></a><span class="co"># ------------</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="load-in-data-and-create-train-and-test-datasets" class="level3">
<h3 class="anchored" data-anchor-id="load-in-data-and-create-train-and-test-datasets">Load in data and create train and test datasets</h3>
<p>We’re going to be using the tiny Shakespeare dataset. Data is tokenized according to a simple character based tokenizer. Data is split into a train and test set so we have something to test after performing training (9:1 split).</p>
<div id="1e9c3871" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a><span class="op">!</span> [ <span class="op">!</span> <span class="op">-</span>f <span class="st">"input.txt"</span> ] <span class="op">&amp;&amp;</span> wget https:<span class="op">//</span>raw.githubusercontent.com<span class="op">/</span>argonne<span class="op">-</span>lcf<span class="op">/</span>ATPESC_MachineLearning<span class="op">/</span>refs<span class="op">/</span>heads<span class="op">/</span>master<span class="op">/</span><span class="dv">0</span><span class="er">2_intro_to_LLMs</span><span class="op">/</span>dataset<span class="op">/</span><span class="bu">input</span>.txt</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)</code></pre>
</div>
</div>
<div id="3b8def5b" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"input.txt"</span>, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb27-2"><a href="#cb27-2"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb27-3"><a href="#cb27-3"></a></span>
<span id="cb27-4"><a href="#cb27-4"></a><span class="co"># here are all the unique characters that occur in this text</span></span>
<span id="cb27-5"><a href="#cb27-5"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb27-6"><a href="#cb27-6"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb27-7"><a href="#cb27-7"></a><span class="co"># create a mapping from characters to integers</span></span>
<span id="cb27-8"><a href="#cb27-8"></a>stoi <span class="op">=</span> {ch: i <span class="cf">for</span> i, ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb27-9"><a href="#cb27-9"></a>itos <span class="op">=</span> {i: ch <span class="cf">for</span> i, ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb27-10"><a href="#cb27-10"></a>encode <span class="op">=</span> <span class="kw">lambda</span> s: [</span>
<span id="cb27-11"><a href="#cb27-11"></a>    stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s</span>
<span id="cb27-12"><a href="#cb27-12"></a>]  <span class="co"># encoder: take a string, output a list of integers</span></span>
<span id="cb27-13"><a href="#cb27-13"></a>decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">""</span>.join(</span>
<span id="cb27-14"><a href="#cb27-14"></a>    [itos[i] <span class="cf">for</span> i <span class="kw">in</span> l]</span>
<span id="cb27-15"><a href="#cb27-15"></a>)  <span class="co"># decoder: take a list of integers, output a string</span></span>
<span id="cb27-16"><a href="#cb27-16"></a></span>
<span id="cb27-17"><a href="#cb27-17"></a><span class="co"># Train and test splits</span></span>
<span id="cb27-18"><a href="#cb27-18"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb27-19"><a href="#cb27-19"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span> <span class="op">*</span> <span class="bu">len</span>(data))  <span class="co"># first 90% will be train, rest val</span></span>
<span id="cb27-20"><a href="#cb27-20"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb27-21"><a href="#cb27-21"></a>val_data <span class="op">=</span> data[n:]</span>
<span id="cb27-22"><a href="#cb27-22"></a></span>
<span id="cb27-23"><a href="#cb27-23"></a></span>
<span id="cb27-24"><a href="#cb27-24"></a><span class="co"># data loading</span></span>
<span id="cb27-25"><a href="#cb27-25"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb27-26"><a href="#cb27-26"></a>    <span class="co"># generate a small batch of data of inputs x and targets y</span></span>
<span id="cb27-27"><a href="#cb27-27"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">"train"</span> <span class="cf">else</span> val_data</span>
<span id="cb27-28"><a href="#cb27-28"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb27-29"><a href="#cb27-29"></a>    x <span class="op">=</span> torch.stack([data[i : i <span class="op">+</span> block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb27-30"><a href="#cb27-30"></a>    y <span class="op">=</span> torch.stack([data[i <span class="op">+</span> <span class="dv">1</span> : i <span class="op">+</span> block_size <span class="op">+</span> <span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb27-31"><a href="#cb27-31"></a>    x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb27-32"><a href="#cb27-32"></a>    <span class="cf">return</span> x, y</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="f4778e22" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a><span class="bu">print</span>(text[:<span class="dv">1000</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">First Citizen:
Before we proceed any further, hear me speak.

All:
Speak, speak.

First Citizen:
You are all resolved rather to die than to famish?

All:
Resolved. resolved.

First Citizen:
First, you know Caius Marcius is chief enemy to the people.

All:
We know't, we know't.

First Citizen:
Let us kill him, and we'll have corn at our own price.
Is't a verdict?

All:
No more talking on't; let it be done: away, away!

Second Citizen:
One word, good citizens.

First Citizen:
We are accounted poor citizens, the patricians good.
What authority surfeits on would relieve us: if they
would yield us but the superfluity, while it were
wholesome, we might guess they relieved us humanely;
but they think we are too dear: the leanness that
afflicts us, the object of our misery, is as an
inventory to particularise their abundance; our
sufferance is a gain to them Let us revenge this with
our pikes, ere we become rakes: for the gods know I
speak this in hunger for bread, not in thirst for revenge.


</pre>
</div>
</div>
</section>
<section id="set-up-the-components-of-the-decoder-block" class="level3">
<h3 class="anchored" data-anchor-id="set-up-the-components-of-the-decoder-block">Set up the components of the Decoder block:</h3>
<ul>
<li>MultiHeadAttention</li>
<li>FeedForward Network</li>
</ul>
<div id="c610fd2f" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a><span class="kw">class</span> Head(nn.Module):</span>
<span id="cb29-2"><a href="#cb29-2"></a>    <span class="co">"""one head of self-attention"""</span></span>
<span id="cb29-3"><a href="#cb29-3"></a></span>
<span id="cb29-4"><a href="#cb29-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, head_size):</span>
<span id="cb29-5"><a href="#cb29-5"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb29-6"><a href="#cb29-6"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb29-7"><a href="#cb29-7"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb29-8"><a href="#cb29-8"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb29-9"><a href="#cb29-9"></a>        <span class="va">self</span>.register_buffer(<span class="st">"tril"</span>, torch.tril(torch.ones(block_size, block_size)))</span>
<span id="cb29-10"><a href="#cb29-10"></a></span>
<span id="cb29-11"><a href="#cb29-11"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb29-12"><a href="#cb29-12"></a></span>
<span id="cb29-13"><a href="#cb29-13"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb29-14"><a href="#cb29-14"></a>        B, T, C <span class="op">=</span> x.shape</span>
<span id="cb29-15"><a href="#cb29-15"></a>        k <span class="op">=</span> <span class="va">self</span>.key(x)  <span class="co"># (B,T,C) 16,32,16</span></span>
<span id="cb29-16"><a href="#cb29-16"></a>        q <span class="op">=</span> <span class="va">self</span>.query(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb29-17"><a href="#cb29-17"></a>        <span class="co"># compute attention scores ("affinities")</span></span>
<span id="cb29-18"><a href="#cb29-18"></a>        wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> C<span class="op">**-</span><span class="fl">0.5</span>  <span class="co"># (B, T, C) @ (B, C, T) -&gt; (B, T, T)</span></span>
<span id="cb29-19"><a href="#cb29-19"></a>        wei <span class="op">=</span> wei.masked_fill(<span class="va">self</span>.tril[:T, :T] <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">"-inf"</span>))  <span class="co"># (B, T, T)</span></span>
<span id="cb29-20"><a href="#cb29-20"></a>        wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, T, T)</span></span>
<span id="cb29-21"><a href="#cb29-21"></a>        wei <span class="op">=</span> <span class="va">self</span>.dropout(wei)</span>
<span id="cb29-22"><a href="#cb29-22"></a>        <span class="co"># perform the weighted aggregation of the values</span></span>
<span id="cb29-23"><a href="#cb29-23"></a>        v <span class="op">=</span> <span class="va">self</span>.value(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb29-24"><a href="#cb29-24"></a>        out <span class="op">=</span> wei <span class="op">@</span> v  <span class="co"># (B, T, T) @ (B, T, C) -&gt; (B, T, C)</span></span>
<span id="cb29-25"><a href="#cb29-25"></a>        <span class="cf">return</span> out</span>
<span id="cb29-26"><a href="#cb29-26"></a></span>
<span id="cb29-27"><a href="#cb29-27"></a></span>
<span id="cb29-28"><a href="#cb29-28"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb29-29"><a href="#cb29-29"></a>    <span class="co">"""multiple heads of self-attention in parallel"""</span></span>
<span id="cb29-30"><a href="#cb29-30"></a></span>
<span id="cb29-31"><a href="#cb29-31"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads, head_size):</span>
<span id="cb29-32"><a href="#cb29-32"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb29-33"><a href="#cb29-33"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([Head(head_size) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)])</span>
<span id="cb29-34"><a href="#cb29-34"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(n_embd, n_embd)</span>
<span id="cb29-35"><a href="#cb29-35"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb29-36"><a href="#cb29-36"></a></span>
<span id="cb29-37"><a href="#cb29-37"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb29-38"><a href="#cb29-38"></a>        out <span class="op">=</span> torch.cat([h(x) <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.heads], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb29-39"><a href="#cb29-39"></a>        out <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.proj(out))</span>
<span id="cb29-40"><a href="#cb29-40"></a>        <span class="cf">return</span> out</span>
<span id="cb29-41"><a href="#cb29-41"></a></span>
<span id="cb29-42"><a href="#cb29-42"></a></span>
<span id="cb29-43"><a href="#cb29-43"></a><span class="kw">class</span> FeedFoward(nn.Module):</span>
<span id="cb29-44"><a href="#cb29-44"></a>    <span class="co">"""a simple linear layer followed by a non-linearity"""</span></span>
<span id="cb29-45"><a href="#cb29-45"></a></span>
<span id="cb29-46"><a href="#cb29-46"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd):</span>
<span id="cb29-47"><a href="#cb29-47"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb29-48"><a href="#cb29-48"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb29-49"><a href="#cb29-49"></a>            nn.Linear(n_embd, <span class="dv">4</span> <span class="op">*</span> n_embd),</span>
<span id="cb29-50"><a href="#cb29-50"></a>            nn.ReLU(),</span>
<span id="cb29-51"><a href="#cb29-51"></a>            nn.Linear(</span>
<span id="cb29-52"><a href="#cb29-52"></a>                <span class="dv">4</span> <span class="op">*</span> n_embd, n_embd</span>
<span id="cb29-53"><a href="#cb29-53"></a>            ),  <span class="co"># Projection layer going back into the residual pathway</span></span>
<span id="cb29-54"><a href="#cb29-54"></a>            nn.Dropout(dropout),</span>
<span id="cb29-55"><a href="#cb29-55"></a>        )</span>
<span id="cb29-56"><a href="#cb29-56"></a></span>
<span id="cb29-57"><a href="#cb29-57"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb29-58"><a href="#cb29-58"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="combine-components-into-the-decoder-block" class="level3">
<h3 class="anchored" data-anchor-id="combine-components-into-the-decoder-block">Combine components into the Decoder block</h3>
<div id="7db45e7f" class="cell" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb30-2"><a href="#cb30-2"></a>    <span class="co">"""Transformer block: communication followed by computation"""</span></span>
<span id="cb30-3"><a href="#cb30-3"></a></span>
<span id="cb30-4"><a href="#cb30-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd, n_head):</span>
<span id="cb30-5"><a href="#cb30-5"></a>        <span class="co"># n_embd: embedding dimension, n_head: the number of heads we'd like</span></span>
<span id="cb30-6"><a href="#cb30-6"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb30-7"><a href="#cb30-7"></a>        head_size <span class="op">=</span> n_embd <span class="op">//</span> n_head</span>
<span id="cb30-8"><a href="#cb30-8"></a>        <span class="va">self</span>.sa <span class="op">=</span> MultiHeadAttention(n_head, head_size)</span>
<span id="cb30-9"><a href="#cb30-9"></a>        <span class="va">self</span>.ffwd <span class="op">=</span> FeedFoward(n_embd)</span>
<span id="cb30-10"><a href="#cb30-10"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb30-11"><a href="#cb30-11"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb30-12"><a href="#cb30-12"></a></span>
<span id="cb30-13"><a href="#cb30-13"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb30-14"><a href="#cb30-14"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x))  <span class="co"># Communication</span></span>
<span id="cb30-15"><a href="#cb30-15"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x))  <span class="co"># Computation</span></span>
<span id="cb30-16"><a href="#cb30-16"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="set-up-the-full-transformer-model" class="level3">
<h3 class="anchored" data-anchor-id="set-up-the-full-transformer-model">Set up the full Transformer model</h3>
<p>This is a combination of the Token embeddings, Positional embeddings, a stack of Transformer blocks and an output block.</p>
<div id="35f16027" class="cell" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a><span class="co"># super simple language model</span></span>
<span id="cb31-2"><a href="#cb31-2"></a><span class="kw">class</span> LanguageModel(nn.Module):</span>
<span id="cb31-3"><a href="#cb31-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb31-4"><a href="#cb31-4"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb31-5"><a href="#cb31-5"></a>        <span class="co"># each token directly reads off the logits for the next token from a lookup table</span></span>
<span id="cb31-6"><a href="#cb31-6"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb31-7"><a href="#cb31-7"></a>        <span class="va">self</span>.position_embedding_table <span class="op">=</span> nn.Embedding(block_size, n_embd)</span>
<span id="cb31-8"><a href="#cb31-8"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(</span>
<span id="cb31-9"><a href="#cb31-9"></a>            <span class="op">*</span>[Block(n_embd, n_head<span class="op">=</span>n_head) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layer)]</span>
<span id="cb31-10"><a href="#cb31-10"></a>        )</span>
<span id="cb31-11"><a href="#cb31-11"></a>        <span class="va">self</span>.ln_f <span class="op">=</span> nn.LayerNorm(n_embd)  <span class="co"># final layer norm</span></span>
<span id="cb31-12"><a href="#cb31-12"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(n_embd, vocab_size)</span>
<span id="cb31-13"><a href="#cb31-13"></a></span>
<span id="cb31-14"><a href="#cb31-14"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb31-15"><a href="#cb31-15"></a>        B, T <span class="op">=</span> idx.shape</span>
<span id="cb31-16"><a href="#cb31-16"></a></span>
<span id="cb31-17"><a href="#cb31-17"></a>        <span class="co"># idx and targets are both (B,T) tensor of integers</span></span>
<span id="cb31-18"><a href="#cb31-18"></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx)  <span class="co"># (B,T,C)</span></span>
<span id="cb31-19"><a href="#cb31-19"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.position_embedding_table(torch.arange(T, device<span class="op">=</span>device))  <span class="co"># (T,C)</span></span>
<span id="cb31-20"><a href="#cb31-20"></a>        x <span class="op">=</span> tok_emb <span class="op">+</span> pos_emb  <span class="co"># (B,T,C)</span></span>
<span id="cb31-21"><a href="#cb31-21"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb31-22"><a href="#cb31-22"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_f(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb31-23"><a href="#cb31-23"></a>        logits <span class="op">=</span> <span class="va">self</span>.lm_head(x)  <span class="co"># (B,T,vocab_size)</span></span>
<span id="cb31-24"><a href="#cb31-24"></a></span>
<span id="cb31-25"><a href="#cb31-25"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb31-26"><a href="#cb31-26"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb31-27"><a href="#cb31-27"></a>        <span class="cf">else</span>:</span>
<span id="cb31-28"><a href="#cb31-28"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb31-29"><a href="#cb31-29"></a>            logits <span class="op">=</span> logits.view(B <span class="op">*</span> T, C)</span>
<span id="cb31-30"><a href="#cb31-30"></a>            targets <span class="op">=</span> targets.view(B <span class="op">*</span> T)</span>
<span id="cb31-31"><a href="#cb31-31"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb31-32"><a href="#cb31-32"></a></span>
<span id="cb31-33"><a href="#cb31-33"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb31-34"><a href="#cb31-34"></a></span>
<span id="cb31-35"><a href="#cb31-35"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb31-36"><a href="#cb31-36"></a>        <span class="co"># idx is (B, T) array of indices in the current context</span></span>
<span id="cb31-37"><a href="#cb31-37"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb31-38"><a href="#cb31-38"></a>            <span class="co"># crop idx to the last block_size tokens</span></span>
<span id="cb31-39"><a href="#cb31-39"></a>            idx_cond <span class="op">=</span> idx[:, <span class="op">-</span>block_size:]</span>
<span id="cb31-40"><a href="#cb31-40"></a>            <span class="co"># get the predictions</span></span>
<span id="cb31-41"><a href="#cb31-41"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx_cond)</span>
<span id="cb31-42"><a href="#cb31-42"></a>            <span class="co"># focus only on the last time step</span></span>
<span id="cb31-43"><a href="#cb31-43"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]  <span class="co"># becomes (B, C)</span></span>
<span id="cb31-44"><a href="#cb31-44"></a>            <span class="co"># apply softmax to get probabilities</span></span>
<span id="cb31-45"><a href="#cb31-45"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, C)</span></span>
<span id="cb31-46"><a href="#cb31-46"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb31-47"><a href="#cb31-47"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (B, 1)</span></span>
<span id="cb31-48"><a href="#cb31-48"></a>            <span class="co"># append sampled index to the running sequence</span></span>
<span id="cb31-49"><a href="#cb31-49"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (B, T+1)</span></span>
<span id="cb31-50"><a href="#cb31-50"></a>        <span class="cf">return</span> idx</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
<section id="homework" class="level2">
<h2 class="anchored" data-anchor-id="homework">Homework</h2>
<ol type="1">
<li>In this notebook, we learned the various components of an LLM.<br>
Take the mini LLM we created from scratch and run your own training loop. Show how the training and validation perplexity change over the steps.<br>
Hint: this function might be useful for you:</li>
</ol>
<div id="b67da557" class="cell" data-execution_count="26">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb32-2"><a href="#cb32-2"></a><span class="kw">def</span> estimate_loss():</span>
<span id="cb32-3"><a href="#cb32-3"></a>    out <span class="op">=</span> {}</span>
<span id="cb32-4"><a href="#cb32-4"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb32-5"><a href="#cb32-5"></a>    <span class="cf">for</span> split <span class="kw">in</span> [<span class="st">"train"</span>, <span class="st">"val"</span>]:</span>
<span id="cb32-6"><a href="#cb32-6"></a>        losses <span class="op">=</span> torch.zeros(eval_iters)</span>
<span id="cb32-7"><a href="#cb32-7"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(eval_iters):</span>
<span id="cb32-8"><a href="#cb32-8"></a>            X, Y <span class="op">=</span> get_batch(split)</span>
<span id="cb32-9"><a href="#cb32-9"></a>            logits, loss <span class="op">=</span> model(X, Y)</span>
<span id="cb32-10"><a href="#cb32-10"></a>            losses[k] <span class="op">=</span> loss.item()</span>
<span id="cb32-11"><a href="#cb32-11"></a>        out[split] <span class="op">=</span> losses.mean()</span>
<span id="cb32-12"><a href="#cb32-12"></a>    model.train()</span>
<span id="cb32-13"><a href="#cb32-13"></a>    <span class="cf">return</span> out</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<ol start="2" type="1">
<li>Run the same training loop but modify one of the hyperparameters from the below list. Run this at least 4 times with a different value and plot each perplexity over training step.</li>
</ol>
<div id="1e8c52b6" class="cell" data-execution_count="27">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a><span class="co"># hyperparameters</span></span>
<span id="cb33-2"><a href="#cb33-2"></a>n_embd <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb33-3"><a href="#cb33-3"></a>n_head <span class="op">=</span> <span class="dv">4</span>  <span class="co">## so head_size = 16</span></span>
<span id="cb33-4"><a href="#cb33-4"></a>n_layer <span class="op">=</span> <span class="dv">4</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<ol start="3" type="1">
<li><p>Output some generated text from each model you trained. Did the output make more sense with some hyperparameters than others?</p></li>
<li><p>We saw a cool visualization of attention mechanisms with BertViz. Take a more complicated model than GPT2 such as “meta-llama/Llama-2-7b-chat-hf” and see how the attention mechanisms are different</p></li>
</ol>
</section>
<section id="different-types-of-transformers" class="level2">
<h2 class="anchored" data-anchor-id="different-types-of-transformers">Different types of Transformers</h2>
<section id="encoder-decoder-architecture" class="level3">
<h3 class="anchored" data-anchor-id="encoder-decoder-architecture">Encoder-Decoder architecture</h3>
<p>Incorporates both an encoder + decoder architecture</p>
<p>The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.</p>
<p>In the decoder, the self-attention layer only attends to earlier positions in the output sequence. The future positions are masked (setting them to -inf) before the softmax step in the self-attention calculation.</p>
<p>The “Encoder-Decoder Attention” layer creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.</p>
<p>The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output.</p>
<p>The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did.</p>
<p>And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.</p>
<div id="fig-animated-transformer" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-animated-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/transformer_decoding_2.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;10: Illustration of the Encoder-Decoder architecture"><img src="images/transformer_decoding_2.gif" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-animated-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Illustration of the Encoder-Decoder architecture
</figcaption>
</figure>
</div>
<p>Image credit: https://jalammar.github.io/illustrated-transformer/</p>
</section>
<section id="encoder-only-transformers" class="level3">
<h3 class="anchored" data-anchor-id="encoder-only-transformers">Encoder-only Transformers</h3>
<p>In addition to the encoder-decoder architecture shown here there various other architectures which are either only encoder or decoder models.</p>
</section>
<section id="bidirectional-encoder-representations-from-transformers-bert-model" class="level3">
<h3 class="anchored" data-anchor-id="bidirectional-encoder-representations-from-transformers-bert-model">Bidirectional Encoder Representations from Transformers (BERT) model</h3>
<p>Encoder-only models only use the encoder layer of the Transformer.</p>
<p>These models are usually used for “understanding” natural language; however, they typically are not used for text generation. Examples of uses for these models are:</p>
<ol type="1">
<li>Determining how positive or negative a movie’s reviews are. (Sentiment Analysis)</li>
<li>Summarizing long legal contracts. (Summarization)</li>
<li>Differentiating words that have multiple meanings (like ‘bank’) based on the surrounding text. (Polysemy resolution)</li>
</ol>
<p>These models are often characterized as having “bi-directional” attention, and are often called auto-encoding models. The attention mechanisms of these models can access all the words in the initial sentence.</p>
<p>The most common encoder only architectures are:</p>
<ul>
<li>ALBERT</li>
<li>BERT</li>
<li>DistilBERT</li>
<li>ELECTRA</li>
<li>RoBERTa</li>
</ul>
<p>As example, let’s consider BERT model in a little more detail.</p>
<div id="fig-bert-explanation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bert-explanation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/BERT_Explanation.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;11: BERT Explanation"><img src="images/BERT_Explanation.webp" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bert-explanation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: BERT Explanation
</figcaption>
</figure>
</div>
<p>Image credit: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270</p>
<p>The BERT model is bidirectionally trained to have a deeper sense of language context and flow than single-direction language models.</p>
<p>The Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).</p>
<p>In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence.</p>
<p>To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:</p>
<ol type="1">
<li>A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.</li>
<li>A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.</li>
<li>A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper.</li>
</ol>
<div id="fig-bert-input" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bert-input-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/BERT_input_sent.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;12: Illustration of BERT input"><img src="images/BERT_input_sent.webp" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bert-input-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Illustration of BERT input
</figcaption>
</figure>
</div>
<p>Image credit: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270</p>
<p>To predict if the second sentence is indeed connected to the first, the following steps are performed:</p>
<ol type="1">
<li>The entire input sequence goes through the Transformer model.</li>
<li>The output of the [CLS] token is transformed into a 2×1 shaped vector, using a simple classification layer (learned matrices of weights and biases).</li>
<li>Calculating the probability of IsNextSequence with softmax.</li>
</ol>
<section id="advantages-and-disadvantages" class="level4">
<h4 class="anchored" data-anchor-id="advantages-and-disadvantages">Advantages and disadvantages:</h4>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Contextualized embeddings: Good for tasks where contextualized embeddings of input tokens are crucial, such as natural language understanding.</li>
<li>Parallel processing: Allows for parallel processing of input tokens, making it computationally efficient.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Not designed for sequence generation: Might not perform well on tasks that require sequential generation of output, as there is no inherent mechanism for auto-regressive decoding.</li>
</ul>
<p>Here is an example of a BERT code that can be used to</p>
</section>
</section>
<section id="decoder-only-models" class="level3">
<h3 class="anchored" data-anchor-id="decoder-only-models">Decoder-only models</h3>
<p>An important difference of the GPT-2 architecture compared to the encoder-Transformer architecture has to do with the type of attention mechanism used.</p>
<p>In models such as BERT, the self-attention mechanism has access to tokens to the left and right of the query token. However, in decoder-based models such as GPT-2, masked self-attention is used instead which allows access only to tokens to the left of the query.</p>
<p>The masked self-attention mechanism is important for GPT-2 since it allows the model to be trained for token-by-token generation without simply “memorizing” the future tokens.</p>
<div id="fig-self-attention-and-masked-self-attention" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-self-attention-and-masked-self-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/self-attention-and-masked-self-attention.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;13: "><img src="images/self-attention-and-masked-self-attention.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-self-attention-and-masked-self-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13
</figcaption>
</figure>
</div>
<p>Image credit: https://jalammar.github.io/illustrated-gpt2/</p>
<p>The masked self-attention adds understanding of associated words to explain contexts of certain words before passing it through a neural network. It assigns scores to how relevant each word in the segment is, and then adds up the vector representation. This is then passed through the feed-forward network resulting in an output vector.</p>
<div id="fig-gpt2-self-attention" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gpt2-self-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/gpt2-self-attention-example-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;14: "><img src="images/gpt2-self-attention-example-2.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-gpt2-self-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14
</figcaption>
</figure>
</div>
<p>Image credit: https://jalammar.github.io/illustrated-gpt2/</p>
<p>The resulting vector then needs to be converted to an output token. A common method of obtaining this output token is known as top-k.</p>
<p>Here, the output vector is multiplied by the token embeddings which results in probabilities for each token in the vocabulary. Then the output token is sampled according to this probability.</p>
<div id="fig-gpt2-output" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gpt2-output-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/gpt2-output.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;15: "><img src="images/gpt2-output.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-gpt2-output-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15
</figcaption>
</figure>
</div>
<p>Image credit: https://jalammar.github.io/illustrated-gpt2/</p>
</section>
<section id="advantages-and-disadvantages-1" class="level3">
<h3 class="anchored" data-anchor-id="advantages-and-disadvantages-1">Advantages and disadvantages</h3>
<p><strong>Advantages:</strong></p>
<ul>
<li>Auto-regressive generation: Well-suited for tasks that require sequential generation, as the model can generate one token at a time based on the previous tokens.</li>
<li>Variable-length output: Can handle tasks where the output sequence length is not fixed.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>No direct access to input context: The decoder doesn’t directly consider the input context during decoding, which might be a limitation for certain tasks.</li>
<li>Potential for inefficiency: Decoding token by token can be less computationally efficient compared to parallel processing.</li>
</ul>
</section>
</section>
<section id="additional-architectures" class="level2">
<h2 class="anchored" data-anchor-id="additional-architectures">Additional architectures</h2>
<p>In addition to text, LLMs have also been applied on other data sources such as images and graphs. Here I will describe two particular architectures: 1. Vision Transformers 2. Graph Transformers</p>
<section id="vision-transformers" class="level3">
<h3 class="anchored" data-anchor-id="vision-transformers">Vision Transformers</h3>
<p>Vision Transformers (ViT) is an architecture that uses self-attention mechanisms to process images.</p>
<p>The way this works is:</p>
<ol type="1">
<li>Split image into patches (size is fixed)</li>
<li>Flatten the image patches</li>
<li>Create lower-dimensional linear embeddings from these flattened image patches and include positional embeddings</li>
<li>Feed the sequence as an input to a transformer encoder</li>
<li>Pre-train the ViT model with image labels, which is then fully supervised on a big dataset Fine-tune the downstream dataset for image classification</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/vision-transformer-vit.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="vision-transformer-vit.png"><img src="images/vision-transformer-vit.png" class="img-fluid figure-img" alt="vision-transformer-vit.png"></a></p>
<figcaption>vision-transformer-vit.png</figcaption>
</figure>
</div>
<p>Image credit: Dosovitskiy, Alexey, et al.&nbsp;“An image is worth 16x16 words: Transformers for image recognition at scale.” arXiv preprint arXiv:2010.11929 (2020).</p>
</section>
<section id="graph-transformers" class="level3">
<h3 class="anchored" data-anchor-id="graph-transformers">Graph Transformers</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Graphformer.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="Graphformer.png"><img src="images/Graphformer.png" class="img-fluid figure-img" alt="Graphformer.png"></a></p>
<figcaption>Graphformer.png</figcaption>
</figure>
</div>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Here are some recommendations for further reading and additional code for review.</p>
<ul>
<li>“The Illustrated Transformer” by Jay Alammar</li>
<li>“Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)”</li>
<li>“The Illustrated GPT-2 (Visualizing Transformer Language Models)”</li>
<li>“A gentle introduction to positional encoding”</li>
<li>“LLM Tutorial Workshop (Argonne National Laboratory)”</li>
<li>“LLM Tutorial Workshop Part 2 (Argonne National Laboratory)”</li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{foreman2025,
  author = {Foreman, Sam},
  title = {Language Models {(LMs)}},
  date = {2025-08-05},
  url = {https://saforem2.github.io/hpc-bootcamp-2025/02-llms/00-intro-to-llms/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-foreman2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Foreman, Sam. 2025. <span>“Language Models (LMs).”</span> August 5,
2025. <a href="https://saforem2.github.io/hpc-bootcamp-2025/02-llms/00-intro-to-llms/">https://saforem2.github.io/hpc-bootcamp-2025/02-llms/00-intro-to-llms/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/saforem2\.github\.io\/hpc-bootcamp-2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="dark">
<input type="hidden" id="giscus-alt-theme" value="light">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "saforem2/intro-hpc-bootcamp-2025";
    script.dataset.repoId = "R_kgDOPQ7DhQ";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOPQ7Dhc4CtXSR";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../02-llms/index.html" class="pagination-link" aria-label="[02] Large Language Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">[02] Large Language Models</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../02-llms/01-hands-on-llms/index.html" class="pagination-link" aria-label="[1] Hands-on LLMs">
        <span class="nav-page-text">[1] Hands-on LLMs</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb34" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb34-1"><a href="#cb34-1"></a><span class="co">---</span></span>
<span id="cb34-2"><a href="#cb34-2"></a><span class="an">title:</span><span class="co"> Language models (LMs)</span></span>
<span id="cb34-3"><a href="#cb34-3"></a><span class="an">description:</span><span class="co"> "An introduction to High Performance Computing (HPC) and Artificial Intelligence (AI) for scientific applications, with a focus on practical skills and hands-on experience."</span></span>
<span id="cb34-4"><a href="#cb34-4"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb34-5"><a href="#cb34-5"></a><span class="an">date:</span><span class="co"> 2025-08-05</span></span>
<span id="cb34-6"><a href="#cb34-6"></a><span class="an">date-modified:</span><span class="co"> last-modified</span></span>
<span id="cb34-7"><a href="#cb34-7"></a><span class="co">---</span></span>
<span id="cb34-8"><a href="#cb34-8"></a></span>
<span id="cb34-9"><a href="#cb34-9"></a></span>
<span id="cb34-10"><a href="#cb34-10"></a>::: {.callout-note title="Authors" collapse="false"}</span>
<span id="cb34-11"><a href="#cb34-11"></a>Author: Archit Vasan , including materials on LLMs by Varuni Sastri and Carlo</span>
<span id="cb34-12"><a href="#cb34-12"></a>Graziani at Argonne, and discussion/editorial work by Taylor Childers, Bethany</span>
<span id="cb34-13"><a href="#cb34-13"></a>Lusch, and Venkat Vishwanath (Argonne)</span>
<span id="cb34-14"><a href="#cb34-14"></a></span>
<span id="cb34-15"><a href="#cb34-15"></a>Modification by Huihuo Zheng on August 1, 2025</span>
<span id="cb34-16"><a href="#cb34-16"></a>:::</span>
<span id="cb34-17"><a href="#cb34-17"></a></span>
<span id="cb34-18"><a href="#cb34-18"></a>Inspiration from the blog posts "The Illustrated Transformer" and "The</span>
<span id="cb34-19"><a href="#cb34-19"></a>Illustrated GPT2" by Jay Alammar, highly recommended reading.</span>
<span id="cb34-20"><a href="#cb34-20"></a></span>
<span id="cb34-21"><a href="#cb34-21"></a>Although the name "language models" is derived from Natural Language</span>
<span id="cb34-22"><a href="#cb34-22"></a>Processing, the models used in these approaches can be applied to diverse</span>
<span id="cb34-23"><a href="#cb34-23"></a>scientific applications as illustrated below. </span>
<span id="cb34-24"><a href="#cb34-24"></a></span>
<span id="cb34-25"><a href="#cb34-25"></a>This session is dedicated to setting out the basics of sequential data</span>
<span id="cb34-26"><a href="#cb34-26"></a>modeling, and introducing a few key elements required for DL approaches to such</span>
<span id="cb34-27"><a href="#cb34-27"></a>modeling---principally Transformers.</span>
<span id="cb34-28"><a href="#cb34-28"></a></span>
<span id="cb34-29"><a href="#cb34-29"></a><span class="fu">## Overview</span></span>
<span id="cb34-30"><a href="#cb34-30"></a></span>
<span id="cb34-31"><a href="#cb34-31"></a>During this session I will cover:</span>
<span id="cb34-32"><a href="#cb34-32"></a></span>
<span id="cb34-33"><a href="#cb34-33"></a><span class="ss">1. </span>Scientific applications modeling sequential data</span>
<span id="cb34-34"><a href="#cb34-34"></a><span class="ss">2. </span>Brief History of Language Models</span>
<span id="cb34-35"><a href="#cb34-35"></a><span class="ss">3. </span>Tokenization and embedding of sequential data</span>
<span id="cb34-36"><a href="#cb34-36"></a><span class="ss">5. </span>Elements of a Transformer</span>
<span id="cb34-37"><a href="#cb34-37"></a><span class="ss">6. </span>Attention mechanisms</span>
<span id="cb34-38"><a href="#cb34-38"></a><span class="ss">7. </span>Output layers</span>
<span id="cb34-39"><a href="#cb34-39"></a><span class="ss">8. </span>Training loops</span>
<span id="cb34-40"><a href="#cb34-40"></a><span class="ss">9. </span>Different types of Transformers</span>
<span id="cb34-41"><a href="#cb34-41"></a></span>
<span id="cb34-42"><a href="#cb34-42"></a><span class="fu">## Modeling Sequential Data</span></span>
<span id="cb34-43"><a href="#cb34-43"></a></span>
<span id="cb34-44"><a href="#cb34-44"></a>Sequences are variable-length lists with data in subsequent iterations that</span>
<span id="cb34-45"><a href="#cb34-45"></a>depends on previous iterations (or tokens).</span>
<span id="cb34-46"><a href="#cb34-46"></a></span>
<span id="cb34-47"><a href="#cb34-47"></a>Mathematically: </span>
<span id="cb34-48"><a href="#cb34-48"></a>A sequence is a list of tokens:</span>
<span id="cb34-49"><a href="#cb34-49"></a></span>
<span id="cb34-50"><a href="#cb34-50"></a>$$T = <span class="co">[</span><span class="ot">t_1, t_2, t_3,...,t_N</span><span class="co">]</span>$$</span>
<span id="cb34-51"><a href="#cb34-51"></a></span>
<span id="cb34-52"><a href="#cb34-52"></a>where each token within the list depends on the others with a particular</span>
<span id="cb34-53"><a href="#cb34-53"></a>probability:</span>
<span id="cb34-54"><a href="#cb34-54"></a></span>
<span id="cb34-55"><a href="#cb34-55"></a>$$P(t_2 | t_1, t_3, t_4, ..., t_N)$$</span>
<span id="cb34-56"><a href="#cb34-56"></a></span>
<span id="cb34-57"><a href="#cb34-57"></a>The purpose of sequential modeling is to learn these probabilities for possible</span>
<span id="cb34-58"><a href="#cb34-58"></a>tokens in a distribution to perform various tasks including:</span>
<span id="cb34-59"><a href="#cb34-59"></a></span>
<span id="cb34-60"><a href="#cb34-60"></a><span class="ss">- </span>Sequence generation based on a prompt</span>
<span id="cb34-61"><a href="#cb34-61"></a><span class="ss">- </span>Language translation (e.g. English --&gt; French)</span>
<span id="cb34-62"><a href="#cb34-62"></a><span class="ss">- </span>Property prediction (predicting a property based on an entire sequence)</span>
<span id="cb34-63"><a href="#cb34-63"></a><span class="ss">- </span>Identifying mistakes or missing elements in sequential data</span>
<span id="cb34-64"><a href="#cb34-64"></a></span>
<span id="cb34-65"><a href="#cb34-65"></a><span class="fu">## Scientific sequential data modeling examples</span></span>
<span id="cb34-66"><a href="#cb34-66"></a></span>
<span id="cb34-67"><a href="#cb34-67"></a><span class="fu">### Nucleic acid sequences + genomic data</span></span>
<span id="cb34-68"><a href="#cb34-68"></a></span>
<span id="cb34-69"><a href="#cb34-69"></a>Nucleic acid sequences can be used to predict translation of proteins,</span>
<span id="cb34-70"><a href="#cb34-70"></a>mutations, and gene expression levels.</span>
<span id="cb34-71"><a href="#cb34-71"></a></span>
<span id="cb34-72"><a href="#cb34-72"></a>::: {#fig-rna-sequences}</span>
<span id="cb34-73"><a href="#cb34-73"></a><span class="al">![](images/RNA-codons.svg.png)</span></span>
<span id="cb34-74"><a href="#cb34-74"></a></span>
<span id="cb34-75"><a href="#cb34-75"></a>RNA sequences</span>
<span id="cb34-76"><a href="#cb34-76"></a>:::</span>
<span id="cb34-77"><a href="#cb34-77"></a></span>
<span id="cb34-78"><a href="#cb34-78"></a>Here is an image of GenSLM.</span>
<span id="cb34-79"><a href="#cb34-79"></a>This is a language model developed by Argonne researchers that can model</span>
<span id="cb34-80"><a href="#cb34-80"></a>genomic information in a single model.</span>
<span id="cb34-81"><a href="#cb34-81"></a>It was shown to model the evolution of SARS-COV2 without expensive experiments.</span>
<span id="cb34-82"><a href="#cb34-82"></a></span>
<span id="cb34-83"><a href="#cb34-83"></a>::: {#fig-genslm}</span>
<span id="cb34-84"><a href="#cb34-84"></a></span>
<span id="cb34-85"><a href="#cb34-85"></a><span class="al">![](images/genslm.png)</span></span>
<span id="cb34-86"><a href="#cb34-86"></a></span>
<span id="cb34-87"><a href="#cb34-87"></a>GenSLM. Image credit: Zvyagin et. al 2022. BioRXiv</span>
<span id="cb34-88"><a href="#cb34-88"></a>:::</span>
<span id="cb34-89"><a href="#cb34-89"></a></span>
<span id="cb34-90"><a href="#cb34-90"></a><span class="fu">### Protein sequences</span></span>
<span id="cb34-91"><a href="#cb34-91"></a></span>
<span id="cb34-92"><a href="#cb34-92"></a>Protein sequences can be used to predict folding structure, protein-protein</span>
<span id="cb34-93"><a href="#cb34-93"></a>interactions, chemical/binding properties, protein function and many more</span>
<span id="cb34-94"><a href="#cb34-94"></a>properties.</span>
<span id="cb34-95"><a href="#cb34-95"></a></span>
<span id="cb34-96"><a href="#cb34-96"></a>::: {#fig-protein-sequences}</span>
<span id="cb34-97"><a href="#cb34-97"></a><span class="al">![](images/Protein-Structure-06.png)</span></span>
<span id="cb34-98"><a href="#cb34-98"></a></span>
<span id="cb34-99"><a href="#cb34-99"></a>Protein sequences</span>
<span id="cb34-100"><a href="#cb34-100"></a>:::</span>
<span id="cb34-101"><a href="#cb34-101"></a></span>
<span id="cb34-102"><a href="#cb34-102"></a><span class="fu">### Other applications:</span></span>
<span id="cb34-103"><a href="#cb34-103"></a></span>
<span id="cb34-104"><a href="#cb34-104"></a><span class="ss">* </span>Biomedical text</span>
<span id="cb34-105"><a href="#cb34-105"></a><span class="ss">* </span>SMILES strings</span>
<span id="cb34-106"><a href="#cb34-106"></a><span class="ss">* </span>Weather predictions</span>
<span id="cb34-107"><a href="#cb34-107"></a><span class="ss">* </span>Interfacing with simulations such as molecular dynamics simulation</span>
<span id="cb34-108"><a href="#cb34-108"></a></span>
<span id="cb34-109"><a href="#cb34-109"></a><span class="fu">## Overview of Language models</span></span>
<span id="cb34-110"><a href="#cb34-110"></a></span>
<span id="cb34-111"><a href="#cb34-111"></a>We will now briefly talk about the progression of language models.</span>
<span id="cb34-112"><a href="#cb34-112"></a></span>
<span id="cb34-113"><a href="#cb34-113"></a><span class="fu">### RNNs</span></span>
<span id="cb34-114"><a href="#cb34-114"></a></span>
<span id="cb34-115"><a href="#cb34-115"></a>Recurrent Neural Newtorks(RNNs) were a traditional model used to determine</span>
<span id="cb34-116"><a href="#cb34-116"></a>temporal dependencies within data. </span>
<span id="cb34-117"><a href="#cb34-117"></a></span>
<span id="cb34-118"><a href="#cb34-118"></a>In RNNs, the hidden state from the previous time step is fed back into the</span>
<span id="cb34-119"><a href="#cb34-119"></a>network, allowing it to maintain a “memory” of past inputs. </span>
<span id="cb34-120"><a href="#cb34-120"></a></span>
<span id="cb34-121"><a href="#cb34-121"></a>They were ideal for tasks with short sequences such as natural language</span>
<span id="cb34-122"><a href="#cb34-122"></a>processing and time-series prediction.</span>
<span id="cb34-123"><a href="#cb34-123"></a></span>
<span id="cb34-124"><a href="#cb34-124"></a>::: {#fig-rnn}</span>
<span id="cb34-125"><a href="#cb34-125"></a><span class="al">![](images/recurrent_nn.png)</span></span>
<span id="cb34-126"><a href="#cb34-126"></a></span>
<span id="cb34-127"><a href="#cb34-127"></a>RNN</span>
<span id="cb34-128"><a href="#cb34-128"></a>:::</span>
<span id="cb34-129"><a href="#cb34-129"></a></span>
<span id="cb34-130"><a href="#cb34-130"></a>However, these networks had significant challenges.</span>
<span id="cb34-131"><a href="#cb34-131"></a></span>
<span id="cb34-132"><a href="#cb34-132"></a><span class="ss">- </span>**Slow to train**:</span>
<span id="cb34-133"><a href="#cb34-133"></a>  RNNs process data one element at a time, maintaining an internal hidden state</span>
<span id="cb34-134"><a href="#cb34-134"></a>  that is updated at each step. They operate recurrently, where each output</span>
<span id="cb34-135"><a href="#cb34-135"></a>  depends on the previous hidden state and the current input; thus, parallel</span>
<span id="cb34-136"><a href="#cb34-136"></a>  computation is not possible.</span>
<span id="cb34-137"><a href="#cb34-137"></a><span class="ss">- </span>**Cannot handle large sequences**:</span>
<span id="cb34-138"><a href="#cb34-138"></a>  Exploding and vanishing gradients limit the RNN modelling of long</span>
<span id="cb34-139"><a href="#cb34-139"></a>  sequences. Some variants of RNNs such as LSTM and GRU addressed this problem,</span>
<span id="cb34-140"><a href="#cb34-140"></a>  they cannot engage with very large sequences.</span>
<span id="cb34-141"><a href="#cb34-141"></a></span>
<span id="cb34-142"><a href="#cb34-142"></a><span class="fu">### Transformers</span></span>
<span id="cb34-143"><a href="#cb34-143"></a></span>
<span id="cb34-144"><a href="#cb34-144"></a>The newest LMs referred to as "large language models" (since they have large</span>
<span id="cb34-145"><a href="#cb34-145"></a>parameter size) were developed to address many of these challenges.</span>
<span id="cb34-146"><a href="#cb34-146"></a></span>
<span id="cb34-147"><a href="#cb34-147"></a>These new models base their desin on the Transformer architecture that was</span>
<span id="cb34-148"><a href="#cb34-148"></a>introduced in 2017 in the "Attention is all you need" paper.</span>
<span id="cb34-149"><a href="#cb34-149"></a></span>
<span id="cb34-150"><a href="#cb34-150"></a>Since then a multitude of LLM architectures have been designed.</span>
<span id="cb34-151"><a href="#cb34-151"></a></span>
<span id="cb34-152"><a href="#cb34-152"></a>::: {#fig-chapter1-transformers-chrono}</span>
<span id="cb34-153"><a href="#cb34-153"></a></span>
<span id="cb34-154"><a href="#cb34-154"></a><span class="al">![](images/en_chapter1_transformers_chrono.svg)</span></span>
<span id="cb34-155"><a href="#cb34-155"></a>Image credit: https://huggingface.co/learn/nlp-course/chapter1/4</span>
<span id="cb34-156"><a href="#cb34-156"></a>:::</span>
<span id="cb34-157"><a href="#cb34-157"></a></span>
<span id="cb34-158"><a href="#cb34-158"></a>The power of these models comes from the "attention mechanism" defined in the</span>
<span id="cb34-159"><a href="#cb34-159"></a>Vaswani 2017 seminal paper. </span>
<span id="cb34-160"><a href="#cb34-160"></a></span>
<span id="cb34-161"><a href="#cb34-161"></a>::: {#fig-transformer-arch}</span>
<span id="cb34-162"><a href="#cb34-162"></a><span class="al">![](images/attention_is_all_you_need.png)</span></span>
<span id="cb34-163"><a href="#cb34-163"></a></span>
<span id="cb34-164"><a href="#cb34-164"></a>Transformer architecture</span>
<span id="cb34-165"><a href="#cb34-165"></a>:::</span>
<span id="cb34-166"><a href="#cb34-166"></a></span>
<span id="cb34-167"><a href="#cb34-167"></a><span class="fu">## Coding example of LLMs in action!</span></span>
<span id="cb34-168"><a href="#cb34-168"></a></span>
<span id="cb34-169"><a href="#cb34-169"></a>Let's look at an example of running inference with a LLM as a block box to</span>
<span id="cb34-170"><a href="#cb34-170"></a>generate text given a prompt and we will also initiate a training loop for an</span>
<span id="cb34-171"><a href="#cb34-171"></a>LLM:</span>
<span id="cb34-172"><a href="#cb34-172"></a></span>
<span id="cb34-173"><a href="#cb34-173"></a>Here, we will use the <span class="in">`transformers`</span> library which is as part of HuggingFace, a</span>
<span id="cb34-174"><a href="#cb34-174"></a>repository of different models, tokenizers and information on how to apply</span>
<span id="cb34-175"><a href="#cb34-175"></a>these models</span>
<span id="cb34-176"><a href="#cb34-176"></a></span>
<span id="cb34-177"><a href="#cb34-177"></a>*Warning: Large Language Models are only as good as their training data. They</span>
<span id="cb34-178"><a href="#cb34-178"></a>have no ethics, no judgement, or editing ability. We will be using some</span>
<span id="cb34-179"><a href="#cb34-179"></a>pretrained models from Hugging Face which used wide samples of internet hosted</span>
<span id="cb34-180"><a href="#cb34-180"></a>text. The datasets have not been strictly filtered to restrict all malign</span>
<span id="cb34-181"><a href="#cb34-181"></a>content so the generated text may be surprisingly dark or questionable. They do</span>
<span id="cb34-182"><a href="#cb34-182"></a>not reflect our core values and are only used for demonstration purposes.*</span>
<span id="cb34-183"><a href="#cb34-183"></a></span>
<span id="cb34-186"><a href="#cb34-186"></a><span class="in">```{python}</span></span>
<span id="cb34-187"><a href="#cb34-187"></a><span class="co">#| scrolled: true</span></span>
<span id="cb34-188"><a href="#cb34-188"></a><span class="co"># !pip install transformers</span></span>
<span id="cb34-189"><a href="#cb34-189"></a><span class="co"># !pip install pandas</span></span>
<span id="cb34-190"><a href="#cb34-190"></a><span class="co"># !pip install torch</span></span>
<span id="cb34-191"><a href="#cb34-191"></a><span class="in">```</span></span>
<span id="cb34-192"><a href="#cb34-192"></a></span>
<span id="cb34-195"><a href="#cb34-195"></a><span class="in">```{python}</span></span>
<span id="cb34-196"><a href="#cb34-196"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb34-197"><a href="#cb34-197"></a><span class="op">%</span>autoreload <span class="dv">2</span></span>
<span id="cb34-198"><a href="#cb34-198"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb34-199"><a href="#cb34-199"></a><span class="co"># settings for jupyter book: svg for html version, high-resolution png for pdf</span></span>
<span id="cb34-200"><a href="#cb34-200"></a><span class="im">import</span> matplotlib_inline.backend_inline</span>
<span id="cb34-201"><a href="#cb34-201"></a>matplotlib_inline.backend_inline.set_matplotlib_formats(<span class="st">'retina'</span>, <span class="st">'svg'</span>, <span class="st">'png'</span>)</span>
<span id="cb34-202"><a href="#cb34-202"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb34-203"><a href="#cb34-203"></a><span class="co"># mpl.rcParams['figure.dpi'] = 400</span></span>
<span id="cb34-204"><a href="#cb34-204"></a><span class="im">from</span> rich <span class="im">import</span> <span class="bu">print</span></span>
<span id="cb34-205"><a href="#cb34-205"></a><span class="in">```</span></span>
<span id="cb34-206"><a href="#cb34-206"></a></span>
<span id="cb34-209"><a href="#cb34-209"></a><span class="in">```{python}</span></span>
<span id="cb34-210"><a href="#cb34-210"></a><span class="co">#| scrolled: true</span></span>
<span id="cb34-211"><a href="#cb34-211"></a><span class="im">from</span> transformers <span class="im">import</span> AutoConfig, AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb34-212"><a href="#cb34-212"></a></span>
<span id="cb34-213"><a href="#cb34-213"></a>input_text <span class="op">=</span> <span class="st">"I got an A+ in my final exam; I am very"</span></span>
<span id="cb34-214"><a href="#cb34-214"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb34-215"><a href="#cb34-215"></a></span>
<span id="cb34-216"><a href="#cb34-216"></a>generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>, model<span class="op">=</span><span class="st">"openai-community/gpt2"</span>)</span>
<span id="cb34-217"><a href="#cb34-217"></a><span class="bu">print</span>(</span>
<span id="cb34-218"><a href="#cb34-218"></a>    [</span>
<span id="cb34-219"><a href="#cb34-219"></a>        i[<span class="st">"generated_text"</span>]</span>
<span id="cb34-220"><a href="#cb34-220"></a>        <span class="cf">for</span> i <span class="kw">in</span> generator(input_text, max_length<span class="op">=</span><span class="dv">20</span>, num_return_sequences<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb34-221"><a href="#cb34-221"></a>    ]</span>
<span id="cb34-222"><a href="#cb34-222"></a>)</span>
<span id="cb34-223"><a href="#cb34-223"></a><span class="in">```</span></span>
<span id="cb34-224"><a href="#cb34-224"></a></span>
<span id="cb34-225"><a href="#cb34-225"></a>We can also train a language model given input data:</span>
<span id="cb34-226"><a href="#cb34-226"></a></span>
<span id="cb34-227"><a href="#cb34-227"></a><span class="fu">## What's going on under the hood?</span></span>
<span id="cb34-228"><a href="#cb34-228"></a>There are two components that are "black-boxes" here:</span>
<span id="cb34-229"><a href="#cb34-229"></a></span>
<span id="cb34-230"><a href="#cb34-230"></a><span class="ss">1. </span>The method for tokenization</span>
<span id="cb34-231"><a href="#cb34-231"></a><span class="ss">2. </span>The model that generates novel text.</span>
<span id="cb34-232"><a href="#cb34-232"></a></span>
<span id="cb34-233"><a href="#cb34-233"></a>Image credit: https://blog.floydhub.com/tokenization-nlp/</span>
<span id="cb34-234"><a href="#cb34-234"></a></span>
<span id="cb34-235"><a href="#cb34-235"></a><span class="fu">### Tokenization</span></span>
<span id="cb34-236"><a href="#cb34-236"></a></span>
<span id="cb34-239"><a href="#cb34-239"></a><span class="in">```{python}</span></span>
<span id="cb34-240"><a href="#cb34-240"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb34-241"><a href="#cb34-241"></a></span>
<span id="cb34-242"><a href="#cb34-242"></a><span class="co"># A utility function to tokenize a sequence and print out some information about it.</span></span>
<span id="cb34-243"><a href="#cb34-243"></a></span>
<span id="cb34-244"><a href="#cb34-244"></a></span>
<span id="cb34-245"><a href="#cb34-245"></a><span class="kw">def</span> tokenization_summary(tokenizer, sequence):</span>
<span id="cb34-246"><a href="#cb34-246"></a>    <span class="co"># get the vocabulary</span></span>
<span id="cb34-247"><a href="#cb34-247"></a>    vocab <span class="op">=</span> tokenizer.vocab</span>
<span id="cb34-248"><a href="#cb34-248"></a>    <span class="co"># Number of entries to print</span></span>
<span id="cb34-249"><a href="#cb34-249"></a>    n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb34-250"><a href="#cb34-250"></a></span>
<span id="cb34-251"><a href="#cb34-251"></a>    <span class="co"># Print subset of the vocabulary</span></span>
<span id="cb34-252"><a href="#cb34-252"></a>    <span class="bu">print</span>(<span class="st">"Subset of tokenizer.vocab:"</span>)</span>
<span id="cb34-253"><a href="#cb34-253"></a>    <span class="cf">for</span> i, (token, index) <span class="kw">in</span> <span class="bu">enumerate</span>(tokenizer.vocab.items()):</span>
<span id="cb34-254"><a href="#cb34-254"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>token<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>index<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-255"><a href="#cb34-255"></a>        <span class="cf">if</span> i <span class="op">&gt;=</span> n <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb34-256"><a href="#cb34-256"></a>            <span class="cf">break</span></span>
<span id="cb34-257"><a href="#cb34-257"></a></span>
<span id="cb34-258"><a href="#cb34-258"></a>    <span class="bu">print</span>(<span class="st">"Vocab size of the tokenizer = "</span>, <span class="bu">len</span>(vocab))</span>
<span id="cb34-259"><a href="#cb34-259"></a>    <span class="bu">print</span>(<span class="st">"------------------------------------------"</span>)</span>
<span id="cb34-260"><a href="#cb34-260"></a></span>
<span id="cb34-261"><a href="#cb34-261"></a>    <span class="co"># .tokenize chunks the existing sequence into different tokens based on the rules and vocab of the tokenizer.</span></span>
<span id="cb34-262"><a href="#cb34-262"></a>    tokens <span class="op">=</span> tokenizer.tokenize(sequence)</span>
<span id="cb34-263"><a href="#cb34-263"></a>    <span class="bu">print</span>(<span class="st">"Tokens : "</span>, tokens)</span>
<span id="cb34-264"><a href="#cb34-264"></a>    <span class="bu">print</span>(<span class="st">"------------------------------------------"</span>)</span>
<span id="cb34-265"><a href="#cb34-265"></a></span>
<span id="cb34-266"><a href="#cb34-266"></a>    <span class="co"># .convert_tokens_to_ids or .encode or .tokenize converts the tokens to their corresponding numerical representation.</span></span>
<span id="cb34-267"><a href="#cb34-267"></a>    <span class="co">#  .convert_tokens_to_ids has a 1-1 mapping between tokens and numerical representation</span></span>
<span id="cb34-268"><a href="#cb34-268"></a>    <span class="co"># ids = tokenizer.convert_tokens_to_ids(tokens)</span></span>
<span id="cb34-269"><a href="#cb34-269"></a>    <span class="co"># print("encoded Ids: ", ids)</span></span>
<span id="cb34-270"><a href="#cb34-270"></a></span>
<span id="cb34-271"><a href="#cb34-271"></a>    <span class="co"># .encode also adds additional information like Start of sequence tokens and End of sequene</span></span>
<span id="cb34-272"><a href="#cb34-272"></a>    <span class="bu">print</span>(<span class="st">"tokenized sequence : "</span>, tokenizer.encode(sequence))</span>
<span id="cb34-273"><a href="#cb34-273"></a></span>
<span id="cb34-274"><a href="#cb34-274"></a>    <span class="co"># .tokenizer has additional information about attention_mask.</span></span>
<span id="cb34-275"><a href="#cb34-275"></a>    <span class="co"># encode = tokenizer(sequence)</span></span>
<span id="cb34-276"><a href="#cb34-276"></a>    <span class="co"># print("Encode sequence : ", encode)</span></span>
<span id="cb34-277"><a href="#cb34-277"></a>    <span class="co"># print("------------------------------------------")</span></span>
<span id="cb34-278"><a href="#cb34-278"></a></span>
<span id="cb34-279"><a href="#cb34-279"></a>    <span class="co"># .decode decodes the ids to raw text</span></span>
<span id="cb34-280"><a href="#cb34-280"></a>    ids <span class="op">=</span> tokenizer.convert_tokens_to_ids(tokens)</span>
<span id="cb34-281"><a href="#cb34-281"></a>    decode <span class="op">=</span> tokenizer.decode(ids)</span>
<span id="cb34-282"><a href="#cb34-282"></a>    <span class="bu">print</span>(<span class="st">"Decode sequence : "</span>, decode)</span>
<span id="cb34-283"><a href="#cb34-283"></a></span>
<span id="cb34-284"><a href="#cb34-284"></a></span>
<span id="cb34-285"><a href="#cb34-285"></a>tokenizer_1 <span class="op">=</span> AutoTokenizer.from_pretrained(</span>
<span id="cb34-286"><a href="#cb34-286"></a>    <span class="st">"gpt2"</span></span>
<span id="cb34-287"><a href="#cb34-287"></a>)  <span class="co"># GPT-2 uses "Byte-Pair Encoding (BPE)"</span></span>
<span id="cb34-288"><a href="#cb34-288"></a></span>
<span id="cb34-289"><a href="#cb34-289"></a>sequence <span class="op">=</span> <span class="st">"I got an A+ in my final exam; I am very"</span></span>
<span id="cb34-290"><a href="#cb34-290"></a></span>
<span id="cb34-291"><a href="#cb34-291"></a>tokenization_summary(tokenizer_1, sequence)</span>
<span id="cb34-292"><a href="#cb34-292"></a><span class="in">```</span></span>
<span id="cb34-293"><a href="#cb34-293"></a></span>
<span id="cb34-294"><a href="#cb34-294"></a><span class="fu">### Token embedding:</span></span>
<span id="cb34-295"><a href="#cb34-295"></a></span>
<span id="cb34-296"><a href="#cb34-296"></a>Words are turned into vectors based on their location within a vocabulary.</span>
<span id="cb34-297"><a href="#cb34-297"></a></span>
<span id="cb34-298"><a href="#cb34-298"></a>The strategy of choice for learning language structure from tokenized text is to find a clever way to map each token into a moderate-dimension vector space, adjusting the mapping so that</span>
<span id="cb34-299"><a href="#cb34-299"></a></span>
<span id="cb34-300"><a href="#cb34-300"></a>Similar, or associated tokens take up residence nearby each other, and different regions of the space correspond to different position in the sequence.</span>
<span id="cb34-301"><a href="#cb34-301"></a>Such a mapping from token ID to a point in a vector space is called a token embedding. The dimension of the vector space is often high (e.g. 1024-dimensional), but much smaller than the vocabulary size (30,000--500,000). </span>
<span id="cb34-302"><a href="#cb34-302"></a></span>
<span id="cb34-303"><a href="#cb34-303"></a>Various approaches have been attempted for generating such embeddings, including static algorithms that operate on a corpus of tokenized data as preprocessors for NLP tasks. Transformers, however, adjust their embeddings during training.</span>
<span id="cb34-304"><a href="#cb34-304"></a></span>
<span id="cb34-305"><a href="#cb34-305"></a><span class="fu">### We can visualize these embeddings of the popular BERT model using PCA!</span></span>
<span id="cb34-306"><a href="#cb34-306"></a></span>
<span id="cb34-309"><a href="#cb34-309"></a><span class="in">```{python}</span></span>
<span id="cb34-310"><a href="#cb34-310"></a><span class="co">#| scrolled: true</span></span>
<span id="cb34-311"><a href="#cb34-311"></a><span class="co"># !pip install umap</span></span>
<span id="cb34-312"><a href="#cb34-312"></a><span class="co"># !pip install plotly</span></span>
<span id="cb34-313"><a href="#cb34-313"></a><span class="co"># !pip install scikit-learn</span></span>
<span id="cb34-314"><a href="#cb34-314"></a><span class="co"># !pip install nltk</span></span>
<span id="cb34-315"><a href="#cb34-315"></a><span class="in">```</span></span>
<span id="cb34-316"><a href="#cb34-316"></a></span>
<span id="cb34-319"><a href="#cb34-319"></a><span class="in">```{python}</span></span>
<span id="cb34-320"><a href="#cb34-320"></a><span class="co">#| scrolled: true</span></span>
<span id="cb34-321"><a href="#cb34-321"></a><span class="im">import</span> nltk</span>
<span id="cb34-322"><a href="#cb34-322"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-323"><a href="#cb34-323"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb34-324"><a href="#cb34-324"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb34-325"><a href="#cb34-325"></a><span class="im">import</span> umap</span>
<span id="cb34-326"><a href="#cb34-326"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb34-327"><a href="#cb34-327"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb34-328"><a href="#cb34-328"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb34-329"><a href="#cb34-329"></a><span class="im">from</span> transformers <span class="im">import</span> BertModel, BertTokenizer</span>
<span id="cb34-330"><a href="#cb34-330"></a></span>
<span id="cb34-331"><a href="#cb34-331"></a>nltk.download(<span class="st">"stopwords"</span>)</span>
<span id="cb34-332"><a href="#cb34-332"></a><span class="im">import</span> torch</span>
<span id="cb34-333"><a href="#cb34-333"></a></span>
<span id="cb34-334"><a href="#cb34-334"></a><span class="co"># Load BERT model and tokenizer</span></span>
<span id="cb34-335"><a href="#cb34-335"></a>model_name <span class="op">=</span> <span class="st">"bert-base-uncased"</span></span>
<span id="cb34-336"><a href="#cb34-336"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(model_name)</span>
<span id="cb34-337"><a href="#cb34-337"></a>model <span class="op">=</span> BertModel.from_pretrained(model_name)</span>
<span id="cb34-338"><a href="#cb34-338"></a></span>
<span id="cb34-339"><a href="#cb34-339"></a><span class="cf">if</span> <span class="va">True</span>:</span>
<span id="cb34-340"><a href="#cb34-340"></a>    text <span class="op">=</span> <span class="st">"The diligent student diligently studied hard for his upcoming exams He was incredibly conscientious in his efforts and committed himself to mastering every subject"</span></span>
<span id="cb34-341"><a href="#cb34-341"></a></span>
<span id="cb34-342"><a href="#cb34-342"></a>    <span class="co"># Tokenize and get BERT embeddings</span></span>
<span id="cb34-343"><a href="#cb34-343"></a>    tokens <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-344"><a href="#cb34-344"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb34-345"><a href="#cb34-345"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>tokens)</span>
<span id="cb34-346"><a href="#cb34-346"></a>        embeddings <span class="op">=</span> outputs.last_hidden_state.squeeze(</span>
<span id="cb34-347"><a href="#cb34-347"></a>            <span class="dv">0</span></span>
<span id="cb34-348"><a href="#cb34-348"></a>        ).numpy()  <span class="co"># Shape: (num_tokens, 768) for BERT-base</span></span>
<span id="cb34-349"><a href="#cb34-349"></a></span>
<span id="cb34-350"><a href="#cb34-350"></a>    <span class="co"># Get the list of token labels without special tokens and subword tokens</span></span>
<span id="cb34-351"><a href="#cb34-351"></a>    labels <span class="op">=</span> [</span>
<span id="cb34-352"><a href="#cb34-352"></a>        tokenizer.convert_ids_to_tokens(<span class="bu">id</span>) <span class="cf">for</span> <span class="bu">id</span> <span class="kw">in</span> tokens.input_ids[<span class="dv">0</span>].tolist()</span>
<span id="cb34-353"><a href="#cb34-353"></a>    ]</span>
<span id="cb34-354"><a href="#cb34-354"></a>    filtered_labels <span class="op">=</span> [</span>
<span id="cb34-355"><a href="#cb34-355"></a>        label</span>
<span id="cb34-356"><a href="#cb34-356"></a>        <span class="cf">for</span> label <span class="kw">in</span> labels</span>
<span id="cb34-357"><a href="#cb34-357"></a>        <span class="cf">if</span> <span class="kw">not</span> (label.startswith(<span class="st">"["</span>) <span class="kw">and</span> label.endswith(<span class="st">"]"</span>)) <span class="kw">and</span> <span class="st">"##"</span> <span class="kw">not</span> <span class="kw">in</span> label</span>
<span id="cb34-358"><a href="#cb34-358"></a>    ]</span>
<span id="cb34-359"><a href="#cb34-359"></a></span>
<span id="cb34-360"><a href="#cb34-360"></a>    <span class="co"># Remove stopwords from labels and embeddings</span></span>
<span id="cb34-361"><a href="#cb34-361"></a>    stop_words <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">"english"</span>))</span>
<span id="cb34-362"><a href="#cb34-362"></a>    filtered_labels <span class="op">=</span> [</span>
<span id="cb34-363"><a href="#cb34-363"></a>        label <span class="cf">for</span> label <span class="kw">in</span> filtered_labels <span class="cf">if</span> label.lower() <span class="kw">not</span> <span class="kw">in</span> stop_words</span>
<span id="cb34-364"><a href="#cb34-364"></a>    ]</span>
<span id="cb34-365"><a href="#cb34-365"></a>    filtered_embeddings <span class="op">=</span> embeddings[: <span class="bu">len</span>(filtered_labels)]</span>
<span id="cb34-366"><a href="#cb34-366"></a></span>
<span id="cb34-367"><a href="#cb34-367"></a>    <span class="co"># Perform PCA for dimensionality reduction (3D)</span></span>
<span id="cb34-368"><a href="#cb34-368"></a>    pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb34-369"><a href="#cb34-369"></a>    embeddings_pca <span class="op">=</span> pca.fit_transform(filtered_embeddings)</span>
<span id="cb34-370"><a href="#cb34-370"></a></span>
<span id="cb34-371"><a href="#cb34-371"></a>    <span class="co"># Convert embeddings and labels to DataFrame for Plotly</span></span>
<span id="cb34-372"><a href="#cb34-372"></a>    data_pca <span class="op">=</span> {</span>
<span id="cb34-373"><a href="#cb34-373"></a>        <span class="st">"x"</span>: embeddings_pca[:, <span class="dv">0</span>],</span>
<span id="cb34-374"><a href="#cb34-374"></a>        <span class="st">"y"</span>: embeddings_pca[:, <span class="dv">1</span>],</span>
<span id="cb34-375"><a href="#cb34-375"></a>        <span class="st">"z"</span>: embeddings_pca[:, <span class="dv">2</span>],</span>
<span id="cb34-376"><a href="#cb34-376"></a>        <span class="st">"label"</span>: filtered_labels,</span>
<span id="cb34-377"><a href="#cb34-377"></a>    }</span>
<span id="cb34-378"><a href="#cb34-378"></a>    df_pca <span class="op">=</span> pd.DataFrame(data_pca)</span>
<span id="cb34-379"><a href="#cb34-379"></a></span>
<span id="cb34-380"><a href="#cb34-380"></a>    <span class="co"># Plot PCA in 3D with Plotly (interactive)</span></span>
<span id="cb34-381"><a href="#cb34-381"></a>    fig_pca <span class="op">=</span> px.scatter_3d(</span>
<span id="cb34-382"><a href="#cb34-382"></a>        df_pca,</span>
<span id="cb34-383"><a href="#cb34-383"></a>        x<span class="op">=</span><span class="st">"x"</span>,</span>
<span id="cb34-384"><a href="#cb34-384"></a>        y<span class="op">=</span><span class="st">"y"</span>,</span>
<span id="cb34-385"><a href="#cb34-385"></a>        z<span class="op">=</span><span class="st">"z"</span>,</span>
<span id="cb34-386"><a href="#cb34-386"></a>        text<span class="op">=</span><span class="st">"label"</span>,</span>
<span id="cb34-387"><a href="#cb34-387"></a>        title<span class="op">=</span><span class="st">"PCA 3D Visualization of Token Embeddings"</span>,</span>
<span id="cb34-388"><a href="#cb34-388"></a>        labels<span class="op">=</span>{<span class="st">"x"</span>: <span class="st">"Dimension 1"</span>, <span class="st">"y"</span>: <span class="st">"Dimension 2"</span>, <span class="st">"z"</span>: <span class="st">"Dimension 3"</span>},</span>
<span id="cb34-389"><a href="#cb34-389"></a>        hover_name<span class="op">=</span><span class="st">"label"</span>,</span>
<span id="cb34-390"><a href="#cb34-390"></a>    )</span>
<span id="cb34-391"><a href="#cb34-391"></a>    fig_pca.update_traces(marker<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">5</span>), textfont<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">8</span>))</span>
<span id="cb34-392"><a href="#cb34-392"></a>    fig_pca.show()</span>
<span id="cb34-393"><a href="#cb34-393"></a><span class="in">```</span></span>
<span id="cb34-394"><a href="#cb34-394"></a></span>
<span id="cb34-395"><a href="#cb34-395"></a>You should see common words grouped together!</span>
<span id="cb34-396"><a href="#cb34-396"></a></span>
<span id="cb34-397"><a href="#cb34-397"></a><span class="fu">## Elements of a Transformer</span></span>
<span id="cb34-398"><a href="#cb34-398"></a></span>
<span id="cb34-399"><a href="#cb34-399"></a>Now let's look at the base elements that make up a Transformer by dissecting the popular GPT2 model</span>
<span id="cb34-400"><a href="#cb34-400"></a></span>
<span id="cb34-403"><a href="#cb34-403"></a><span class="in">```{python}</span></span>
<span id="cb34-404"><a href="#cb34-404"></a><span class="im">from</span> transformers <span class="im">import</span> GPT2LMHeadModel, GPT2Tokenizer</span>
<span id="cb34-405"><a href="#cb34-405"></a></span>
<span id="cb34-406"><a href="#cb34-406"></a>model <span class="op">=</span> GPT2LMHeadModel.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb34-407"><a href="#cb34-407"></a><span class="bu">print</span>(model)</span>
<span id="cb34-408"><a href="#cb34-408"></a><span class="in">```</span></span>
<span id="cb34-409"><a href="#cb34-409"></a></span>
<span id="cb34-410"><a href="#cb34-410"></a>GPT2 is an example of a Transformer Decoder which is used to generate novel text. </span>
<span id="cb34-411"><a href="#cb34-411"></a></span>
<span id="cb34-412"><a href="#cb34-412"></a>Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.</span>
<span id="cb34-413"><a href="#cb34-413"></a></span>
<span id="cb34-414"><a href="#cb34-414"></a>The pretraining of decoder models usually revolves around predicting the next word in the sentence.</span>
<span id="cb34-415"><a href="#cb34-415"></a></span>
<span id="cb34-416"><a href="#cb34-416"></a>These models are best suited for tasks involving text generation.</span>
<span id="cb34-417"><a href="#cb34-417"></a></span>
<span id="cb34-418"><a href="#cb34-418"></a>Examples of these include:</span>
<span id="cb34-419"><a href="#cb34-419"></a><span class="ss">* </span>CTRL</span>
<span id="cb34-420"><a href="#cb34-420"></a><span class="ss">* </span>GPT</span>
<span id="cb34-421"><a href="#cb34-421"></a><span class="ss">* </span>GPT-2</span>
<span id="cb34-422"><a href="#cb34-422"></a><span class="ss">* </span>Transformer XL</span>
<span id="cb34-423"><a href="#cb34-423"></a></span>
<span id="cb34-424"><a href="#cb34-424"></a>Let's discuss one of the most popular models, GPT-2 in a little more detail.</span>
<span id="cb34-425"><a href="#cb34-425"></a></span>
<span id="cb34-426"><a href="#cb34-426"></a>The architecture of GPT-2 is inspired by the paper: "Generating Wikipedia by Summarizing Long Sequences" which is another arrangement of the transformer block that can do language modeling. This model threw away the encoder and thus is known as the “Transformer-Decoder”.</span>
<span id="cb34-427"><a href="#cb34-427"></a></span>
<span id="cb34-428"><a href="#cb34-428"></a><span class="al">![transformer-decoder-intro.png](images/transformer-decoder-intro.png)</span></span>
<span id="cb34-429"><a href="#cb34-429"></a></span>
<span id="cb34-430"><a href="#cb34-430"></a>Image credit: https://jalammar.github.io/illustrated-gpt2/</span>
<span id="cb34-431"><a href="#cb34-431"></a></span>
<span id="cb34-432"><a href="#cb34-432"></a>The Transformer-Decoder is composed of Decoder blocks stacked ontop of each other where each contains two types of layers: </span>
<span id="cb34-433"><a href="#cb34-433"></a><span class="ss">1. </span>Masked Self-Attention and </span>
<span id="cb34-434"><a href="#cb34-434"></a><span class="ss">2. </span>Feed Forward Neural Networks.</span>
<span id="cb34-435"><a href="#cb34-435"></a></span>
<span id="cb34-436"><a href="#cb34-436"></a>In this lecture, we will </span>
<span id="cb34-437"><a href="#cb34-437"></a><span class="ss">* </span>First, discuss attention mechanisms at length as this is arguably the greatest contribution by Transformers.</span>
<span id="cb34-438"><a href="#cb34-438"></a><span class="ss">* </span>Second, extend the discussion from last week (https://github.com/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/Sequential_Data_Models.ipynb) on embedding input data while taking into account position.</span>
<span id="cb34-439"><a href="#cb34-439"></a><span class="ss">* </span>Third, discuss outputting real text/sequences from the models.</span>
<span id="cb34-440"><a href="#cb34-440"></a><span class="ss">* </span>Fourth, build a training loop for a mini-LLM.</span>
<span id="cb34-441"><a href="#cb34-441"></a></span>
<span id="cb34-444"><a href="#cb34-444"></a><span class="in">```{python}</span></span>
<span id="cb34-445"><a href="#cb34-445"></a><span class="co">## IMPORTS</span></span>
<span id="cb34-446"><a href="#cb34-446"></a></span>
<span id="cb34-447"><a href="#cb34-447"></a><span class="im">import</span> torch</span>
<span id="cb34-448"><a href="#cb34-448"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb34-449"><a href="#cb34-449"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb34-450"><a href="#cb34-450"></a></span>
<span id="cb34-451"><a href="#cb34-451"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb34-452"><a href="#cb34-452"></a><span class="co"># hyperparameters</span></span>
<span id="cb34-453"><a href="#cb34-453"></a>batch_size <span class="op">=</span> <span class="dv">16</span>  <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb34-454"><a href="#cb34-454"></a>block_size <span class="op">=</span> <span class="dv">32</span>  <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb34-455"><a href="#cb34-455"></a>max_iters <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb34-456"><a href="#cb34-456"></a>eval_interval <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb34-457"><a href="#cb34-457"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb34-458"><a href="#cb34-458"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb34-459"><a href="#cb34-459"></a>eval_iters <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb34-460"><a href="#cb34-460"></a>n_embd <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb34-461"><a href="#cb34-461"></a>n_head <span class="op">=</span> <span class="dv">4</span>  <span class="co">## so head_size = 16</span></span>
<span id="cb34-462"><a href="#cb34-462"></a>n_layer <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb34-463"><a href="#cb34-463"></a>dropout <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb34-464"><a href="#cb34-464"></a><span class="co"># ------------</span></span>
<span id="cb34-465"><a href="#cb34-465"></a></span>
<span id="cb34-466"><a href="#cb34-466"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb34-467"><a href="#cb34-467"></a><span class="in">```</span></span>
<span id="cb34-468"><a href="#cb34-468"></a></span>
<span id="cb34-469"><a href="#cb34-469"></a><span class="fu">## Attention mechanisms</span></span>
<span id="cb34-470"><a href="#cb34-470"></a></span>
<span id="cb34-471"><a href="#cb34-471"></a>Suppose the following sentence is an input sentence we want to translate using an LLM:</span>
<span id="cb34-472"><a href="#cb34-472"></a></span>
<span id="cb34-473"><a href="#cb34-473"></a><span class="in">`”The animal didn't cross the street because it was too tired”`</span></span>
<span id="cb34-474"><a href="#cb34-474"></a></span>
<span id="cb34-475"><a href="#cb34-475"></a>Earlier, we mentioned that the Transformer learns an embedding of all words allowing interpretation of meanings of words.</span>
<span id="cb34-476"><a href="#cb34-476"></a></span>
<span id="cb34-477"><a href="#cb34-477"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/viz-bert-voc-verbs.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">"Drawing"</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width: 400px;"</span><span class="dt">/&gt;</span></span>
<span id="cb34-478"><a href="#cb34-478"></a></span>
<span id="cb34-479"><a href="#cb34-479"></a>So, if the model did a good job in token embedding, it will "know" what all the words in this sentence mean. </span>
<span id="cb34-480"><a href="#cb34-480"></a></span>
<span id="cb34-481"><a href="#cb34-481"></a>But to understand a full sentence, the model also need to understand what each word means in relation to other words.</span>
<span id="cb34-482"><a href="#cb34-482"></a></span>
<span id="cb34-483"><a href="#cb34-483"></a>For example, when we read the sentence:</span>
<span id="cb34-484"><a href="#cb34-484"></a><span class="in">`”The animal didn't cross the street because it was too tired”`</span></span>
<span id="cb34-485"><a href="#cb34-485"></a>we know intuitively that the word <span class="in">`"it"`</span> refers to <span class="in">`"animal"`</span>, the state for <span class="in">`"it"`</span> is <span class="in">`"tired"`</span>, and the associated action is <span class="in">`"didn't cross"`</span>.</span>
<span id="cb34-486"><a href="#cb34-486"></a></span>
<span id="cb34-487"><a href="#cb34-487"></a>However, the model needs a way to learn all of this information in a simple yet generalizable way.</span>
<span id="cb34-488"><a href="#cb34-488"></a>What makes Transformers particularly powerful compared to earlier sequential architectures is how it encodes context with the **self-attention mechanism**.</span>
<span id="cb34-489"><a href="#cb34-489"></a></span>
<span id="cb34-490"><a href="#cb34-490"></a>As the model processes each word in the input sequence, attention looks at other positions in the input sequence for clues to a better understanding for this word.</span>
<span id="cb34-491"><a href="#cb34-491"></a></span>
<span id="cb34-492"><a href="#cb34-492"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/transformer_self-attention_visualization.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">"Drawing"</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width: 300px;"</span><span class="dt">/&gt;</span></span>
<span id="cb34-493"><a href="#cb34-493"></a></span>
<span id="cb34-494"><a href="#cb34-494"></a>Image credit: https://jalammar.github.io/illustrated-transformer/</span>
<span id="cb34-495"><a href="#cb34-495"></a></span>
<span id="cb34-496"><a href="#cb34-496"></a>Self-attention mechanisms use 3 vectors to encode the context of a word in a sequence with another word:</span>
<span id="cb34-497"><a href="#cb34-497"></a><span class="ss">1. </span>Query: the word representation we score other words against using the other word's keys</span>
<span id="cb34-498"><a href="#cb34-498"></a><span class="ss">2. </span>Key: labels for the words in a sequence that we match against the query</span>
<span id="cb34-499"><a href="#cb34-499"></a><span class="ss">3. </span>Value: actual word representation. We will use the queries and keys to score the word's relevance to the query, and multiply this by the value. </span>
<span id="cb34-500"><a href="#cb34-500"></a></span>
<span id="cb34-501"><a href="#cb34-501"></a>An analogy provided by Jay Alammar is thinking about attention as choosing a file from a file cabinet according to information on a post-it note. You can use the post-it note (query) to identify the folder (key) that most matches the topic you are looking up. Then you access the contents of the file (value) according to its relevance to your query.</span>
<span id="cb34-502"><a href="#cb34-502"></a></span>
<span id="cb34-503"><a href="#cb34-503"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/self-attention-example-folders-3.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">"Drawing"</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width: 500px;"</span><span class="dt">/&gt;</span></span>
<span id="cb34-504"><a href="#cb34-504"></a>Image credit: https://jalammar.github.io/illustrated-gpt2/</span>
<span id="cb34-505"><a href="#cb34-505"></a></span>
<span id="cb34-506"><a href="#cb34-506"></a>In our models, we can encode queries, keys, and values using simple linear layers with the same size (<span class="in">`sequence length, head_size`</span>). During the training process, these layers will be updated to best encode context.</span>
<span id="cb34-507"><a href="#cb34-507"></a></span>
<span id="cb34-510"><a href="#cb34-510"></a><span class="in">```{python}</span></span>
<span id="cb34-511"><a href="#cb34-511"></a>C <span class="op">=</span> <span class="dv">32</span>  <span class="co"># channels</span></span>
<span id="cb34-512"><a href="#cb34-512"></a>head_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb34-513"><a href="#cb34-513"></a></span>
<span id="cb34-514"><a href="#cb34-514"></a>key <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb34-515"><a href="#cb34-515"></a>query <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb34-516"><a href="#cb34-516"></a>value <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb34-517"><a href="#cb34-517"></a><span class="in">```</span></span>
<span id="cb34-518"><a href="#cb34-518"></a></span>
<span id="cb34-519"><a href="#cb34-519"></a>The algorithm for self-attention is as follows:</span>
<span id="cb34-520"><a href="#cb34-520"></a></span>
<span id="cb34-521"><a href="#cb34-521"></a><span class="ss">1. </span>Generate query, key and value vectors for each word</span>
<span id="cb34-522"><a href="#cb34-522"></a><span class="ss">2. </span>Calculate a score for each word in the input sentence against each other.</span>
<span id="cb34-523"><a href="#cb34-523"></a><span class="ss">3. </span>Divide the scores by the square root of the dimension of the key vectors to stabilize the gradients. This is then passed through a softmax operation.</span>
<span id="cb34-524"><a href="#cb34-524"></a><span class="ss">4. </span>Multiply each value vector by the softmax score.</span>
<span id="cb34-525"><a href="#cb34-525"></a><span class="ss">5. </span>Sum up the weighted value vectors to produce the output.</span>
<span id="cb34-526"><a href="#cb34-526"></a></span>
<span id="cb34-527"><a href="#cb34-527"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/self-attention-output.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">"Drawing"</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width: 450px;"</span><span class="dt">/&gt;</span></span>
<span id="cb34-528"><a href="#cb34-528"></a></span>
<span id="cb34-529"><a href="#cb34-529"></a>Image credit: https://jalammar.github.io/illustrated-transformer/</span>
<span id="cb34-530"><a href="#cb34-530"></a></span>
<span id="cb34-531"><a href="#cb34-531"></a>Let's see how attention is performed in the code.</span>
<span id="cb34-532"><a href="#cb34-532"></a></span>
<span id="cb34-535"><a href="#cb34-535"></a><span class="in">```{python}</span></span>
<span id="cb34-536"><a href="#cb34-536"></a><span class="im">import</span> torch</span>
<span id="cb34-537"><a href="#cb34-537"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb34-538"><a href="#cb34-538"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb34-539"><a href="#cb34-539"></a></span>
<span id="cb34-540"><a href="#cb34-540"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb34-541"><a href="#cb34-541"></a>B, T, C <span class="op">=</span> <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">32</span>  <span class="co"># batch, time, channels</span></span>
<span id="cb34-542"><a href="#cb34-542"></a>x <span class="op">=</span> torch.randn(B, T, C)</span>
<span id="cb34-543"><a href="#cb34-543"></a></span>
<span id="cb34-544"><a href="#cb34-544"></a><span class="co"># Here we want the wei to be data dependent - ie gather info from the past but in a data dependant way</span></span>
<span id="cb34-545"><a href="#cb34-545"></a></span>
<span id="cb34-546"><a href="#cb34-546"></a>head_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb34-547"><a href="#cb34-547"></a>key <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb34-548"><a href="#cb34-548"></a>query <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb34-549"><a href="#cb34-549"></a>value <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb34-550"><a href="#cb34-550"></a>k <span class="op">=</span> key(</span>
<span id="cb34-551"><a href="#cb34-551"></a>    x</span>
<span id="cb34-552"><a href="#cb34-552"></a>)  <span class="co"># (B, T, 16) # each token here (totally B*T) produce a key and query in parallel and independently</span></span>
<span id="cb34-553"><a href="#cb34-553"></a>q <span class="op">=</span> query(x)  <span class="co"># (B, T, 16)</span></span>
<span id="cb34-554"><a href="#cb34-554"></a>v <span class="op">=</span> value(x)</span>
<span id="cb34-555"><a href="#cb34-555"></a></span>
<span id="cb34-556"><a href="#cb34-556"></a>wei <span class="op">=</span> (</span>
<span id="cb34-557"><a href="#cb34-557"></a>    q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> head_size<span class="op">**-</span><span class="fl">0.5</span></span>
<span id="cb34-558"><a href="#cb34-558"></a>)  <span class="co"># (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T). #</span></span>
<span id="cb34-559"><a href="#cb34-559"></a>wei <span class="op">=</span> F.softmax(</span>
<span id="cb34-560"><a href="#cb34-560"></a>    wei, dim<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb34-561"><a href="#cb34-561"></a>)  <span class="co"># exponentiate and normalize giving a nice distibution that sums to 1 and</span></span>
<span id="cb34-562"><a href="#cb34-562"></a><span class="co"># now it tells us that in a data dependent manner how much of info to aggregate from</span></span>
<span id="cb34-563"><a href="#cb34-563"></a></span>
<span id="cb34-564"><a href="#cb34-564"></a>out <span class="op">=</span> wei <span class="op">@</span> v  <span class="co"># aggregate the attention scores and value vector.</span></span>
<span id="cb34-565"><a href="#cb34-565"></a><span class="in">```</span></span>
<span id="cb34-566"><a href="#cb34-566"></a></span>
<span id="cb34-569"><a href="#cb34-569"></a><span class="in">```{python}</span></span>
<span id="cb34-570"><a href="#cb34-570"></a><span class="bu">print</span>(out[<span class="dv">0</span>])</span>
<span id="cb34-571"><a href="#cb34-571"></a><span class="in">```</span></span>
<span id="cb34-572"><a href="#cb34-572"></a></span>
<span id="cb34-573"><a href="#cb34-573"></a><span class="fu">### Multi-head attention</span></span>
<span id="cb34-574"><a href="#cb34-574"></a></span>
<span id="cb34-575"><a href="#cb34-575"></a>In practice, multiple attention heads are used which</span>
<span id="cb34-576"><a href="#cb34-576"></a><span class="ss">1. </span>Expands the model’s ability to focus on different positions and prevent the attention to be dominated by the word itself.</span>
<span id="cb34-577"><a href="#cb34-577"></a><span class="ss">2. </span>Have multiple “representation subspaces”. Have multiple sets of Query/Key/Value weight matrices</span>
<span id="cb34-578"><a href="#cb34-578"></a></span>
<span id="cb34-579"><a href="#cb34-579"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/transformer_multi-headed_self-attention-recap.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">"Drawing"</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width: 700px;"</span><span class="dt">/&gt;</span></span>
<span id="cb34-580"><a href="#cb34-580"></a></span>
<span id="cb34-581"><a href="#cb34-581"></a>Image credit: https://jalammar.github.io/illustrated-transformer/</span>
<span id="cb34-582"><a href="#cb34-582"></a></span>
<span id="cb34-583"><a href="#cb34-583"></a><span class="fu">### Let's see attention mechanisms in action!</span></span>
<span id="cb34-584"><a href="#cb34-584"></a></span>
<span id="cb34-585"><a href="#cb34-585"></a>We are going to use the powerful visualization tool bertviz, which allows an interactive experience of the attention mechanisms. Normally these mechanisms are abstracted away but this will allow us to inspect our model in more detail.</span>
<span id="cb34-586"><a href="#cb34-586"></a></span>
<span id="cb34-589"><a href="#cb34-589"></a><span class="in">```{python}</span></span>
<span id="cb34-590"><a href="#cb34-590"></a><span class="co">#| scrolled: true</span></span>
<span id="cb34-591"><a href="#cb34-591"></a><span class="op">!</span>pip install bertviz</span>
<span id="cb34-592"><a href="#cb34-592"></a><span class="in">```</span></span>
<span id="cb34-593"><a href="#cb34-593"></a></span>
<span id="cb34-594"><a href="#cb34-594"></a>Let's load in the model, GPT2 and look at the attention mechanisms. </span>
<span id="cb34-595"><a href="#cb34-595"></a></span>
<span id="cb34-596"><a href="#cb34-596"></a>**Hint... click on the different blocks in the visualization to see the attention**</span>
<span id="cb34-597"><a href="#cb34-597"></a></span>
<span id="cb34-600"><a href="#cb34-600"></a><span class="in">```{python}</span></span>
<span id="cb34-601"><a href="#cb34-601"></a><span class="co">#| scrolled: true</span></span>
<span id="cb34-602"><a href="#cb34-602"></a><span class="im">from</span> bertviz <span class="im">import</span> model_view</span>
<span id="cb34-603"><a href="#cb34-603"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoModelForCausalLM, AutoTokenizer, utils</span>
<span id="cb34-604"><a href="#cb34-604"></a></span>
<span id="cb34-605"><a href="#cb34-605"></a>utils.logging.set_verbosity_error()  <span class="co"># Suppress standard warnings</span></span>
<span id="cb34-606"><a href="#cb34-606"></a></span>
<span id="cb34-607"><a href="#cb34-607"></a>model_name <span class="op">=</span> <span class="st">"openai-community/gpt2"</span></span>
<span id="cb34-608"><a href="#cb34-608"></a>input_text <span class="op">=</span> <span class="st">"The animal didn't cross the street because it was too tired"</span></span>
<span id="cb34-609"><a href="#cb34-609"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_name, output_attentions<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-610"><a href="#cb34-610"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb34-611"><a href="#cb34-611"></a>inputs <span class="op">=</span> tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)  <span class="co"># Tokenize input text</span></span>
<span id="cb34-612"><a href="#cb34-612"></a>outputs <span class="op">=</span> model(inputs)  <span class="co"># Run model</span></span>
<span id="cb34-613"><a href="#cb34-613"></a>attention <span class="op">=</span> outputs[<span class="op">-</span><span class="dv">1</span>]  <span class="co"># Retrieve attention from model outputs</span></span>
<span id="cb34-614"><a href="#cb34-614"></a>tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(</span>
<span id="cb34-615"><a href="#cb34-615"></a>    inputs[<span class="dv">0</span>]</span>
<span id="cb34-616"><a href="#cb34-616"></a>)  <span class="co"># Convert input ids to token strings</span></span>
<span id="cb34-617"><a href="#cb34-617"></a>model_view(attention, tokens)  <span class="co"># Display model view</span></span>
<span id="cb34-618"><a href="#cb34-618"></a><span class="in">```</span></span>
<span id="cb34-619"><a href="#cb34-619"></a></span>
<span id="cb34-620"><a href="#cb34-620"></a><span class="fu">## Positional encoding</span></span>
<span id="cb34-621"><a href="#cb34-621"></a></span>
<span id="cb34-622"><a href="#cb34-622"></a>We just discussed attention mechanisms which account for context between words. Another question we should ask is how do we account for the order of words in an input sentence</span>
<span id="cb34-623"><a href="#cb34-623"></a></span>
<span id="cb34-624"><a href="#cb34-624"></a>Consider the following two sentences to see why this is important:</span>
<span id="cb34-625"><a href="#cb34-625"></a></span>
<span id="cb34-626"><a href="#cb34-626"></a>``The man ate the sandwich.``</span>
<span id="cb34-627"><a href="#cb34-627"></a></span>
<span id="cb34-628"><a href="#cb34-628"></a>``The sandwich ate the man.``</span>
<span id="cb34-629"><a href="#cb34-629"></a></span>
<span id="cb34-630"><a href="#cb34-630"></a>Clearly, these are two vastly different situations even though they have the same words. The Transformer can </span>
<span id="cb34-631"><a href="#cb34-631"></a></span>
<span id="cb34-632"><a href="#cb34-632"></a>Transformers differentiate between these situations by adding a **Positional encoding** vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word.</span>
<span id="cb34-633"><a href="#cb34-633"></a></span>
<span id="cb34-634"><a href="#cb34-634"></a>::: {#fig-positional-encoding}</span>
<span id="cb34-635"><a href="#cb34-635"></a><span class="al">![](images/positional_encoding.png)</span></span>
<span id="cb34-636"><a href="#cb34-636"></a></span>
<span id="cb34-637"><a href="#cb34-637"></a>Positional encoding</span>
<span id="cb34-638"><a href="#cb34-638"></a>:::</span>
<span id="cb34-639"><a href="#cb34-639"></a></span>
<span id="cb34-640"><a href="#cb34-640"></a>Image credit: <span class="ot">&lt;https://medium.com/@xuer.chen.human/llm-study-notes-positional-encoding-0639a1002ec0&gt;</span></span>
<span id="cb34-641"><a href="#cb34-641"></a></span>
<span id="cb34-642"><a href="#cb34-642"></a>We set up positional encoding similarly as token embedding using the ``nn.Embedding`` tool. We use a simple embedding here but there are more complex positional encodings used such as sinusoidal. </span>
<span id="cb34-643"><a href="#cb34-643"></a></span>
<span id="cb34-644"><a href="#cb34-644"></a>For an explanation of different positional encodings, refer to this post: https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/</span>
<span id="cb34-645"><a href="#cb34-645"></a></span>
<span id="cb34-648"><a href="#cb34-648"></a><span class="in">```{python}</span></span>
<span id="cb34-649"><a href="#cb34-649"></a>vocab_size <span class="op">=</span> <span class="dv">65</span></span>
<span id="cb34-650"><a href="#cb34-650"></a>n_embd <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb34-651"><a href="#cb34-651"></a></span>
<span id="cb34-652"><a href="#cb34-652"></a>token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb34-653"><a href="#cb34-653"></a>block_size <span class="op">=</span> <span class="dv">32</span>  <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb34-654"><a href="#cb34-654"></a>position_embedding_table <span class="op">=</span> nn.Embedding(block_size, n_embd)</span>
<span id="cb34-655"><a href="#cb34-655"></a><span class="in">```</span></span>
<span id="cb34-656"><a href="#cb34-656"></a></span>
<span id="cb34-657"><a href="#cb34-657"></a>You will notice the positional encoding size is <span class="in">`(block_size, n_embed)`</span> because it encodes for the postion of a token within the sequence of size <span class="in">`block_size`</span></span>
<span id="cb34-658"><a href="#cb34-658"></a></span>
<span id="cb34-659"><a href="#cb34-659"></a>Then, the position embedding used is simply added to the token embedding to apply positional embedding.</span>
<span id="cb34-660"><a href="#cb34-660"></a></span>
<span id="cb34-661"><a href="#cb34-661"></a>Let's look at token embedding alone:</span>
<span id="cb34-662"><a href="#cb34-662"></a></span>
<span id="cb34-665"><a href="#cb34-665"></a><span class="in">```{python}</span></span>
<span id="cb34-666"><a href="#cb34-666"></a>x <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">15</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">9</span>])</span>
<span id="cb34-667"><a href="#cb34-667"></a>x <span class="op">=</span> token_embedding_table(x)</span>
<span id="cb34-668"><a href="#cb34-668"></a><span class="bu">print</span>(x[<span class="dv">0</span>])</span>
<span id="cb34-669"><a href="#cb34-669"></a><span class="in">```</span></span>
<span id="cb34-670"><a href="#cb34-670"></a></span>
<span id="cb34-671"><a href="#cb34-671"></a>And token + positional embeddings:</span>
<span id="cb34-672"><a href="#cb34-672"></a></span>
<span id="cb34-675"><a href="#cb34-675"></a><span class="in">```{python}</span></span>
<span id="cb34-676"><a href="#cb34-676"></a>x <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">15</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">9</span>])</span>
<span id="cb34-677"><a href="#cb34-677"></a>x <span class="op">=</span> position_embedding_table(x) <span class="op">+</span> token_embedding_table(x)</span>
<span id="cb34-678"><a href="#cb34-678"></a><span class="bu">print</span>(x[<span class="dv">0</span>])</span>
<span id="cb34-679"><a href="#cb34-679"></a><span class="in">```</span></span>
<span id="cb34-680"><a href="#cb34-680"></a></span>
<span id="cb34-681"><a href="#cb34-681"></a>You can see a clear offset between these two embeddings.</span>
<span id="cb34-682"><a href="#cb34-682"></a></span>
<span id="cb34-683"><a href="#cb34-683"></a>During the training process, these embeddings will be learned to best encode the token and positional embeddings of the sequences.</span>
<span id="cb34-684"><a href="#cb34-684"></a></span>
<span id="cb34-685"><a href="#cb34-685"></a><span class="fu">## Output layers</span></span>
<span id="cb34-686"><a href="#cb34-686"></a></span>
<span id="cb34-687"><a href="#cb34-687"></a>At the end of our Transformer model, we are left with a vector, so how do we turn this into a word?</span>
<span id="cb34-688"><a href="#cb34-688"></a></span>
<span id="cb34-689"><a href="#cb34-689"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/transformer-decoder-intro.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">"Drawing"</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width: 400px;"</span><span class="dt">/&gt;</span></span>
<span id="cb34-690"><a href="#cb34-690"></a></span>
<span id="cb34-691"><a href="#cb34-691"></a>Using a final Linear layer and a Softmax Layer.</span>
<span id="cb34-692"><a href="#cb34-692"></a>The Linear layer projects the vector produced by the stack of decoders, into a larger vector called a logits vector.</span>
<span id="cb34-693"><a href="#cb34-693"></a></span>
<span id="cb34-694"><a href="#cb34-694"></a>If our model knows 10,000 unique English words learned from its training dataset the logits vector is 10,000 cells wide – each cell corresponds to the score of a unique word.</span>
<span id="cb34-695"><a href="#cb34-695"></a></span>
<span id="cb34-696"><a href="#cb34-696"></a>The softmax layer turns those scores into probabilities. The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.</span>
<span id="cb34-697"><a href="#cb34-697"></a></span>
<span id="cb34-698"><a href="#cb34-698"></a>::: {#fig-transformer-decoder-output-softmax}</span>
<span id="cb34-699"><a href="#cb34-699"></a></span>
<span id="cb34-700"><a href="#cb34-700"></a><span class="al">![](images/transformer_decoder_output_softmax.png)</span></span>
<span id="cb34-701"><a href="#cb34-701"></a></span>
<span id="cb34-702"><a href="#cb34-702"></a>Transformer decoder output softmax</span>
<span id="cb34-703"><a href="#cb34-703"></a>:::</span>
<span id="cb34-704"><a href="#cb34-704"></a></span>
<span id="cb34-705"><a href="#cb34-705"></a>Image credit: https://jalammar.github.io/illustrated-transformer/</span>
<span id="cb34-706"><a href="#cb34-706"></a></span>
<span id="cb34-707"><a href="#cb34-707"></a><span class="fu">## Training</span></span>
<span id="cb34-708"><a href="#cb34-708"></a></span>
<span id="cb34-709"><a href="#cb34-709"></a>How does an LLM improve over time?</span>
<span id="cb34-710"><a href="#cb34-710"></a>We want to compare the probabilitiy distribution for each token generated by our model to the ground truths.</span>
<span id="cb34-711"><a href="#cb34-711"></a>Our model produces a probability distribution for each token. We want to compare these probability distributions to the ground truths.</span>
<span id="cb34-712"><a href="#cb34-712"></a>For example, when translating the sentence: “je suis étudiant” into “i am a student” as can be seen in the example:</span>
<span id="cb34-713"><a href="#cb34-713"></a></span>
<span id="cb34-714"><a href="#cb34-714"></a>::: {#fig-output-target-probability-distribution}</span>
<span id="cb34-715"><a href="#cb34-715"></a></span>
<span id="cb34-716"><a href="#cb34-716"></a><span class="al">![](images/output_target_probability_distributions.png)</span></span>
<span id="cb34-717"><a href="#cb34-717"></a></span>
<span id="cb34-718"><a href="#cb34-718"></a>Output target probability distributions</span>
<span id="cb34-719"><a href="#cb34-719"></a>:::</span>
<span id="cb34-720"><a href="#cb34-720"></a></span>
<span id="cb34-721"><a href="#cb34-721"></a>Image credit: https://jalammar.github.io/illustrated-transformer/</span>
<span id="cb34-722"><a href="#cb34-722"></a></span>
<span id="cb34-723"><a href="#cb34-723"></a>The model can calculate the loss between the vector it generates and the ground truth vector seen in this example. A commonly used loss function is cross entropy loss:</span>
<span id="cb34-724"><a href="#cb34-724"></a></span>
<span id="cb34-725"><a href="#cb34-725"></a>$$CE = -\sum_{x \in X} p(x) log q(x)$$</span>
<span id="cb34-726"><a href="#cb34-726"></a></span>
<span id="cb34-727"><a href="#cb34-727"></a>where p(x) represents the true distribution and q(x) represents the predicted distribution.</span>
<span id="cb34-728"><a href="#cb34-728"></a></span>
<span id="cb34-731"><a href="#cb34-731"></a><span class="in">```{python}</span></span>
<span id="cb34-732"><a href="#cb34-732"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb34-733"><a href="#cb34-733"></a></span>
<span id="cb34-734"><a href="#cb34-734"></a>logits <span class="op">=</span> torch.tensor([<span class="fl">0.5</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>])</span>
<span id="cb34-735"><a href="#cb34-735"></a>targets <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>])</span>
<span id="cb34-736"><a href="#cb34-736"></a>loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb34-737"><a href="#cb34-737"></a><span class="bu">print</span>(loss)</span>
<span id="cb34-738"><a href="#cb34-738"></a><span class="in">```</span></span>
<span id="cb34-739"><a href="#cb34-739"></a></span>
<span id="cb34-740"><a href="#cb34-740"></a>Another important metric commonly used in LLMs is **perplexity**.</span>
<span id="cb34-741"><a href="#cb34-741"></a></span>
<span id="cb34-742"><a href="#cb34-742"></a>Intuitively, perplexity means to be surprised. We measure how much the model is surprised by seeing new data. The lower the perplexity, the better the training is.</span>
<span id="cb34-743"><a href="#cb34-743"></a></span>
<span id="cb34-744"><a href="#cb34-744"></a>Mathematically, perplexity is just the exponent of the negative cross entropy loss:</span>
<span id="cb34-745"><a href="#cb34-745"></a></span>
<span id="cb34-746"><a href="#cb34-746"></a>$$\text{perplexity} = exp(\text{CE})$$</span>
<span id="cb34-747"><a href="#cb34-747"></a></span>
<span id="cb34-750"><a href="#cb34-750"></a><span class="in">```{python}</span></span>
<span id="cb34-751"><a href="#cb34-751"></a>perplexity <span class="op">=</span> torch.exp(loss)</span>
<span id="cb34-752"><a href="#cb34-752"></a><span class="bu">print</span>(perplexity)</span>
<span id="cb34-753"><a href="#cb34-753"></a><span class="in">```</span></span>
<span id="cb34-754"><a href="#cb34-754"></a></span>
<span id="cb34-755"><a href="#cb34-755"></a><span class="fu">## Let's train a mini-LLM from scratch</span></span>
<span id="cb34-756"><a href="#cb34-756"></a></span>
<span id="cb34-757"><a href="#cb34-757"></a><span class="fu">### Set up hyperparameters:</span></span>
<span id="cb34-758"><a href="#cb34-758"></a></span>
<span id="cb34-761"><a href="#cb34-761"></a><span class="in">```{python}</span></span>
<span id="cb34-762"><a href="#cb34-762"></a><span class="co"># hyperparameters</span></span>
<span id="cb34-763"><a href="#cb34-763"></a>batch_size <span class="op">=</span> <span class="dv">4</span>  <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb34-764"><a href="#cb34-764"></a>block_size <span class="op">=</span> <span class="dv">32</span>  <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb34-765"><a href="#cb34-765"></a>max_iters <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb34-766"><a href="#cb34-766"></a>eval_interval <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb34-767"><a href="#cb34-767"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb34-768"><a href="#cb34-768"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb34-769"><a href="#cb34-769"></a>eval_iters <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb34-770"><a href="#cb34-770"></a>n_embd <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb34-771"><a href="#cb34-771"></a>n_head <span class="op">=</span> <span class="dv">4</span>  <span class="co">## so head_size = 16</span></span>
<span id="cb34-772"><a href="#cb34-772"></a>n_layer <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb34-773"><a href="#cb34-773"></a>dropout <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb34-774"><a href="#cb34-774"></a><span class="co"># ------------</span></span>
<span id="cb34-775"><a href="#cb34-775"></a><span class="in">```</span></span>
<span id="cb34-776"><a href="#cb34-776"></a></span>
<span id="cb34-777"><a href="#cb34-777"></a><span class="fu">### Load in data and create train and test datasets</span></span>
<span id="cb34-778"><a href="#cb34-778"></a></span>
<span id="cb34-779"><a href="#cb34-779"></a>We're going to be using the tiny Shakespeare dataset.</span>
<span id="cb34-780"><a href="#cb34-780"></a>Data is tokenized according to a simple character based tokenizer.</span>
<span id="cb34-781"><a href="#cb34-781"></a>Data is split into a train and test set so we have something to test after performing training (9:1 split).</span>
<span id="cb34-782"><a href="#cb34-782"></a></span>
<span id="cb34-785"><a href="#cb34-785"></a><span class="in">```{python}</span></span>
<span id="cb34-786"><a href="#cb34-786"></a><span class="op">!</span> [ <span class="op">!</span> <span class="op">-</span>f <span class="st">"input.txt"</span> ] <span class="op">&amp;&amp;</span> wget https:<span class="op">//</span>raw.githubusercontent.com<span class="op">/</span>argonne<span class="op">-</span>lcf<span class="op">/</span>ATPESC_MachineLearning<span class="op">/</span>refs<span class="op">/</span>heads<span class="op">/</span>master<span class="op">/</span><span class="dv">0</span><span class="er">2_intro_to_LLMs</span><span class="op">/</span>dataset<span class="op">/</span><span class="bu">input</span>.txt</span>
<span id="cb34-787"><a href="#cb34-787"></a><span class="in">```</span></span>
<span id="cb34-788"><a href="#cb34-788"></a></span>
<span id="cb34-791"><a href="#cb34-791"></a><span class="in">```{python}</span></span>
<span id="cb34-792"><a href="#cb34-792"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"input.txt"</span>, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb34-793"><a href="#cb34-793"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb34-794"><a href="#cb34-794"></a></span>
<span id="cb34-795"><a href="#cb34-795"></a><span class="co"># here are all the unique characters that occur in this text</span></span>
<span id="cb34-796"><a href="#cb34-796"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb34-797"><a href="#cb34-797"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb34-798"><a href="#cb34-798"></a><span class="co"># create a mapping from characters to integers</span></span>
<span id="cb34-799"><a href="#cb34-799"></a>stoi <span class="op">=</span> {ch: i <span class="cf">for</span> i, ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb34-800"><a href="#cb34-800"></a>itos <span class="op">=</span> {i: ch <span class="cf">for</span> i, ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb34-801"><a href="#cb34-801"></a>encode <span class="op">=</span> <span class="kw">lambda</span> s: [</span>
<span id="cb34-802"><a href="#cb34-802"></a>    stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s</span>
<span id="cb34-803"><a href="#cb34-803"></a>]  <span class="co"># encoder: take a string, output a list of integers</span></span>
<span id="cb34-804"><a href="#cb34-804"></a>decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">""</span>.join(</span>
<span id="cb34-805"><a href="#cb34-805"></a>    [itos[i] <span class="cf">for</span> i <span class="kw">in</span> l]</span>
<span id="cb34-806"><a href="#cb34-806"></a>)  <span class="co"># decoder: take a list of integers, output a string</span></span>
<span id="cb34-807"><a href="#cb34-807"></a></span>
<span id="cb34-808"><a href="#cb34-808"></a><span class="co"># Train and test splits</span></span>
<span id="cb34-809"><a href="#cb34-809"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb34-810"><a href="#cb34-810"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span> <span class="op">*</span> <span class="bu">len</span>(data))  <span class="co"># first 90% will be train, rest val</span></span>
<span id="cb34-811"><a href="#cb34-811"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb34-812"><a href="#cb34-812"></a>val_data <span class="op">=</span> data[n:]</span>
<span id="cb34-813"><a href="#cb34-813"></a></span>
<span id="cb34-814"><a href="#cb34-814"></a></span>
<span id="cb34-815"><a href="#cb34-815"></a><span class="co"># data loading</span></span>
<span id="cb34-816"><a href="#cb34-816"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb34-817"><a href="#cb34-817"></a>    <span class="co"># generate a small batch of data of inputs x and targets y</span></span>
<span id="cb34-818"><a href="#cb34-818"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">"train"</span> <span class="cf">else</span> val_data</span>
<span id="cb34-819"><a href="#cb34-819"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb34-820"><a href="#cb34-820"></a>    x <span class="op">=</span> torch.stack([data[i : i <span class="op">+</span> block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb34-821"><a href="#cb34-821"></a>    y <span class="op">=</span> torch.stack([data[i <span class="op">+</span> <span class="dv">1</span> : i <span class="op">+</span> block_size <span class="op">+</span> <span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb34-822"><a href="#cb34-822"></a>    x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb34-823"><a href="#cb34-823"></a>    <span class="cf">return</span> x, y</span>
<span id="cb34-824"><a href="#cb34-824"></a><span class="in">```</span></span>
<span id="cb34-825"><a href="#cb34-825"></a></span>
<span id="cb34-828"><a href="#cb34-828"></a><span class="in">```{python}</span></span>
<span id="cb34-829"><a href="#cb34-829"></a><span class="co">#| scrolled: true</span></span>
<span id="cb34-830"><a href="#cb34-830"></a><span class="bu">print</span>(text[:<span class="dv">1000</span>])</span>
<span id="cb34-831"><a href="#cb34-831"></a><span class="in">```</span></span>
<span id="cb34-832"><a href="#cb34-832"></a></span>
<span id="cb34-833"><a href="#cb34-833"></a><span class="fu">### Set up the components of the Decoder block: </span></span>
<span id="cb34-834"><a href="#cb34-834"></a><span class="ss">* </span>MultiHeadAttention</span>
<span id="cb34-835"><a href="#cb34-835"></a><span class="ss">* </span>FeedForward Network</span>
<span id="cb34-836"><a href="#cb34-836"></a></span>
<span id="cb34-839"><a href="#cb34-839"></a><span class="in">```{python}</span></span>
<span id="cb34-840"><a href="#cb34-840"></a><span class="kw">class</span> Head(nn.Module):</span>
<span id="cb34-841"><a href="#cb34-841"></a>    <span class="co">"""one head of self-attention"""</span></span>
<span id="cb34-842"><a href="#cb34-842"></a></span>
<span id="cb34-843"><a href="#cb34-843"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, head_size):</span>
<span id="cb34-844"><a href="#cb34-844"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-845"><a href="#cb34-845"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb34-846"><a href="#cb34-846"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb34-847"><a href="#cb34-847"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb34-848"><a href="#cb34-848"></a>        <span class="va">self</span>.register_buffer(<span class="st">"tril"</span>, torch.tril(torch.ones(block_size, block_size)))</span>
<span id="cb34-849"><a href="#cb34-849"></a></span>
<span id="cb34-850"><a href="#cb34-850"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb34-851"><a href="#cb34-851"></a></span>
<span id="cb34-852"><a href="#cb34-852"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-853"><a href="#cb34-853"></a>        B, T, C <span class="op">=</span> x.shape</span>
<span id="cb34-854"><a href="#cb34-854"></a>        k <span class="op">=</span> <span class="va">self</span>.key(x)  <span class="co"># (B,T,C) 16,32,16</span></span>
<span id="cb34-855"><a href="#cb34-855"></a>        q <span class="op">=</span> <span class="va">self</span>.query(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb34-856"><a href="#cb34-856"></a>        <span class="co"># compute attention scores ("affinities")</span></span>
<span id="cb34-857"><a href="#cb34-857"></a>        wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> C<span class="op">**-</span><span class="fl">0.5</span>  <span class="co"># (B, T, C) @ (B, C, T) -&gt; (B, T, T)</span></span>
<span id="cb34-858"><a href="#cb34-858"></a>        wei <span class="op">=</span> wei.masked_fill(<span class="va">self</span>.tril[:T, :T] <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">"-inf"</span>))  <span class="co"># (B, T, T)</span></span>
<span id="cb34-859"><a href="#cb34-859"></a>        wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, T, T)</span></span>
<span id="cb34-860"><a href="#cb34-860"></a>        wei <span class="op">=</span> <span class="va">self</span>.dropout(wei)</span>
<span id="cb34-861"><a href="#cb34-861"></a>        <span class="co"># perform the weighted aggregation of the values</span></span>
<span id="cb34-862"><a href="#cb34-862"></a>        v <span class="op">=</span> <span class="va">self</span>.value(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb34-863"><a href="#cb34-863"></a>        out <span class="op">=</span> wei <span class="op">@</span> v  <span class="co"># (B, T, T) @ (B, T, C) -&gt; (B, T, C)</span></span>
<span id="cb34-864"><a href="#cb34-864"></a>        <span class="cf">return</span> out</span>
<span id="cb34-865"><a href="#cb34-865"></a></span>
<span id="cb34-866"><a href="#cb34-866"></a></span>
<span id="cb34-867"><a href="#cb34-867"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb34-868"><a href="#cb34-868"></a>    <span class="co">"""multiple heads of self-attention in parallel"""</span></span>
<span id="cb34-869"><a href="#cb34-869"></a></span>
<span id="cb34-870"><a href="#cb34-870"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads, head_size):</span>
<span id="cb34-871"><a href="#cb34-871"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-872"><a href="#cb34-872"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([Head(head_size) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)])</span>
<span id="cb34-873"><a href="#cb34-873"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(n_embd, n_embd)</span>
<span id="cb34-874"><a href="#cb34-874"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb34-875"><a href="#cb34-875"></a></span>
<span id="cb34-876"><a href="#cb34-876"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-877"><a href="#cb34-877"></a>        out <span class="op">=</span> torch.cat([h(x) <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.heads], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb34-878"><a href="#cb34-878"></a>        out <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.proj(out))</span>
<span id="cb34-879"><a href="#cb34-879"></a>        <span class="cf">return</span> out</span>
<span id="cb34-880"><a href="#cb34-880"></a></span>
<span id="cb34-881"><a href="#cb34-881"></a></span>
<span id="cb34-882"><a href="#cb34-882"></a><span class="kw">class</span> FeedFoward(nn.Module):</span>
<span id="cb34-883"><a href="#cb34-883"></a>    <span class="co">"""a simple linear layer followed by a non-linearity"""</span></span>
<span id="cb34-884"><a href="#cb34-884"></a></span>
<span id="cb34-885"><a href="#cb34-885"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd):</span>
<span id="cb34-886"><a href="#cb34-886"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-887"><a href="#cb34-887"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb34-888"><a href="#cb34-888"></a>            nn.Linear(n_embd, <span class="dv">4</span> <span class="op">*</span> n_embd),</span>
<span id="cb34-889"><a href="#cb34-889"></a>            nn.ReLU(),</span>
<span id="cb34-890"><a href="#cb34-890"></a>            nn.Linear(</span>
<span id="cb34-891"><a href="#cb34-891"></a>                <span class="dv">4</span> <span class="op">*</span> n_embd, n_embd</span>
<span id="cb34-892"><a href="#cb34-892"></a>            ),  <span class="co"># Projection layer going back into the residual pathway</span></span>
<span id="cb34-893"><a href="#cb34-893"></a>            nn.Dropout(dropout),</span>
<span id="cb34-894"><a href="#cb34-894"></a>        )</span>
<span id="cb34-895"><a href="#cb34-895"></a></span>
<span id="cb34-896"><a href="#cb34-896"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-897"><a href="#cb34-897"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span>
<span id="cb34-898"><a href="#cb34-898"></a><span class="in">```</span></span>
<span id="cb34-899"><a href="#cb34-899"></a></span>
<span id="cb34-900"><a href="#cb34-900"></a><span class="fu">### Combine components into the Decoder block</span></span>
<span id="cb34-901"><a href="#cb34-901"></a></span>
<span id="cb34-904"><a href="#cb34-904"></a><span class="in">```{python}</span></span>
<span id="cb34-905"><a href="#cb34-905"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb34-906"><a href="#cb34-906"></a>    <span class="co">"""Transformer block: communication followed by computation"""</span></span>
<span id="cb34-907"><a href="#cb34-907"></a></span>
<span id="cb34-908"><a href="#cb34-908"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd, n_head):</span>
<span id="cb34-909"><a href="#cb34-909"></a>        <span class="co"># n_embd: embedding dimension, n_head: the number of heads we'd like</span></span>
<span id="cb34-910"><a href="#cb34-910"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-911"><a href="#cb34-911"></a>        head_size <span class="op">=</span> n_embd <span class="op">//</span> n_head</span>
<span id="cb34-912"><a href="#cb34-912"></a>        <span class="va">self</span>.sa <span class="op">=</span> MultiHeadAttention(n_head, head_size)</span>
<span id="cb34-913"><a href="#cb34-913"></a>        <span class="va">self</span>.ffwd <span class="op">=</span> FeedFoward(n_embd)</span>
<span id="cb34-914"><a href="#cb34-914"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb34-915"><a href="#cb34-915"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb34-916"><a href="#cb34-916"></a></span>
<span id="cb34-917"><a href="#cb34-917"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-918"><a href="#cb34-918"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x))  <span class="co"># Communication</span></span>
<span id="cb34-919"><a href="#cb34-919"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x))  <span class="co"># Computation</span></span>
<span id="cb34-920"><a href="#cb34-920"></a>        <span class="cf">return</span> x</span>
<span id="cb34-921"><a href="#cb34-921"></a><span class="in">```</span></span>
<span id="cb34-922"><a href="#cb34-922"></a></span>
<span id="cb34-923"><a href="#cb34-923"></a><span class="fu">### Set up the full Transformer model </span></span>
<span id="cb34-924"><a href="#cb34-924"></a>This is a combination of the Token embeddings, Positional embeddings, a stack of Transformer blocks and an output block.</span>
<span id="cb34-925"><a href="#cb34-925"></a></span>
<span id="cb34-928"><a href="#cb34-928"></a><span class="in">```{python}</span></span>
<span id="cb34-929"><a href="#cb34-929"></a><span class="co"># super simple language model</span></span>
<span id="cb34-930"><a href="#cb34-930"></a><span class="kw">class</span> LanguageModel(nn.Module):</span>
<span id="cb34-931"><a href="#cb34-931"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb34-932"><a href="#cb34-932"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-933"><a href="#cb34-933"></a>        <span class="co"># each token directly reads off the logits for the next token from a lookup table</span></span>
<span id="cb34-934"><a href="#cb34-934"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb34-935"><a href="#cb34-935"></a>        <span class="va">self</span>.position_embedding_table <span class="op">=</span> nn.Embedding(block_size, n_embd)</span>
<span id="cb34-936"><a href="#cb34-936"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(</span>
<span id="cb34-937"><a href="#cb34-937"></a>            <span class="op">*</span>[Block(n_embd, n_head<span class="op">=</span>n_head) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layer)]</span>
<span id="cb34-938"><a href="#cb34-938"></a>        )</span>
<span id="cb34-939"><a href="#cb34-939"></a>        <span class="va">self</span>.ln_f <span class="op">=</span> nn.LayerNorm(n_embd)  <span class="co"># final layer norm</span></span>
<span id="cb34-940"><a href="#cb34-940"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(n_embd, vocab_size)</span>
<span id="cb34-941"><a href="#cb34-941"></a></span>
<span id="cb34-942"><a href="#cb34-942"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb34-943"><a href="#cb34-943"></a>        B, T <span class="op">=</span> idx.shape</span>
<span id="cb34-944"><a href="#cb34-944"></a></span>
<span id="cb34-945"><a href="#cb34-945"></a>        <span class="co"># idx and targets are both (B,T) tensor of integers</span></span>
<span id="cb34-946"><a href="#cb34-946"></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx)  <span class="co"># (B,T,C)</span></span>
<span id="cb34-947"><a href="#cb34-947"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.position_embedding_table(torch.arange(T, device<span class="op">=</span>device))  <span class="co"># (T,C)</span></span>
<span id="cb34-948"><a href="#cb34-948"></a>        x <span class="op">=</span> tok_emb <span class="op">+</span> pos_emb  <span class="co"># (B,T,C)</span></span>
<span id="cb34-949"><a href="#cb34-949"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb34-950"><a href="#cb34-950"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_f(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb34-951"><a href="#cb34-951"></a>        logits <span class="op">=</span> <span class="va">self</span>.lm_head(x)  <span class="co"># (B,T,vocab_size)</span></span>
<span id="cb34-952"><a href="#cb34-952"></a></span>
<span id="cb34-953"><a href="#cb34-953"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb34-954"><a href="#cb34-954"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb34-955"><a href="#cb34-955"></a>        <span class="cf">else</span>:</span>
<span id="cb34-956"><a href="#cb34-956"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb34-957"><a href="#cb34-957"></a>            logits <span class="op">=</span> logits.view(B <span class="op">*</span> T, C)</span>
<span id="cb34-958"><a href="#cb34-958"></a>            targets <span class="op">=</span> targets.view(B <span class="op">*</span> T)</span>
<span id="cb34-959"><a href="#cb34-959"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb34-960"><a href="#cb34-960"></a></span>
<span id="cb34-961"><a href="#cb34-961"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb34-962"><a href="#cb34-962"></a></span>
<span id="cb34-963"><a href="#cb34-963"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb34-964"><a href="#cb34-964"></a>        <span class="co"># idx is (B, T) array of indices in the current context</span></span>
<span id="cb34-965"><a href="#cb34-965"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb34-966"><a href="#cb34-966"></a>            <span class="co"># crop idx to the last block_size tokens</span></span>
<span id="cb34-967"><a href="#cb34-967"></a>            idx_cond <span class="op">=</span> idx[:, <span class="op">-</span>block_size:]</span>
<span id="cb34-968"><a href="#cb34-968"></a>            <span class="co"># get the predictions</span></span>
<span id="cb34-969"><a href="#cb34-969"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx_cond)</span>
<span id="cb34-970"><a href="#cb34-970"></a>            <span class="co"># focus only on the last time step</span></span>
<span id="cb34-971"><a href="#cb34-971"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]  <span class="co"># becomes (B, C)</span></span>
<span id="cb34-972"><a href="#cb34-972"></a>            <span class="co"># apply softmax to get probabilities</span></span>
<span id="cb34-973"><a href="#cb34-973"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, C)</span></span>
<span id="cb34-974"><a href="#cb34-974"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb34-975"><a href="#cb34-975"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (B, 1)</span></span>
<span id="cb34-976"><a href="#cb34-976"></a>            <span class="co"># append sampled index to the running sequence</span></span>
<span id="cb34-977"><a href="#cb34-977"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (B, T+1)</span></span>
<span id="cb34-978"><a href="#cb34-978"></a>        <span class="cf">return</span> idx</span>
<span id="cb34-979"><a href="#cb34-979"></a><span class="in">```</span></span>
<span id="cb34-980"><a href="#cb34-980"></a></span>
<span id="cb34-981"><a href="#cb34-981"></a><span class="fu">## Homework</span></span>
<span id="cb34-982"><a href="#cb34-982"></a></span>
<span id="cb34-983"><a href="#cb34-983"></a><span class="ss">1. </span>In this notebook, we learned the various components of an LLM.  </span>
<span id="cb34-984"><a href="#cb34-984"></a>   Take the mini LLM we created from scratch and run your own training loop. Show how the training and validation perplexity change over the steps.  </span>
<span id="cb34-985"><a href="#cb34-985"></a>   Hint: this function might be useful for you:</span>
<span id="cb34-986"><a href="#cb34-986"></a></span>
<span id="cb34-989"><a href="#cb34-989"></a><span class="in">```{python}</span></span>
<span id="cb34-990"><a href="#cb34-990"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb34-991"><a href="#cb34-991"></a><span class="kw">def</span> estimate_loss():</span>
<span id="cb34-992"><a href="#cb34-992"></a>    out <span class="op">=</span> {}</span>
<span id="cb34-993"><a href="#cb34-993"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb34-994"><a href="#cb34-994"></a>    <span class="cf">for</span> split <span class="kw">in</span> [<span class="st">"train"</span>, <span class="st">"val"</span>]:</span>
<span id="cb34-995"><a href="#cb34-995"></a>        losses <span class="op">=</span> torch.zeros(eval_iters)</span>
<span id="cb34-996"><a href="#cb34-996"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(eval_iters):</span>
<span id="cb34-997"><a href="#cb34-997"></a>            X, Y <span class="op">=</span> get_batch(split)</span>
<span id="cb34-998"><a href="#cb34-998"></a>            logits, loss <span class="op">=</span> model(X, Y)</span>
<span id="cb34-999"><a href="#cb34-999"></a>            losses[k] <span class="op">=</span> loss.item()</span>
<span id="cb34-1000"><a href="#cb34-1000"></a>        out[split] <span class="op">=</span> losses.mean()</span>
<span id="cb34-1001"><a href="#cb34-1001"></a>    model.train()</span>
<span id="cb34-1002"><a href="#cb34-1002"></a>    <span class="cf">return</span> out</span>
<span id="cb34-1003"><a href="#cb34-1003"></a><span class="in">```</span></span>
<span id="cb34-1004"><a href="#cb34-1004"></a></span>
<span id="cb34-1005"><a href="#cb34-1005"></a><span class="ss">2. </span>Run the same training loop but modify one of the hyperparameters from the below list. Run this at least 4 times with a different value and plot each perplexity over training step.</span>
<span id="cb34-1006"><a href="#cb34-1006"></a></span>
<span id="cb34-1009"><a href="#cb34-1009"></a><span class="in">```{python}</span></span>
<span id="cb34-1010"><a href="#cb34-1010"></a><span class="co"># hyperparameters</span></span>
<span id="cb34-1011"><a href="#cb34-1011"></a>n_embd <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb34-1012"><a href="#cb34-1012"></a>n_head <span class="op">=</span> <span class="dv">4</span>  <span class="co">## so head_size = 16</span></span>
<span id="cb34-1013"><a href="#cb34-1013"></a>n_layer <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb34-1014"><a href="#cb34-1014"></a><span class="in">```</span></span>
<span id="cb34-1015"><a href="#cb34-1015"></a></span>
<span id="cb34-1016"><a href="#cb34-1016"></a><span class="ss">3. </span>Output some generated text from each model you trained. Did the output make more sense with some hyperparameters than others?</span>
<span id="cb34-1017"><a href="#cb34-1017"></a></span>
<span id="cb34-1018"><a href="#cb34-1018"></a><span class="ss">4. </span>We saw a cool visualization of attention mechanisms with BertViz. Take a more complicated model than GPT2 such as "meta-llama/Llama-2-7b-chat-hf" and see how the attention mechanisms are different</span>
<span id="cb34-1019"><a href="#cb34-1019"></a></span>
<span id="cb34-1020"><a href="#cb34-1020"></a><span class="fu">## Different types of Transformers</span></span>
<span id="cb34-1021"><a href="#cb34-1021"></a></span>
<span id="cb34-1022"><a href="#cb34-1022"></a><span class="fu">### Encoder-Decoder architecture</span></span>
<span id="cb34-1023"><a href="#cb34-1023"></a>Incorporates both an encoder + decoder architecture</span>
<span id="cb34-1024"><a href="#cb34-1024"></a></span>
<span id="cb34-1025"><a href="#cb34-1025"></a>The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.</span>
<span id="cb34-1026"><a href="#cb34-1026"></a></span>
<span id="cb34-1027"><a href="#cb34-1027"></a>In the decoder, the self-attention layer only attends to earlier positions in the output sequence. The future positions are masked (setting them to -inf) before the softmax step in the self-attention calculation.</span>
<span id="cb34-1028"><a href="#cb34-1028"></a></span>
<span id="cb34-1029"><a href="#cb34-1029"></a>The “Encoder-Decoder Attention” layer creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.</span>
<span id="cb34-1030"><a href="#cb34-1030"></a></span>
<span id="cb34-1031"><a href="#cb34-1031"></a>The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output.</span>
<span id="cb34-1032"><a href="#cb34-1032"></a></span>
<span id="cb34-1033"><a href="#cb34-1033"></a>The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did.</span>
<span id="cb34-1034"><a href="#cb34-1034"></a></span>
<span id="cb34-1035"><a href="#cb34-1035"></a>And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.</span>
<span id="cb34-1036"><a href="#cb34-1036"></a></span>
<span id="cb34-1037"><a href="#cb34-1037"></a>::: {#fig-animated-transformer}</span>
<span id="cb34-1038"><a href="#cb34-1038"></a></span>
<span id="cb34-1039"><a href="#cb34-1039"></a><span class="al">![](images/transformer_decoding_2.gif)</span></span>
<span id="cb34-1040"><a href="#cb34-1040"></a></span>
<span id="cb34-1041"><a href="#cb34-1041"></a>Illustration of the Encoder-Decoder architecture</span>
<span id="cb34-1042"><a href="#cb34-1042"></a>:::</span>
<span id="cb34-1043"><a href="#cb34-1043"></a></span>
<span id="cb34-1044"><a href="#cb34-1044"></a>Image credit: https://jalammar.github.io/illustrated-transformer/</span>
<span id="cb34-1045"><a href="#cb34-1045"></a></span>
<span id="cb34-1046"><a href="#cb34-1046"></a><span class="fu">### Encoder-only Transformers</span></span>
<span id="cb34-1047"><a href="#cb34-1047"></a></span>
<span id="cb34-1048"><a href="#cb34-1048"></a>In addition to the encoder-decoder architecture shown here there various other architectures which are either only encoder or decoder models.</span>
<span id="cb34-1049"><a href="#cb34-1049"></a></span>
<span id="cb34-1050"><a href="#cb34-1050"></a><span class="fu">### Bidirectional Encoder Representations from Transformers (BERT) model</span></span>
<span id="cb34-1051"><a href="#cb34-1051"></a></span>
<span id="cb34-1052"><a href="#cb34-1052"></a></span>
<span id="cb34-1053"><a href="#cb34-1053"></a>Encoder-only models only use the encoder layer of the Transformer.</span>
<span id="cb34-1054"><a href="#cb34-1054"></a></span>
<span id="cb34-1055"><a href="#cb34-1055"></a>These models are usually used for "understanding" natural language; however, they typically are not used for text generation. Examples of uses for these models are:</span>
<span id="cb34-1056"><a href="#cb34-1056"></a></span>
<span id="cb34-1057"><a href="#cb34-1057"></a><span class="ss">1. </span>Determining how positive or negative a movie’s reviews are. (Sentiment Analysis)</span>
<span id="cb34-1058"><a href="#cb34-1058"></a><span class="ss">2. </span>Summarizing long legal contracts. (Summarization)</span>
<span id="cb34-1059"><a href="#cb34-1059"></a><span class="ss">3. </span>Differentiating words that have multiple meanings (like ‘bank’) based on the surrounding text. (Polysemy resolution)</span>
<span id="cb34-1060"><a href="#cb34-1060"></a></span>
<span id="cb34-1061"><a href="#cb34-1061"></a>These models are often characterized as having “bi-directional” attention, and are often called auto-encoding models.</span>
<span id="cb34-1062"><a href="#cb34-1062"></a>The attention mechanisms of these models can access all the words in the initial sentence.</span>
<span id="cb34-1063"><a href="#cb34-1063"></a></span>
<span id="cb34-1064"><a href="#cb34-1064"></a>The most common encoder only architectures are:</span>
<span id="cb34-1065"><a href="#cb34-1065"></a></span>
<span id="cb34-1066"><a href="#cb34-1066"></a><span class="ss">* </span>ALBERT</span>
<span id="cb34-1067"><a href="#cb34-1067"></a><span class="ss">* </span>BERT</span>
<span id="cb34-1068"><a href="#cb34-1068"></a><span class="ss">* </span>DistilBERT</span>
<span id="cb34-1069"><a href="#cb34-1069"></a><span class="ss">* </span>ELECTRA</span>
<span id="cb34-1070"><a href="#cb34-1070"></a><span class="ss">* </span>RoBERTa</span>
<span id="cb34-1071"><a href="#cb34-1071"></a></span>
<span id="cb34-1072"><a href="#cb34-1072"></a>As example, let's consider BERT model in a little more detail.</span>
<span id="cb34-1073"><a href="#cb34-1073"></a></span>
<span id="cb34-1074"><a href="#cb34-1074"></a>::: {#fig-bert-explanation}</span>
<span id="cb34-1075"><a href="#cb34-1075"></a></span>
<span id="cb34-1076"><a href="#cb34-1076"></a><span class="al">![](images/BERT_Explanation.webp)</span></span>
<span id="cb34-1077"><a href="#cb34-1077"></a></span>
<span id="cb34-1078"><a href="#cb34-1078"></a>BERT Explanation</span>
<span id="cb34-1079"><a href="#cb34-1079"></a>:::</span>
<span id="cb34-1080"><a href="#cb34-1080"></a></span>
<span id="cb34-1081"><a href="#cb34-1081"></a>Image credit: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270</span>
<span id="cb34-1082"><a href="#cb34-1082"></a></span>
<span id="cb34-1083"><a href="#cb34-1083"></a>The BERT model is bidirectionally trained to have a deeper sense of language context and flow than single-direction language models.</span>
<span id="cb34-1084"><a href="#cb34-1084"></a></span>
<span id="cb34-1085"><a href="#cb34-1085"></a>The Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).</span>
<span id="cb34-1086"><a href="#cb34-1086"></a></span>
<span id="cb34-1087"><a href="#cb34-1087"></a>In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence.</span>
<span id="cb34-1088"><a href="#cb34-1088"></a></span>
<span id="cb34-1089"><a href="#cb34-1089"></a>To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:</span>
<span id="cb34-1090"><a href="#cb34-1090"></a></span>
<span id="cb34-1091"><a href="#cb34-1091"></a><span class="ss">1. </span>A <span class="co">[</span><span class="ot">CLS</span><span class="co">]</span> token is inserted at the beginning of the first sentence and a <span class="co">[</span><span class="ot">SEP</span><span class="co">]</span> token is inserted at the end of each sentence.</span>
<span id="cb34-1092"><a href="#cb34-1092"></a><span class="ss">2. </span>A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.</span>
<span id="cb34-1093"><a href="#cb34-1093"></a><span class="ss">3. </span>A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper.</span>
<span id="cb34-1094"><a href="#cb34-1094"></a></span>
<span id="cb34-1095"><a href="#cb34-1095"></a>::: {#fig-bert-input}</span>
<span id="cb34-1096"><a href="#cb34-1096"></a></span>
<span id="cb34-1097"><a href="#cb34-1097"></a><span class="al">![](images/BERT_input_sent.webp)</span></span>
<span id="cb34-1098"><a href="#cb34-1098"></a></span>
<span id="cb34-1099"><a href="#cb34-1099"></a>Illustration of BERT input</span>
<span id="cb34-1100"><a href="#cb34-1100"></a>:::</span>
<span id="cb34-1101"><a href="#cb34-1101"></a></span>
<span id="cb34-1102"><a href="#cb34-1102"></a>Image credit: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270</span>
<span id="cb34-1103"><a href="#cb34-1103"></a></span>
<span id="cb34-1104"><a href="#cb34-1104"></a>To predict if the second sentence is indeed connected to the first, the following steps are performed:</span>
<span id="cb34-1105"><a href="#cb34-1105"></a></span>
<span id="cb34-1106"><a href="#cb34-1106"></a><span class="ss">1. </span>The entire input sequence goes through the Transformer model.</span>
<span id="cb34-1107"><a href="#cb34-1107"></a><span class="ss">2. </span>The output of the <span class="co">[</span><span class="ot">CLS</span><span class="co">]</span> token is transformed into a 2×1 shaped vector, using a simple classification layer (learned matrices of weights and biases).</span>
<span id="cb34-1108"><a href="#cb34-1108"></a><span class="ss">3. </span>Calculating the probability of IsNextSequence with softmax.</span>
<span id="cb34-1109"><a href="#cb34-1109"></a></span>
<span id="cb34-1110"><a href="#cb34-1110"></a><span class="fu">#### Advantages and disadvantages:</span></span>
<span id="cb34-1111"><a href="#cb34-1111"></a></span>
<span id="cb34-1112"><a href="#cb34-1112"></a>**Advantages**:</span>
<span id="cb34-1113"><a href="#cb34-1113"></a></span>
<span id="cb34-1114"><a href="#cb34-1114"></a><span class="ss">* </span>Contextualized embeddings: Good for tasks where contextualized embeddings of input tokens are crucial, such as natural language understanding.</span>
<span id="cb34-1115"><a href="#cb34-1115"></a><span class="ss">* </span>Parallel processing: Allows for parallel processing of input tokens, making it computationally efficient.</span>
<span id="cb34-1116"><a href="#cb34-1116"></a></span>
<span id="cb34-1117"><a href="#cb34-1117"></a>**Disadvantages:**</span>
<span id="cb34-1118"><a href="#cb34-1118"></a></span>
<span id="cb34-1119"><a href="#cb34-1119"></a><span class="ss">* </span>Not designed for sequence generation: Might not perform well on tasks that require sequential generation of output, as there is no inherent mechanism for auto-regressive decoding.</span>
<span id="cb34-1120"><a href="#cb34-1120"></a></span>
<span id="cb34-1121"><a href="#cb34-1121"></a>Here is an example of a BERT code that can be used to</span>
<span id="cb34-1122"><a href="#cb34-1122"></a></span>
<span id="cb34-1123"><a href="#cb34-1123"></a><span class="fu">### Decoder-only models</span></span>
<span id="cb34-1124"><a href="#cb34-1124"></a></span>
<span id="cb34-1125"><a href="#cb34-1125"></a>An important difference of the GPT-2 architecture compared to the encoder-Transformer architecture has to do with the type of attention mechanism used.</span>
<span id="cb34-1126"><a href="#cb34-1126"></a></span>
<span id="cb34-1127"><a href="#cb34-1127"></a>In models such as BERT, the self-attention mechanism has access to tokens to the left and right of the query token. However, in decoder-based models such as GPT-2, masked self-attention is used instead which allows access only to tokens to the left of the query.</span>
<span id="cb34-1128"><a href="#cb34-1128"></a></span>
<span id="cb34-1129"><a href="#cb34-1129"></a>The masked self-attention mechanism is important for GPT-2 since it allows the model to be trained for token-by-token generation without simply "memorizing" the future tokens.</span>
<span id="cb34-1130"><a href="#cb34-1130"></a></span>
<span id="cb34-1131"><a href="#cb34-1131"></a>::: {#fig-self-attention-and-masked-self-attention}</span>
<span id="cb34-1132"><a href="#cb34-1132"></a></span>
<span id="cb34-1133"><a href="#cb34-1133"></a><span class="al">![](images/self-attention-and-masked-self-attention.png)</span></span>
<span id="cb34-1134"><a href="#cb34-1134"></a></span>
<span id="cb34-1135"><a href="#cb34-1135"></a>:::</span>
<span id="cb34-1136"><a href="#cb34-1136"></a></span>
<span id="cb34-1137"><a href="#cb34-1137"></a>Image credit: https://jalammar.github.io/illustrated-gpt2/</span>
<span id="cb34-1138"><a href="#cb34-1138"></a></span>
<span id="cb34-1139"><a href="#cb34-1139"></a>The masked self-attention adds understanding of associated words to explain contexts of certain words before passing it through a neural network. It assigns scores to how relevant each word in the segment is, and then adds up the vector representation. This is then passed through the feed-forward network resulting in an output vector.</span>
<span id="cb34-1140"><a href="#cb34-1140"></a></span>
<span id="cb34-1141"><a href="#cb34-1141"></a>::: {#fig-gpt2-self-attention}</span>
<span id="cb34-1142"><a href="#cb34-1142"></a></span>
<span id="cb34-1143"><a href="#cb34-1143"></a><span class="al">![](images/gpt2-self-attention-example-2.png)</span></span>
<span id="cb34-1144"><a href="#cb34-1144"></a></span>
<span id="cb34-1145"><a href="#cb34-1145"></a>:::</span>
<span id="cb34-1146"><a href="#cb34-1146"></a></span>
<span id="cb34-1147"><a href="#cb34-1147"></a>Image credit: https://jalammar.github.io/illustrated-gpt2/</span>
<span id="cb34-1148"><a href="#cb34-1148"></a></span>
<span id="cb34-1149"><a href="#cb34-1149"></a>The resulting vector then needs to be converted to an output token. A common method of obtaining this output token is known as top-k.</span>
<span id="cb34-1150"><a href="#cb34-1150"></a></span>
<span id="cb34-1151"><a href="#cb34-1151"></a>Here, the output vector is multiplied by the token embeddings which results in probabilities for each token in the vocabulary. Then the output token is sampled according to this probability.</span>
<span id="cb34-1152"><a href="#cb34-1152"></a></span>
<span id="cb34-1153"><a href="#cb34-1153"></a>::: {#fig-gpt2-output}</span>
<span id="cb34-1154"><a href="#cb34-1154"></a></span>
<span id="cb34-1155"><a href="#cb34-1155"></a><span class="al">![](images/gpt2-output.png)</span></span>
<span id="cb34-1156"><a href="#cb34-1156"></a></span>
<span id="cb34-1157"><a href="#cb34-1157"></a>:::</span>
<span id="cb34-1158"><a href="#cb34-1158"></a></span>
<span id="cb34-1159"><a href="#cb34-1159"></a>Image credit: https://jalammar.github.io/illustrated-gpt2/</span>
<span id="cb34-1160"><a href="#cb34-1160"></a></span>
<span id="cb34-1161"><a href="#cb34-1161"></a><span class="fu">### Advantages and disadvantages</span></span>
<span id="cb34-1162"><a href="#cb34-1162"></a></span>
<span id="cb34-1163"><a href="#cb34-1163"></a>**Advantages:**</span>
<span id="cb34-1164"><a href="#cb34-1164"></a></span>
<span id="cb34-1165"><a href="#cb34-1165"></a><span class="ss">* </span>Auto-regressive generation: Well-suited for tasks that require sequential generation, as the model can generate one token at a time based on the previous tokens.</span>
<span id="cb34-1166"><a href="#cb34-1166"></a><span class="ss">* </span>Variable-length output: Can handle tasks where the output sequence length is not fixed.</span>
<span id="cb34-1167"><a href="#cb34-1167"></a></span>
<span id="cb34-1168"><a href="#cb34-1168"></a>**Disadvantages:**</span>
<span id="cb34-1169"><a href="#cb34-1169"></a></span>
<span id="cb34-1170"><a href="#cb34-1170"></a><span class="ss">* </span>No direct access to input context: The decoder doesn't directly consider the input context during decoding, which might be a limitation for certain tasks.</span>
<span id="cb34-1171"><a href="#cb34-1171"></a><span class="ss">* </span>Potential for inefficiency: Decoding token by token can be less computationally efficient compared to parallel processing.</span>
<span id="cb34-1172"><a href="#cb34-1172"></a></span>
<span id="cb34-1173"><a href="#cb34-1173"></a><span class="fu">## Additional architectures</span></span>
<span id="cb34-1174"><a href="#cb34-1174"></a></span>
<span id="cb34-1175"><a href="#cb34-1175"></a>In addition to text, LLMs have also been applied on other data sources such as images and graphs. Here I will describe two particular architectures:</span>
<span id="cb34-1176"><a href="#cb34-1176"></a><span class="ss">1. </span>Vision Transformers</span>
<span id="cb34-1177"><a href="#cb34-1177"></a><span class="ss">2. </span>Graph Transformers</span>
<span id="cb34-1178"><a href="#cb34-1178"></a></span>
<span id="cb34-1179"><a href="#cb34-1179"></a><span class="fu">### Vision Transformers</span></span>
<span id="cb34-1180"><a href="#cb34-1180"></a></span>
<span id="cb34-1181"><a href="#cb34-1181"></a>Vision Transformers (ViT) is an architecture that uses self-attention mechanisms to process images.</span>
<span id="cb34-1182"><a href="#cb34-1182"></a></span>
<span id="cb34-1183"><a href="#cb34-1183"></a>The way this works is:</span>
<span id="cb34-1184"><a href="#cb34-1184"></a></span>
<span id="cb34-1185"><a href="#cb34-1185"></a><span class="ss">1. </span>Split image into patches (size is fixed)</span>
<span id="cb34-1186"><a href="#cb34-1186"></a><span class="ss">2. </span>Flatten the image patches</span>
<span id="cb34-1187"><a href="#cb34-1187"></a><span class="ss">3. </span>Create lower-dimensional linear embeddings from these flattened image patches and include positional embeddings</span>
<span id="cb34-1188"><a href="#cb34-1188"></a><span class="ss">4. </span>Feed the sequence as an input to a transformer encoder</span>
<span id="cb34-1189"><a href="#cb34-1189"></a><span class="ss">5. </span>Pre-train the ViT model with image labels, which is then fully supervised on a big dataset Fine-tune the downstream dataset for image classification</span>
<span id="cb34-1190"><a href="#cb34-1190"></a></span>
<span id="cb34-1191"><a href="#cb34-1191"></a><span class="al">![vision-transformer-vit.png](images/vision-transformer-vit.png)</span></span>
<span id="cb34-1192"><a href="#cb34-1192"></a></span>
<span id="cb34-1193"><a href="#cb34-1193"></a>Image credit: Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020).</span>
<span id="cb34-1194"><a href="#cb34-1194"></a></span>
<span id="cb34-1195"><a href="#cb34-1195"></a><span class="fu">### Graph Transformers</span></span>
<span id="cb34-1196"><a href="#cb34-1196"></a></span>
<span id="cb34-1197"><a href="#cb34-1197"></a><span class="al">![Graphformer.png](images/Graphformer.png)</span></span>
<span id="cb34-1198"><a href="#cb34-1198"></a></span>
<span id="cb34-1199"><a href="#cb34-1199"></a><span class="fu">## References</span></span>
<span id="cb34-1200"><a href="#cb34-1200"></a></span>
<span id="cb34-1201"><a href="#cb34-1201"></a>Here are some recommendations for further reading and additional code for review.</span>
<span id="cb34-1202"><a href="#cb34-1202"></a></span>
<span id="cb34-1203"><a href="#cb34-1203"></a><span class="ss">* </span>"The Illustrated Transformer" by Jay Alammar </span>
<span id="cb34-1204"><a href="#cb34-1204"></a><span class="ss">* </span>"Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)"</span>
<span id="cb34-1205"><a href="#cb34-1205"></a><span class="ss">* </span>"The Illustrated GPT-2 (Visualizing Transformer Language Models)"</span>
<span id="cb34-1206"><a href="#cb34-1206"></a><span class="ss">* </span>"A gentle introduction to positional encoding"</span>
<span id="cb34-1207"><a href="#cb34-1207"></a><span class="ss">* </span>"LLM Tutorial Workshop (Argonne National Laboratory)"</span>
<span id="cb34-1208"><a href="#cb34-1208"></a><span class="ss">* </span>"LLM Tutorial Workshop Part 2 (Argonne National Laboratory)"</span>
<span id="cb34-1209"><a href="#cb34-1209"></a></span>
<span id="cb34-1210"><a href="#cb34-1210"></a></span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://saforem2.github.io/intro-hpc-bootcamp-2025/">
<p>saforem2.github.io/intro-hpc-bootcamp-2025</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/blob/main/02-llms/00-intro-to-llms/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/edit/main/02-llms/00-intro-to-llms/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/saforem2">
      <i class="bi bi-twitter" role="img" aria-label="Sam Foreman Twitter">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link active" href="https://github.com/saforem2/intro-hpc-bootcamp-2025" aria-current="page">
      <i class="bi bi-github" role="img" aria-label="Sam Foreman GitHub">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>