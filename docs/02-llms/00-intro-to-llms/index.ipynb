{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Large Language Models\n",
        "\n",
        "[Sam Foreman](https://samforeman.me)\n",
        "(\\[[ALCF](https://alcf.anl.gov/about/people/sam-foreman)\\](<https://alcf.anl.gov/about/people/sam-foreman>))  \n",
        "2025-07-15\n",
        "\n",
        "> **Authors**\n",
        ">\n",
        "> Content modified from original content written by Archit Vasan ,\n",
        "> including materials on LLMs by Varuni Sastri and Carlo Graziani at\n",
        "> Argonne, and discussion/editorial work by Taylor Childers, Bethany\n",
        "> Lusch, and Venkat Vishwanath (Argonne)\n",
        "\n",
        "Inspiration from the blog posts ‚ÄúThe Illustrated Transformer‚Äù and ‚ÄúThe\n",
        "Illustrated GPT2‚Äù by Jay Alammar, highly recommended reading.\n",
        "\n",
        "This tutorial covers the some fundamental concepts necessary to to study\n",
        "of large language models (LLMs).\n",
        "\n",
        "## Brief overview\n",
        "\n",
        "-   Scientific applications for language models\n",
        "-   General overview of Transformers\n",
        "-   Tokenization\n",
        "-   Model Architecture\n",
        "-   Pipeline using HuggingFace\n",
        "-   Model loading\n",
        "\n",
        "## References\n",
        "\n",
        "I strongly recommend reading:\n",
        "\n",
        "-   [‚ÄúThe Illustrated\n",
        "    Transformer‚Äù](https://jalammar.github.io/illustrated-transformer/)\n",
        "    by Jay AlammarAlammar also has a useful post dedicated more\n",
        "    generally to Sequence-to-Sequence modeling\n",
        "-   [LLM Course by ü§ó\n",
        "    HuggingFace](https://huggingface.co/learn/llm-course/chapter1/1)\n",
        "-   [‚ÄúVisualizing A Neural Machine Translation Model (Mechanics of\n",
        "    Seq2seq Models With\n",
        "    Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/),\n",
        "    which illustrates the attention mechanism in the context of a more\n",
        "    generic language translation model.\n",
        "-   [GPT in 60 Lines of\n",
        "    NumPy](https://jaykmody.com/blog/gpt-from-scratch/)"
      ],
      "id": "ae6e3225-a196-43cf-8523-b5dae4b6d426"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}