<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.18">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sam Foreman">
<meta name="dcterms.date" content="2025-07-15">
<meta name="description" content="Simple example illustrating how to train a linear regression model using stochastic gradient descent (SGD) on a dataset of house prices.">

<title>Example: Linear Regression – Intro to HPC Bootcamp 2025</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../00-intro-AI-HPC/7-statistical-learning/index.html" rel="next">
<link href="../../00-intro-AI-HPC/5-mcmc-example/index.html" rel="prev">
<link href="../../assets/favicon-sf.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-9bf6f933b760cbb4c99ada293d62cb55.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-22c6c940514060718923bc7c0517e30f.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-213ee8eb1e84f2f28ac8c726e3715433.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-7ed7b956390c4b79159f05cb27058288.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=GTM-MRCG68M5"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'GTM-MRCG68M5', { 'anonymize_ip': true});
</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous">
<script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js" integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6" crossorigin="anonymous"></script>
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;family=IBM+Plex+Sans+Condensed:ital,wght@0,400;0,500;0,600;0,700&amp;family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans&amp;family=IBM+Plex+Sans+Condensed&amp;family=IBM+Plex+Mono&amp;display=swap" rel="stylesheet">
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-TC329HJ');</script>
<!-- End Google Tag Manager -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link href="https://iosevka-webfonts.github.io/iosevka/iosevka.css" rel="stylesheet">

  <script defer="" src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Example: Linear Regression – Intro to HPC Bootcamp 2025">
<meta property="og:description" content="Simple example illustrating how to train a linear regression model using stochastic gradient descent (SGD) on a dataset of house prices.">
<meta property="og:image" content="https://saforem2.github.io/hpc-bootcamp-2025/00-intro-AI-HPC/figures/jupyter1.png">
<meta property="og:site_name" content="Intro to HPC Bootcamp 2025">
<meta property="og:image:height" content="1704">
<meta property="og:image:width" content="1630">
<meta name="twitter:title" content="Example: Linear Regression – Intro to HPC Bootcamp 2025">
<meta name="twitter:description" content="Simple example illustrating how to train a linear regression model using stochastic gradient descent (SGD) on a dataset of house prices.">
<meta name="twitter:image" content="https://saforem2.github.io/hpc-bootcamp-2025/00-intro-AI-HPC/figures/jupyter1.png">
<meta name="twitter:site" content="saforem2">
<meta name="twitter:image-height" content="1704">
<meta name="twitter:image-width" content="1630">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="Example: Linear Regression">
<meta name="citation_author" content="Sam Foreman">
<meta name="citation_publication_date" content="2025-07-15">
<meta name="citation_cover_date" content="2025-07-15">
<meta name="citation_year" content="2025">
<meta name="citation_online_date" content="2025-07-15">
<meta name="citation_fulltext_html_url" content="https://saforem2.github.io/hpc-bootcamp-2025/00-intro-AI-HPC/6-linear-regression/">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=The hugging face course, 2022;,citation_author=Hugging Face;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_publisher=https://huggingface.co/course;">
<meta name="citation_reference" content="citation_title=HiPerRAG: High-performance retrieval augmented generation for scientific insights;,citation_author=Ozan Gokdemir;,citation_author=Carlo Siebenschuh;,citation_author=Alexander Brace;,citation_author=Azton Wells;,citation_author=Brian Hsu;,citation_author=Kyle Hippe;,citation_author=Priyanka V. Setty;,citation_author=Aswathy Ajith;,citation_author=J. Gregory Pauloski;,citation_author=Varuni Sastry;,citation_author=Sam Foreman;,citation_author=Huihuo Zheng;,citation_author=Heng Ma;,citation_author=Bharat Kale;,citation_author=Nicholas Chia;,citation_author=Thomas Gibbs;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Francis J. Alexander;,citation_author=Anima Anandkumar;,citation_author=Ian Foster;,citation_author=Rick Stevens;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://arxiv.org/abs/2505.04846;">
<meta name="citation_reference" content="citation_title=MOFA: Discovering materials for carbon capture with a GenAI- and simulation-based workflow;,citation_author=Xiaoli Yan;,citation_author=Nathaniel Hudson;,citation_author=Hyun Park;,citation_author=Daniel Grzenda;,citation_author=J. Gregory Pauloski;,citation_author=Marcus Schwarting;,citation_author=Haochen Pan;,citation_author=Hassan Harb;,citation_author=Samuel Foreman;,citation_author=Chris Knight;,citation_author=Tom Gibbs;,citation_author=Kyle Chard;,citation_author=Santanu Chaudhuri;,citation_author=Emad Tajkhorshid;,citation_author=Ian Foster;,citation_author=Mohamad Moosavi;,citation_author=Logan Ward;,citation_author=E. A. Huerta;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://arxiv.org/abs/2501.10651;">
<meta name="citation_reference" content="citation_title=MProt-DPO: Breaking the ExaFLOPS barrier for multimodal protein design workflows with direct preference optimization;,citation_abstract=We present a scalable, end-to-end workflow for protein design. By augmenting protein sequences with natural language descriptions of their biochemical properties, we train generative models that can be preferentially aligned with protein fitness landscapes. Through complex experimental- and simulation-based observations, we integrate these measures as preferred parameters for generating new protein variants and demonstrate our workflow on five diverse supercomputers. We achieve &amp;amp;amp;gt;1 ExaFLOPS sustained performance in mixed precision on each supercomputer and a maximum sustained performance of 4.11 ExaFLOPS and peak performance of 5.57 ExaFLOPS. We establish the scientific performance of our model on two tasks: (1) across a predetermined benchmark dataset of deep mutational scanning experiments to optimize the fitness-determining mutations in the yeast protein HIS7, and (2) in optimizing the design of the enzyme malate dehydrogenase to achieve lower activation barriers (and therefore increased catalytic rates) using simulation data. Our implementation thus sets high watermarks for multimodal protein design workflows.;,citation_author=Gautham Dharuman;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Sam Foreman;,citation_author=Väinö Hatanpää;,citation_author=Varuni K. Sastry;,citation_author=Huihuo Zheng;,citation_author=Logan Ward;,citation_author=Servesh Muralidharan;,citation_author=Archit Vasan;,citation_author=Bharat Kale;,citation_author=Carla M. Mann;,citation_author=Heng Ma;,citation_author=Yun-Hsuan Cheng;,citation_author=Yuliana Zamora;,citation_author=Shengchao Liu;,citation_author=Chaowei Xiao;,citation_author=Murali Emani;,citation_author=Tom Gibbs;,citation_author=Mahidhar Tatineni;,citation_author=Deepak Canchi;,citation_author=Jerome Mitchell;,citation_author=Koichi Yamada;,citation_author=Maria Garzaran;,citation_author=Michael E. Papka;,citation_author=Ian Foster;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://doi.org/10.1109/SC41406.2024.00013;,citation_doi=10.1109/SC41406.2024.00013;,citation_isbn=9798350352917;,citation_conference_title=Proceedings of the international conference for high performance computing, networking, storage, and analysis;,citation_conference=IEEE Press;,citation_series_title=SC ’24;">
<meta name="citation_reference" content="citation_title=Quality measures for dynamic graph generative models;,citation_author=Ryien Hosseini;,citation_author=Filippo Simini;,citation_author=Venkatram Vishwanath;,citation_author=Rebecca Willett;,citation_author=Henry Hoffmann;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://openreview.net/forum?id=8bjspmAMBk;,citation_conference_title=The thirteenth international conference on learning representations;">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=RG-inspired machine learning for lattice field theory;,citation_author=Sam Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_volume=175;,citation_conference_title=EPJ web of conferences;,citation_conference=EDP Sciences;">
<meta name="citation_reference" content="citation_title=Large energy density in three-plate nanocapacitors due to coulomb blockade;,citation_author=A Hubler;,citation_author=S Foreman;,citation_author=J Liu;,citation_author=L Wortsmann;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=10;,citation_volume=123;,citation_journal_title=Journal of Applied Physics;,citation_publisher=AIP Publishing;">
<meta name="citation_reference" content="citation_title=Examples of renormalization group transformations for image sets;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=5;,citation_volume=98;,citation_journal_title=Physical Review E;,citation_publisher=American Physical Society;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the Ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.22323/1.334.0245;,citation_volume=LATTICE2018;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=Lattice 2018;">
<meta name="citation_reference" content="citation_title=Learning better physics: A machine learning approach to lattice gauge theory;,citation_author=Samuel Alfred Foreman;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_dissertation_institution=University of Iowa;">
<meta name="citation_reference" content="citation_title=Machine learning and neural networks for field theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=HMC with normalizing flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_journal_title=arXiv preprint arXiv:2112.01586;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy storage in quantum resonators;,citation_author=Jiaqi Liu;,citation_author=Alfred W Hubler;,citation_author=Samuel Alfred Foreman;,citation_author=Katharina Ott;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Applications of machine learning to lattice quantum field theory;,citation_author=Denis Boyda;,citation_author=Salvatore Calı̀;,citation_author=Sam Foreman;,citation_author=Lena Funcke;,citation_author=Daniel C Hackett;,citation_author=Yin Lin;,citation_author=Gert Aarts;,citation_author=Andrei Alexandru;,citation_author=Xiao-Yong Jin;,citation_author=Biagio Lucini;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_journal_title=arXiv preprint arXiv:2202.05838;">
<meta name="citation_reference" content="citation_title=Lattice QCD and particle physics;,citation_author=Andreas S Kronfeld;,citation_author=Tanmoy Bhattacharya;,citation_author=Thomas Blum;,citation_author=Norman H Christ;,citation_author=Carleton DeTar;,citation_author=William Detmold;,citation_author=Robert Edwards;,citation_author=Anna Hasenfratz;,citation_author=Huey-Wen Lin;,citation_author=Swagato Mukherjee;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2207.07641;,citation_journal_title=arXiv preprint arXiv:2207.07641;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=6;,citation_volume=37;,citation_journal_title=The International Journal of High Performance Computing Applications;,citation_publisher=SAGE Publications Sage UK: London, England;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=The international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=A comprehensive performance study of large language models on novel AI accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04607;,citation_journal_title=arXiv preprint arXiv:2310.04607;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04610;,citation_journal_title=arXiv preprint arXiv:2310.04610;">
<meta name="citation_reference" content="citation_title=Protein generation via genome-scale language models with bio-physical scoring;,citation_author=Gautham Dharuman;,citation_author=Logan Ward;,citation_author=Heng Ma;,citation_author=Priyanka V Setty;,citation_author=Ozan Gokdemir;,citation_author=Sam Foreman;,citation_author=Murali Emani;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Kristopher Keipert;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=Proceedings of the SC’23 workshops of the international conference on high performance computing, network, storage, and analysis;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2312.08936;,citation_journal_title=arXiv preprint arXiv:2312.08936;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 computational frontier CompF03 topical group report: Machine learning;,citation_author=Phiala Shanahan;,citation_author=Kazuhiro Terao;,citation_author=Daniel Whiteson;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;,citation_journal_title=arXiv preprint arXiv:2209.07559;">
<meta name="citation_reference" content="citation_title=Thorough characterization and analysis of large transformer model training at-scale;,citation_author=Scott Cheng;,citation_author=Jun-Liang Lin;,citation_author=Murali Emani;,citation_author=Siddhisanket Raskar;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Venkatram Vishwanath;,citation_author=Mahmut Taylan Kandemir;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=8;,citation_journal_title=Proceedings of the ACM on Measurement and Analysis of Computing Systems;,citation_publisher=ACM New York, NY, USA;">
<meta name="citation_reference" content="citation_title=Communities through energy justice projects;,citation_author=Mary Ann Leung;,citation_author=Katharine Cahill;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois Curfman McInnes;,citation_author=Suzanne Parete-Koon;,citation_author=Subil Abraham;,citation_author=Lacy Beach Barrier;,citation_author=Gladys Chen;,citation_author=Lizanne DeStefano;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science;">
<meta name="citation_reference" content="citation_title=Applications of a foundation model approach for weather and climate;,citation_author=Troy Arcomano;,citation_author=Alexander Wikner;,citation_author=Romit Maulik;,citation_author=Veerabhadra Rao Kotamarthi;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_volume=2023;,citation_conference_title=AGU fall meeting abstracts;">
<meta name="citation_reference" content="citation_title=Toward a holistic performance evaluation of large language models across diverse ai accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_author=Sanjif Shanmugavelu;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 IEEE international parallel and distributed processing symposium workshops (IPDPSW);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Intro to HPC bootcamp: Engaging new communities through energy justice projects;,citation_author=Suzanne Parete-Koon;,citation_author=Michael Sandoval;,citation_author=Kellen Leland;,citation_author=Subil Abraham;,citation_author=Mary Ann Leung;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois McInnes;,citation_author=Sreeranjani Ramprakash;,citation_author=Lacy Beach Barrier;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science Education;,citation_publisher=Oak Ridge National Laboratory (ORNL), Oak Ridge, TN (United States);">
<meta name="citation_reference" content="citation_title=MProt-DPO: Breaking the ExaFLOPS barrier for multimodal protein design workflows with direct preference optimization;,citation_author=Gautham Dharuman;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Sam Foreman;,citation_author=Väinä Hatanpää;,citation_author=Varuni K Sastry;,citation_author=Huihuo Zheng;,citation_author=Logan Ward;,citation_author=Servesh Muralidharan;,citation_author=Archit Vasan;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 SC24: International conference for high performance computing, networking, storage and analysis SC;,citation_conference=IEEE Computer Society;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=Connor Holmes;,citation_author=Martin Cai;,citation_author=Adam Ghanem;,citation_author=Zhongzhu Zhou;,citation_author=Yuxiong He;,citation_author=Pete Luferenko;,citation_author=Divya Kumar;,citation_author=Jonathan Weyn;,citation_author=Ruixiong Zhang;,citation_author=Sylwester Klocek;,citation_author=Volodymyr Vragov;,citation_author=Mohammed AlQuraishi;,citation_author=Gustaf Ahdritz;,citation_author=Christina Floristean;,citation_author=Cristina Negri;,citation_author=Rao Kotamarthi;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_author=Sam Foreman;,citation_author=Kyle Hippe;,citation_author=Troy Arcomano;,citation_author=Romit Maulik;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot;,citation_author=Murali Emani;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Prasanna Balaprakash;,citation_author=Gina Tourassi;,citation_author=John Gounley;,citation_author=Heidi Hanson;,citation_author=Thomas E Potok;,citation_author=Massimiliano Lupo Pasini;,citation_author=Kate Evans;,citation_author=Dan Lu;,citation_author=Dalton Lunga;,citation_author=Junqi Yin;,citation_author=Sajal Dash;,citation_author=Feiyi Wang;,citation_author=Mallikarjun Shankar;,citation_author=Isaac Lyngaas;,citation_author=Xiao Wang;,citation_author=Guojing Cong;,citation_author=Pei Zhang;,citation_author=Ming Fan;,citation_author=Siyan Liu;,citation_author=Adolfy Hoisie;,citation_author=Shinjae Yoo;,citation_author=Yihui Ren;,citation_author=William Tang;,citation_author=Kyle Felker;,citation_author=Alexey Svyatkovskiy;,citation_author=Hang Liu;,citation_author=Ashwin Aji;,citation_author=Angela Dalton;,citation_author=Michael Schulte;,citation_author=Karl Schulz;,citation_author=Yuntian Deng;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Anima Anandkumar;,citation_author=Rick Stevens;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04610;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=The climate risk &amp;amp;amp; resilience portal (ClimRR) metadata and data dictionary;,citation_author=C. Burdi;,citation_author=Wall. T Branham;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://dub.sh/ClimRR-Metadata;">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2105.03418;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy Justice Analysis of Climate Data with ClimRR;,citation_author=Sam Foreman;,citation_publication_date=2023-08-07;,citation_cover_date=2023-08-07;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/climate-analysis;,citation_language=en;">
<meta name="citation_reference" content="citation_author=Sam Foreman;,citation_publication_date=2023-08-19;,citation_cover_date=2023-08-19;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/l2hmc-qcd;,citation_language=en;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James Osborn;,citation_publication_date=00;,citation_cover_date=00;,citation_year=0;,citation_conference_title=40th international symposium on lattice field theory (lattice 2023) (batavia, IL, united states, 07/31/2023 - 08/04/2023);">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=S. Foreman;,citation_author=X. Jin;,citation_author=J. Osborn;,citation_publication_date=2022-07;,citation_cover_date=2022-07;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_conference_title=The 38th international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Mastering language models;,citation_author=Samuel Montgomery;,citation_publication_date=2023-10;,citation_cover_date=2023-10;,citation_year=2023;,citation_fulltext_html_url=https://towardsdatascience.com/mastering-language-models-32e1d891511a
           ;,citation_journal_title=Medium;,citation_publisher=Towards Data Science;">
<meta name="citation_reference" content="citation_title=Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond;,citation_author=Jingfeng Yang;,citation_author=Hongye Jin;,citation_author=Ruixiang Tang;,citation_author=Xiaotian Han;,citation_author=Qizhang Feng;,citation_author=Haoming Jiang;,citation_author=Bing Yin;,citation_author=Xia Hu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2304.13712;">
<meta name="citation_reference" content="citation_title=Training tips for the transformer model;,citation_author=Martin Popel;,citation_author=Ondřej Bojar;,citation_publication_date=2018-04;,citation_cover_date=2018-04;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.2478%2Fpralin-2018-0002;,citation_issue=1;,citation_doi=10.2478/pralin-2018-0002;,citation_volume=110;,citation_journal_title=The Prague Bulletin of Mathematical Linguistics;,citation_publisher=Charles University in Prague, Karolinum Press;">
<meta name="citation_reference" content="citation_title=Attention is all you need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N. Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03762;">
<meta name="citation_reference" content="citation_title=Tree of thoughts: Deliberate problem solving with large language models;,citation_author=Shunyu Yao;,citation_author=Dian Yu;,citation_author=Jeffrey Zhao;,citation_author=Izhak Shafran;,citation_author=Thomas L. Griffiths;,citation_author=Yuan Cao;,citation_author=Karthik Narasimhan;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2305.10601;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_abstract=We seek to transform how new and emergent variants of pandemiccausing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.Competing Interest StatementThe authors have declared no competing interest.;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot-Sasson;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.biorxiv.org/content/early/2022/11/23/2022.10.10.511571;,citation_doi=10.1101/2022.10.10.511571;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
</head>

<body class="nav-sidebar floating quarto-dark"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = true;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../00-intro-AI-HPC/index.html">[00] Intro to AI and HPC</a></li><li class="breadcrumb-item"><a href="../../00-intro-AI-HPC/6-linear-regression/index.html">[6] Linear Regression</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Intro to HPC Bootcamp 2025</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="../../index.html" title="Home" class="quarto-navigation-tool px-1" aria-label="Home"><i class="bi bi-home"></i></a>
    <a href="https://github.com/saforem2/intro-hpc-bootcamp-2025" title="GitHub" class="quarto-navigation-tool px-1" aria-label="GitHub"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../00-intro-AI-HPC/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[00] Intro to AI and HPC</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/0-compute-systems/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] Compute Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/1-shared-resources/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[1] Shared Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/2-jupyter-notebooks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[2] Jupyter Notebooks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/3-python/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[3] Using Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/4-data/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[4] Working with Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/5-mcmc-example/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[5] MCMC Example</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/6-linear-regression/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">[6] Linear Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/7-statistical-learning/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[7] Statistical Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/8-clustering/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[8] Clustering</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../01-neural-networks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[01] Neural Networks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/0-intro/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] Intro to NNs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/1-mnist/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[1] MNIST Example</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/1-mnist-ipynb/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[1] MNIST Example (ipynb)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/2-advanced/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[2] Advanced</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/3-conv-nets/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[3] Conv. Nets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/4-representation-learning/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[4] Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/5-distributed-training/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[5] Distributed Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../02-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[02] Large Language Models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/00-intro-to-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] Intro to LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/01-hands-on-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[1] Hands-on LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/06-parallel-training/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[6] Parallel Training</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/08-shakespeare-example-colab/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[8] Shakespeare Example (Colab)</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#how-to-run-this-notebook-on-polaris" id="toc-how-to-run-this-notebook-on-polaris" class="nav-link active" data-scroll-target="#how-to-run-this-notebook-on-polaris">How to run this notebook on Polaris</a></li>
  <li><a href="#what-is-ai-training" id="toc-what-is-ai-training" class="nav-link" data-scroll-target="#what-is-ai-training">What is AI training?</a></li>
  <li><a href="#how-does-large-language-model-work" id="toc-how-does-large-language-model-work" class="nav-link" data-scroll-target="#how-does-large-language-model-work">How does large language model work?</a></li>
  <li><a href="#simplest-ai-model-example-linear-regression" id="toc-simplest-ai-model-example-linear-regression" class="nav-link" data-scroll-target="#simplest-ai-model-example-linear-regression">Simplest AI model example: linear regression</a>
  <ul class="collapse">
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">Dataset</a></li>
  <li><a href="#theory-of-linear-regression" id="toc-theory-of-linear-regression" class="nav-link" data-scroll-target="#theory-of-linear-regression">Theory of linear regression</a></li>
  <li><a href="#training-through-stochastic-gradient-descent-sgd" id="toc-training-through-stochastic-gradient-descent-sgd" class="nav-link" data-scroll-target="#training-through-stochastic-gradient-descent-sgd">Training through Stochastic Gradient Descent (SGD)</a></li>
  </ul></li>
  <li><a href="#putting-it-together" id="toc-putting-it-together" class="nav-link" data-scroll-target="#putting-it-together">Putting it together</a></li>
  <li><a href="#homework" id="toc-homework" class="nav-link" data-scroll-target="#homework">Homework</a>
  <ul class="collapse">
  <li><a href="#mini-batch-training" id="toc-mini-batch-training" class="nav-link" data-scroll-target="#mini-batch-training">Mini Batch Training</a></li>
  <li><a href="#learning-rate-issue-bonus" id="toc-learning-rate-issue-bonus" class="nav-link" data-scroll-target="#learning-rate-issue-bonus">Learning rate issue (Bonus)</a></li>
  </ul></li>
  <li><a href="#minibatch-training" id="toc-minibatch-training" class="nav-link" data-scroll-target="#minibatch-training">Minibatch training</a></li>
  <li><a href="#learning-rate" id="toc-learning-rate" class="nav-link" data-scroll-target="#learning-rate">Learning rate</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/blob/main/00-intro-AI-HPC/6-linear-regression/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/edit/main/00-intro-AI-HPC/6-linear-regression/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.ipynb" download="index.ipynb"><i class="bi bi-journal-code"></i>Jupyter</a></li><li><a href="README.md"><i class="bi bi-file-code"></i>Github (GFM)</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TC329HJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../00-intro-AI-HPC/index.html">[00] Intro to AI and HPC</a></li><li class="breadcrumb-item"><a href="../../00-intro-AI-HPC/6-linear-regression/index.html">[6] Linear Regression</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Example: Linear Regression</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">ai</div>
    <div class="quarto-category">hpc</div>
    <div class="quarto-category">ai4science</div>
  </div>
  </div>

<div>
  <div class="description">
    Simple example illustrating how to train a linear regression model using stochastic gradient descent (SGD) on a dataset of house prices.
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://samforeman.me">Sam Foreman</a> <a href="mailto:foremans@anl.gov" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://alcf.anl.gov/about/people/sam-foreman">
            </a><a href="https://alcf.anl.gov/about/people/sam-foreman">ALCF</a>
            
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 15, 2025</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">August 6, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<p><a href="https://colab.research.google.com/github/saforem2/intro-hpc-bootcamp-2025/blob/main/docs/00-intro-AI-HPC/6-linear-regression/index.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid"></a></p>
<p>In this notebook, we will talk about:</p>
<ul>
<li>What is AI training?</li>
<li>How does large language model work?</li>
<li>A simple AI model: linear regression</li>
</ul>
<details closed="">
<summary>
<h2 class="anchored">
ALCF Specific Setup
</h2>
</summary>
<section id="how-to-run-this-notebook-on-polaris" class="level2">
<h2 class="anchored" data-anchor-id="how-to-run-this-notebook-on-polaris">How to run this notebook on Polaris</h2>
<ul>
<li>Go to <a href="https://jupyter.alcf.anl.gov" class="uri">https://jupyter.alcf.anl.gov</a>, and click “Login Polaris”</li>
<li>After login, select <code>ALCFAITP</code> project and <code>ALCFAITP</code> queue during the lecture (use <code>debug</code> queue outside of the lecture)</li>
<li>Load the notebook and select “datascience/conda-2023-01-10” python kernel</li>
</ul>
<div id="fig-jupyter" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-jupyter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../figures/jupyter1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: "><img src="../figures/jupyter1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-jupyter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1
</figcaption>
</figure>
</div>
<p><strong>How to run this notebook on Google Colab</strong></p>
<ul>
<li><p>Go to https://colab.research.google.com/, sign in or sign up</p></li>
<li><p>“File”-&gt; “open notebook”</p></li>
<li><p>Choose <code>01_intro_AI_on_Supercomputer/01_linear_regression_sgd.ipynb</code> from the list</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../figures/colab.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Google Colab"><img src="../figures/colab.png" class="img-fluid figure-img" alt="Google Colab"></a></p>
<figcaption>Google Colab</figcaption>
</figure>
</div></li>
</ul>
</section></details>

<section id="what-is-ai-training" class="level2">
<h2 class="anchored" data-anchor-id="what-is-ai-training">What is AI training?</h2>
<p><strong>Two ways of learning English</strong>:</p>
<ul>
<li>through learning rules;</li>
<li>through hearing a lot of speakings</li>
</ul>
<div id="fig-data-driven" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-driven-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../figures/data_driven.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;2: Data Driven Learning"><img src="../figures/data_driven.png" class="img-fluid figure-img" style="width:90.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-driven-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Data Driven Learning
</figcaption>
</figure>
</div>
<p>I learned English in my middle school, and memorized a lot of grammar rules in my mind. Every time when I speak, I try to follow the grammar rules as much as I can. But I always break the rules.</p>
<p>However, my daugher learned English differently. She learns speaking by hearing a lot of speaking from TV, teachers, classmates, and her older brother. The fact is that, she seldomly breaks grammar rules. This way of learning by observing patterns is very powerful! This is the essence of AI or data driven science.</p>
</section>
<section id="how-does-large-language-model-work" class="level2">
<h2 class="anchored" data-anchor-id="how-does-large-language-model-work">How does large language model work?</h2>
<p>Large Language Models, like GPT, function by pre-training on extensive datasets to learn language patterns, utilizing transformer architecture for contextual understanding, and can be fine-tuned for specific tasks, enabling them to generate coherent and contextually relevant text based on provided inputs.</p>
<div id="fig-llm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-llm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../figures/llm2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;3: "><img src="../figures/llm2.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-llm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3
</figcaption>
</figure>
</div>
<p><strong>More complicated example</strong>:</p>
<div class="flex-container">
<div class="flex-item" style="width:43%">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../figures/chatgpt_poem.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Poem about Albert Einstein"><img src="../figures/chatgpt_poem.png" class="img-fluid figure-img" alt="Poem about Albert Einstein"></a></p>
<figcaption>Poem about Albert Einstein</figcaption>
</figure>
</div>
</div>
<div class="flex-item" style="width:49.2%">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../figures/martin.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Poem about Martin Luther"><img src="../figures/martin.png" class="img-fluid figure-img" alt="Poem about Martin Luther"></a></p>
<figcaption>Poem about Martin Luther</figcaption>
</figure>
</div>
</div>
</div>
<p>You can do this on https://chat.openai.com</p>
</section>
<section id="simplest-ai-model-example-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="simplest-ai-model-example-linear-regression">Simplest AI model example: linear regression</h2>
<p>This example is adopted from Bethany Lusch, ALCF.</p>
<p>Linear regression is the simplest example learning from existing data for future prediction.</p>
<div id="fig-excel-linear-regression" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-excel-linear-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../figures/excel_linear_regression.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;4: Linear regression in Excel"><img src="../figures/excel_linear_regression.jpg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-excel-linear-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Linear regression in Excel
</figcaption>
</figure>
</div>
<p>We’re going to review the math involved in this process to help understand how training an AI works.</p>
<p>First we will load some tools that others wrote and we can use to help us work.</p>
<ul>
<li><a href="https://pandas.pydata.org/docs/">Pandas</a>: a toolkit for working with row vs. column data, like excel sheets, and CSV (Comma Seperated Values) files.</li>
<li><a href="https://numpy.org/doc/">Numpy</a>: a toolkit for managing arrays, vectors, matrices, etc, doing math with them, slicing them up, and many other handy things.</li>
<li><a href="https://matplotlib.org/stable/index.html">Matplotlib</a>: a toolkit for plotting data</li>
</ul>
<div id="bc8acfc0" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2"></a>os.environ[<span class="st">"FORCE_COLOR"</span>] <span class="op">=</span> <span class="st">"1"</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>os.environ[<span class="st">"TTY_INTERACTIVE"</span>] <span class="op">=</span> <span class="st">"1"</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> ambivalent</span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a>plt.style.use(ambivalent.STYLES[<span class="st">'ambivalent'</span>])</span>
<span id="cb1-10"><a href="#cb1-10"></a>sns.set_context(<span class="st">"notebook"</span>)</span>
<span id="cb1-11"><a href="#cb1-11"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> [<span class="fl">6.4</span>, <span class="fl">4.8</span>]</span>
<span id="cb1-12"><a href="#cb1-12"></a></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="im">import</span> IPython.display <span class="im">as</span> ipydis</span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="im">import</span> time</span>
<span id="cb1-18"><a href="#cb1-18"></a></span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="im">import</span> ezpz</span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="im">from</span> rich <span class="im">import</span> <span class="bu">print</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">[<span style="color: #7f7f7f; text-decoration-color: #7f7f7f">2025-08-05 12:51:50,140357</span>][<span style="color: #008000; text-decoration-color: #008000">I</span>][<span style="color: #008080; text-decoration-color: #008080; font-style: italic">ezpz</span>/<span style="color: #000080; text-decoration-color: #000080">__init__</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #800080; text-decoration-color: #800080">265</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">ezpz</span>]<span style="color: #c0c0c0; text-decoration-color: #c0c0c0"> </span>Setting logging level to <span style="color: #008000; text-decoration-color: #008000">'INFO'</span> on <span style="color: #008000; text-decoration-color: #008000">'RANK == 0'</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">[<span style="color: #7f7f7f; text-decoration-color: #7f7f7f">2025-08-05 12:51:50,142475</span>][<span style="color: #008000; text-decoration-color: #008000">I</span>][<span style="color: #008080; text-decoration-color: #008080; font-style: italic">ezpz</span>/<span style="color: #000080; text-decoration-color: #000080">__init__</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #800080; text-decoration-color: #800080">266</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">ezpz</span>]<span style="color: #c0c0c0; text-decoration-color: #c0c0c0"> </span>Setting logging level to <span style="color: #008000; text-decoration-color: #008000">'CRITICAL'</span> on all others <span style="color: #008000; text-decoration-color: #008000">'RANK != 0'</span>
</pre>
</div>
</div>
<section id="dataset" class="level3">
<h3 class="anchored" data-anchor-id="dataset">Dataset</h3>
<p>We used a realestate dataset from Kaggle to produce this reduced dataset.</p>
<p>This dataset contains the <em>sale price</em> and <em>above ground square feet</em> of many houses. We can use this data for our linear regression.</p>
<p>We use Pandas to read the data file which is stored as Comma Separated Values (CSV) and print the column labels.</p>
<p>CSV files are similar to excel sheets.</p>
<div id="5431b5b3" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="op">!</span> [ <span class="op">-</span>e .<span class="op">/</span>slimmed_realestate_data.csv ] <span class="op">||</span> wget https:<span class="op">//</span>raw.githubusercontent.com<span class="op">/</span>argonne<span class="op">-</span>lcf<span class="op">/</span>ai<span class="op">-</span>science<span class="op">-</span>training<span class="op">-</span>series<span class="op">/</span>main<span class="op">/</span><span class="dv">0</span><span class="er">1_intro_AI_on_Supercomputer</span><span class="op">/</span>slimmed_realestate_data.csv</span>
<span id="cb2-2"><a href="#cb2-2"></a>data <span class="op">=</span> pd.read_csv(<span class="st">'slimmed_realestate_data.csv'</span>)</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="bu">print</span>(data.columns)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Index</span><span style="font-weight: bold">([</span><span style="color: #008000; text-decoration-color: #008000">'Unnamed: 0'</span>, <span style="color: #008000; text-decoration-color: #008000">'SalePrice'</span>, <span style="color: #008000; text-decoration-color: #008000">'GrLivArea'</span><span style="font-weight: bold">]</span>, <span style="color: #808000; text-decoration-color: #808000">dtype</span>=<span style="color: #008000; text-decoration-color: #008000">'object'</span><span style="font-weight: bold">)</span>
</pre>
</div>
</div>
<p>Now pandas provides some helpful tools for us to inspect our data.</p>
<p>It provides a <code>plot()</code> function that, behind the scenes, is calling into the <em>Matplotlib</em> library and calling the function <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html">matplotlib.pyplot.plot()</a>.</p>
<p>In this case, we simply tell it the names of the columns we want as our <em>x</em> and <em>y</em> values and the <code>style</code> (<code>'.'</code> tells <code>matplotlib</code> to use a small dot to represent each data point).</p>
<div id="49fdebe2" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>data.plot(x<span class="op">=</span><span class="st">'GrLivArea'</span>, y<span class="op">=</span><span class="st">'SalePrice'</span>,style<span class="op">=</span><span class="st">'o'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, markeredgecolor<span class="op">=</span><span class="st">"#222"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-4-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="index_files/figure-html/cell-4-output-1.png" width="578" height="421" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>or, even better yet, use <a href="https://seaborn.pydata.org/"><code>seaborn</code></a> to plot the data:</p>
<div id="7a2cf0e5" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>sns.jointplot(</span>
<span id="cb4-2"><a href="#cb4-2"></a>    x<span class="op">=</span><span class="st">"GrLivArea"</span>,</span>
<span id="cb4-3"><a href="#cb4-3"></a>    y<span class="op">=</span><span class="st">"SalePrice"</span>,</span>
<span id="cb4-4"><a href="#cb4-4"></a>    data<span class="op">=</span>data,</span>
<span id="cb4-5"><a href="#cb4-5"></a>    kind<span class="op">=</span><span class="st">'reg'</span>,</span>
<span id="cb4-6"><a href="#cb4-6"></a>    color<span class="op">=</span>(<span class="dv">216</span> <span class="op">/</span> <span class="fl">255.0</span>, <span class="dv">100</span> <span class="op">/</span> <span class="fl">255.0</span>, <span class="dv">50</span> <span class="op">/</span> <span class="fl">255.0</span>, <span class="fl">0.33</span>),</span>
<span id="cb4-7"><a href="#cb4-7"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-5-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="index_files/figure-html/cell-5-output-1.png" width="591" height="559" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="theory-of-linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="theory-of-linear-regression">Theory of linear regression</h3>
<p>The goal of learning regression is to find a line that is closest to all the points.</p>
<p>The slope and intercept of such a line <span class="math inline">y = m x + b</span> can be found as:</p>
<p><span class="math display">m = { n (\Sigma xy) - (\Sigma x) (\Sigma y) \over n (\Sigma x^2) - (\Sigma x)^2 } </span></p>
<p><span class="math display">b = { (\Sigma y) (\Sigma x^2) - (\Sigma x) (\Sigma xy) \over n (\Sigma x^2) - (\Sigma x)^2 } </span></p>
<p>Details derivation of this can be found <a href="https://en.wikipedia.org/wiki/Simple_linear_regression">here</a>.</p>
<p>We’ll break this calculation into a few steps to help make it easier.</p>
<p>First lets define <span class="math inline">x</span> and <span class="math inline">y</span>. <span class="math inline">x</span> will be our <em>above ground square footage</em> and <span class="math inline">y</span> will be <em>sale price</em>. In our equations we have a few different values we need, such as <span class="math inline">n</span> which is just the number of points we have:</p>
<div id="0eefb63f" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>n <span class="op">=</span> <span class="bu">len</span>(data)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Then we need our <span class="math inline">x</span> and <span class="math inline">y</span> by selecting only the column we care about for each one. Note about data formats: <code>data</code> is a Pandas <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame">DataFrame</a> object which has rows and columns; <code>data['GrLivArea']</code> is a Pandas <a href="https://pandas.pydata.org/docs/reference/api/pandas.Series.html">Series</a> object which only has rows; then we also convert from <em>Pandas</em> data formats (in this case a <em>Series</em>) to <em>Numpy</em> data formats using the <code>to_numpy()</code> function which is part of the Pandas <em>Series</em> object.</p>
<div id="69aca1c2" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>x <span class="op">=</span> data[<span class="st">'GrLivArea'</span>].to_numpy()</span>
<span id="cb6-2"><a href="#cb6-2"></a>y <span class="op">=</span> data[<span class="st">'SalePrice'</span>].to_numpy()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now we will calculate <span class="math inline">\Sigma xy</span>, <span class="math inline">\Sigma x</span>, <span class="math inline">\Sigma y</span>, and <span class="math inline">\Sigma x^2</span>:</p>
<div id="5f05b1d6" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>sum_xy <span class="op">=</span> np.<span class="bu">sum</span>(x<span class="op">*</span>y)</span>
<span id="cb7-2"><a href="#cb7-2"></a>sum_x <span class="op">=</span> np.<span class="bu">sum</span>(x)</span>
<span id="cb7-3"><a href="#cb7-3"></a>sum_y <span class="op">=</span> np.<span class="bu">sum</span>(y)</span>
<span id="cb7-4"><a href="#cb7-4"></a>sum_x2 <span class="op">=</span> np.<span class="bu">sum</span>(x<span class="op">*</span>x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The denominator in the equation for <span class="math inline">m</span> and <span class="math inline">b</span> are the same so we can calculate that once:</p>
<div id="5153fffb" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>denominator <span class="op">=</span> n <span class="op">*</span> sum_x2 <span class="op">-</span> sum_x <span class="op">*</span> sum_x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Then we can calculate our fit values:</p>
<div id="58dd3c20" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>m <span class="op">=</span> (n <span class="op">*</span> sum_xy <span class="op">-</span> sum_x <span class="op">*</span> sum_y) <span class="op">/</span> denominator</span>
<span id="cb9-2"><a href="#cb9-2"></a>b <span class="op">=</span> (sum_y <span class="op">*</span> sum_x2 <span class="op">-</span> sum_x <span class="op">*</span> sum_xy) <span class="op">/</span> denominator</span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="bu">print</span>(<span class="st">'y = </span><span class="sc">%f</span><span class="st"> * x + </span><span class="sc">%f</span><span class="st">'</span> <span class="op">%</span> (m,b))</span>
<span id="cb9-4"><a href="#cb9-4"></a></span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="co"># saving these for later comparison</span></span>
<span id="cb9-6"><a href="#cb9-6"></a>m_calc <span class="op">=</span> m</span>
<span id="cb9-7"><a href="#cb9-7"></a>b_calc <span class="op">=</span> b</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">y = <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">87.688145</span> * x + <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">34754.077892</span>
</pre>
</div>
</div>
<p>Now we can plot the fit results with our data to see how we did.</p>
<p>First we define a plotting function because we’re going to do this often and we want to reuse our code:</p>
<div id="75deb891" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="kw">def</span> plot_data(x,y,m,b,plt <span class="op">=</span> plt):</span>
<span id="cb10-2"><a href="#cb10-2"></a>   <span class="co"># plot our data points with 'bo' = blue circles</span></span>
<span id="cb10-3"><a href="#cb10-3"></a>   plt.plot(x, y, <span class="st">'o'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, markeredgecolor<span class="op">=</span><span class="st">"#222"</span>)</span>
<span id="cb10-4"><a href="#cb10-4"></a>   <span class="co"># create the line based on our linear fit</span></span>
<span id="cb10-5"><a href="#cb10-5"></a>   <span class="co"># first we need to make x points</span></span>
<span id="cb10-6"><a href="#cb10-6"></a>   <span class="co"># the 'arange' function generates points between two limits (min,max)</span></span>
<span id="cb10-7"><a href="#cb10-7"></a>   linear_x <span class="op">=</span> np.arange(x.<span class="bu">min</span>(),x.<span class="bu">max</span>())</span>
<span id="cb10-8"><a href="#cb10-8"></a>   <span class="co"># now we use our fit parameters to calculate the y points based on our x points</span></span>
<span id="cb10-9"><a href="#cb10-9"></a>   linear_y <span class="op">=</span> linear_x <span class="op">*</span> m <span class="op">+</span> b</span>
<span id="cb10-10"><a href="#cb10-10"></a>   <span class="co"># plot the linear points using 'r-' = red line</span></span>
<span id="cb10-11"><a href="#cb10-11"></a>   plt.plot(linear_x, linear_y, <span class="st">'r-'</span>, label<span class="op">=</span><span class="st">'fit'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now can use this function to plot our results:</p>
<div id="b7d688ac" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>plot_data(x,y,m,b)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-12-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="index_files/figure-html/cell-12-output-1.png" width="578" height="400" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="training-through-stochastic-gradient-descent-sgd" class="level3">
<h3 class="anchored" data-anchor-id="training-through-stochastic-gradient-descent-sgd">Training through Stochastic Gradient Descent (SGD)</h3>
<p>SGD is a common method in AI for training deep neural networks on large datasets. It is an iterative method for optimizing a loss function that we get to define. We will use this simple linear regression to demonstrate how it works.</p>
<section id="the-model" class="level4">
<h4 class="anchored" data-anchor-id="the-model">The model</h4>
<p>In AI, neural networks are often referred to as a <em>model</em> because, once fully trained, they should model (AKA predict) the behavior of our system. In our example, the system is how house prices vary based on house size. We know our system is roughly driven by a linear function:</p>
<p><span class="math display">\hat{y_i}(x_i) = m * x_i + b </span></p>
<p>We just need to figure out <span class="math inline">m</span> and <span class="math inline">b</span>. Let’s create a function that calculates our model given <span class="math inline">x</span>, <span class="math inline">m</span>, and <span class="math inline">b</span>.</p>
<div id="3044320b" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="kw">def</span> model(x,m,b):</span>
<span id="cb12-2"><a href="#cb12-2"></a>   <span class="cf">return</span> m <span class="op">*</span> x <span class="op">+</span> b</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="the-loss-function" class="level4">
<h4 class="anchored" data-anchor-id="the-loss-function">The Loss Function</h4>
<p>A <em>loss function</em>, or <em>objective function</em>, is something we define and is based on what we want to achieve. In the method of SGD, it is our goal to minimize (or make close to zero) the values calculated from the <em>loss function</em>. In our example, we ideally want the prediction of our <em>model</em> to be equal to the actual data, though we will settle for “as close as possible”.</p>
<p>So we will select our <em>loss function</em> to be the <a href="https://en.wikipedia.org/wiki/Mean_squared_error">Mean Squared Error</a> function:</p>
<p><span class="math display"> L(y_i,\hat{y_i}) = (y_i - \hat{y_i}(x_i))^2 </span></p>
<p>where <span class="math inline">y_i</span> is our <span class="math inline">i^{th}</span> entry in the <code>data['SalePrice']</code> vector and <span class="math inline">\hat{y_i}</span> is the prediction based on evaluting <span class="math inline">m * x_i + b</span>.</p>
<p>This function looks like the figure below when we plot it with <span class="math inline">x=y_i - \hat{y_i}(x_i)</span> and we we want to be down near <span class="math inline">y_i - \hat{y_i}(x_i) = 0</span> which indicates that our <span class="math inline">y_i</span> is as close as possible to <span class="math inline">\hat{y_i}</span>.</p>
<div id="fig-loss-func" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-loss-func-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../figures/loss_func.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;5: Loss function for linear regression"><img src="../figures/loss_func.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-loss-func-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Loss function for linear regression
</figcaption>
</figure>
</div>
<p>Here we crate a function that calculates this for us.</p>
<div id="ca67ff34" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="kw">def</span> loss(x,y,m,b):</span>
<span id="cb13-2"><a href="#cb13-2"></a>   y_predicted <span class="op">=</span> model(x,m,b)</span>
<span id="cb13-3"><a href="#cb13-3"></a>   <span class="cf">return</span> np.power( y <span class="op">-</span> y_predicted, <span class="dv">2</span> )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="minimizing-the-loss-function" class="level4">
<h4 class="anchored" data-anchor-id="minimizing-the-loss-function">Minimizing the Loss Function</h4>
<p>We want to use the loss function in order to guide how to update <span class="math inline">m</span> and <span class="math inline">b</span> to better model our system. In calculus we learn to minimize a function with respect to a variable you calculate the <em>partial derivative</em> with respect to the variable you want to vary.</p>
<p><span class="math display"> { \partial L \over \partial m } = 0 </span></p>
<p>The location of the solution to this is the minimum as shown in the figure above. We can write down the partial derivative of the loss function as:</p>
<p><span class="math display"> { \partial L \over \partial m } = -2 x_i (y_i - \hat{y_i}(x_i)) </span> <span class="math display"> { \partial L \over \partial b } = -2 (y_i - \hat{y_i}(x_i)) </span></p>
<p>We can use this to calculate an adjustment to <span class="math inline">m</span> and <span class="math inline">b</span> that will reduce the loss function, effectively improving our fitting parameters. This is done using this equation:</p>
<p><span class="math display"> m' = m - \eta { \partial L \over \partial m }</span> <span class="math display"> b' = b - \eta { \partial L \over \partial b }</span></p>
<p>Here our original <span class="math inline">m</span> and <span class="math inline">b</span> are adjusted by the partial derivative multiplied by some small factor, <span class="math inline">\eta</span>, called the <em>learning rate</em>. This learning rate is very important in our process and must be tuned for every problem.</p>
<p>In our example, the selection of the learning rate essentially defines how close we can get to the minimum, AKA the best fit solution. This figure shows what happens when we pick a large learning rate. We first select a starting point in our loss function (typically randomly), then every update from <span class="math inline">m</span>/<span class="math inline">b</span> to <span class="math inline">m'</span>/<span class="math inline">b'</span> results in a shift to somewhere else on our loss function (following the red arrows). In this example, our learning rate (<span class="math inline">\eta</span>) has been selected too large such that we bounce back and forth around the minimum, never reaching it.</p>
<div id="fig-parabola-largeLR" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-parabola-largeLR-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../figures/parabola_largeLR.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;6: Large LR"><img src="../figures/parabola_largeLR.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-parabola-largeLR-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Large LR
</figcaption>
</figure>
</div>
<p>If we select a smaller learning we can see better behavior in the next figure.</p>
<div id="fig-parabola-smallLR" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-parabola-smallLR-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../figures/parabola_smallLR.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;7: Small LR"><img src="../figures/parabola_smallLR.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-parabola-smallLR-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Small LR
</figcaption>
</figure>
</div>
<p>Though, keep in mind, too small a learning rate results is so little progress toward the minimum that you may never reach it!</p>
<p>A pit fall of SGD that one must be aware of is when your loss function is complex, with many minima. The next figure shows such a case, in which we select a small learning rate and our starting point happens to be near a local minimum that is not the lowest minimum. As shown, we do reach a minimum, but it isn’t the lowest minimum in our loss function. It could be that we randomly select a starting point near the minimum we care about, but we should build methods that are more robust against randomly getting the right answer.</p>
<div id="fig-local-min-smallLR" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-local-min-smallLR-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../figures/local_min_smallLR.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;8: Local minimal with small LR"><img src="../figures/local_min_smallLR.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-local-min-smallLR-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Local minimal with small LR
</figcaption>
</figure>
</div>
<p>Then, if we increase our learning rate too much, we bounce around again.</p>
<div id="fig-local-min-largeLR" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-local-min-largeLR-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../figures/local_min_largeLR.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;9: Local minimal with large LR"><img src="../figures/local_min_largeLR.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-local-min-largeLR-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Local minimal with large LR
</figcaption>
</figure>
</div>
<p>What we want to do in this situation is start with a large learning rate and slowly reduce its size as we progress. That is shown in this next figure.</p>
<div id="fig-local-min-variableLR" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-local-min-variableLR-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../figures/local_min_variableLR.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;10: Local min with variable LR"><img src="../figures/local_min_variableLR.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-local-min-variableLR-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Local min with variable LR
</figcaption>
</figure>
</div>
<p>As you can see, this process is not perfect and could still land in a local minimum, but it is important to be aware of these behaviors as you utilize SGD in machine learning.</p>
<p>So let’s continue, we’ll build functions we can use to update our fit parameters, <span class="math inline">m</span> and <span class="math inline">b</span>.</p>
<div id="33f3c3d0" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="kw">def</span> updated_m(x,y,m,b,learning_rate):</span>
<span id="cb14-2"><a href="#cb14-2"></a>   dL_dm <span class="op">=</span> <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> x <span class="op">*</span> (y <span class="op">-</span> model(x,m,b))</span>
<span id="cb14-3"><a href="#cb14-3"></a>   dL_dm <span class="op">=</span> np.mean(dL_dm)</span>
<span id="cb14-4"><a href="#cb14-4"></a>   <span class="cf">return</span> m <span class="op">-</span> learning_rate <span class="op">*</span> dL_dm</span>
<span id="cb14-5"><a href="#cb14-5"></a></span>
<span id="cb14-6"><a href="#cb14-6"></a><span class="kw">def</span> updated_b(x,y,m,b,learning_rate):</span>
<span id="cb14-7"><a href="#cb14-7"></a>   dL_db <span class="op">=</span> <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> (y <span class="op">-</span> model(x,m,b))</span>
<span id="cb14-8"><a href="#cb14-8"></a>   dL_db <span class="op">=</span> np.mean(dL_db)</span>
<span id="cb14-9"><a href="#cb14-9"></a>   <span class="cf">return</span> b <span class="op">-</span> learning_rate <span class="op">*</span> dL_db</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
</section>
<section id="putting-it-together" class="level2">
<h2 class="anchored" data-anchor-id="putting-it-together">Putting it together</h2>
<p>We can now randomly select our initial slope and intercept:</p>
<div id="69fb7c9a" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>m <span class="op">=</span> <span class="fl">5.</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>b <span class="op">=</span> <span class="fl">1000.</span></span>
<span id="cb15-3"><a href="#cb15-3"></a><span class="bu">print</span>(<span class="ss">f"y_i = </span><span class="sc">{</span>m<span class="sc">:.2f}</span><span class="ss"> * x + </span><span class="sc">{</span>b<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="co"># print('y_i = %.2f * x + %.2f' % (m,b))</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">y_i = <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5.00</span> * x + <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1000.00</span>
</pre>
</div>
</div>
<p>Then we can calculate our Loss function:</p>
<div id="4362a66d" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>l <span class="op">=</span> loss(x,y,m,b)</span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="bu">print</span>(<span class="ss">f'first 10 loss values: </span><span class="sc">{</span>l[:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">first <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span> loss values: <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3.03421561e+10</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3.55511025e+10</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.24579082e+10</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.91656336e+10</span>
 <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.60604929e+10</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.04432804e+10</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.72410030e+10</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.76517796e+10</span>
 <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.52769600e+10</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.18152900e+10</span><span style="font-weight: bold">]</span>
</pre>
</div>
</div>
<div id="4ade74f6" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-9</span></span>
<span id="cb17-2"><a href="#cb17-2"></a>m <span class="op">=</span> updated_m(x,y,m,b,learning_rate)</span>
<span id="cb17-3"><a href="#cb17-3"></a>b <span class="op">=</span> updated_b(x,y,m,b,learning_rate)</span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="bu">print</span>(<span class="st">'y_i = </span><span class="sc">%.2f</span><span class="st"> * x + </span><span class="sc">%.2f</span><span class="st">     previously calculated: y_i = </span><span class="sc">%.2f</span><span class="st"> * x + </span><span class="sc">%.2f</span><span class="st">'</span> <span class="op">%</span> (m,b,m_calc,b_calc))</span>
<span id="cb17-5"><a href="#cb17-5"></a>plot_data(x,y,m,b)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">y_i = <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5.47</span> * x + <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1000.00</span>     previously calculated: y_i = <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">87.69</span> * x + <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">34754.08</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-18-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img src="index_files/figure-html/cell-18-output-2.png" width="578" height="400" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<div id="1135e018" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="co"># set our initial slope and intercept</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>m <span class="op">=</span> <span class="fl">5.</span></span>
<span id="cb18-3"><a href="#cb18-3"></a>b <span class="op">=</span> <span class="fl">1000.</span></span>
<span id="cb18-4"><a href="#cb18-4"></a><span class="co"># batch_size = 60</span></span>
<span id="cb18-5"><a href="#cb18-5"></a><span class="co"># set a learning rate for each parameter</span></span>
<span id="cb18-6"><a href="#cb18-6"></a>learning_rate_m <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb18-7"><a href="#cb18-7"></a>learning_rate_b <span class="op">=</span> <span class="fl">1e-1</span></span>
<span id="cb18-8"><a href="#cb18-8"></a><span class="co"># use these to plot our progress over time</span></span>
<span id="cb18-9"><a href="#cb18-9"></a>loss_history <span class="op">=</span> []</span>
<span id="cb18-10"><a href="#cb18-10"></a><span class="co"># convert panda data to numpy arrays, one for the "Ground Living Area" and one for "Sale Price"</span></span>
<span id="cb18-11"><a href="#cb18-11"></a>data_x <span class="op">=</span> data[<span class="st">'GrLivArea'</span>].to_numpy()</span>
<span id="cb18-12"><a href="#cb18-12"></a>data_y <span class="op">=</span> data[<span class="st">'SalePrice'</span>].to_numpy()</span>
<span id="cb18-13"><a href="#cb18-13"></a><span class="co"># we run our loop N times</span></span>
<span id="cb18-14"><a href="#cb18-14"></a>loop_N <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb18-15"><a href="#cb18-15"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(loop_N):</span>
<span id="cb18-16"><a href="#cb18-16"></a>    <span class="co"># update our slope and intercept based on the current values</span></span>
<span id="cb18-17"><a href="#cb18-17"></a>    m <span class="op">=</span> updated_m(data_x,data_y,m,b,learning_rate_m)</span>
<span id="cb18-18"><a href="#cb18-18"></a>    b <span class="op">=</span> updated_b(data_x,data_y,m,b,learning_rate_b)</span>
<span id="cb18-19"><a href="#cb18-19"></a></span>
<span id="cb18-20"><a href="#cb18-20"></a>    <span class="co"># calculate the loss value</span></span>
<span id="cb18-21"><a href="#cb18-21"></a>    loss_value <span class="op">=</span> np.mean(loss(data_x,data_y,m,b))</span>
<span id="cb18-22"><a href="#cb18-22"></a></span>
<span id="cb18-23"><a href="#cb18-23"></a>    <span class="co"># keep a history of our loss values</span></span>
<span id="cb18-24"><a href="#cb18-24"></a>    loss_history.append(loss_value)</span>
<span id="cb18-25"><a href="#cb18-25"></a></span>
<span id="cb18-26"><a href="#cb18-26"></a>    <span class="co"># print our progress</span></span>
<span id="cb18-27"><a href="#cb18-27"></a>    mstr <span class="op">=</span> <span class="st">" "</span>.join([</span>
<span id="cb18-28"><a href="#cb18-28"></a>        <span class="ss">f"[</span><span class="sc">{</span>i<span class="sc">:03d}</span><span class="ss">]"</span>,</span>
<span id="cb18-29"><a href="#cb18-29"></a>        <span class="ss">f"dy_i = </span><span class="sc">{</span>m<span class="sc">:.2f}</span><span class="ss"> * x + </span><span class="sc">{</span>b<span class="sc">:.2f}</span><span class="ss">"</span>,</span>
<span id="cb18-30"><a href="#cb18-30"></a>        <span class="ss">f"previously calculated: y_i = </span><span class="sc">{</span>m_calc<span class="sc">:.2f}</span><span class="ss"> * x + </span><span class="sc">{</span>b_calc<span class="sc">:.2f}</span><span class="ss">"</span>,</span>
<span id="cb18-31"><a href="#cb18-31"></a>        <span class="ss">f"loss: </span><span class="sc">{</span>loss_value<span class="sc">:.2f}</span><span class="ss">"</span>,</span>
<span id="cb18-32"><a href="#cb18-32"></a>    ])</span>
<span id="cb18-33"><a href="#cb18-33"></a>    <span class="bu">print</span>(mstr)</span>
<span id="cb18-34"><a href="#cb18-34"></a>    <span class="co"># print(</span></span>
<span id="cb18-35"><a href="#cb18-35"></a>    <span class="co">#         '[%03d]  dy_i = %.2f * x + %.2f     previously calculated: y_i = %.2f * x + %.2f    loss: %f' % (i,m,b,m_calc,b_calc,loss_value))</span></span>
<span id="cb18-36"><a href="#cb18-36"></a></span>
<span id="cb18-37"><a href="#cb18-37"></a>    <span class="co"># close/delete previous plots</span></span>
<span id="cb18-38"><a href="#cb18-38"></a>    plt.close(<span class="st">'all'</span>)</span>
<span id="cb18-39"><a href="#cb18-39"></a></span>
<span id="cb18-40"><a href="#cb18-40"></a>    dfigsize <span class="op">=</span> plt.rcParams[<span class="st">'figure.figsize'</span>]</span>
<span id="cb18-41"><a href="#cb18-41"></a>    <span class="co"># create a 1 by 2 plot grid</span></span>
<span id="cb18-42"><a href="#cb18-42"></a>    fig,ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(dfigsize[<span class="dv">0</span>]<span class="op">*</span><span class="dv">2</span>,dfigsize[<span class="dv">1</span>]))</span>
<span id="cb18-43"><a href="#cb18-43"></a>    <span class="co"># lot our usual output</span></span>
<span id="cb18-44"><a href="#cb18-44"></a>    plot_data(data_x,data_y,m,b,ax[<span class="dv">0</span>])</span>
<span id="cb18-45"><a href="#cb18-45"></a></span>
<span id="cb18-46"><a href="#cb18-46"></a>    <span class="co"># here we also plot the calculated linear fit for comparison</span></span>
<span id="cb18-47"><a href="#cb18-47"></a>    line_x <span class="op">=</span> np.arange(data_x.<span class="bu">min</span>(),data_x.<span class="bu">max</span>())</span>
<span id="cb18-48"><a href="#cb18-48"></a>    line_y <span class="op">=</span> line_x <span class="op">*</span> m_calc <span class="op">+</span> b_calc</span>
<span id="cb18-49"><a href="#cb18-49"></a>    ax[<span class="dv">0</span>].plot(line_x,line_y, color<span class="op">=</span><span class="st">"#000"</span>, linestyle<span class="op">=</span><span class="st">":"</span> ,label<span class="op">=</span><span class="st">'calculated'</span>)</span>
<span id="cb18-50"><a href="#cb18-50"></a>    <span class="co"># add a legend to the plot and x/y labels</span></span>
<span id="cb18-51"><a href="#cb18-51"></a>    ax[<span class="dv">0</span>].legend()</span>
<span id="cb18-52"><a href="#cb18-52"></a>    ax[<span class="dv">0</span>].set_xlabel(<span class="st">'square footage'</span>)</span>
<span id="cb18-53"><a href="#cb18-53"></a>    ax[<span class="dv">0</span>].set_ylabel(<span class="st">'sale price'</span>)</span>
<span id="cb18-54"><a href="#cb18-54"></a></span>
<span id="cb18-55"><a href="#cb18-55"></a>    <span class="co"># plot the loss </span></span>
<span id="cb18-56"><a href="#cb18-56"></a>    loss_x <span class="op">=</span> np.arange(<span class="dv">0</span>,<span class="bu">len</span>(loss_history))</span>
<span id="cb18-57"><a href="#cb18-57"></a>    loss_y <span class="op">=</span> np.asarray(loss_history)</span>
<span id="cb18-58"><a href="#cb18-58"></a>    ax[<span class="dv">1</span>].plot(loss_x,loss_y, <span class="st">'o-'</span>)</span>
<span id="cb18-59"><a href="#cb18-59"></a>    ax[<span class="dv">1</span>].set_yscale(<span class="st">'log'</span>)</span>
<span id="cb18-60"><a href="#cb18-60"></a>    ax[<span class="dv">1</span>].set_xlabel(<span class="st">'loop step'</span>)</span>
<span id="cb18-61"><a href="#cb18-61"></a>    ax[<span class="dv">1</span>].set_ylabel(<span class="st">'loss'</span>)</span>
<span id="cb18-62"><a href="#cb18-62"></a>    plt.show()</span>
<span id="cb18-63"><a href="#cb18-63"></a>    <span class="co"># gives us time to see the plot</span></span>
<span id="cb18-64"><a href="#cb18-64"></a>    time.sleep(<span class="fl">2.5</span>)</span>
<span id="cb18-65"><a href="#cb18-65"></a>    <span class="co"># clears the plot when the next plot is ready to show.</span></span>
<span id="cb18-66"><a href="#cb18-66"></a>    ipydis.clear_output(wait<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">029</span><span style="font-weight: bold">]</span> dy_i = <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">88.89</span> * x + <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32912.24</span> previously calculated: y_i = <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">87.69</span> * x + <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">34754.08</span> loss: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1478200827.64</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-19-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img src="index_files/figure-html/cell-19-output-2.png" width="1060" height="421" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="homework" class="level2">
<h2 class="anchored" data-anchor-id="homework">Homework</h2>
<section id="mini-batch-training" class="level3">
<h3 class="anchored" data-anchor-id="mini-batch-training">Mini Batch Training</h3>
<p>In AI, datasets are often very large and cannot be processed all at once as is done in the loop above. The data is instead randomly sampled in smaller <em>batches</em> where each <em>batch</em> contains <code>batch_size</code> inputs.</p>
<p>How can you change the loop above to sample the dataset in smaller batches?</p>
<p>Hint: Our <code>data</code> variable is a Pandas <code>DataFrame</code> object, search for “how to sample a DataFrame”.</p>
<p>Instead of using the entire dataset like:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>data_x <span class="op">=</span> data[<span class="st">'GrLivArea'</span>].to_numpy()</span>
<span id="cb19-2"><a href="#cb19-2"></a>data_y <span class="op">=</span> data[<span class="st">'SalePrice'</span>].to_numpy()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Use</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>data_batch <span class="op">=</span> data.sample(batch_size)</span>
<span id="cb20-2"><a href="#cb20-2"></a>data_x <span class="op">=</span> data_batch[<span class="st">'GrLivArea'</span>].to_numpy()</span>
<span id="cb20-3"><a href="#cb20-3"></a>data_y <span class="op">=</span> data_batch[<span class="st">'SalePrice'</span>].to_numpy()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>You also have to adjust the loop_N accordingly to make sure that it loop over the entire datasets the same number of times.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>loop_N <span class="op">=</span> <span class="dv">30</span><span class="op">*</span><span class="bu">len</span>(data)<span class="op">//</span>batch_size</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Please plot your learning curve for different batch size, such as 32, 64, 128, 256, 512.</p>
</section>
<section id="learning-rate-issue-bonus" class="level3">
<h3 class="anchored">Learning rate issue (Bonus)</h3>
<p>As described above, if the learning rate is too large, it will affect the convergence. Do your training with (batch_size = 64, learning_rate_m = 1e-7, learning_rate_b = 1e-1). Then linearly increase the batch size and learning rate until you see the training does not converge.</p>
<pre><code>(64, 1e-7, 1e-1)*1
(64, 1e-7, 1e-1)*2
(64, 1e-7, 1e-1)*4
(64, 1e-7, 1e-1)*8
...</code></pre>
<p><strong>How to submit your homework</strong></p>
<ul>
<li>Fork the github repo to your personal github</li>
<li>Make change to the 01_linear_regression_sgd.ipynb, and then push to your personal github</li>
<li>Provide the link of 01_linear_regression_sgd in the personal github.</li>
</ul>
<details closed="">
<summary>
<h2 class="anchored" data-anchor-id="learning-rate-issue-bonus">
Homework Answer
</h2>
</summary>
<p>Let us define a train function which allow us to try different hyperparameter setups.</p>
<div id="9d90dc9a" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>x <span class="op">=</span> data[<span class="st">'GrLivArea'</span>].to_numpy()</span>
<span id="cb23-2"><a href="#cb23-2"></a>y <span class="op">=</span> data[<span class="st">'SalePrice'</span>].to_numpy()</span>
<span id="cb23-3"><a href="#cb23-3"></a></span>
<span id="cb23-4"><a href="#cb23-4"></a><span class="kw">def</span> train(batch_size, epochs<span class="op">=</span><span class="dv">30</span>, learning_rate_m <span class="op">=</span> <span class="fl">1e-7</span>, learning_rate_b <span class="op">=</span> <span class="fl">1e-1</span>):</span>
<span id="cb23-5"><a href="#cb23-5"></a>    loss_history <span class="op">=</span> []</span>
<span id="cb23-6"><a href="#cb23-6"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(data)<span class="op">//</span>batch_size</span>
<span id="cb23-7"><a href="#cb23-7"></a>    loop_N <span class="op">=</span> epochs<span class="op">*</span>num_batches</span>
<span id="cb23-8"><a href="#cb23-8"></a>    m <span class="op">=</span> <span class="fl">5.</span></span>
<span id="cb23-9"><a href="#cb23-9"></a>    b <span class="op">=</span> <span class="fl">1000.</span></span>
<span id="cb23-10"><a href="#cb23-10"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(loop_N):</span>
<span id="cb23-11"><a href="#cb23-11"></a>        data_batch <span class="op">=</span> data.sample(batch_size)</span>
<span id="cb23-12"><a href="#cb23-12"></a>        data_x <span class="op">=</span> data_batch[<span class="st">'GrLivArea'</span>].to_numpy()</span>
<span id="cb23-13"><a href="#cb23-13"></a>        data_y <span class="op">=</span> data_batch[<span class="st">'SalePrice'</span>].to_numpy()</span>
<span id="cb23-14"><a href="#cb23-14"></a>        <span class="co"># update our slope and intercept based on the current values</span></span>
<span id="cb23-15"><a href="#cb23-15"></a>        m <span class="op">=</span> updated_m(data_x,data_y,m,b,learning_rate_m)</span>
<span id="cb23-16"><a href="#cb23-16"></a>        b <span class="op">=</span> updated_b(data_x,data_y,m,b,learning_rate_b)</span>
<span id="cb23-17"><a href="#cb23-17"></a></span>
<span id="cb23-18"><a href="#cb23-18"></a>        <span class="co"># calculate the loss value</span></span>
<span id="cb23-19"><a href="#cb23-19"></a>        loss_value <span class="op">=</span> np.mean(loss(data_x,data_y,m,b))</span>
<span id="cb23-20"><a href="#cb23-20"></a></span>
<span id="cb23-21"><a href="#cb23-21"></a>        <span class="co"># keep a history of our loss values</span></span>
<span id="cb23-22"><a href="#cb23-22"></a>        loss_history.append(loss_value)</span>
<span id="cb23-23"><a href="#cb23-23"></a>    <span class="co">#loss_last_epoch = np.sum(loss_history[-num_batches:]*batch_size)/len(data)</span></span>
<span id="cb23-24"><a href="#cb23-24"></a>    <span class="cf">return</span> m, b, np.mean(loss(x,y,m,b))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</details></section>
</section>
<section id="minibatch-training" class="level2">
<h2 class="anchored" data-anchor-id="minibatch-training">Minibatch training</h2>
<div id="d01866e1" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="bu">print</span>(<span class="st">'previously calculated: y_i = </span><span class="sc">%.2f</span><span class="st"> * x + </span><span class="sc">%.2f</span><span class="st">    loss: </span><span class="sc">%f</span><span class="ch">\n</span><span class="st">======================================='</span> <span class="op">%</span> (m_calc,b_calc,loss_value))</span>
<span id="cb24-2"><a href="#cb24-2"></a></span>
<span id="cb24-3"><a href="#cb24-3"></a></span>
<span id="cb24-4"><a href="#cb24-4"></a><span class="cf">for</span> bs <span class="kw">in</span> <span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">512</span>:</span>
<span id="cb24-5"><a href="#cb24-5"></a>    m, b, l <span class="op">=</span> train(bs, epochs<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb24-6"><a href="#cb24-6"></a>    <span class="bu">print</span>(<span class="ss">f"batch size: </span><span class="sc">{</span>bs<span class="sc">}</span><span class="ss">, m=</span><span class="sc">{</span>m<span class="sc">:.4f}</span><span class="ss">, b=</span><span class="sc">{</span>b<span class="sc">:.4f}</span><span class="ss">, loss=</span><span class="sc">{</span>l<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">previously calculated: y_i = <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">87.69</span> * x + <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">34754.08</span>    loss: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1478200827.641291</span>
=======================================
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">batch size: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">64</span>, <span style="color: #808000; text-decoration-color: #808000">m</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">81.4346</span>, <span style="color: #808000; text-decoration-color: #808000">b</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">37598.9686</span>, <span style="color: #808000; text-decoration-color: #808000">loss</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1524042838.5582</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">batch size: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">m</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">82.9546</span>, <span style="color: #808000; text-decoration-color: #808000">b</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">36569.2526</span>, <span style="color: #808000; text-decoration-color: #808000">loss</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1507453099.5719</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">batch size: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #808000; text-decoration-color: #808000">m</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">88.9217</span>, <span style="color: #808000; text-decoration-color: #808000">b</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">33983.0016</span>, <span style="color: #808000; text-decoration-color: #808000">loss</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1479132254.9839</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">batch size: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">512</span>, <span style="color: #808000; text-decoration-color: #808000">m</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">88.9744</span>, <span style="color: #808000; text-decoration-color: #808000">b</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">33121.6436</span>, <span style="color: #808000; text-decoration-color: #808000">loss</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1478267991.4074</span>
</pre>
</div>
</div>
<p>We see that eventually, we all get similar results with the minibatch training. Of course, here, we still keep the same learning rate. A gene</p>
</section>
<section id="learning-rate" class="level2">
<h2 class="anchored" data-anchor-id="learning-rate">Learning rate</h2>
<div id="82efcc22" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>:</span>
<span id="cb25-2"><a href="#cb25-2"></a>    bs, lrm, lrb <span class="op">=</span> np.array([<span class="dv">64</span>, <span class="fl">1e-7</span>, <span class="fl">1e-1</span>])<span class="op">*</span>i</span>
<span id="cb25-3"><a href="#cb25-3"></a>    bs <span class="op">=</span> <span class="bu">int</span>(bs)</span>
<span id="cb25-4"><a href="#cb25-4"></a>    m, b, l <span class="op">=</span> train(<span class="bu">int</span>(bs), epochs<span class="op">=</span><span class="dv">30</span>, learning_rate_m <span class="op">=</span> lrm, learning_rate_b <span class="op">=</span> lrb)</span>
<span id="cb25-5"><a href="#cb25-5"></a>    <span class="bu">print</span>(<span class="ss">f"batch size: </span><span class="sc">{</span>bs<span class="sc">}</span><span class="ss">, m=</span><span class="sc">{</span>m<span class="sc">:.4f}</span><span class="ss">, b=</span><span class="sc">{</span>b<span class="sc">:.4f}</span><span class="ss">, loss=</span><span class="sc">{</span>l<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">batch size: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">64</span>, <span style="color: #808000; text-decoration-color: #808000">m</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">89.9543</span>, <span style="color: #808000; text-decoration-color: #808000">b</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">33462.2763</span>, <span style="color: #808000; text-decoration-color: #808000">loss</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1482797783.8074</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">batch size: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">m</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">89.8994</span>, <span style="color: #808000; text-decoration-color: #808000">b</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">34131.8465</span>, <span style="color: #808000; text-decoration-color: #808000">loss</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1485330197.9274</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">batch size: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #808000; text-decoration-color: #808000">m</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">83.4639</span>, <span style="color: #808000; text-decoration-color: #808000">b</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">38002.0392</span>, <span style="color: #808000; text-decoration-color: #808000">loss</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1489970643.3773</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">batch size: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">512</span>, <span style="color: #808000; text-decoration-color: #808000">m</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">60644.7371</span>, <span style="color: #808000; text-decoration-color: #808000">b</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">28757847.8287</span>, <span style="color: #808000; text-decoration-color: #808000">loss</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13999940210285804.0000</span>
</pre>
</div>
</div>
<p>We can see that, if we increase the batch size and the learning rate proportionally, at certain point, it does not converge for example for the case batch size = 512. To increase the learning rate proportional to the batch size is a general practice. However, if the learning rate is too large, it will continue to move around without finding a local minimum. One trick, people can do is to start with a smaller learning rate in the first few steps / epochs, and once the optimization becomes stable, increase the learning rate proportional to the batch size.</p>




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{foreman2025,
  author = {Foreman, Sam},
  title = {Example: {Linear} {Regression}},
  date = {2025-07-15},
  url = {https://saforem2.github.io/hpc-bootcamp-2025/00-intro-AI-HPC/6-linear-regression/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-foreman2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Foreman, Sam. 2025. <span>“Example: Linear Regression.”</span> July 15,
2025. <a href="https://saforem2.github.io/hpc-bootcamp-2025/00-intro-AI-HPC/6-linear-regression/">https://saforem2.github.io/hpc-bootcamp-2025/00-intro-AI-HPC/6-linear-regression/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/saforem2\.github\.io\/hpc-bootcamp-2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="dark">
<input type="hidden" id="giscus-alt-theme" value="light">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "saforem2/intro-hpc-bootcamp-2025";
    script.dataset.repoId = "R_kgDOPQ7DhQ";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOPQ7Dhc4CtXSR";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../00-intro-AI-HPC/5-mcmc-example/index.html" class="pagination-link" aria-label="[5] MCMC Example">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">[5] MCMC Example</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../00-intro-AI-HPC/7-statistical-learning/index.html" class="pagination-link" aria-label="[7] Statistical Learning">
        <span class="nav-page-text">[7] Statistical Learning</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb26" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb26-1"><a href="#cb26-1"></a><span class="co">---</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="an">title:</span><span class="co"> "Example: Linear Regression"</span></span>
<span id="cb26-3"><a href="#cb26-3"></a><span class="an">description:</span><span class="co"> "Simple example illustrating how to train a linear regression model using stochastic gradient descent (SGD) on a dataset of house prices."</span></span>
<span id="cb26-4"><a href="#cb26-4"></a><span class="an">categories:</span></span>
<span id="cb26-5"><a href="#cb26-5"></a><span class="co">  - ai</span></span>
<span id="cb26-6"><a href="#cb26-6"></a><span class="co">  - hpc</span></span>
<span id="cb26-7"><a href="#cb26-7"></a><span class="co">  - ai4science</span></span>
<span id="cb26-8"><a href="#cb26-8"></a><span class="an">date:</span><span class="co"> '2025-07-15'</span></span>
<span id="cb26-9"><a href="#cb26-9"></a><span class="an">date-modified:</span><span class="co"> last-modified</span></span>
<span id="cb26-10"><a href="#cb26-10"></a><span class="an">format:</span></span>
<span id="cb26-11"><a href="#cb26-11"></a><span class="co">  ipynb:</span></span>
<span id="cb26-12"><a href="#cb26-12"></a><span class="co">    default-image-extension: svg</span></span>
<span id="cb26-13"><a href="#cb26-13"></a><span class="co">  html: default</span></span>
<span id="cb26-14"><a href="#cb26-14"></a><span class="co">  gfm:</span></span>
<span id="cb26-15"><a href="#cb26-15"></a><span class="co">    toc: true</span></span>
<span id="cb26-16"><a href="#cb26-16"></a><span class="co">---</span></span>
<span id="cb26-17"><a href="#cb26-17"></a></span>
<span id="cb26-18"><a href="#cb26-18"></a><span class="co">[</span><span class="al">![](https://colab.research.google.com/assets/colab-badge.svg)</span><span class="co">](https://colab.research.google.com/github/saforem2/intro-hpc-bootcamp-2025/blob/main/docs/00-intro-AI-HPC/6-linear-regression/index.ipynb)</span></span>
<span id="cb26-19"><a href="#cb26-19"></a></span>
<span id="cb26-20"><a href="#cb26-20"></a>In this notebook, we will talk about:</span>
<span id="cb26-21"><a href="#cb26-21"></a></span>
<span id="cb26-22"><a href="#cb26-22"></a><span class="ss">- </span>What is AI training?</span>
<span id="cb26-23"><a href="#cb26-23"></a><span class="ss">- </span>How does large language model work?</span>
<span id="cb26-24"><a href="#cb26-24"></a><span class="ss">- </span>A simple AI model: linear regression</span>
<span id="cb26-25"><a href="#cb26-25"></a></span>
<span id="cb26-26"><a href="#cb26-26"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="ot"> closed</span><span class="dt">&gt;&lt;</span><span class="kw">summary</span><span class="dt">&gt;&lt;</span><span class="kw">h2</span><span class="dt">&gt;</span>ALCF Specific Setup<span class="dt">&lt;/</span><span class="kw">h2</span><span class="dt">&gt;&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb26-27"><a href="#cb26-27"></a></span>
<span id="cb26-28"><a href="#cb26-28"></a><span class="fu">## How to run this notebook on Polaris</span></span>
<span id="cb26-29"><a href="#cb26-29"></a></span>
<span id="cb26-30"><a href="#cb26-30"></a><span class="ss">- </span>Go to <span class="ot">&lt;https://jupyter.alcf.anl.gov&gt;</span>, and click "Login Polaris"</span>
<span id="cb26-31"><a href="#cb26-31"></a><span class="ss">- </span>After login, select <span class="in">`ALCFAITP`</span> project and <span class="in">`ALCFAITP`</span> queue during the</span>
<span id="cb26-32"><a href="#cb26-32"></a>  lecture (use <span class="in">`debug`</span> queue outside of the lecture)</span>
<span id="cb26-33"><a href="#cb26-33"></a><span class="ss">- </span>Load the notebook and select "datascience/conda-2023-01-10" python kernel</span>
<span id="cb26-34"><a href="#cb26-34"></a></span>
<span id="cb26-35"><a href="#cb26-35"></a>::: {#fig-jupyter}</span>
<span id="cb26-36"><a href="#cb26-36"></a></span>
<span id="cb26-37"><a href="#cb26-37"></a><span class="al">![](../figures/jupyter1.png)</span></span>
<span id="cb26-38"><a href="#cb26-38"></a><span class="al">![](../figures/jupyter2.png)</span></span>
<span id="cb26-39"><a href="#cb26-39"></a></span>
<span id="cb26-40"><a href="#cb26-40"></a>:::</span>
<span id="cb26-41"><a href="#cb26-41"></a></span>
<span id="cb26-42"><a href="#cb26-42"></a>**How to run this notebook on Google Colab**</span>
<span id="cb26-43"><a href="#cb26-43"></a></span>
<span id="cb26-44"><a href="#cb26-44"></a><span class="ss">- </span>Go to https://colab.research.google.com/, sign in or sign up</span>
<span id="cb26-45"><a href="#cb26-45"></a><span class="ss">- </span>"File"-&gt; "open notebook"</span>
<span id="cb26-46"><a href="#cb26-46"></a><span class="ss">- </span>Choose <span class="in">`01_intro_AI_on_Supercomputer/01_linear_regression_sgd.ipynb`</span> from the list</span>
<span id="cb26-47"><a href="#cb26-47"></a></span>
<span id="cb26-48"><a href="#cb26-48"></a>  <span class="al">![Google Colab](../figures/colab.png)</span></span>
<span id="cb26-49"><a href="#cb26-49"></a></span>
<span id="cb26-50"><a href="#cb26-50"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb26-51"><a href="#cb26-51"></a></span>
<span id="cb26-52"><a href="#cb26-52"></a><span class="fu">## What is AI training?</span></span>
<span id="cb26-53"><a href="#cb26-53"></a></span>
<span id="cb26-54"><a href="#cb26-54"></a>**Two ways of learning English**:</span>
<span id="cb26-55"><a href="#cb26-55"></a></span>
<span id="cb26-56"><a href="#cb26-56"></a><span class="ss">- </span>through learning rules;</span>
<span id="cb26-57"><a href="#cb26-57"></a><span class="ss">- </span>through hearing a lot of speakings</span>
<span id="cb26-58"><a href="#cb26-58"></a></span>
<span id="cb26-59"><a href="#cb26-59"></a>::: {#fig-data-driven}</span>
<span id="cb26-60"><a href="#cb26-60"></a></span>
<span id="cb26-61"><a href="#cb26-61"></a><span class="al">![](../figures/data_driven.png)</span>{width="90%"}</span>
<span id="cb26-62"><a href="#cb26-62"></a></span>
<span id="cb26-63"><a href="#cb26-63"></a>Data Driven Learning</span>
<span id="cb26-64"><a href="#cb26-64"></a>:::</span>
<span id="cb26-65"><a href="#cb26-65"></a></span>
<span id="cb26-66"><a href="#cb26-66"></a>I learned English in my middle school, and memorized a lot of grammar rules in</span>
<span id="cb26-67"><a href="#cb26-67"></a>my mind.</span>
<span id="cb26-68"><a href="#cb26-68"></a>Every time when I speak, I try to follow the grammar rules as much as I can.</span>
<span id="cb26-69"><a href="#cb26-69"></a>But I always break the rules.</span>
<span id="cb26-70"><a href="#cb26-70"></a></span>
<span id="cb26-71"><a href="#cb26-71"></a>However, my daugher learned English differently.</span>
<span id="cb26-72"><a href="#cb26-72"></a>She learns speaking by hearing a lot of speaking from TV, teachers, classmates,</span>
<span id="cb26-73"><a href="#cb26-73"></a>and her older brother.</span>
<span id="cb26-74"><a href="#cb26-74"></a>The fact is that, she seldomly breaks grammar rules. This way of learning by</span>
<span id="cb26-75"><a href="#cb26-75"></a>observing patterns is very powerful! This is the essence of AI or data driven</span>
<span id="cb26-76"><a href="#cb26-76"></a>science.</span>
<span id="cb26-77"><a href="#cb26-77"></a></span>
<span id="cb26-78"><a href="#cb26-78"></a><span class="fu">## How does large language model work?</span></span>
<span id="cb26-79"><a href="#cb26-79"></a></span>
<span id="cb26-80"><a href="#cb26-80"></a>Large Language Models, like GPT, function by pre-training on extensive datasets</span>
<span id="cb26-81"><a href="#cb26-81"></a>to learn language patterns, utilizing transformer architecture for contextual</span>
<span id="cb26-82"><a href="#cb26-82"></a>understanding, and can be fine-tuned for specific tasks, enabling them to</span>
<span id="cb26-83"><a href="#cb26-83"></a>generate coherent and contextually relevant text based on provided inputs.</span>
<span id="cb26-84"><a href="#cb26-84"></a></span>
<span id="cb26-85"><a href="#cb26-85"></a>::: {#fig-llm}</span>
<span id="cb26-86"><a href="#cb26-86"></a></span>
<span id="cb26-87"><a href="#cb26-87"></a><span class="al">![](../figures/llm2.png)</span></span>
<span id="cb26-88"><a href="#cb26-88"></a><span class="al">![](../figures/chatgpt_response.png)</span></span>
<span id="cb26-89"><a href="#cb26-89"></a></span>
<span id="cb26-90"><a href="#cb26-90"></a>:::</span>
<span id="cb26-91"><a href="#cb26-91"></a></span>
<span id="cb26-92"><a href="#cb26-92"></a></span>
<span id="cb26-93"><a href="#cb26-93"></a>**More complicated example**:</span>
<span id="cb26-94"><a href="#cb26-94"></a></span>
<span id="cb26-95"><a href="#cb26-95"></a></span>
<span id="cb26-96"><a href="#cb26-96"></a>::: {.flex-container}</span>
<span id="cb26-97"><a href="#cb26-97"></a></span>
<span id="cb26-98"><a href="#cb26-98"></a>::: {.flex-item style="width:43%"}</span>
<span id="cb26-99"><a href="#cb26-99"></a><span class="al">![Poem about Albert Einstein](../figures/chatgpt_poem.png)</span></span>
<span id="cb26-100"><a href="#cb26-100"></a>:::</span>
<span id="cb26-101"><a href="#cb26-101"></a></span>
<span id="cb26-102"><a href="#cb26-102"></a>::: {.flex-item style="width:49.2%"}</span>
<span id="cb26-103"><a href="#cb26-103"></a><span class="al">![Poem about Martin Luther](../figures/martin.png)</span></span>
<span id="cb26-104"><a href="#cb26-104"></a>:::</span>
<span id="cb26-105"><a href="#cb26-105"></a></span>
<span id="cb26-106"><a href="#cb26-106"></a>:::</span>
<span id="cb26-107"><a href="#cb26-107"></a></span>
<span id="cb26-108"><a href="#cb26-108"></a>You can do this on https://chat.openai.com</span>
<span id="cb26-109"><a href="#cb26-109"></a></span>
<span id="cb26-110"><a href="#cb26-110"></a><span class="fu">## Simplest AI model example: linear regression</span></span>
<span id="cb26-111"><a href="#cb26-111"></a></span>
<span id="cb26-112"><a href="#cb26-112"></a>This example is adopted from Bethany Lusch, ALCF.</span>
<span id="cb26-113"><a href="#cb26-113"></a></span>
<span id="cb26-114"><a href="#cb26-114"></a>Linear regression is the simplest example learning from existing data for</span>
<span id="cb26-115"><a href="#cb26-115"></a>future prediction.</span>
<span id="cb26-116"><a href="#cb26-116"></a></span>
<span id="cb26-117"><a href="#cb26-117"></a>::: {#fig-excel-linear-regression}</span>
<span id="cb26-118"><a href="#cb26-118"></a></span>
<span id="cb26-119"><a href="#cb26-119"></a><span class="al">![](../figures/excel_linear_regression.jpg)</span></span>
<span id="cb26-120"><a href="#cb26-120"></a></span>
<span id="cb26-121"><a href="#cb26-121"></a>Linear regression in Excel</span>
<span id="cb26-122"><a href="#cb26-122"></a>:::</span>
<span id="cb26-123"><a href="#cb26-123"></a></span>
<span id="cb26-124"><a href="#cb26-124"></a></span>
<span id="cb26-125"><a href="#cb26-125"></a>We're going to review the math involved in this process to help understand how</span>
<span id="cb26-126"><a href="#cb26-126"></a>training an AI works.</span>
<span id="cb26-127"><a href="#cb26-127"></a></span>
<span id="cb26-128"><a href="#cb26-128"></a>First we will load some tools that others wrote and we can use to help us work.</span>
<span id="cb26-129"><a href="#cb26-129"></a></span>
<span id="cb26-130"><a href="#cb26-130"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Pandas</span><span class="co">](https://pandas.pydata.org/docs/)</span>: a toolkit for working with row vs.</span>
<span id="cb26-131"><a href="#cb26-131"></a>  column data, like excel sheets, and CSV (Comma Seperated Values) files.</span>
<span id="cb26-132"><a href="#cb26-132"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Numpy</span><span class="co">](https://numpy.org/doc/)</span>: a toolkit for managing arrays, vectors,</span>
<span id="cb26-133"><a href="#cb26-133"></a>  matrices, etc, doing math with them, slicing them up, and many other handy</span>
<span id="cb26-134"><a href="#cb26-134"></a>  things.</span>
<span id="cb26-135"><a href="#cb26-135"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Matplotlib</span><span class="co">](https://matplotlib.org/stable/index.html)</span>: a toolkit for</span>
<span id="cb26-136"><a href="#cb26-136"></a>  plotting data</span>
<span id="cb26-137"><a href="#cb26-137"></a></span>
<span id="cb26-138"><a href="#cb26-138"></a></span>
<span id="cb26-141"><a href="#cb26-141"></a><span class="in">```{python}</span></span>
<span id="cb26-142"><a href="#cb26-142"></a><span class="im">import</span> os</span>
<span id="cb26-143"><a href="#cb26-143"></a>os.environ[<span class="st">"FORCE_COLOR"</span>] <span class="op">=</span> <span class="st">"1"</span></span>
<span id="cb26-144"><a href="#cb26-144"></a>os.environ[<span class="st">"TTY_INTERACTIVE"</span>] <span class="op">=</span> <span class="st">"1"</span></span>
<span id="cb26-145"><a href="#cb26-145"></a><span class="im">import</span> ambivalent</span>
<span id="cb26-146"><a href="#cb26-146"></a></span>
<span id="cb26-147"><a href="#cb26-147"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb26-148"><a href="#cb26-148"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb26-149"><a href="#cb26-149"></a></span>
<span id="cb26-150"><a href="#cb26-150"></a>plt.style.use(ambivalent.STYLES[<span class="st">'ambivalent'</span>])</span>
<span id="cb26-151"><a href="#cb26-151"></a>sns.set_context(<span class="st">"notebook"</span>)</span>
<span id="cb26-152"><a href="#cb26-152"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> [<span class="fl">6.4</span>, <span class="fl">4.8</span>]</span>
<span id="cb26-153"><a href="#cb26-153"></a></span>
<span id="cb26-154"><a href="#cb26-154"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb26-155"><a href="#cb26-155"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb26-156"><a href="#cb26-156"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb26-157"><a href="#cb26-157"></a><span class="im">import</span> IPython.display <span class="im">as</span> ipydis</span>
<span id="cb26-158"><a href="#cb26-158"></a><span class="im">import</span> time</span>
<span id="cb26-159"><a href="#cb26-159"></a></span>
<span id="cb26-160"><a href="#cb26-160"></a><span class="im">import</span> ezpz</span>
<span id="cb26-161"><a href="#cb26-161"></a><span class="im">from</span> rich <span class="im">import</span> <span class="bu">print</span></span>
<span id="cb26-162"><a href="#cb26-162"></a><span class="in">```</span></span>
<span id="cb26-163"><a href="#cb26-163"></a></span>
<span id="cb26-164"><a href="#cb26-164"></a><span class="fu">### Dataset</span></span>
<span id="cb26-165"><a href="#cb26-165"></a></span>
<span id="cb26-166"><a href="#cb26-166"></a>We used a realestate dataset from Kaggle to produce this reduced dataset.</span>
<span id="cb26-167"><a href="#cb26-167"></a></span>
<span id="cb26-168"><a href="#cb26-168"></a>This dataset contains the _sale price_ and _above ground square feet_ of many</span>
<span id="cb26-169"><a href="#cb26-169"></a>houses.</span>
<span id="cb26-170"><a href="#cb26-170"></a>We can use this data for our linear regression.</span>
<span id="cb26-171"><a href="#cb26-171"></a></span>
<span id="cb26-172"><a href="#cb26-172"></a>We use Pandas to read the data file which is stored as Comma Separated Values</span>
<span id="cb26-173"><a href="#cb26-173"></a>(CSV) and print the column labels.</span>
<span id="cb26-174"><a href="#cb26-174"></a></span>
<span id="cb26-175"><a href="#cb26-175"></a>CSV files are similar to excel sheets.</span>
<span id="cb26-176"><a href="#cb26-176"></a></span>
<span id="cb26-179"><a href="#cb26-179"></a><span class="in">```{python}</span></span>
<span id="cb26-180"><a href="#cb26-180"></a><span class="op">!</span> [ <span class="op">-</span>e .<span class="op">/</span>slimmed_realestate_data.csv ] <span class="op">||</span> wget https:<span class="op">//</span>raw.githubusercontent.com<span class="op">/</span>argonne<span class="op">-</span>lcf<span class="op">/</span>ai<span class="op">-</span>science<span class="op">-</span>training<span class="op">-</span>series<span class="op">/</span>main<span class="op">/</span><span class="dv">0</span><span class="er">1_intro_AI_on_Supercomputer</span><span class="op">/</span>slimmed_realestate_data.csv</span>
<span id="cb26-181"><a href="#cb26-181"></a>data <span class="op">=</span> pd.read_csv(<span class="st">'slimmed_realestate_data.csv'</span>)</span>
<span id="cb26-182"><a href="#cb26-182"></a><span class="bu">print</span>(data.columns)</span>
<span id="cb26-183"><a href="#cb26-183"></a><span class="in">```</span></span>
<span id="cb26-184"><a href="#cb26-184"></a></span>
<span id="cb26-185"><a href="#cb26-185"></a>Now pandas provides some helpful tools for us to inspect our data.</span>
<span id="cb26-186"><a href="#cb26-186"></a></span>
<span id="cb26-187"><a href="#cb26-187"></a>It provides a <span class="in">`plot()`</span> function that, behind the scenes, is calling into the</span>
<span id="cb26-188"><a href="#cb26-188"></a>_Matplotlib_ library and calling the function</span>
<span id="cb26-189"><a href="#cb26-189"></a><span class="co">[</span><span class="ot">matplotlib.pyplot.plot()</span><span class="co">](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html)</span>.</span>
<span id="cb26-190"><a href="#cb26-190"></a></span>
<span id="cb26-191"><a href="#cb26-191"></a>In this case, we simply tell it the names of the columns we want as our _x_ and</span>
<span id="cb26-192"><a href="#cb26-192"></a>_y_ values and the <span class="in">`style`</span> (<span class="in">`'.'`</span> tells <span class="in">`matplotlib`</span> to use a small dot to</span>
<span id="cb26-193"><a href="#cb26-193"></a>represent each data point).</span>
<span id="cb26-194"><a href="#cb26-194"></a></span>
<span id="cb26-197"><a href="#cb26-197"></a><span class="in">```{python}</span></span>
<span id="cb26-198"><a href="#cb26-198"></a>data.plot(x<span class="op">=</span><span class="st">'GrLivArea'</span>, y<span class="op">=</span><span class="st">'SalePrice'</span>,style<span class="op">=</span><span class="st">'o'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, markeredgecolor<span class="op">=</span><span class="st">"#222"</span>)</span>
<span id="cb26-199"><a href="#cb26-199"></a><span class="in">```</span></span>
<span id="cb26-200"><a href="#cb26-200"></a></span>
<span id="cb26-201"><a href="#cb26-201"></a>or, even better yet, use <span class="co">[</span><span class="ot">`seaborn`</span><span class="co">](https://seaborn.pydata.org/)</span> to plot the</span>
<span id="cb26-202"><a href="#cb26-202"></a>data:</span>
<span id="cb26-203"><a href="#cb26-203"></a></span>
<span id="cb26-206"><a href="#cb26-206"></a><span class="in">```{python}</span></span>
<span id="cb26-207"><a href="#cb26-207"></a>sns.jointplot(</span>
<span id="cb26-208"><a href="#cb26-208"></a>    x<span class="op">=</span><span class="st">"GrLivArea"</span>,</span>
<span id="cb26-209"><a href="#cb26-209"></a>    y<span class="op">=</span><span class="st">"SalePrice"</span>,</span>
<span id="cb26-210"><a href="#cb26-210"></a>    data<span class="op">=</span>data,</span>
<span id="cb26-211"><a href="#cb26-211"></a>    kind<span class="op">=</span><span class="st">'reg'</span>,</span>
<span id="cb26-212"><a href="#cb26-212"></a>    color<span class="op">=</span>(<span class="dv">216</span> <span class="op">/</span> <span class="fl">255.0</span>, <span class="dv">100</span> <span class="op">/</span> <span class="fl">255.0</span>, <span class="dv">50</span> <span class="op">/</span> <span class="fl">255.0</span>, <span class="fl">0.33</span>),</span>
<span id="cb26-213"><a href="#cb26-213"></a>)</span>
<span id="cb26-214"><a href="#cb26-214"></a><span class="in">```</span></span>
<span id="cb26-215"><a href="#cb26-215"></a></span>
<span id="cb26-216"><a href="#cb26-216"></a><span class="fu">### Theory of linear regression</span></span>
<span id="cb26-217"><a href="#cb26-217"></a></span>
<span id="cb26-218"><a href="#cb26-218"></a>The goal of learning regression is to find a line that is closest to all the points. </span>
<span id="cb26-219"><a href="#cb26-219"></a></span>
<span id="cb26-220"><a href="#cb26-220"></a>The slope and intercept of such a line $y = m x + b$ can be found as: </span>
<span id="cb26-221"><a href="#cb26-221"></a></span>
<span id="cb26-222"><a href="#cb26-222"></a>$$m = { n (\Sigma xy) - (\Sigma x) (\Sigma y) \over n (\Sigma x^2) - (\Sigma x)^2 } $$</span>
<span id="cb26-223"><a href="#cb26-223"></a></span>
<span id="cb26-224"><a href="#cb26-224"></a>$$b = { (\Sigma y) (\Sigma x^2) - (\Sigma x) (\Sigma xy) \over n (\Sigma x^2) - (\Sigma x)^2 } $$</span>
<span id="cb26-225"><a href="#cb26-225"></a></span>
<span id="cb26-226"><a href="#cb26-226"></a>Details derivation of this can be found <span class="co">[</span><span class="ot">here</span><span class="co">](https://en.wikipedia.org/wiki/Simple_linear_regression)</span>.</span>
<span id="cb26-227"><a href="#cb26-227"></a></span>
<span id="cb26-228"><a href="#cb26-228"></a>We'll break this calculation into a few steps to help make it easier.</span>
<span id="cb26-229"><a href="#cb26-229"></a></span>
<span id="cb26-230"><a href="#cb26-230"></a>First lets define $x$ and $y$. $x$ will be our _above ground square footage_</span>
<span id="cb26-231"><a href="#cb26-231"></a>and $y$ will be _sale price_.</span>
<span id="cb26-232"><a href="#cb26-232"></a>In our equations we have a few different values</span>
<span id="cb26-233"><a href="#cb26-233"></a>we need, such as $n$ which is just the number of points we have:</span>
<span id="cb26-234"><a href="#cb26-234"></a></span>
<span id="cb26-237"><a href="#cb26-237"></a><span class="in">```{python}</span></span>
<span id="cb26-238"><a href="#cb26-238"></a>n <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb26-239"><a href="#cb26-239"></a><span class="in">```</span></span>
<span id="cb26-240"><a href="#cb26-240"></a></span>
<span id="cb26-241"><a href="#cb26-241"></a>Then we need our $x$ and $y$ by selecting only the column we care about for</span>
<span id="cb26-242"><a href="#cb26-242"></a>each one.</span>
<span id="cb26-243"><a href="#cb26-243"></a>Note about data formats: <span class="in">`data`</span> is a Pandas</span>
<span id="cb26-244"><a href="#cb26-244"></a><span class="co">[</span><span class="ot">DataFrame</span><span class="co">](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame)</span></span>
<span id="cb26-245"><a href="#cb26-245"></a>object which has rows and columns; <span class="in">`data['GrLivArea']`</span> is a Pandas</span>
<span id="cb26-246"><a href="#cb26-246"></a><span class="co">[</span><span class="ot">Series</span><span class="co">](https://pandas.pydata.org/docs/reference/api/pandas.Series.html)</span></span>
<span id="cb26-247"><a href="#cb26-247"></a>object which only has rows; then we also convert from _Pandas_ data formats (in</span>
<span id="cb26-248"><a href="#cb26-248"></a>this case a _Series_) to _Numpy_ data formats using the <span class="in">`to_numpy()`</span> function</span>
<span id="cb26-249"><a href="#cb26-249"></a>which is part of the Pandas _Series_ object.</span>
<span id="cb26-250"><a href="#cb26-250"></a></span>
<span id="cb26-253"><a href="#cb26-253"></a><span class="in">```{python}</span></span>
<span id="cb26-254"><a href="#cb26-254"></a>x <span class="op">=</span> data[<span class="st">'GrLivArea'</span>].to_numpy()</span>
<span id="cb26-255"><a href="#cb26-255"></a>y <span class="op">=</span> data[<span class="st">'SalePrice'</span>].to_numpy()</span>
<span id="cb26-256"><a href="#cb26-256"></a><span class="in">```</span></span>
<span id="cb26-257"><a href="#cb26-257"></a></span>
<span id="cb26-258"><a href="#cb26-258"></a>Now we will calculate $\Sigma xy$, $\Sigma x$, $\Sigma y$, and $\Sigma x^2$:</span>
<span id="cb26-259"><a href="#cb26-259"></a></span>
<span id="cb26-262"><a href="#cb26-262"></a><span class="in">```{python}</span></span>
<span id="cb26-263"><a href="#cb26-263"></a>sum_xy <span class="op">=</span> np.<span class="bu">sum</span>(x<span class="op">*</span>y)</span>
<span id="cb26-264"><a href="#cb26-264"></a>sum_x <span class="op">=</span> np.<span class="bu">sum</span>(x)</span>
<span id="cb26-265"><a href="#cb26-265"></a>sum_y <span class="op">=</span> np.<span class="bu">sum</span>(y)</span>
<span id="cb26-266"><a href="#cb26-266"></a>sum_x2 <span class="op">=</span> np.<span class="bu">sum</span>(x<span class="op">*</span>x)</span>
<span id="cb26-267"><a href="#cb26-267"></a><span class="in">```</span></span>
<span id="cb26-268"><a href="#cb26-268"></a></span>
<span id="cb26-269"><a href="#cb26-269"></a>The denominator in the equation for $m$ and $b$ are the same so we can</span>
<span id="cb26-270"><a href="#cb26-270"></a>calculate that once:</span>
<span id="cb26-271"><a href="#cb26-271"></a></span>
<span id="cb26-274"><a href="#cb26-274"></a><span class="in">```{python}</span></span>
<span id="cb26-275"><a href="#cb26-275"></a>denominator <span class="op">=</span> n <span class="op">*</span> sum_x2 <span class="op">-</span> sum_x <span class="op">*</span> sum_x</span>
<span id="cb26-276"><a href="#cb26-276"></a><span class="in">```</span></span>
<span id="cb26-277"><a href="#cb26-277"></a></span>
<span id="cb26-278"><a href="#cb26-278"></a>Then we can calculate our fit values:</span>
<span id="cb26-279"><a href="#cb26-279"></a></span>
<span id="cb26-282"><a href="#cb26-282"></a><span class="in">```{python}</span></span>
<span id="cb26-283"><a href="#cb26-283"></a>m <span class="op">=</span> (n <span class="op">*</span> sum_xy <span class="op">-</span> sum_x <span class="op">*</span> sum_y) <span class="op">/</span> denominator</span>
<span id="cb26-284"><a href="#cb26-284"></a>b <span class="op">=</span> (sum_y <span class="op">*</span> sum_x2 <span class="op">-</span> sum_x <span class="op">*</span> sum_xy) <span class="op">/</span> denominator</span>
<span id="cb26-285"><a href="#cb26-285"></a><span class="bu">print</span>(<span class="st">'y = </span><span class="sc">%f</span><span class="st"> * x + </span><span class="sc">%f</span><span class="st">'</span> <span class="op">%</span> (m,b))</span>
<span id="cb26-286"><a href="#cb26-286"></a></span>
<span id="cb26-287"><a href="#cb26-287"></a><span class="co"># saving these for later comparison</span></span>
<span id="cb26-288"><a href="#cb26-288"></a>m_calc <span class="op">=</span> m</span>
<span id="cb26-289"><a href="#cb26-289"></a>b_calc <span class="op">=</span> b</span>
<span id="cb26-290"><a href="#cb26-290"></a><span class="in">```</span></span>
<span id="cb26-291"><a href="#cb26-291"></a></span>
<span id="cb26-292"><a href="#cb26-292"></a>Now we can plot the fit results with our data to see how we did.</span>
<span id="cb26-293"><a href="#cb26-293"></a></span>
<span id="cb26-294"><a href="#cb26-294"></a>First we define a plotting function because we're going to do this often and we</span>
<span id="cb26-295"><a href="#cb26-295"></a>want to reuse our code:</span>
<span id="cb26-296"><a href="#cb26-296"></a></span>
<span id="cb26-299"><a href="#cb26-299"></a><span class="in">```{python}</span></span>
<span id="cb26-300"><a href="#cb26-300"></a><span class="kw">def</span> plot_data(x,y,m,b,plt <span class="op">=</span> plt):</span>
<span id="cb26-301"><a href="#cb26-301"></a>   <span class="co"># plot our data points with 'bo' = blue circles</span></span>
<span id="cb26-302"><a href="#cb26-302"></a>   plt.plot(x, y, <span class="st">'o'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, markeredgecolor<span class="op">=</span><span class="st">"#222"</span>)</span>
<span id="cb26-303"><a href="#cb26-303"></a>   <span class="co"># create the line based on our linear fit</span></span>
<span id="cb26-304"><a href="#cb26-304"></a>   <span class="co"># first we need to make x points</span></span>
<span id="cb26-305"><a href="#cb26-305"></a>   <span class="co"># the 'arange' function generates points between two limits (min,max)</span></span>
<span id="cb26-306"><a href="#cb26-306"></a>   linear_x <span class="op">=</span> np.arange(x.<span class="bu">min</span>(),x.<span class="bu">max</span>())</span>
<span id="cb26-307"><a href="#cb26-307"></a>   <span class="co"># now we use our fit parameters to calculate the y points based on our x points</span></span>
<span id="cb26-308"><a href="#cb26-308"></a>   linear_y <span class="op">=</span> linear_x <span class="op">*</span> m <span class="op">+</span> b</span>
<span id="cb26-309"><a href="#cb26-309"></a>   <span class="co"># plot the linear points using 'r-' = red line</span></span>
<span id="cb26-310"><a href="#cb26-310"></a>   plt.plot(linear_x, linear_y, <span class="st">'r-'</span>, label<span class="op">=</span><span class="st">'fit'</span>)</span>
<span id="cb26-311"><a href="#cb26-311"></a><span class="in">```</span></span>
<span id="cb26-312"><a href="#cb26-312"></a></span>
<span id="cb26-313"><a href="#cb26-313"></a>Now can use this function to plot our results:</span>
<span id="cb26-314"><a href="#cb26-314"></a></span>
<span id="cb26-317"><a href="#cb26-317"></a><span class="in">```{python}</span></span>
<span id="cb26-318"><a href="#cb26-318"></a>plot_data(x,y,m,b)</span>
<span id="cb26-319"><a href="#cb26-319"></a><span class="in">```</span></span>
<span id="cb26-320"><a href="#cb26-320"></a></span>
<span id="cb26-321"><a href="#cb26-321"></a><span class="fu">### Training through Stochastic Gradient Descent (SGD)</span></span>
<span id="cb26-322"><a href="#cb26-322"></a></span>
<span id="cb26-323"><a href="#cb26-323"></a>SGD is a common method in AI for training deep neural networks on large</span>
<span id="cb26-324"><a href="#cb26-324"></a>datasets.</span>
<span id="cb26-325"><a href="#cb26-325"></a>It is an iterative method for optimizing a loss function that we get</span>
<span id="cb26-326"><a href="#cb26-326"></a>to define.</span>
<span id="cb26-327"><a href="#cb26-327"></a>We will use this simple linear regression to demonstrate how it</span>
<span id="cb26-328"><a href="#cb26-328"></a>works.</span>
<span id="cb26-329"><a href="#cb26-329"></a></span>
<span id="cb26-330"><a href="#cb26-330"></a><span class="fu">#### The model</span></span>
<span id="cb26-331"><a href="#cb26-331"></a></span>
<span id="cb26-332"><a href="#cb26-332"></a>In AI, neural networks are often referred to as a _model_ because, once fully</span>
<span id="cb26-333"><a href="#cb26-333"></a>trained, they should model (AKA predict) the behavior of our system.</span>
<span id="cb26-334"><a href="#cb26-334"></a>In our example, the system is how house prices vary based on house size.</span>
<span id="cb26-335"><a href="#cb26-335"></a>We know our system is roughly driven by a linear function:</span>
<span id="cb26-336"><a href="#cb26-336"></a></span>
<span id="cb26-337"><a href="#cb26-337"></a>$$\hat{y_i}(x_i) = m * x_i + b $$</span>
<span id="cb26-338"><a href="#cb26-338"></a></span>
<span id="cb26-339"><a href="#cb26-339"></a>We just need to figure out $m$ and $b$. Let's create a function that calculates</span>
<span id="cb26-340"><a href="#cb26-340"></a>our model given $x$, $m$, and $b$.</span>
<span id="cb26-341"><a href="#cb26-341"></a></span>
<span id="cb26-344"><a href="#cb26-344"></a><span class="in">```{python}</span></span>
<span id="cb26-345"><a href="#cb26-345"></a><span class="kw">def</span> model(x,m,b):</span>
<span id="cb26-346"><a href="#cb26-346"></a>   <span class="cf">return</span> m <span class="op">*</span> x <span class="op">+</span> b</span>
<span id="cb26-347"><a href="#cb26-347"></a><span class="in">```</span></span>
<span id="cb26-348"><a href="#cb26-348"></a></span>
<span id="cb26-349"><a href="#cb26-349"></a><span class="fu">#### The Loss Function</span></span>
<span id="cb26-350"><a href="#cb26-350"></a></span>
<span id="cb26-351"><a href="#cb26-351"></a>A _loss function_, or _objective function_, is something we define and is based</span>
<span id="cb26-352"><a href="#cb26-352"></a>on what we want to achieve.</span>
<span id="cb26-353"><a href="#cb26-353"></a>In the method of SGD, it is our goal to minimize (or make close to zero) the</span>
<span id="cb26-354"><a href="#cb26-354"></a>values calculated from the _loss function_.</span>
<span id="cb26-355"><a href="#cb26-355"></a>In our example, we ideally want the prediction of our _model_ to be equal to</span>
<span id="cb26-356"><a href="#cb26-356"></a>the actual data, though we will settle for "as close as possible".</span>
<span id="cb26-357"><a href="#cb26-357"></a></span>
<span id="cb26-358"><a href="#cb26-358"></a>So we will select our _loss function_ to be the <span class="co">[</span><span class="ot">Mean Squared Error</span><span class="co">](https://en.wikipedia.org/wiki/Mean_squared_error)</span> function:</span>
<span id="cb26-359"><a href="#cb26-359"></a></span>
<span id="cb26-360"><a href="#cb26-360"></a>$$ L(y_i,\hat{y_i}) = (y_i - \hat{y_i}(x_i))^2 $$</span>
<span id="cb26-361"><a href="#cb26-361"></a></span>
<span id="cb26-362"><a href="#cb26-362"></a>where $y_i$ is our $i^{th}$ entry in the <span class="in">`data['SalePrice']`</span> vector and</span>
<span id="cb26-363"><a href="#cb26-363"></a>$\hat{y_i}$ is the prediction based on evaluting $m * x_i + b$.</span>
<span id="cb26-364"><a href="#cb26-364"></a></span>
<span id="cb26-365"><a href="#cb26-365"></a>This function looks like the figure below when we plot it with </span>
<span id="cb26-366"><a href="#cb26-366"></a>$x=y_i - \hat{y_i}(x_i)$</span>
<span id="cb26-367"><a href="#cb26-367"></a>and we we want to be down near </span>
<span id="cb26-368"><a href="#cb26-368"></a>$y_i - \hat{y_i}(x_i) = 0$</span>
<span id="cb26-369"><a href="#cb26-369"></a>which indicates that our $y_i$ is as close as possible to $\hat{y_i}$.</span>
<span id="cb26-370"><a href="#cb26-370"></a></span>
<span id="cb26-371"><a href="#cb26-371"></a>::: {#fig-loss-func}</span>
<span id="cb26-372"><a href="#cb26-372"></a></span>
<span id="cb26-373"><a href="#cb26-373"></a><span class="al">![](../figures/loss_func.png)</span></span>
<span id="cb26-374"><a href="#cb26-374"></a></span>
<span id="cb26-375"><a href="#cb26-375"></a>Loss function for linear regression</span>
<span id="cb26-376"><a href="#cb26-376"></a>:::</span>
<span id="cb26-377"><a href="#cb26-377"></a></span>
<span id="cb26-378"><a href="#cb26-378"></a>Here we crate a function that calculates this for us.</span>
<span id="cb26-379"><a href="#cb26-379"></a></span>
<span id="cb26-382"><a href="#cb26-382"></a><span class="in">```{python}</span></span>
<span id="cb26-383"><a href="#cb26-383"></a><span class="kw">def</span> loss(x,y,m,b):</span>
<span id="cb26-384"><a href="#cb26-384"></a>   y_predicted <span class="op">=</span> model(x,m,b)</span>
<span id="cb26-385"><a href="#cb26-385"></a>   <span class="cf">return</span> np.power( y <span class="op">-</span> y_predicted, <span class="dv">2</span> )</span>
<span id="cb26-386"><a href="#cb26-386"></a><span class="in">```</span></span>
<span id="cb26-387"><a href="#cb26-387"></a></span>
<span id="cb26-388"><a href="#cb26-388"></a><span class="fu">#### Minimizing the Loss Function</span></span>
<span id="cb26-389"><a href="#cb26-389"></a></span>
<span id="cb26-390"><a href="#cb26-390"></a>We want to use the loss function in order to guide how to update $m$ and $b$ to</span>
<span id="cb26-391"><a href="#cb26-391"></a>better model our system.</span>
<span id="cb26-392"><a href="#cb26-392"></a>In calculus we learn to minimize a function with</span>
<span id="cb26-393"><a href="#cb26-393"></a>respect to a variable you calculate the _partial derivative_ with respect to</span>
<span id="cb26-394"><a href="#cb26-394"></a>the variable you want to vary.</span>
<span id="cb26-395"><a href="#cb26-395"></a></span>
<span id="cb26-396"><a href="#cb26-396"></a>$$ { \partial L \over \partial m } = 0 $$</span>
<span id="cb26-397"><a href="#cb26-397"></a></span>
<span id="cb26-398"><a href="#cb26-398"></a>The location of the solution to this is the minimum as shown in the figure</span>
<span id="cb26-399"><a href="#cb26-399"></a>above.</span>
<span id="cb26-400"><a href="#cb26-400"></a>We can write down the partial derivative of the loss function as:</span>
<span id="cb26-401"><a href="#cb26-401"></a></span>
<span id="cb26-402"><a href="#cb26-402"></a>$$ { \partial L \over \partial m } = -2 x_i (y_i - \hat{y_i}(x_i)) $$</span>
<span id="cb26-403"><a href="#cb26-403"></a>$$ { \partial L \over \partial b } = -2 (y_i - \hat{y_i}(x_i)) $$</span>
<span id="cb26-404"><a href="#cb26-404"></a></span>
<span id="cb26-405"><a href="#cb26-405"></a>We can use this to calculate an adjustment to $m$ and $b$ that will reduce the</span>
<span id="cb26-406"><a href="#cb26-406"></a>loss function, effectively improving our fitting parameters.</span>
<span id="cb26-407"><a href="#cb26-407"></a>This is done using this equation:</span>
<span id="cb26-408"><a href="#cb26-408"></a></span>
<span id="cb26-409"><a href="#cb26-409"></a>$$ m' = m - \eta { \partial L \over \partial m }$$</span>
<span id="cb26-410"><a href="#cb26-410"></a>$$ b' = b - \eta { \partial L \over \partial b }$$</span>
<span id="cb26-411"><a href="#cb26-411"></a></span>
<span id="cb26-412"><a href="#cb26-412"></a>Here our original $m$ and $b$ are adjusted by the partial derivative multiplied</span>
<span id="cb26-413"><a href="#cb26-413"></a>by some small factor, $\eta$, called the _learning rate_. This learning rate is</span>
<span id="cb26-414"><a href="#cb26-414"></a>very important in our process and must be tuned for every problem.</span>
<span id="cb26-415"><a href="#cb26-415"></a></span>
<span id="cb26-416"><a href="#cb26-416"></a>In our example, the selection of the learning rate essentially defines how</span>
<span id="cb26-417"><a href="#cb26-417"></a>close we can get to the minimum, AKA the best fit solution.</span>
<span id="cb26-418"><a href="#cb26-418"></a>This figure shows</span>
<span id="cb26-419"><a href="#cb26-419"></a>what happens when we pick a large learning rate.</span>
<span id="cb26-420"><a href="#cb26-420"></a>We first select a starting</span>
<span id="cb26-421"><a href="#cb26-421"></a>point in our loss function (typically randomly), then every update from $m$/$b$</span>
<span id="cb26-422"><a href="#cb26-422"></a>to $m'$/$b'$ results in a shift to somewhere else on our loss function</span>
<span id="cb26-423"><a href="#cb26-423"></a>(following the red arrows).</span>
<span id="cb26-424"><a href="#cb26-424"></a>In this example, our learning rate ($\eta$) has</span>
<span id="cb26-425"><a href="#cb26-425"></a>been selected too large such that we bounce back and forth around the minimum,</span>
<span id="cb26-426"><a href="#cb26-426"></a>never reaching it.</span>
<span id="cb26-427"><a href="#cb26-427"></a></span>
<span id="cb26-428"><a href="#cb26-428"></a>::: {#fig-parabola-largeLR}</span>
<span id="cb26-429"><a href="#cb26-429"></a><span class="al">![](../figures/parabola_largeLR.png)</span></span>
<span id="cb26-430"><a href="#cb26-430"></a></span>
<span id="cb26-431"><a href="#cb26-431"></a>Large LR</span>
<span id="cb26-432"><a href="#cb26-432"></a>:::</span>
<span id="cb26-433"><a href="#cb26-433"></a></span>
<span id="cb26-434"><a href="#cb26-434"></a>If we select a smaller learning we can see better behavior in the next figure.</span>
<span id="cb26-435"><a href="#cb26-435"></a></span>
<span id="cb26-436"><a href="#cb26-436"></a>::: {#fig-parabola-smallLR}</span>
<span id="cb26-437"><a href="#cb26-437"></a></span>
<span id="cb26-438"><a href="#cb26-438"></a><span class="al">![](../figures/parabola_smallLR.png)</span></span>
<span id="cb26-439"><a href="#cb26-439"></a></span>
<span id="cb26-440"><a href="#cb26-440"></a>Small LR</span>
<span id="cb26-441"><a href="#cb26-441"></a>:::</span>
<span id="cb26-442"><a href="#cb26-442"></a></span>
<span id="cb26-443"><a href="#cb26-443"></a>Though, keep in mind, too small a learning rate results is so little progress</span>
<span id="cb26-444"><a href="#cb26-444"></a>toward the minimum that you may never reach it!</span>
<span id="cb26-445"><a href="#cb26-445"></a></span>
<span id="cb26-446"><a href="#cb26-446"></a>A pit fall of SGD that one must be aware of is when your loss function is</span>
<span id="cb26-447"><a href="#cb26-447"></a>complex, with many minima.</span>
<span id="cb26-448"><a href="#cb26-448"></a>The next figure shows such a case, in which we</span>
<span id="cb26-449"><a href="#cb26-449"></a>select a small learning rate and our starting point happens to be near a local</span>
<span id="cb26-450"><a href="#cb26-450"></a>minimum that is not the lowest minimum.</span>
<span id="cb26-451"><a href="#cb26-451"></a>As shown, we do reach a minimum, but it</span>
<span id="cb26-452"><a href="#cb26-452"></a>isn't the lowest minimum in our loss function.</span>
<span id="cb26-453"><a href="#cb26-453"></a>It could be that we randomly</span>
<span id="cb26-454"><a href="#cb26-454"></a>select a starting point near the minimum we care about, but we should build</span>
<span id="cb26-455"><a href="#cb26-455"></a>methods that are more robust against randomly getting the right answer.</span>
<span id="cb26-456"><a href="#cb26-456"></a></span>
<span id="cb26-457"><a href="#cb26-457"></a>::: {#fig-local-min-smallLR}</span>
<span id="cb26-458"><a href="#cb26-458"></a></span>
<span id="cb26-459"><a href="#cb26-459"></a><span class="al">![](../figures/local_min_smallLR.png)</span></span>
<span id="cb26-460"><a href="#cb26-460"></a></span>
<span id="cb26-461"><a href="#cb26-461"></a>Local minimal with small LR</span>
<span id="cb26-462"><a href="#cb26-462"></a>:::</span>
<span id="cb26-463"><a href="#cb26-463"></a></span>
<span id="cb26-464"><a href="#cb26-464"></a>Then, if we increase our learning rate too much, we bounce around again.</span>
<span id="cb26-465"><a href="#cb26-465"></a></span>
<span id="cb26-466"><a href="#cb26-466"></a>::: {#fig-local-min-largeLR}</span>
<span id="cb26-467"><a href="#cb26-467"></a></span>
<span id="cb26-468"><a href="#cb26-468"></a><span class="al">![](../figures/local_min_largeLR.png)</span></span>
<span id="cb26-469"><a href="#cb26-469"></a></span>
<span id="cb26-470"><a href="#cb26-470"></a>Local minimal with large LR</span>
<span id="cb26-471"><a href="#cb26-471"></a>:::</span>
<span id="cb26-472"><a href="#cb26-472"></a></span>
<span id="cb26-473"><a href="#cb26-473"></a>What we want to do in this situation is start with a large learning rate and</span>
<span id="cb26-474"><a href="#cb26-474"></a>slowly reduce its size as we progress.</span>
<span id="cb26-475"><a href="#cb26-475"></a>That is shown in this next figure.</span>
<span id="cb26-476"><a href="#cb26-476"></a></span>
<span id="cb26-477"><a href="#cb26-477"></a>::: {#fig-local-min-variableLR}</span>
<span id="cb26-478"><a href="#cb26-478"></a></span>
<span id="cb26-479"><a href="#cb26-479"></a><span class="al">![](../figures/local_min_variableLR.png)</span></span>
<span id="cb26-480"><a href="#cb26-480"></a></span>
<span id="cb26-481"><a href="#cb26-481"></a>Local min with variable LR</span>
<span id="cb26-482"><a href="#cb26-482"></a>:::</span>
<span id="cb26-483"><a href="#cb26-483"></a></span>
<span id="cb26-484"><a href="#cb26-484"></a>As you can see, this process is not perfect and could still land in a local</span>
<span id="cb26-485"><a href="#cb26-485"></a>minimum, but it is important to be aware of these behaviors as you utilize SGD</span>
<span id="cb26-486"><a href="#cb26-486"></a>in machine learning.</span>
<span id="cb26-487"><a href="#cb26-487"></a></span>
<span id="cb26-488"><a href="#cb26-488"></a>So let's continue, we'll build functions we can use to update our fit</span>
<span id="cb26-489"><a href="#cb26-489"></a>parameters, $m$ and $b$.</span>
<span id="cb26-490"><a href="#cb26-490"></a></span>
<span id="cb26-493"><a href="#cb26-493"></a><span class="in">```{python}</span></span>
<span id="cb26-494"><a href="#cb26-494"></a><span class="kw">def</span> updated_m(x,y,m,b,learning_rate):</span>
<span id="cb26-495"><a href="#cb26-495"></a>   dL_dm <span class="op">=</span> <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> x <span class="op">*</span> (y <span class="op">-</span> model(x,m,b))</span>
<span id="cb26-496"><a href="#cb26-496"></a>   dL_dm <span class="op">=</span> np.mean(dL_dm)</span>
<span id="cb26-497"><a href="#cb26-497"></a>   <span class="cf">return</span> m <span class="op">-</span> learning_rate <span class="op">*</span> dL_dm</span>
<span id="cb26-498"><a href="#cb26-498"></a></span>
<span id="cb26-499"><a href="#cb26-499"></a><span class="kw">def</span> updated_b(x,y,m,b,learning_rate):</span>
<span id="cb26-500"><a href="#cb26-500"></a>   dL_db <span class="op">=</span> <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> (y <span class="op">-</span> model(x,m,b))</span>
<span id="cb26-501"><a href="#cb26-501"></a>   dL_db <span class="op">=</span> np.mean(dL_db)</span>
<span id="cb26-502"><a href="#cb26-502"></a>   <span class="cf">return</span> b <span class="op">-</span> learning_rate <span class="op">*</span> dL_db</span>
<span id="cb26-503"><a href="#cb26-503"></a><span class="in">```</span></span>
<span id="cb26-504"><a href="#cb26-504"></a></span>
<span id="cb26-505"><a href="#cb26-505"></a><span class="fu">## Putting it together</span></span>
<span id="cb26-506"><a href="#cb26-506"></a></span>
<span id="cb26-507"><a href="#cb26-507"></a>We can now randomly select our initial slope and intercept:</span>
<span id="cb26-508"><a href="#cb26-508"></a></span>
<span id="cb26-511"><a href="#cb26-511"></a><span class="in">```{python}</span></span>
<span id="cb26-512"><a href="#cb26-512"></a>m <span class="op">=</span> <span class="fl">5.</span></span>
<span id="cb26-513"><a href="#cb26-513"></a>b <span class="op">=</span> <span class="fl">1000.</span></span>
<span id="cb26-514"><a href="#cb26-514"></a><span class="bu">print</span>(<span class="ss">f"y_i = </span><span class="sc">{</span>m<span class="sc">:.2f}</span><span class="ss"> * x + </span><span class="sc">{</span>b<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb26-515"><a href="#cb26-515"></a><span class="co"># print('y_i = %.2f * x + %.2f' % (m,b))</span></span>
<span id="cb26-516"><a href="#cb26-516"></a><span class="in">```</span></span>
<span id="cb26-517"><a href="#cb26-517"></a></span>
<span id="cb26-518"><a href="#cb26-518"></a>Then we can calculate our Loss function:</span>
<span id="cb26-519"><a href="#cb26-519"></a></span>
<span id="cb26-522"><a href="#cb26-522"></a><span class="in">```{python}</span></span>
<span id="cb26-523"><a href="#cb26-523"></a>l <span class="op">=</span> loss(x,y,m,b)</span>
<span id="cb26-524"><a href="#cb26-524"></a><span class="bu">print</span>(<span class="ss">f'first 10 loss values: </span><span class="sc">{</span>l[:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb26-525"><a href="#cb26-525"></a><span class="in">```</span></span>
<span id="cb26-526"><a href="#cb26-526"></a></span>
<span id="cb26-529"><a href="#cb26-529"></a><span class="in">```{python}</span></span>
<span id="cb26-530"><a href="#cb26-530"></a>learning_rate <span class="op">=</span> <span class="fl">1e-9</span></span>
<span id="cb26-531"><a href="#cb26-531"></a>m <span class="op">=</span> updated_m(x,y,m,b,learning_rate)</span>
<span id="cb26-532"><a href="#cb26-532"></a>b <span class="op">=</span> updated_b(x,y,m,b,learning_rate)</span>
<span id="cb26-533"><a href="#cb26-533"></a><span class="bu">print</span>(<span class="st">'y_i = </span><span class="sc">%.2f</span><span class="st"> * x + </span><span class="sc">%.2f</span><span class="st">     previously calculated: y_i = </span><span class="sc">%.2f</span><span class="st"> * x + </span><span class="sc">%.2f</span><span class="st">'</span> <span class="op">%</span> (m,b,m_calc,b_calc))</span>
<span id="cb26-534"><a href="#cb26-534"></a>plot_data(x,y,m,b)</span>
<span id="cb26-535"><a href="#cb26-535"></a><span class="in">```</span></span>
<span id="cb26-536"><a href="#cb26-536"></a></span>
<span id="cb26-539"><a href="#cb26-539"></a><span class="in">```{python}</span></span>
<span id="cb26-540"><a href="#cb26-540"></a><span class="co"># set our initial slope and intercept</span></span>
<span id="cb26-541"><a href="#cb26-541"></a>m <span class="op">=</span> <span class="fl">5.</span></span>
<span id="cb26-542"><a href="#cb26-542"></a>b <span class="op">=</span> <span class="fl">1000.</span></span>
<span id="cb26-543"><a href="#cb26-543"></a><span class="co"># batch_size = 60</span></span>
<span id="cb26-544"><a href="#cb26-544"></a><span class="co"># set a learning rate for each parameter</span></span>
<span id="cb26-545"><a href="#cb26-545"></a>learning_rate_m <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb26-546"><a href="#cb26-546"></a>learning_rate_b <span class="op">=</span> <span class="fl">1e-1</span></span>
<span id="cb26-547"><a href="#cb26-547"></a><span class="co"># use these to plot our progress over time</span></span>
<span id="cb26-548"><a href="#cb26-548"></a>loss_history <span class="op">=</span> []</span>
<span id="cb26-549"><a href="#cb26-549"></a><span class="co"># convert panda data to numpy arrays, one for the "Ground Living Area" and one for "Sale Price"</span></span>
<span id="cb26-550"><a href="#cb26-550"></a>data_x <span class="op">=</span> data[<span class="st">'GrLivArea'</span>].to_numpy()</span>
<span id="cb26-551"><a href="#cb26-551"></a>data_y <span class="op">=</span> data[<span class="st">'SalePrice'</span>].to_numpy()</span>
<span id="cb26-552"><a href="#cb26-552"></a><span class="co"># we run our loop N times</span></span>
<span id="cb26-553"><a href="#cb26-553"></a>loop_N <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb26-554"><a href="#cb26-554"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(loop_N):</span>
<span id="cb26-555"><a href="#cb26-555"></a>    <span class="co"># update our slope and intercept based on the current values</span></span>
<span id="cb26-556"><a href="#cb26-556"></a>    m <span class="op">=</span> updated_m(data_x,data_y,m,b,learning_rate_m)</span>
<span id="cb26-557"><a href="#cb26-557"></a>    b <span class="op">=</span> updated_b(data_x,data_y,m,b,learning_rate_b)</span>
<span id="cb26-558"><a href="#cb26-558"></a></span>
<span id="cb26-559"><a href="#cb26-559"></a>    <span class="co"># calculate the loss value</span></span>
<span id="cb26-560"><a href="#cb26-560"></a>    loss_value <span class="op">=</span> np.mean(loss(data_x,data_y,m,b))</span>
<span id="cb26-561"><a href="#cb26-561"></a></span>
<span id="cb26-562"><a href="#cb26-562"></a>    <span class="co"># keep a history of our loss values</span></span>
<span id="cb26-563"><a href="#cb26-563"></a>    loss_history.append(loss_value)</span>
<span id="cb26-564"><a href="#cb26-564"></a></span>
<span id="cb26-565"><a href="#cb26-565"></a>    <span class="co"># print our progress</span></span>
<span id="cb26-566"><a href="#cb26-566"></a>    mstr <span class="op">=</span> <span class="st">" "</span>.join([</span>
<span id="cb26-567"><a href="#cb26-567"></a>        <span class="ss">f"[</span><span class="sc">{</span>i<span class="sc">:03d}</span><span class="ss">]"</span>,</span>
<span id="cb26-568"><a href="#cb26-568"></a>        <span class="ss">f"dy_i = </span><span class="sc">{</span>m<span class="sc">:.2f}</span><span class="ss"> * x + </span><span class="sc">{</span>b<span class="sc">:.2f}</span><span class="ss">"</span>,</span>
<span id="cb26-569"><a href="#cb26-569"></a>        <span class="ss">f"previously calculated: y_i = </span><span class="sc">{</span>m_calc<span class="sc">:.2f}</span><span class="ss"> * x + </span><span class="sc">{</span>b_calc<span class="sc">:.2f}</span><span class="ss">"</span>,</span>
<span id="cb26-570"><a href="#cb26-570"></a>        <span class="ss">f"loss: </span><span class="sc">{</span>loss_value<span class="sc">:.2f}</span><span class="ss">"</span>,</span>
<span id="cb26-571"><a href="#cb26-571"></a>    ])</span>
<span id="cb26-572"><a href="#cb26-572"></a>    <span class="bu">print</span>(mstr)</span>
<span id="cb26-573"><a href="#cb26-573"></a>    <span class="co"># print(</span></span>
<span id="cb26-574"><a href="#cb26-574"></a>    <span class="co">#         '[%03d]  dy_i = %.2f * x + %.2f     previously calculated: y_i = %.2f * x + %.2f    loss: %f' % (i,m,b,m_calc,b_calc,loss_value))</span></span>
<span id="cb26-575"><a href="#cb26-575"></a></span>
<span id="cb26-576"><a href="#cb26-576"></a>    <span class="co"># close/delete previous plots</span></span>
<span id="cb26-577"><a href="#cb26-577"></a>    plt.close(<span class="st">'all'</span>)</span>
<span id="cb26-578"><a href="#cb26-578"></a></span>
<span id="cb26-579"><a href="#cb26-579"></a>    dfigsize <span class="op">=</span> plt.rcParams[<span class="st">'figure.figsize'</span>]</span>
<span id="cb26-580"><a href="#cb26-580"></a>    <span class="co"># create a 1 by 2 plot grid</span></span>
<span id="cb26-581"><a href="#cb26-581"></a>    fig,ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(dfigsize[<span class="dv">0</span>]<span class="op">*</span><span class="dv">2</span>,dfigsize[<span class="dv">1</span>]))</span>
<span id="cb26-582"><a href="#cb26-582"></a>    <span class="co"># lot our usual output</span></span>
<span id="cb26-583"><a href="#cb26-583"></a>    plot_data(data_x,data_y,m,b,ax[<span class="dv">0</span>])</span>
<span id="cb26-584"><a href="#cb26-584"></a></span>
<span id="cb26-585"><a href="#cb26-585"></a>    <span class="co"># here we also plot the calculated linear fit for comparison</span></span>
<span id="cb26-586"><a href="#cb26-586"></a>    line_x <span class="op">=</span> np.arange(data_x.<span class="bu">min</span>(),data_x.<span class="bu">max</span>())</span>
<span id="cb26-587"><a href="#cb26-587"></a>    line_y <span class="op">=</span> line_x <span class="op">*</span> m_calc <span class="op">+</span> b_calc</span>
<span id="cb26-588"><a href="#cb26-588"></a>    ax[<span class="dv">0</span>].plot(line_x,line_y, color<span class="op">=</span><span class="st">"#000"</span>, linestyle<span class="op">=</span><span class="st">":"</span> ,label<span class="op">=</span><span class="st">'calculated'</span>)</span>
<span id="cb26-589"><a href="#cb26-589"></a>    <span class="co"># add a legend to the plot and x/y labels</span></span>
<span id="cb26-590"><a href="#cb26-590"></a>    ax[<span class="dv">0</span>].legend()</span>
<span id="cb26-591"><a href="#cb26-591"></a>    ax[<span class="dv">0</span>].set_xlabel(<span class="st">'square footage'</span>)</span>
<span id="cb26-592"><a href="#cb26-592"></a>    ax[<span class="dv">0</span>].set_ylabel(<span class="st">'sale price'</span>)</span>
<span id="cb26-593"><a href="#cb26-593"></a></span>
<span id="cb26-594"><a href="#cb26-594"></a>    <span class="co"># plot the loss </span></span>
<span id="cb26-595"><a href="#cb26-595"></a>    loss_x <span class="op">=</span> np.arange(<span class="dv">0</span>,<span class="bu">len</span>(loss_history))</span>
<span id="cb26-596"><a href="#cb26-596"></a>    loss_y <span class="op">=</span> np.asarray(loss_history)</span>
<span id="cb26-597"><a href="#cb26-597"></a>    ax[<span class="dv">1</span>].plot(loss_x,loss_y, <span class="st">'o-'</span>)</span>
<span id="cb26-598"><a href="#cb26-598"></a>    ax[<span class="dv">1</span>].set_yscale(<span class="st">'log'</span>)</span>
<span id="cb26-599"><a href="#cb26-599"></a>    ax[<span class="dv">1</span>].set_xlabel(<span class="st">'loop step'</span>)</span>
<span id="cb26-600"><a href="#cb26-600"></a>    ax[<span class="dv">1</span>].set_ylabel(<span class="st">'loss'</span>)</span>
<span id="cb26-601"><a href="#cb26-601"></a>    plt.show()</span>
<span id="cb26-602"><a href="#cb26-602"></a>    <span class="co"># gives us time to see the plot</span></span>
<span id="cb26-603"><a href="#cb26-603"></a>    time.sleep(<span class="fl">2.5</span>)</span>
<span id="cb26-604"><a href="#cb26-604"></a>    <span class="co"># clears the plot when the next plot is ready to show.</span></span>
<span id="cb26-605"><a href="#cb26-605"></a>    ipydis.clear_output(wait<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-606"><a href="#cb26-606"></a><span class="in">```</span></span>
<span id="cb26-607"><a href="#cb26-607"></a></span>
<span id="cb26-608"><a href="#cb26-608"></a><span class="fu">## Homework</span></span>
<span id="cb26-609"><a href="#cb26-609"></a></span>
<span id="cb26-610"><a href="#cb26-610"></a><span class="fu">### Mini Batch Training</span></span>
<span id="cb26-611"><a href="#cb26-611"></a></span>
<span id="cb26-612"><a href="#cb26-612"></a>In AI, datasets are often very large and cannot be processed all at once as is</span>
<span id="cb26-613"><a href="#cb26-613"></a>done in the loop above.</span>
<span id="cb26-614"><a href="#cb26-614"></a>The data is instead randomly sampled in smaller</span>
<span id="cb26-615"><a href="#cb26-615"></a>_batches_ where each _batch_ contains <span class="in">`batch_size`</span> inputs.</span>
<span id="cb26-616"><a href="#cb26-616"></a></span>
<span id="cb26-617"><a href="#cb26-617"></a>How can you change the loop above to sample the dataset in smaller batches?</span>
<span id="cb26-618"><a href="#cb26-618"></a></span>
<span id="cb26-619"><a href="#cb26-619"></a>Hint: Our <span class="in">`data`</span> variable is a Pandas <span class="in">`DataFrame`</span> object, search for "how to</span>
<span id="cb26-620"><a href="#cb26-620"></a>sample a DataFrame".</span>
<span id="cb26-621"><a href="#cb26-621"></a></span>
<span id="cb26-622"><a href="#cb26-622"></a>Instead of using the entire dataset like:</span>
<span id="cb26-623"><a href="#cb26-623"></a></span>
<span id="cb26-624"><a href="#cb26-624"></a><span class="in">```python</span></span>
<span id="cb26-625"><a href="#cb26-625"></a>data_x <span class="op">=</span> data[<span class="st">'GrLivArea'</span>].to_numpy()</span>
<span id="cb26-626"><a href="#cb26-626"></a>data_y <span class="op">=</span> data[<span class="st">'SalePrice'</span>].to_numpy()</span>
<span id="cb26-627"><a href="#cb26-627"></a><span class="in">```</span></span>
<span id="cb26-628"><a href="#cb26-628"></a></span>
<span id="cb26-629"><a href="#cb26-629"></a>Use</span>
<span id="cb26-630"><a href="#cb26-630"></a></span>
<span id="cb26-631"><a href="#cb26-631"></a><span class="in">```python</span></span>
<span id="cb26-632"><a href="#cb26-632"></a>data_batch <span class="op">=</span> data.sample(batch_size)</span>
<span id="cb26-633"><a href="#cb26-633"></a>data_x <span class="op">=</span> data_batch[<span class="st">'GrLivArea'</span>].to_numpy()</span>
<span id="cb26-634"><a href="#cb26-634"></a>data_y <span class="op">=</span> data_batch[<span class="st">'SalePrice'</span>].to_numpy()</span>
<span id="cb26-635"><a href="#cb26-635"></a><span class="in">```</span></span>
<span id="cb26-636"><a href="#cb26-636"></a></span>
<span id="cb26-637"><a href="#cb26-637"></a>You also have to adjust the loop_N accordingly to make sure that it loop over the entire datasets the same number of times.</span>
<span id="cb26-638"><a href="#cb26-638"></a></span>
<span id="cb26-639"><a href="#cb26-639"></a><span class="in">```python</span></span>
<span id="cb26-640"><a href="#cb26-640"></a>loop_N <span class="op">=</span> <span class="dv">30</span><span class="op">*</span><span class="bu">len</span>(data)<span class="op">//</span>batch_size</span>
<span id="cb26-641"><a href="#cb26-641"></a><span class="in">```</span></span>
<span id="cb26-642"><a href="#cb26-642"></a></span>
<span id="cb26-643"><a href="#cb26-643"></a>Please plot your learning curve for different batch size, such as 32, 64, 128, 256, 512. </span>
<span id="cb26-644"><a href="#cb26-644"></a></span>
<span id="cb26-645"><a href="#cb26-645"></a><span class="fu">### Learning rate issue (Bonus)</span></span>
<span id="cb26-646"><a href="#cb26-646"></a></span>
<span id="cb26-647"><a href="#cb26-647"></a>As described above, if the learning rate is too large, it will affect the convergence. Do your training with (batch_size = 64, learning_rate_m = 1e-7, learning_rate_b = 1e-1). Then linearly increase the batch size and learning rate until you see the training does not converge. </span>
<span id="cb26-648"><a href="#cb26-648"></a></span>
<span id="cb26-649"><a href="#cb26-649"></a><span class="in">```</span></span>
<span id="cb26-650"><a href="#cb26-650"></a><span class="in">(64, 1e-7, 1e-1)*1</span></span>
<span id="cb26-651"><a href="#cb26-651"></a><span class="in">(64, 1e-7, 1e-1)*2</span></span>
<span id="cb26-652"><a href="#cb26-652"></a><span class="in">(64, 1e-7, 1e-1)*4</span></span>
<span id="cb26-653"><a href="#cb26-653"></a><span class="in">(64, 1e-7, 1e-1)*8</span></span>
<span id="cb26-654"><a href="#cb26-654"></a><span class="in">...</span></span>
<span id="cb26-655"><a href="#cb26-655"></a><span class="in">```</span></span>
<span id="cb26-656"><a href="#cb26-656"></a></span>
<span id="cb26-657"><a href="#cb26-657"></a>**How to submit your homework**</span>
<span id="cb26-658"><a href="#cb26-658"></a></span>
<span id="cb26-659"><a href="#cb26-659"></a><span class="ss">- </span>Fork the github repo to your personal github </span>
<span id="cb26-660"><a href="#cb26-660"></a><span class="ss">- </span>Make change to the 01_linear_regression_sgd.ipynb, and then push to your personal github</span>
<span id="cb26-661"><a href="#cb26-661"></a><span class="ss">- </span>Provide the link of 01_linear_regression_sgd in the personal github. </span>
<span id="cb26-662"><a href="#cb26-662"></a></span>
<span id="cb26-663"><a href="#cb26-663"></a><span class="co">&lt;!--</span></span>
<span id="cb26-664"><a href="#cb26-664"></a><span class="co">Follow the below instruction on how to do this: </span></span>
<span id="cb26-665"><a href="#cb26-665"></a><span class="co">https://github.com/argonne-lcf/ai-science-training-series/blob/main/00_introToAlcf/03_githubHomework.md</span></span>
<span id="cb26-666"><a href="#cb26-666"></a><span class="co">--&gt;</span></span>
<span id="cb26-667"><a href="#cb26-667"></a></span>
<span id="cb26-668"><a href="#cb26-668"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="ot"> closed</span><span class="dt">&gt;&lt;</span><span class="kw">summary</span><span class="dt">&gt;&lt;</span><span class="kw">h2</span><span class="dt">&gt;</span>Homework Answer<span class="dt">&lt;/</span><span class="kw">h2</span><span class="dt">&gt;&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb26-669"><a href="#cb26-669"></a></span>
<span id="cb26-670"><a href="#cb26-670"></a>Let us define a train function which allow us to try different hyperparameter setups.</span>
<span id="cb26-671"><a href="#cb26-671"></a></span>
<span id="cb26-674"><a href="#cb26-674"></a><span class="in">```{python}</span></span>
<span id="cb26-675"><a href="#cb26-675"></a>x <span class="op">=</span> data[<span class="st">'GrLivArea'</span>].to_numpy()</span>
<span id="cb26-676"><a href="#cb26-676"></a>y <span class="op">=</span> data[<span class="st">'SalePrice'</span>].to_numpy()</span>
<span id="cb26-677"><a href="#cb26-677"></a></span>
<span id="cb26-678"><a href="#cb26-678"></a><span class="kw">def</span> train(batch_size, epochs<span class="op">=</span><span class="dv">30</span>, learning_rate_m <span class="op">=</span> <span class="fl">1e-7</span>, learning_rate_b <span class="op">=</span> <span class="fl">1e-1</span>):</span>
<span id="cb26-679"><a href="#cb26-679"></a>    loss_history <span class="op">=</span> []</span>
<span id="cb26-680"><a href="#cb26-680"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(data)<span class="op">//</span>batch_size</span>
<span id="cb26-681"><a href="#cb26-681"></a>    loop_N <span class="op">=</span> epochs<span class="op">*</span>num_batches</span>
<span id="cb26-682"><a href="#cb26-682"></a>    m <span class="op">=</span> <span class="fl">5.</span></span>
<span id="cb26-683"><a href="#cb26-683"></a>    b <span class="op">=</span> <span class="fl">1000.</span></span>
<span id="cb26-684"><a href="#cb26-684"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(loop_N):</span>
<span id="cb26-685"><a href="#cb26-685"></a>        data_batch <span class="op">=</span> data.sample(batch_size)</span>
<span id="cb26-686"><a href="#cb26-686"></a>        data_x <span class="op">=</span> data_batch[<span class="st">'GrLivArea'</span>].to_numpy()</span>
<span id="cb26-687"><a href="#cb26-687"></a>        data_y <span class="op">=</span> data_batch[<span class="st">'SalePrice'</span>].to_numpy()</span>
<span id="cb26-688"><a href="#cb26-688"></a>        <span class="co"># update our slope and intercept based on the current values</span></span>
<span id="cb26-689"><a href="#cb26-689"></a>        m <span class="op">=</span> updated_m(data_x,data_y,m,b,learning_rate_m)</span>
<span id="cb26-690"><a href="#cb26-690"></a>        b <span class="op">=</span> updated_b(data_x,data_y,m,b,learning_rate_b)</span>
<span id="cb26-691"><a href="#cb26-691"></a></span>
<span id="cb26-692"><a href="#cb26-692"></a>        <span class="co"># calculate the loss value</span></span>
<span id="cb26-693"><a href="#cb26-693"></a>        loss_value <span class="op">=</span> np.mean(loss(data_x,data_y,m,b))</span>
<span id="cb26-694"><a href="#cb26-694"></a></span>
<span id="cb26-695"><a href="#cb26-695"></a>        <span class="co"># keep a history of our loss values</span></span>
<span id="cb26-696"><a href="#cb26-696"></a>        loss_history.append(loss_value)</span>
<span id="cb26-697"><a href="#cb26-697"></a>    <span class="co">#loss_last_epoch = np.sum(loss_history[-num_batches:]*batch_size)/len(data)</span></span>
<span id="cb26-698"><a href="#cb26-698"></a>    <span class="cf">return</span> m, b, np.mean(loss(x,y,m,b))</span>
<span id="cb26-699"><a href="#cb26-699"></a><span class="in">```</span></span>
<span id="cb26-700"><a href="#cb26-700"></a></span>
<span id="cb26-701"><a href="#cb26-701"></a><span class="fu">## Minibatch training</span></span>
<span id="cb26-702"><a href="#cb26-702"></a></span>
<span id="cb26-705"><a href="#cb26-705"></a><span class="in">```{python}</span></span>
<span id="cb26-706"><a href="#cb26-706"></a><span class="bu">print</span>(<span class="st">'previously calculated: y_i = </span><span class="sc">%.2f</span><span class="st"> * x + </span><span class="sc">%.2f</span><span class="st">    loss: </span><span class="sc">%f</span><span class="ch">\n</span><span class="st">======================================='</span> <span class="op">%</span> (m_calc,b_calc,loss_value))</span>
<span id="cb26-707"><a href="#cb26-707"></a></span>
<span id="cb26-708"><a href="#cb26-708"></a></span>
<span id="cb26-709"><a href="#cb26-709"></a><span class="cf">for</span> bs <span class="kw">in</span> <span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">512</span>:</span>
<span id="cb26-710"><a href="#cb26-710"></a>    m, b, l <span class="op">=</span> train(bs, epochs<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb26-711"><a href="#cb26-711"></a>    <span class="bu">print</span>(<span class="ss">f"batch size: </span><span class="sc">{</span>bs<span class="sc">}</span><span class="ss">, m=</span><span class="sc">{</span>m<span class="sc">:.4f}</span><span class="ss">, b=</span><span class="sc">{</span>b<span class="sc">:.4f}</span><span class="ss">, loss=</span><span class="sc">{</span>l<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb26-712"><a href="#cb26-712"></a><span class="in">```</span></span>
<span id="cb26-713"><a href="#cb26-713"></a></span>
<span id="cb26-714"><a href="#cb26-714"></a>We see that eventually, we all get similar results with the minibatch training. Of course, here, we still keep the same learning rate. A gene</span>
<span id="cb26-715"><a href="#cb26-715"></a></span>
<span id="cb26-716"><a href="#cb26-716"></a><span class="fu">## Learning rate</span></span>
<span id="cb26-717"><a href="#cb26-717"></a></span>
<span id="cb26-720"><a href="#cb26-720"></a><span class="in">```{python}</span></span>
<span id="cb26-721"><a href="#cb26-721"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>:</span>
<span id="cb26-722"><a href="#cb26-722"></a>    bs, lrm, lrb <span class="op">=</span> np.array([<span class="dv">64</span>, <span class="fl">1e-7</span>, <span class="fl">1e-1</span>])<span class="op">*</span>i</span>
<span id="cb26-723"><a href="#cb26-723"></a>    bs <span class="op">=</span> <span class="bu">int</span>(bs)</span>
<span id="cb26-724"><a href="#cb26-724"></a>    m, b, l <span class="op">=</span> train(<span class="bu">int</span>(bs), epochs<span class="op">=</span><span class="dv">30</span>, learning_rate_m <span class="op">=</span> lrm, learning_rate_b <span class="op">=</span> lrb)</span>
<span id="cb26-725"><a href="#cb26-725"></a>    <span class="bu">print</span>(<span class="ss">f"batch size: </span><span class="sc">{</span>bs<span class="sc">}</span><span class="ss">, m=</span><span class="sc">{</span>m<span class="sc">:.4f}</span><span class="ss">, b=</span><span class="sc">{</span>b<span class="sc">:.4f}</span><span class="ss">, loss=</span><span class="sc">{</span>l<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb26-726"><a href="#cb26-726"></a><span class="in">```</span></span>
<span id="cb26-727"><a href="#cb26-727"></a></span>
<span id="cb26-728"><a href="#cb26-728"></a>We can see that, if we increase the batch size and the learning rate</span>
<span id="cb26-729"><a href="#cb26-729"></a>proportionally, at certain point, it does not converge for example for the case</span>
<span id="cb26-730"><a href="#cb26-730"></a>batch size = 512. To increase the learning rate proportional to the batch size</span>
<span id="cb26-731"><a href="#cb26-731"></a>is a general practice. However, if the learning rate is too large, it will</span>
<span id="cb26-732"><a href="#cb26-732"></a>continue to move around without finding a local minimum. One trick, people can</span>
<span id="cb26-733"><a href="#cb26-733"></a>do is to start with a smaller learning rate in the first few steps / epochs,</span>
<span id="cb26-734"><a href="#cb26-734"></a>and once the optimization becomes stable, increase the learning rate</span>
<span id="cb26-735"><a href="#cb26-735"></a>proportional to the batch size. </span>
<span id="cb26-736"><a href="#cb26-736"></a></span>
<span id="cb26-737"><a href="#cb26-737"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://saforem2.github.io/intro-hpc-bootcamp-2025/">
<p>saforem2.github.io/intro-hpc-bootcamp-2025</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/blob/main/00-intro-AI-HPC/6-linear-regression/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/edit/main/00-intro-AI-HPC/6-linear-regression/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/saforem2">
      <i class="bi bi-twitter" role="img" aria-label="Sam Foreman Twitter">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link active" href="https://github.com/saforem2/intro-hpc-bootcamp-2025" aria-current="page">
      <i class="bi bi-github" role="img" aria-label="Sam Foreman GitHub">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>