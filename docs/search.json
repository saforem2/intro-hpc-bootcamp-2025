[
  {
    "objectID": "03-ai-for-science/0-genslm/index.html#overview-of-genslm",
    "href": "03-ai-for-science/0-genslm/index.html#overview-of-genslm",
    "title": "Genome Scale Language Models (GenSLM)",
    "section": "Overview of GenSLM",
    "text": "Overview of GenSLM",
    "crumbs": [
      "[03] AI for Science",
      "[0] GenSLM"
    ]
  },
  {
    "objectID": "02-llms/7-shakespeare-example/index.html",
    "href": "02-llms/7-shakespeare-example/index.html",
    "title": "wordplay üéÆ üí¨: Shakespeare ‚úçÔ∏è",
    "section": "",
    "text": "We will be using the Shakespeare dataset to train a (~ small) 10M param LLM from scratch.",
    "crumbs": [
      "[02] Large Language Models",
      "[7] Shakespeare Example"
    ]
  },
  {
    "objectID": "02-llms/7-shakespeare-example/index.html#install-setup",
    "href": "02-llms/7-shakespeare-example/index.html#install-setup",
    "title": "wordplay üéÆ üí¨: Shakespeare ‚úçÔ∏è",
    "section": "Install / Setup",
    "text": "Install / Setup\n\nWarning!\nIF YOU ARE EXECUTING ON GOOGLE COLAB:\nYou will need to restart your runtime (Runtime \\rightarrow\\, Restart runtime)\nafter executing the following cell:\n\n\n%%bash\n\npython3 -c 'import wordplay; print(wordplay.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has wordplay installed. Nothing to do.\"\nelse\n    echo \"Does not have wordplay installed. Installing...\"\n    git clone 'https://github.com/saforem2/wordplay'\n    python3 wordplay/data/shakespeare_char/prepare.py\n    python3 wordplay/data/shakespeare/prepare.py\n    python3 -m pip install deepspeed\n    python3 -m pip install -e wordplay\nfi\n\n/Users/samforeman/projects/saforem2/wordplay/src/wordplay/__init__.py\nHas wordplay installed. Nothing to do.",
    "crumbs": [
      "[02] Large Language Models",
      "[7] Shakespeare Example"
    ]
  },
  {
    "objectID": "02-llms/7-shakespeare-example/index.html#post-install",
    "href": "02-llms/7-shakespeare-example/index.html#post-install",
    "title": "wordplay üéÆ üí¨: Shakespeare ‚úçÔ∏è",
    "section": "Post Install",
    "text": "Post Install\nIf installed correctly, you should be able to:\n&gt;&gt;&gt; import wordplay\n&gt;&gt;&gt; wordplay.__file__\n'/path/to/wordplay/src/wordplay/__init__.py'\n\n%load_ext autoreload\n%autoreload 2\nimport os\nimport sys\nimport ezpz\n\nos.environ['COLORTERM'] = 'truecolor'\nif sys.platform == 'darwin':\n    # If running on MacOS:\n    # os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n    os.environ['TORCH_DEVICE'] = 'cpu'\n# -----------------------------------------------\n\nlogger = ezpz.get_logger()\n\nimport wordplay\nlogger.info(wordplay.__file__)\n\n[2025-07-23 08:54:18,553565][I][ezpz/__init__:265:ezpz] Setting logging level to 'INFO' on 'RANK == 0'\n\n\n\n[2025-07-23 08:54:18,556331][I][ezpz/__init__:266:ezpz] Setting logging level to 'CRITICAL' on all others 'RANK != 0'\n\n\n\n[2025-07-23 08:54:18,559068][I][ipykernel_29374/2338663768:17:ezpz.log] /Users/samforeman/projects/saforem2/wordplay/src/wordplay/__init__.py",
    "crumbs": [
      "[02] Large Language Models",
      "[7] Shakespeare Example"
    ]
  },
  {
    "objectID": "02-llms/7-shakespeare-example/index.html#build-trainer",
    "href": "02-llms/7-shakespeare-example/index.html#build-trainer",
    "title": "wordplay üéÆ üí¨: Shakespeare ‚úçÔ∏è",
    "section": "Build Trainer",
    "text": "Build Trainer\nExplicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\n\n\nimport os\nimport numpy as np\nfrom ezpz import setup\nfrom hydra.utils import instantiate\nfrom wordplay.configs import get_config, PROJECT_ROOT\nfrom wordplay.trainer import Trainer\n\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nBACKEND = 'DDP'\n\nrank = setup(\n    framework='pytorch',\n    backend=BACKEND,\n    seed=1234,\n)\n\ncfg = get_config(\n    [\n        'data=shakespeare',\n        'model=shakespeare',\n        'model.batch_size=1',\n        'model.block_size=128',\n        'optimizer=shakespeare',\n        'train=shakespeare',\n        f'train.backend={BACKEND}',\n        'train.compile=false',\n        'train.dtype=bfloat16',\n        'train.max_iters=500',\n        'train.log_interval=10',\n        'train.eval_interval=50',\n    ]\n)\nconfig = instantiate(cfg)\n\n[2025-07-23 08:54:18,729865][I][wordplay/configs:81] Setting HF_DATASETS_CACHE to /Users/samforeman/projects/saforem2/wordplay/.cache/huggingface/datasets\n\n\n\n[2025-07-23 08:54:18,741186][W][ezpz/dist:639] Caught TORCH_DEVICE=cpu from environment!\n\n\n\n[2025-07-23 08:54:18,742428][W][ezpz/dist:639] Caught TORCH_DEVICE=cpu from environment!\n\n\n\n[2025-07-23 08:54:18,743238][W][ezpz/dist:639] Caught TORCH_DEVICE=cpu from environment!\n\n\n\n[2025-07-23 08:54:18,743819][I][ezpz/dist:1159] Using fw='ddp' with torch_{device,backend}= {cpu, gloo}\n\n\n\n[2025-07-23 08:54:18,746514][I][ezpz/dist:1026] Caught MASTER_PORT=65535 from environment!\n\n\n\n[2025-07-23 08:54:18,747704][I][ezpz/dist:1042] Using torch.distributed.init_process_group with\n- master_addr='Sams-MacBook-Pro-2.local'\n- master_port='65535'\n- world_size=1\n- rank=0\n- local_rank=0\n- timeout=datetime.timedelta(seconds=3600)\n- backend='gloo'\n\n\n\n[2025-07-23 08:54:18,748729][I][ezpz/dist:759] Calling torch.distributed.init_process_group_with: rank=0 world_size=1 backend=gloo\n\n\n\n[2025-07-23 08:54:18,779879][W][ezpz/dist:639] Caught TORCH_DEVICE=cpu from environment!\n\n\n\n[2025-07-23 08:54:18,780998][W][ezpz/dist:639] Caught TORCH_DEVICE=cpu from environment!\n\n\n\n[2025-07-23 08:54:18,797388][I][ezpz/dist:1377] Using device='cpu' with backend='gloo' + 'gloo' for distributed training.\n\n\n\n[2025-07-23 08:54:18,798492][I][ezpz/dist:1422] ['Sams-MacBook-Pro-2.local'][0/0] \n\n\n\n[2025-07-23 08:54:18,862320][I][wordplay/configs:317] Loading train from /Users/samforeman/projects/saforem2/wordplay/data/shakespeare_char/train.bin\n\n\n\n[2025-07-23 08:54:18,868654][I][wordplay/configs:317] Loading val from /Users/samforeman/projects/saforem2/wordplay/data/shakespeare_char/val.bin\n\n\n\n[2025-07-23 08:54:18,873931][I][wordplay/configs:442] Tokens per iteration: 128\n\n\n\n[2025-07-23 08:54:18,874763][W][ezpz/dist:639] Caught TORCH_DEVICE=cpu from environment!\n\n\n\n[2025-07-23 08:54:18,875477][I][wordplay/configs:465] Using self.ptdtype=torch.bfloat16 on self.device_type='cpu'\n\n\n\n[2025-07-23 08:54:18,876263][I][wordplay/configs:471] Initializing a new model from scratch\n\n\n\n\nBuild Trainer object\n\ntrainer = Trainer(config)\n\n[2025-07-23 08:54:18,910956][I][wordplay/trainer:235] Initializing a new model from scratch\n\n\n\n[2025-07-23 08:54:19,089818][I][wordplay/model:255] number of parameters: 10.65M\n\n\n\n[2025-07-23 08:54:19,091725][I][wordplay/trainer:252] Model size: num_params=10646784\n\n\n\n[2025-07-23 08:54:19,092649][I][wordplay/model:445] num decayed parameter tensors: 26, with 10,690,944 parameters\n\n\n\n[2025-07-23 08:54:19,093592][I][wordplay/model:449] num non-decayed parameter tensors: 13, with 4,992 parameters\n\n\n\n[2025-07-23 08:54:19,094508][I][wordplay/model:465] using fused AdamW: False\n\n\n\n[2025-07-23 08:54:19,095350][C][wordplay/trainer:308] \"devid='cpu:0'\"\n\n\n\n[2025-07-23 08:54:19,105554][I][wordplay/trainer:347] ‚Ä¢ self.model=GPT(\n  (transformer): ModuleDict(\n    (wte): Embedding(65, 384)\n    (wpe): Embedding(128, 384)\n    (drop): Dropout(p=0.2, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n          (attn_dropout): Dropout(p=0.2, inplace=False)\n          (resid_dropout): Dropout(p=0.2, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n          (act_fn): GELU(approximate='none')\n          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): Linear(in_features=384, out_features=65, bias=False)\n)\n\n\n\n[2025-07-23 08:54:19,108885][I][wordplay/trainer:348] ‚Ä¢ self.grad_scaler=None\n\n\n\n[2025-07-23 08:54:19,109801][I][wordplay/trainer:349] ‚Ä¢ self.model_engine=GPT(\n  (transformer): ModuleDict(\n    (wte): Embedding(65, 384)\n    (wpe): Embedding(128, 384)\n    (drop): Dropout(p=0.2, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n          (attn_dropout): Dropout(p=0.2, inplace=False)\n          (resid_dropout): Dropout(p=0.2, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n          (act_fn): GELU(approximate='none')\n          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): Linear(in_features=384, out_features=65, bias=False)\n)\n\n\n\n[2025-07-23 08:54:19,112698][I][wordplay/trainer:350] ‚Ä¢ self.optimizer=AdamW (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.99)\n    capturable: False\n    decoupled_weight_decay: True\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0.1\n\nParameter Group 1\n    amsgrad: False\n    betas: (0.9, 0.99)\n    capturable: False\n    decoupled_weight_decay: True\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0.0\n)",
    "crumbs": [
      "[02] Large Language Models",
      "[7] Shakespeare Example"
    ]
  },
  {
    "objectID": "02-llms/7-shakespeare-example/index.html#prompt-prior-to-training",
    "href": "02-llms/7-shakespeare-example/index.html#prompt-prior-to-training",
    "title": "wordplay üéÆ üí¨: Shakespeare ‚úçÔ∏è",
    "section": "Prompt (prior to training)",
    "text": "Prompt (prior to training)\n\nquery = \"What is an LLM?\"\noutputs = trainer.evaluate(\n    query,\n    num_samples=1,\n    max_new_tokens=256,\n    top_k=16,\n    display=False\n)\nlogger.info(f\"['prompt']: '{query}'\")\nlogger.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n[2025-07-23 08:54:21,840993][I][ipykernel_29374/3496000222:9:ezpz.log] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:54:21,841974][I][ipykernel_29374/3496000222:10:ezpz.log] ['response']:\n\nWhat is an LLM?A,,osy'exx.ff.fpppxv;;'vt3QjYhhvvYAhowQwwQ,eqeqG;X.YqqQSZQWLsyccccj:ZhaooxkkcfkZ\nffop- f,hqWl\noocpppUqAQ;cc''bQqcWAttrqerrwyqqsrqttqYeqWQs'tottcqestbqbbrpWbWYAppppBqfhcqqYqqM?qttqQU'gYe?A..'S'rtppW'fJf;??qn.pwrrrqqfA;!!A,,,AtqqqqbW;bSoW;;?;;;qQ;;cIA.'M;''g",
    "crumbs": [
      "[02] Large Language Models",
      "[7] Shakespeare Example"
    ]
  },
  {
    "objectID": "02-llms/7-shakespeare-example/index.html#train-model",
    "href": "02-llms/7-shakespeare-example/index.html#train-model",
    "title": "wordplay üéÆ üí¨: Shakespeare ‚úçÔ∏è",
    "section": "Train Model",
    "text": "Train Model\n\n\n\nname\ndescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization1\n\n\n\n^legend: #tbl-legend\n\ntrainer.config.device_type\n\n'cpu'\n\n\n\nfrom rich import print\n\nprint(trainer.model)\n\nGPT(\n  (transformer): ModuleDict(\n    (wte): Embedding(65, 384)\n    (wpe): Embedding(128, 384)\n    (drop): Dropout(p=0.2, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n          (attn_dropout): Dropout(p=0.2, inplace=False)\n          (resid_dropout): Dropout(p=0.2, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n          (act_fn): GELU(approximate='none')\n          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): Linear(in_features=384, out_features=65, bias=False)\n)",
    "crumbs": [
      "[02] Large Language Models",
      "[7] Shakespeare Example"
    ]
  },
  {
    "objectID": "02-llms/7-shakespeare-example/index.html#partial-training",
    "href": "02-llms/7-shakespeare-example/index.html#partial-training",
    "title": "wordplay üéÆ üí¨: Shakespeare ‚úçÔ∏è",
    "section": "(partial) Training:",
    "text": "(partial) Training:\nWe‚Äôll first train for 500 iterations and then evaluate the models performance on the same prompt:\n\nWhat is an LLM?\n\n\ntrainer.train(train_iters=500)\n\n                Training Legend                 \n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ        abbr ‚îÉ desc                           ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ        step ‚îÇ Current training iteration     ‚îÇ\n‚îÇ        loss ‚îÇ Loss value                     ‚îÇ\n‚îÇ          dt ‚îÇ Elapsed time per training step ‚îÇ\n‚îÇ         dtf ‚îÇ Elapsed time per forward step  ‚îÇ\n‚îÇ         dtb ‚îÇ Elapsed time per backward step ‚îÇ\n‚îÇ         sps ‚îÇ Samples per second             ‚îÇ\n‚îÇ sps_per_gpu ‚îÇ Samples per second (per GPU)   ‚îÇ\n‚îÇ         tps ‚îÇ Tokens per second              ‚îÇ\n‚îÇ tps_per_gpu ‚îÇ Tokens per second (per GPU)    ‚îÇ\n‚îÇ         mfu ‚îÇ Model flops utilization        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n[2025-07-23 08:54:24,920084][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:54:24,921309][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?wCw'.AAAfxo..'yfAQfppyybvFYerr.MfYZAcLyQQCkkexx-3lllrpMqxkko-rZx3b'3j-ffSSoqq3hhdf'Q''aq'wqqsoKZb'ec3ZAAA;;o,qff..'fArttgbYtturcbcSYrS-Fff'wwwerwPgJ;.e;yY-SpuyeexqYqgQtpMSYqYgbtQqq''';pfsw,';oA;qqeqcckSAo,,rooMgyQha'''fAA..gg;;'ggtSvrupptkeweqqcqqkk-SvYYIv\n\n\n\n[2025-07-23 08:54:29,910957][I][wordplay/trainer:850] step=10 loss=4.28757 dt=0.0205305 dtf=0.020182 dtb=0.000130875 sps=48.7081 sps_per_gpu=48.7081 tps=6234.64 tps_per_gpu=6234.64 mfu=0.134723\n\n\n\n[2025-07-23 08:54:30,124334][I][wordplay/trainer:850] step=20 loss=4.28569 dt=0.0211633 dtf=0.0207717 dtb=0.000151708 sps=47.2515 sps_per_gpu=47.2515 tps=6048.2 tps_per_gpu=6048.2 mfu=0.134321\n\n\n\n[2025-07-23 08:54:30,338380][I][wordplay/trainer:850] step=30 loss=4.19012 dt=0.0213609 dtf=0.0209663 dtb=0.000157583 sps=46.8145 sps_per_gpu=46.8145 tps=5992.25 tps_per_gpu=5992.25 mfu=0.133837\n\n\n\n[2025-07-23 08:54:30,544003][I][wordplay/trainer:850] step=40 loss=4.26634 dt=0.019542 dtf=0.0192077 dtb=0.000129459 sps=51.1717 sps_per_gpu=51.1717 tps=6549.98 tps_per_gpu=6549.98 mfu=0.134607\n\n\n\n[2025-07-23 08:54:30,759775][I][wordplay/trainer:850] step=50 loss=4.22804 dt=0.0207559 dtf=0.0204128 dtb=0.000124667 sps=48.179 sps_per_gpu=48.179 tps=6166.92 tps_per_gpu=6166.92 mfu=0.134472\n\n\n\n[2025-07-23 08:54:32,852336][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:54:32,853302][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?fwxx yY'eyffpCx?ZZZ.eevfeesxqQQYoqapxxxsZ\nvrvb'oZ3qoh33roArW;aafAA''f''QYqAob.aqo.Qyyegg'VcqqYbq3AaFskkcAkfvjb'QQtqQfArWA;Qp'k'goWoq;bbrppfQSYy,,,qqqqMsQuAQ'qgoowqqstSpgli-gggggjGG;cttSAA.pYYIoMSYu;QQSv;?gjJf'eQQQ;yg'Mgo-b';ccIffQSqAA'rqqcII?;;'ecWWllc;'';\n\n\n\n[2025-07-23 08:54:36,602185][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:54:36,603102][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:54:36,718576][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:54:37,029372][I][wordplay/trainer:850] step=60 loss=4.20216 dt=0.0231466 dtf=0.0227005 dtb=0.00016675 sps=43.2028 sps_per_gpu=43.2028 tps=5529.96 tps_per_gpu=5529.96 mfu=0.132975\n\n\n\n[2025-07-23 08:54:37,273605][I][wordplay/trainer:850] step=70 loss=4.20029 dt=0.0200086 dtf=0.0195733 dtb=0.000169 sps=49.9785 sps_per_gpu=49.9785 tps=6397.25 tps_per_gpu=6397.25 mfu=0.133501\n\n\n\n[2025-07-23 08:54:37,486325][I][wordplay/trainer:850] step=80 loss=4.14463 dt=0.0233564 dtf=0.0229816 dtb=0.000128458 sps=42.8149 sps_per_gpu=42.8149 tps=5480.3 tps_per_gpu=5480.3 mfu=0.131993\n\n\n\n[2025-07-23 08:54:37,698746][I][wordplay/trainer:850] step=90 loss=4.14377 dt=0.0222375 dtf=0.0218198 dtb=0.00015025 sps=44.9692 sps_per_gpu=44.9692 tps=5756.05 tps_per_gpu=5756.05 mfu=0.131232\n\n\n\n[2025-07-23 08:54:37,914091][I][wordplay/trainer:850] step=100 loss=4.24105 dt=0.0202152 dtf=0.0198747 dtb=0.000130875 sps=49.4676 sps_per_gpu=49.4676 tps=6331.85 tps_per_gpu=6331.85 mfu=0.131791\n\n\n\n[2025-07-23 08:54:40,036858][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:54:40,037974][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?f'xfAhf.qYEZQyyoo--AA,QQAAstpMfYhjc'c..MAj'FF,a33lx.adbssxvVhfsMwyQYosoooc'hzgSSrq.vZZZcq33Sk\n''vaq.w3AmA'..aYjye'ksr'gbvv,,hqb'eSJJm',rSeqfvrrrW;;bZSS:SqeWtttuYgJvkoBggSA'wst:Sur'txx'rSSqbb;;Qq-;.MsooowbqqqnSpBqSosgggtoo'e;''kG;'g-bWWoqetQ''os'q'tptSSSYe;\n\n\n\n[2025-07-23 08:54:43,774802][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:54:43,775825][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:54:43,865234][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:54:44,064170][I][wordplay/trainer:850] step=110 loss=4.30091 dt=0.0202677 dtf=0.019876 dtb=0.000133916 sps=49.3397 sps_per_gpu=49.3397 tps=6315.48 tps_per_gpu=6315.48 mfu=0.132259\n\n\n\n[2025-07-23 08:54:44,282222][I][wordplay/trainer:850] step=120 loss=4.23854 dt=0.0216304 dtf=0.0212699 dtb=0.000137625 sps=46.2312 sps_per_gpu=46.2312 tps=5917.59 tps_per_gpu=5917.59 mfu=0.131821\n\n\n\n[2025-07-23 08:54:44,492602][I][wordplay/trainer:850] step=130 loss=4.21194 dt=0.0206588 dtf=0.020206 dtb=0.000130542 sps=48.4054 sps_per_gpu=48.4054 tps=6195.9 tps_per_gpu=6195.9 mfu=0.132027\n\n\n\n[2025-07-23 08:54:44,698801][I][wordplay/trainer:850] step=140 loss=4.30343 dt=0.0219738 dtf=0.021581 dtb=0.000134709 sps=45.5088 sps_per_gpu=45.5088 tps=5825.13 tps_per_gpu=5825.13 mfu=0.131412\n\n\n\n[2025-07-23 08:54:44,904603][I][wordplay/trainer:850] step=150 loss=4.25562 dt=0.0204389 dtf=0.019997 dtb=0.00020075 sps=48.9264 sps_per_gpu=48.9264 tps=6262.58 tps_per_gpu=6262.58 mfu=0.131803\n\n\n\n[2025-07-23 08:54:46,968399][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:54:46,969492][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?vXvZoQQoLqQewerA'-''.qqQtXxx'V333jo'gQUoojxttYyfQOCCAASc-sseS\n\nr.GexS-\nDv'acQqjpwptxxqqZ!!fqzAAf.v3aag;vYgg'fqY:n;QsrkoBQhbYYQQgoMbZg;;cLf..WSSJhppMSkggkkkkooqWWQ'';xheuAA;pppcSQQqq;??ZppBkqeQsgb'SpWbrr;.gSbbqq;;f.t'gIBq;;WtgbW,rWWYAAqttMA''ggQQQnxrrrrh;;!\n\n\n\n[2025-07-23 08:54:50,724433][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:54:50,725436][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:54:50,838728][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:54:51,042991][I][wordplay/trainer:850] step=160 loss=4.22457 dt=0.0204011 dtf=0.0199948 dtb=0.00014175 sps=49.017 sps_per_gpu=49.017 tps=6274.18 tps_per_gpu=6274.18 mfu=0.132181\n\n\n\n[2025-07-23 08:54:51,270432][I][wordplay/trainer:850] step=170 loss=4.20268 dt=0.0210567 dtf=0.0206596 dtb=0.000150583 sps=47.4907 sps_per_gpu=47.4907 tps=6078.81 tps_per_gpu=6078.81 mfu=0.132098\n\n\n\n[2025-07-23 08:54:51,504752][I][wordplay/trainer:850] step=180 loss=4.23688 dt=0.0334785 dtf=0.0329052 dtb=0.000172458 sps=29.8699 sps_per_gpu=29.8699 tps=3823.34 tps_per_gpu=3823.34 mfu=0.12715\n\n\n\n[2025-07-23 08:54:51,762815][I][wordplay/trainer:850] step=190 loss=4.28941 dt=0.0236564 dtf=0.022554 dtb=0.000789458 sps=42.2718 sps_per_gpu=42.2718 tps=5410.79 tps_per_gpu=5410.79 mfu=0.126127\n\n\n\n[2025-07-23 08:54:52,052638][I][wordplay/trainer:850] step=200 loss=4.25317 dt=0.0247167 dtf=0.0243248 dtb=0.000136708 sps=40.4585 sps_per_gpu=40.4585 tps=5178.68 tps_per_gpu=5178.68 mfu=0.124705\n\n\n\n[2025-07-23 08:54:54,255692][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:54:54,257282][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?qervyyf.af3VAAowAoooooBQecAAqrxxxtXptxGQUVVcNYhhhck;;ooc'DaVqLZZZcP'''GGl..ooosZppV!333QqYYfQSYUUoofkm.tpcq'e''3esseeqqe;;!f'sx'MBfQttopp,qccQn3tgQSk-sffQnpSoo'gYpqqQn';qqecAAS'?AAASYf';pMt??pSSpptSbbYj-tWWYQY?gYIfkqg.nn'gqqc'gtqqtS??A'tu?MBBp???qq;;??A,,,\n\n\n\n[2025-07-23 08:54:58,085337][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:54:58,086467][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:54:58,178323][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:54:58,404043][I][wordplay/trainer:850] step=210 loss=4.22371 dt=0.0212232 dtf=0.0208848 dtb=0.000128375 sps=47.1183 sps_per_gpu=47.1183 tps=6031.15 tps_per_gpu=6031.15 mfu=0.125267\n\n\n\n[2025-07-23 08:54:58,631257][I][wordplay/trainer:850] step=220 loss=4.23227 dt=0.0211864 dtf=0.0207989 dtb=0.00013875 sps=47.2001 sps_per_gpu=47.2001 tps=6041.62 tps_per_gpu=6041.62 mfu=0.125796\n\n\n\n[2025-07-23 08:54:58,871084][I][wordplay/trainer:850] step=230 loss=4.22308 dt=0.0219266 dtf=0.0215225 dtb=0.000167417 sps=45.6067 sps_per_gpu=45.6067 tps=5837.65 tps_per_gpu=5837.65 mfu=0.125831\n\n\n\n[2025-07-23 08:54:59,108555][I][wordplay/trainer:850] step=240 loss=4.23777 dt=0.0239137 dtf=0.0235578 dtb=0.000133 sps=41.8171 sps_per_gpu=41.8171 tps=5352.59 tps_per_gpu=5352.59 mfu=0.124814\n\n\n\n[2025-07-23 08:54:59,335829][I][wordplay/trainer:850] step=250 loss=4.24408 dt=0.0253892 dtf=0.0250439 dtb=0.000129542 sps=39.3868 sps_per_gpu=39.3868 tps=5041.51 tps_per_gpu=5041.51 mfu=0.123227\n\n\n\n[2025-07-23 08:55:01,392866][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:55:01,393817][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?.rooffAA rW,,aAA'GoA,aUVVcCoGhvZZcd.QEcNAgxvwYa'haccX.aqo?rrQQ;;QbZ '''fc3FqqWk.'oceQ-h!?Yvs'rw--Qc'333-.hq3AwvvcLq','J-w'''rhqWo--;hSQgSqq;?rqYygAA,asso;q33AA'rbv,J-fof'g'SJJ,;ttcqq;'wgybqppaqttof;;;'''qtqaJpuuYf;paeyfhqg''''qWWbwAA-bbQyg'Sqqos''qYrM;a;??\n\n\n\n[2025-07-23 08:55:05,038967][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:55:05,040038][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:55:05,136361][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:55:05,336983][I][wordplay/trainer:850] step=260 loss=4.2759 dt=0.0211354 dtf=0.0207171 dtb=0.000155917 sps=47.3139 sps_per_gpu=47.3139 tps=6056.19 tps_per_gpu=6056.19 mfu=0.123991\n\n\n\n[2025-07-23 08:55:05,559342][I][wordplay/trainer:850] step=270 loss=4.31702 dt=0.0219074 dtf=0.0215387 dtb=0.000134375 sps=45.6466 sps_per_gpu=45.6466 tps=5842.77 tps_per_gpu=5842.77 mfu=0.124217\n\n\n\n[2025-07-23 08:55:05,784304][I][wordplay/trainer:850] step=280 loss=4.20612 dt=0.0211968 dtf=0.020879 dtb=0.000122125 sps=47.1769 sps_per_gpu=47.1769 tps=6038.64 tps_per_gpu=6038.64 mfu=0.124844\n\n\n\n[2025-07-23 08:55:06,006326][I][wordplay/trainer:850] step=290 loss=4.22943 dt=0.0209296 dtf=0.0205276 dtb=0.000143583 sps=47.7792 sps_per_gpu=47.7792 tps=6115.73 tps_per_gpu=6115.73 mfu=0.125575\n\n\n\n[2025-07-23 08:55:06,234678][I][wordplay/trainer:850] step=300 loss=4.11928 dt=0.0223447 dtf=0.0219682 dtb=0.000131958 sps=44.7534 sps_per_gpu=44.7534 tps=5728.44 tps_per_gpu=5728.44 mfu=0.125396\n\n\n\n[2025-07-23 08:55:08,341861][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:55:08,342741][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?L3slghC33vfJQO-eBBBv.Y.Sffs,'gxEUAUCQeswPv,ettLWClrrqeZAtLA.''3NsG..''.sAAmebbqYrv''-\nhTkcxhqqVUvvvfv,lxxlAc..3Zpq''Qsk'st;xlneQssssxS;'tt;cb;??rSQ'k--'t::qqnpYbc;nn;WWqqexSe''ftMqYYttttook;;pgSQQcLgycA;;qqbb''aakqrAAk.h''gYbcLLoopqs:sSSAgZQtiAA.'MMsWllpMt\n\n\n\n[2025-07-23 08:55:11,939051][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:55:11,940287][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:55:12,028965][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:55:12,242106][I][wordplay/trainer:850] step=310 loss=4.23252 dt=0.0224329 dtf=0.0220651 dtb=0.000124708 sps=44.5774 sps_per_gpu=44.5774 tps=5705.91 tps_per_gpu=5705.91 mfu=0.125186\n\n\n\n[2025-07-23 08:55:12,458094][I][wordplay/trainer:850] step=320 loss=4.23608 dt=0.0238395 dtf=0.0234557 dtb=0.000140208 sps=41.9473 sps_per_gpu=41.9473 tps=5369.25 tps_per_gpu=5369.25 mfu=0.12427\n\n\n\n[2025-07-23 08:55:12,684976][I][wordplay/trainer:850] step=330 loss=4.25042 dt=0.0213278 dtf=0.0209162 dtb=0.000149667 sps=46.8873 sps_per_gpu=46.8873 tps=6001.57 tps_per_gpu=6001.57 mfu=0.124812\n\n\n\n[2025-07-23 08:55:12,901223][I][wordplay/trainer:850] step=340 loss=4.19956 dt=0.0214897 dtf=0.0211137 dtb=0.000147791 sps=46.5339 sps_per_gpu=46.5339 tps=5956.34 tps_per_gpu=5956.34 mfu=0.125202\n\n\n\n[2025-07-23 08:55:13,114202][I][wordplay/trainer:850] step=350 loss=4.2746 dt=0.0207375 dtf=0.0203117 dtb=0.000180667 sps=48.2218 sps_per_gpu=48.2218 tps=6172.39 tps_per_gpu=6172.39 mfu=0.126019\n\n\n\n[2025-07-23 08:55:15,200228][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:55:15,201206][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?llBZexQZ wwwZrrxxxcqWa vqqxtqK..aHqQqqqecaask..--'Ve'll3fh3k..ttesscU''aUxhSpepBqqepp\n'QQ-;AqfwetpM vSQwbrrZQqa.CAA,,axqbQu''seyex...'';yyfw'gk:SSWQtrrqW''KKpp?ZQU'''tcb?;;;WufBWbb;f'ggYQttSk;?;;;?fA..Sbt;n''rrWqqMeeq;b'k'eMwQQtpufAAqQYAWASSe'qSpqqtLgWoqSk\n\n\n\n[2025-07-23 08:55:18,747919][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:55:18,748917][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:55:18,838859][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:55:19,038494][I][wordplay/trainer:850] step=360 loss=4.3276 dt=0.0201402 dtf=0.0198174 dtb=0.000121167 sps=49.652 sps_per_gpu=49.652 tps=6355.46 tps_per_gpu=6355.46 mfu=0.127151\n\n\n\n[2025-07-23 08:55:19,243589][I][wordplay/trainer:850] step=370 loss=4.15959 dt=0.0214316 dtf=0.0211187 dtb=0.000119583 sps=46.66 sps_per_gpu=46.66 tps=5972.48 tps_per_gpu=5972.48 mfu=0.127342\n\n\n\n[2025-07-23 08:55:19,461518][I][wordplay/trainer:850] step=380 loss=4.21489 dt=0.0219617 dtf=0.0215623 dtb=0.000121625 sps=45.5337 sps_per_gpu=45.5337 tps=5828.32 tps_per_gpu=5828.32 mfu=0.127202\n\n\n\n[2025-07-23 08:55:19,685327][I][wordplay/trainer:850] step=390 loss=4.18483 dt=0.0243274 dtf=0.0239153 dtb=0.000149958 sps=41.106 sps_per_gpu=41.106 tps=5261.56 tps_per_gpu=5261.56 mfu=0.125851\n\n\n\n[2025-07-23 08:55:19,907185][I][wordplay/trainer:850] step=400 loss=4.2439 dt=0.0202951 dtf=0.0198621 dtb=0.000137083 sps=49.273 sps_per_gpu=49.273 tps=6306.95 tps_per_gpu=6306.95 mfu=0.126895\n\n\n\n[2025-07-23 08:55:21,946919][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:55:21,947949][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?.3YZfxsaskoRbawwqW3fkYfVUB33emX3cxeQ;XAA,E;hqqqAA,VqYoqep.3-S'eh3cPe''bqqQAh\nfSpppp;!cbWA'fff3feNhaAo,Ax.tqq33-33--fCttppaww-gkttttt,,oWbb'glQWb'WWbZexG?b'sWl'tqt?qqQ'M'rhWlfMMe;tc-eqnnfCqYq;'?;t'Mwhqqq'..oooA,rqqfooWkkjGqqqqqq;fs;QYbWkkf',,.SSSbqqqbqeeqff\n\n\n\n[2025-07-23 08:55:25,594874][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:55:25,595965][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:55:25,691313][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:55:25,916580][I][wordplay/trainer:850] step=410 loss=4.23287 dt=0.0242881 dtf=0.0238824 dtb=0.000142625 sps=41.1724 sps_per_gpu=41.1724 tps=5270.07 tps_per_gpu=5270.07 mfu=0.125593\n\n\n\n[2025-07-23 08:55:26,127175][I][wordplay/trainer:850] step=420 loss=4.27257 dt=0.0207173 dtf=0.0202661 dtb=0.000176208 sps=48.2689 sps_per_gpu=48.2689 tps=6178.41 tps_per_gpu=6178.41 mfu=0.126385\n\n\n\n[2025-07-23 08:55:26,342616][I][wordplay/trainer:850] step=430 loss=4.18557 dt=0.0251186 dtf=0.0246748 dtb=0.000158334 sps=39.8112 sps_per_gpu=39.8112 tps=5095.83 tps_per_gpu=5095.83 mfu=0.124758\n\n\n\n[2025-07-23 08:55:26,565083][I][wordplay/trainer:850] step=440 loss=4.21616 dt=0.0220525 dtf=0.0216514 dtb=0.000158709 sps=45.3464 sps_per_gpu=45.3464 tps=5804.34 tps_per_gpu=5804.34 mfu=0.124825\n\n\n\n[2025-07-23 08:55:26,789960][I][wordplay/trainer:850] step=450 loss=4.23928 dt=0.0222061 dtf=0.0214803 dtb=0.00045075 sps=45.0326 sps_per_gpu=45.0326 tps=5764.18 tps_per_gpu=5764.18 mfu=0.124798\n\n\n\n[2025-07-23 08:55:29,818597][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:55:29,823261][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?weeQQQ''QQ'evfhQQ;K.AEsWqb..CfC.h;vvx''bTopBe'gWvXffv3ebssW.;?ptdeeep vrr..CCfkqcptyhpwTssWqsAxrqqqehmuZqZ:qeqGGGGauyfxrrAtgSrqWQ,,t;;ppMMgyeqfvfAAqcWYtqqoopepwySkkqggt3bZMqqq;;yybkSJcSQuuurruqqQtttoo''fAqq;;vSJZZZtM''qqM???gWWAAAt??MYYYe;yglAg;up'exuqqWtu\n\n\n\n[2025-07-23 08:55:33,696280][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:55:33,697421][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:55:33,780308][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:55:33,994982][I][wordplay/trainer:850] step=460 loss=4.24269 dt=0.0216852 dtf=0.0213274 dtb=0.000139167 sps=46.1144 sps_per_gpu=46.1144 tps=5902.64 tps_per_gpu=5902.64 mfu=0.125073\n\n\n\n[2025-07-23 08:55:34,209389][I][wordplay/trainer:850] step=470 loss=4.26877 dt=0.020772 dtf=0.020366 dtb=0.00014975 sps=48.1417 sps_per_gpu=48.1417 tps=6162.14 tps_per_gpu=6162.14 mfu=0.125881\n\n\n\n[2025-07-23 08:55:34,423168][I][wordplay/trainer:850] step=480 loss=4.19188 dt=0.0220768 dtf=0.0217005 dtb=0.000140666 sps=45.2965 sps_per_gpu=45.2965 tps=5797.95 tps_per_gpu=5797.95 mfu=0.125822\n\n\n\n[2025-07-23 08:55:34,673319][I][wordplay/trainer:850] step=490 loss=4.22611 dt=0.0241428 dtf=0.0237415 dtb=0.000156333 sps=41.4202 sps_per_gpu=41.4202 tps=5301.78 tps_per_gpu=5301.78 mfu=0.124696\n\n\n\n[2025-07-23 08:55:34,894285][I][wordplay/trainer:850] step=500 loss=4.21804 dt=0.0227807 dtf=0.0217471 dtb=0.000630167 sps=43.8968 sps_per_gpu=43.8968 tps=5618.79 tps_per_gpu=5618.79 mfu=0.124368\n\n\n\n\nimport time\n\nquery = \"What is an LLM?\"\nt0 = time.perf_counter()\noutputs = trainer.evaluate(\n    query,\n    num_samples=1,\n    max_new_tokens=256,\n    top_k=16,\n    display=False\n)\nlogger.info(f'took: {time.perf_counter() - t0:.4f}s')\nlogger.info(f\"['prompt']: '{query}'\")\nlogger.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n[2025-07-23 08:55:37,291020][I][ipykernel_29374/1425179755:12:ezpz.log] took: 2.1984s\n\n\n\n[2025-07-23 08:55:37,293460][I][ipykernel_29374/1425179755:13:ezpz.log] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:55:37,294820][I][ipykernel_29374/1425179755:14:ezpz.log] ['response']:\n\nWhat is an LLM?fwll\n\nb3afqbZZI,r oppq3A33QoUUye-fwC'3b3.',A'.hhPlVXXqeQyCCC;xfssc;wTTTTcdGoeehQOCXXXB'KZ--qehoF3AqfqqW\ncQAcceffGG,'fSJpppww,txMgQs;;;?qf'fSSrpcg?s,A'rr,aso?''o'MtQrrSSgqfttggSc''Wb'qA,.Apcbb???;pYYySQ'agggScWQgbqWfqYroffSYSYhqfk''qfAA,sgWlnZ:pt,JynS'gJZes",
    "crumbs": [
      "[02] Large Language Models",
      "[7] Shakespeare Example"
    ]
  },
  {
    "objectID": "02-llms/7-shakespeare-example/index.html#resume-training",
    "href": "02-llms/7-shakespeare-example/index.html#resume-training",
    "title": "wordplay üéÆ üí¨: Shakespeare ‚úçÔ∏è",
    "section": "Resume Training‚Ä¶",
    "text": "Resume Training‚Ä¶\n\ntrainer.train()\n\n[2025-07-23 08:55:39,574175][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:55:39,575307][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?wZbbbT'3weew,'foBB.qWWlpwes.qqQevFAA.bbvFF-AkacWWfYhx3fooB'''';vveesppWW\neeWA3ZZppPZe;dCCvres ;ecc--Ws'cqor,JZVVVCCeepfqqWxApBBBBhh;;JeQhMMss,,wshrhW?BiMWYqqwwwAASSwrrroo,rqtWseMq.Ak'ofA,,'t,,..hh;xx'?sAq';cqxrqWkeMqt'gzAAxhrpqt'g't;?btoseq-pqq'qAtttt,eqrM\n\n\n\n[2025-07-23 08:55:43,323114][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:55:43,324064][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:55:43,421604][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:55:43,647671][I][wordplay/trainer:850] step=510 loss=4.25518 dt=0.0215086 dtf=0.0211607 dtb=0.0001255 sps=46.493 sps_per_gpu=46.493 tps=5951.1 tps_per_gpu=5951.1 mfu=0.128597\n\n\n\n[2025-07-23 08:55:43,870914][I][wordplay/trainer:850] step=520 loss=4.20906 dt=0.0229062 dtf=0.0225451 dtb=0.000144583 sps=43.6564 sps_per_gpu=43.6564 tps=5588.02 tps_per_gpu=5588.02 mfu=0.127812\n\n\n\n[2025-07-23 08:55:44,094545][I][wordplay/trainer:850] step=530 loss=4.22394 dt=0.0209522 dtf=0.0205422 dtb=0.000194542 sps=47.7278 sps_per_gpu=47.7278 tps=6109.15 tps_per_gpu=6109.15 mfu=0.128232\n\n\n\n[2025-07-23 08:55:44,311325][I][wordplay/trainer:850] step=540 loss=4.23923 dt=0.0214975 dtf=0.0211227 dtb=0.000145584 sps=46.517 sps_per_gpu=46.517 tps=5954.18 tps_per_gpu=5954.18 mfu=0.128275\n\n\n\n[2025-07-23 08:55:44,549648][I][wordplay/trainer:850] step=550 loss=4.24928 dt=0.0278331 dtf=0.0273527 dtb=0.000198625 sps=35.9285 sps_per_gpu=35.9285 tps=4598.84 tps_per_gpu=4598.84 mfu=0.125385\n\n\n\n[2025-07-23 08:55:46,794937][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:55:46,796111][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?wboG',ZZswPZZhsf'V.h;QrppwAfAa''qWWYYfOOx33fvkkfQ'elccB3kkkm....swevfsssoAkfQss 'f;ehewqs3--seuCeerqfQA,XXqooU;?';QhdI'M;;astc;W;?A;p;p',,'''gosS;;WW?'errs'fwwr''qqWW,w'l;''www''tppwbQWWseSSqYtLtSbQQQ'q;qqM'tbqW,s'r.AAtcbbq-'ttuuA,;;;Q'S;;;ttMglqYetqeSS;Wq\n\n\n\n[2025-07-23 08:55:50,655835][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:55:50,657010][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:55:50,749536][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:55:50,961035][I][wordplay/trainer:850] step=560 loss=4.21979 dt=0.0217063 dtf=0.0212673 dtb=0.00016525 sps=46.0697 sps_per_gpu=46.0697 tps=5896.92 tps_per_gpu=5896.92 mfu=0.125589\n\n\n\n[2025-07-23 08:55:51,179632][I][wordplay/trainer:850] step=570 loss=4.27896 dt=0.0226645 dtf=0.0214423 dtb=0.000155834 sps=44.1219 sps_per_gpu=44.1219 tps=5647.6 tps_per_gpu=5647.6 mfu=0.125234\n\n\n\n[2025-07-23 08:55:51,413390][I][wordplay/trainer:850] step=580 loss=4.25036 dt=0.0281774 dtf=0.0278183 dtb=0.000130375 sps=35.4894 sps_per_gpu=35.4894 tps=4542.64 tps_per_gpu=4542.64 mfu=0.122527\n\n\n\n[2025-07-23 08:55:51,727556][I][wordplay/trainer:850] step=590 loss=4.30325 dt=0.0257962 dtf=0.0253203 dtb=0.000140625 sps=38.7654 sps_per_gpu=38.7654 tps=4961.98 tps_per_gpu=4961.98 mfu=0.120996\n\n\n\n[2025-07-23 08:55:52,008632][I][wordplay/trainer:850] step=600 loss=4.24977 dt=0.0292456 dtf=0.0269955 dtb=0.000195584 sps=34.1932 sps_per_gpu=34.1932 tps=4376.73 tps_per_gpu=4376.73 mfu=0.118354\n\n\n\n[2025-07-23 08:55:54,403985][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:55:54,404886][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?LQ3vvye! wePZ ewbAII''QYUfY.vTcaQlccCfhsZblYe''vS'xqosfoxCx'q33ckkxpppcecZZ-caqAb''fQ-eqb'.AGGGZZ?--s..h.ttppMq3ZQs,e';pwsf..se;;pqtcenr'.nxnqqgbqQYtttM'fSbttcqqqqgYYjjrqfAkkSSSuQqoh'''S;SYYYAG;SSSo'QQQuu;'QSfqo'.tgSggkqWYYbbvqqtuiqrhS;QC'QSrSbWWSJJeuuiWYu\n\n\n\n[2025-07-23 08:55:58,635718][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:55:58,637731][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:55:58,848318][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:55:59,082883][I][wordplay/trainer:850] step=610 loss=4.27699 dt=0.0263469 dtf=0.0259853 dtb=0.000132958 sps=37.9552 sps_per_gpu=37.9552 tps=4858.26 tps_per_gpu=4858.26 mfu=0.117017\n\n\n\n[2025-07-23 08:55:59,347936][I][wordplay/trainer:850] step=620 loss=4.2417 dt=0.02479 dtf=0.024439 dtb=0.000134334 sps=40.3388 sps_per_gpu=40.3388 tps=5163.36 tps_per_gpu=5163.36 mfu=0.116473\n\n\n\n[2025-07-23 08:55:59,648456][I][wordplay/trainer:850] step=630 loss=4.1949 dt=0.0336385 dtf=0.0332351 dtb=0.000137917 sps=29.7279 sps_per_gpu=29.7279 tps=3805.17 tps_per_gpu=3805.17 mfu=0.113048\n\n\n\n[2025-07-23 08:56:00,030250][I][wordplay/trainer:850] step=640 loss=4.21554 dt=0.026985 dtf=0.0266091 dtb=0.000138625 sps=37.0577 sps_per_gpu=37.0577 tps=4743.38 tps_per_gpu=4743.38 mfu=0.111993\n\n\n\n[2025-07-23 08:56:00,261125][I][wordplay/trainer:850] step=650 loss=4.26643 dt=0.0223648 dtf=0.0214353 dtb=0.000123959 sps=44.7131 sps_per_gpu=44.7131 tps=5723.27 tps_per_gpu=5723.27 mfu=0.113161\n\n\n\n[2025-07-23 08:56:02,529277][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:56:02,530240][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?qadZ--e'ovTqro'qE'rpAYvrr;qo3AAwUA-sG..qqbaNNyyep;blgWVe''tkaoo,ebqqUAAAAxttmZS.tGlAxxtccZAk'qffhMM;hqcZ\n'rvsoAAtqWtt,'MqWtt'qqqQ--zpttttuq3brqtrrha;WW'eq;cqqqqrrhh-ppq;'SSJrhSYSJqg'',asqqAhdqbv'?Bqqqb',fqSqt'QqAAWAAqqQQQttttIffvqeWYY--?MfSpppMttttBBM'KK..\n\n\n\n[2025-07-23 08:56:06,206868][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:56:06,207971][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:56:06,315564][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:56:06,557464][I][wordplay/trainer:850] step=660 loss=4.17238 dt=0.0227037 dtf=0.022349 dtb=0.000136125 sps=44.0457 sps_per_gpu=44.0457 tps=5637.85 tps_per_gpu=5637.85 mfu=0.114028\n\n\n\n[2025-07-23 08:56:06,834312][I][wordplay/trainer:850] step=670 loss=4.33205 dt=0.0216763 dtf=0.0213103 dtb=0.000139958 sps=46.1334 sps_per_gpu=46.1334 tps=5905.07 tps_per_gpu=5905.07 mfu=0.115385\n\n\n\n[2025-07-23 08:56:07,173082][I][wordplay/trainer:850] step=680 loss=4.17701 dt=0.0363958 dtf=0.0329993 dtb=0.00308504 sps=27.4757 sps_per_gpu=27.4757 tps=3516.89 tps_per_gpu=3516.89 mfu=0.111446\n\n\n\n[2025-07-23 08:56:07,446497][I][wordplay/trainer:850] step=690 loss=4.23023 dt=0.025179 dtf=0.0245101 dtb=0.00028575 sps=39.7156 sps_per_gpu=39.7156 tps=5083.59 tps_per_gpu=5083.59 mfu=0.111287\n\n\n\n[2025-07-23 08:56:07,753523][I][wordplay/trainer:850] step=700 loss=4.19011 dt=0.0450157 dtf=0.0436069 dtb=0.000475792 sps=22.2145 sps_per_gpu=22.2145 tps=2843.45 tps_per_gpu=2843.45 mfu=0.106302\n\n\n\n[2025-07-23 08:56:10,219282][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:56:10,227521][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?lrvqqrafQEsA,hrccZZ;'rrkf'c x'Xxqad.SSxtaV!XQUxv;a.'g\nZto..herovV-qA'K;aZs3ecAq vqq.!c'fos,ssAAcqfop-;AA.Ag.WYYvvqttxW,,eq;;..Mww';QtMMgqeeqYYppppp;;..MW'tqYf.ff';ccWYrrS'SAsSohegQrr'rhWSASpgj'.A;;.eqqqqqeWWofYQYtcb'Q;;;tttuqcgk;.t3tSbYhhouI;ppp;tSfvgQSuSq\n\n\n\n[2025-07-23 08:56:14,205965][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:56:14,206993][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:56:14,375727][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:56:14,605227][I][wordplay/trainer:850] step=710 loss=4.25752 dt=0.0272084 dtf=0.0268316 dtb=0.000129166 sps=36.7534 sps_per_gpu=36.7534 tps=4704.43 tps_per_gpu=4704.43 mfu=0.105838\n\n\n\n[2025-07-23 08:56:14,825950][I][wordplay/trainer:850] step=720 loss=4.22592 dt=0.0223586 dtf=0.0219346 dtb=0.000147042 sps=44.7255 sps_per_gpu=44.7255 tps=5724.86 tps_per_gpu=5724.86 mfu=0.107625\n\n\n\n[2025-07-23 08:56:15,054193][I][wordplay/trainer:850] step=730 loss=4.18346 dt=0.0232044 dtf=0.0228418 dtb=0.000141417 sps=43.0953 sps_per_gpu=43.0953 tps=5516.2 tps_per_gpu=5516.2 mfu=0.108782\n\n\n\n[2025-07-23 08:56:15,291898][I][wordplay/trainer:850] step=740 loss=4.22937 dt=0.0218458 dtf=0.0211359 dtb=0.000135458 sps=45.7754 sps_per_gpu=45.7754 tps=5859.25 tps_per_gpu=5859.25 mfu=0.110565\n\n\n\n[2025-07-23 08:56:15,522767][I][wordplay/trainer:850] step=750 loss=4.22004 dt=0.0246791 dtf=0.0242991 dtb=0.000149417 sps=40.5201 sps_per_gpu=40.5201 tps=5186.57 tps_per_gpu=5186.57 mfu=0.110716\n\n\n\n[2025-07-23 08:56:17,728891][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:56:17,729995][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?.AvexhjjsAxx3AAAAffyyY'rr.AxZZpaff.yykfAqYEZ\n'koBf''3YYo.hzA,aaqbbZ ttQhhxkeQU'qhqqoqq!!'ffor'f.aZPeG'qW.ttvafA-b??fffvfvYrcL.bWtSS??qtLtQutohdyyppu''rrSqYqc'KKye''''gjjQq'fgJq;;.'gYqrkssW'tp;bqqf.qowqoMM'qQQSqqWssgyttu?qoo'ff''kkSSffAr.MggesgIIBBYeeWqqqqg\n\n\n\n[2025-07-23 08:56:21,581415][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:56:21,582449][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:56:21,762870][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:56:21,979980][I][wordplay/trainer:850] step=760 loss=4.16349 dt=0.0220536 dtf=0.0216752 dtb=0.000128041 sps=45.3441 sps_per_gpu=45.3441 tps=5804.05 tps_per_gpu=5804.05 mfu=0.112187\n\n\n\n[2025-07-23 08:56:22,205426][I][wordplay/trainer:850] step=770 loss=4.22062 dt=0.0235944 dtf=0.0232387 dtb=0.000124708 sps=42.3829 sps_per_gpu=42.3829 tps=5425.01 tps_per_gpu=5425.01 mfu=0.112691\n\n\n\n[2025-07-23 08:56:22,439827][I][wordplay/trainer:850] step=780 loss=4.16916 dt=0.0218494 dtf=0.0214647 dtb=0.000132875 sps=45.7679 sps_per_gpu=45.7679 tps=5858.29 tps_per_gpu=5858.29 mfu=0.114081\n\n\n\n[2025-07-23 08:56:22,713629][I][wordplay/trainer:850] step=790 loss=4.21405 dt=0.0283389 dtf=0.027484 dtb=0.000134458 sps=35.2872 sps_per_gpu=35.2872 tps=4516.76 tps_per_gpu=4516.76 mfu=0.112433\n\n\n\n[2025-07-23 08:56:22,992148][I][wordplay/trainer:850] step=800 loss=4.23569 dt=0.0485556 dtf=0.047787 dtb=0.000512625 sps=20.595 sps_per_gpu=20.595 tps=2636.15 tps_per_gpu=2636.15 mfu=0.106886\n\n\n\n[2025-07-23 08:56:25,427496][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:56:25,428497][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM??.ahoskZqeofpQe'v;.p..hqYwqaarswbbc.ahwbkkA''KyhvX.yp'Vc3;oseo.xeeeaa'WQqfhKKfYqqqf.x33xx--;;;.egMcc-qaaovvKKOsvSpwesfgI;;wwerpMgtcgQsb;uQtggyyptokyy';QCy;;asoW,,Jr''''',AkkfYoAAAAAS::::;;.bWttqeqcbA::gYJJbqgjoBhopwe;.s''ggkk'qk.qkGWYYyqqe;''Sbs'MM;;.qqqqQ\n\n\n\n[2025-07-23 08:56:29,692460][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:56:29,707374][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:56:29,896928][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:56:30,164081][I][wordplay/trainer:850] step=810 loss=4.22317 dt=0.0286232 dtf=0.0282508 dtb=0.000134125 sps=34.9367 sps_per_gpu=34.9367 tps=4471.9 tps_per_gpu=4471.9 mfu=0.105861\n\n\n\n[2025-07-23 08:56:30,431249][I][wordplay/trainer:850] step=820 loss=4.24584 dt=0.0254122 dtf=0.0248138 dtb=0.000128083 sps=39.3512 sps_per_gpu=39.3512 tps=5036.95 tps_per_gpu=5036.95 mfu=0.106159\n\n\n\n[2025-07-23 08:56:30,701193][I][wordplay/trainer:850] step=830 loss=4.1855 dt=0.0262779 dtf=0.0258425 dtb=0.000144167 sps=38.0548 sps_per_gpu=38.0548 tps=4871.01 tps_per_gpu=4871.01 mfu=0.106069\n\n\n\n[2025-07-23 08:56:30,924979][I][wordplay/trainer:850] step=840 loss=4.24083 dt=0.0218259 dtf=0.0214495 dtb=0.000131875 sps=45.8171 sps_per_gpu=45.8171 tps=5864.59 tps_per_gpu=5864.59 mfu=0.108135\n\n\n\n[2025-07-23 08:56:31,161368][I][wordplay/trainer:850] step=850 loss=4.23785 dt=0.0199109 dtf=0.0193973 dtb=0.000151042 sps=50.2238 sps_per_gpu=50.2238 tps=6428.65 tps_per_gpu=6428.65 mfu=0.111213\n\n\n\n[2025-07-23 08:56:33,336937][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:56:33,338045][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?A;QfqrqQ'xxx'aa.hh3vv''wwossqZse'rxfQsseh'.evrpMq''.xxTUeQ'''rqqaxfxtcbqcf3qq3jZbvcepwA,,,ff'hpqcpcA-A'rv::errrvbbZ:pc-qycSScWlbQYhhwwAA-SQCgl;bbrpbSrrrrqqqqq''rWqqtcAkYyqgYtxttttbkkqQWWqaqqqkkk,'qqexrrWSSqyyYj'SyyQYQQ,q''p'---p''tcqzhhhpqWfs.p'foBqqQt::eu\n\n\n\n[2025-07-23 08:56:37,213617][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:56:37,215419][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:56:37,356621][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:56:37,588548][I][wordplay/trainer:850] step=860 loss=4.20116 dt=0.0262202 dtf=0.0254794 dtb=0.00046525 sps=38.1385 sps_per_gpu=38.1385 tps=4881.72 tps_per_gpu=4881.72 mfu=0.11064\n\n\n\n[2025-07-23 08:56:37,872010][I][wordplay/trainer:850] step=870 loss=4.22428 dt=0.0259347 dtf=0.0252386 dtb=0.0001625 sps=38.5584 sps_per_gpu=38.5584 tps=4935.47 tps_per_gpu=4935.47 mfu=0.110241\n\n\n\n[2025-07-23 08:56:38,130826][I][wordplay/trainer:850] step=880 loss=4.22977 dt=0.0227387 dtf=0.0222231 dtb=0.000191125 sps=43.9779 sps_per_gpu=43.9779 tps=5629.17 tps_per_gpu=5629.17 mfu=0.111381\n\n\n\n[2025-07-23 08:56:38,428291][I][wordplay/trainer:850] step=890 loss=4.22047 dt=0.0358749 dtf=0.0341204 dtb=0.000402875 sps=27.8746 sps_per_gpu=27.8746 tps=3567.95 tps_per_gpu=3567.95 mfu=0.107953\n\n\n\n[2025-07-23 08:56:38,713290][I][wordplay/trainer:850] step=900 loss=4.35563 dt=0.0244102 dtf=0.0240346 dtb=0.000135 sps=40.9665 sps_per_gpu=40.9665 tps=5243.72 tps_per_gpu=5243.72 mfu=0.108489\n\n\n\n[2025-07-23 08:56:41,219069][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:56:41,220318][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?wwPA'eeew-3ZAjRwqs33eafCq'ax..xcxc''awA',bsettcCvCqqq33A-.bsor.awQfJ$  3a-3b U' Zq3gQQf',,AqGZ fhhPwU.vfCC.xpqvr.SkkofxsyQrrs';'kGs,rMse''rppb'qqfoktM'qo,qqSqgW,etM'M??Z;auYfSSo??gg'sSvSQQqfftcb;;;;pWQSffttqgQSSSkllbrqqaw,'SqqYQ;;;pqqtpBheW;;;.hn'qYyMMesgl\n\n\n\n[2025-07-23 08:56:45,165007][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:56:45,173286][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:56:45,293191][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:56:45,509566][I][wordplay/trainer:850] step=910 loss=4.19569 dt=0.0203347 dtf=0.0199538 dtb=0.000132334 sps=49.177 sps_per_gpu=49.177 tps=6294.66 tps_per_gpu=6294.66 mfu=0.111242\n\n\n\n[2025-07-23 08:56:45,738822][I][wordplay/trainer:850] step=920 loss=4.23206 dt=0.0204348 dtf=0.0200414 dtb=0.000165542 sps=48.936 sps_per_gpu=48.936 tps=6263.81 tps_per_gpu=6263.81 mfu=0.113653\n\n\n\n[2025-07-23 08:56:45,956799][I][wordplay/trainer:850] step=930 loss=4.29058 dt=0.0210795 dtf=0.0205656 dtb=0.0001655 sps=47.4396 sps_per_gpu=47.4396 tps=6072.26 tps_per_gpu=6072.26 mfu=0.115409\n\n\n\n[2025-07-23 08:56:46,195626][I][wordplay/trainer:850] step=940 loss=4.211 dt=0.0438769 dtf=0.0432041 dtb=0.0002575 sps=22.791 sps_per_gpu=22.791 tps=2917.25 tps_per_gpu=2917.25 mfu=0.110172\n\n\n\n[2025-07-23 08:56:46,443617][I][wordplay/trainer:850] step=950 loss=4.18626 dt=0.0242729 dtf=0.0235941 dtb=0.000165375 sps=41.1983 sps_per_gpu=41.1983 tps=5273.38 tps_per_gpu=5273.38 mfu=0.11055\n\n\n\n[2025-07-23 08:56:48,500097][I][wordplay/trainer:790] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:56:48,502936][I][wordplay/trainer:794] ['response']:\n\nWhat is an LLM?YfQooooRx3xccaHCvj3gllexpjGG,wUxe'oOf.smxxxrq-jj'kxxrkc3fkkeQZZe''YR'JhrZZAcowccpqA,QUJZpcAkkGGGqp--.v'appbYYbeeqbbZrk'MBfq-srksqYee'QQt'J',qWqt;qkGWbrrtqJ-'pa'ggjJSq--'sf'..;''aqfpfx'Sbbq3tooMbb?',AA-AW'MqAAk;ccAGqQqaA;WQhMSq;cffho,eWohpWott3jj---s;?ggIIS\n\n\n\n[2025-07-23 08:56:52,116538][I][wordplay/trainer:733] Saving checkpoint to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example\n\n\n\n[2025-07-23 08:56:52,117689][I][wordplay/trainer:734] Saving model to: /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example/model.pth\n\n\n\n[2025-07-23 08:56:52,203675][I][wordplay/configs:141] Appending /Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/content/02-llms/7-shakespeare-example to /Users/samforeman/projects/saforem2/wordplay/src/ckpts/checkpoints.log\n\n\n\n[2025-07-23 08:56:52,420205][I][wordplay/trainer:850] step=960 loss=4.225 dt=0.0212671 dtf=0.0205828 dtb=0.0001235 sps=47.0209 sps_per_gpu=47.0209 tps=6018.68 tps_per_gpu=6018.68 mfu=0.112501\n\n\n\n[2025-07-23 08:56:52,651407][I][wordplay/trainer:850] step=970 loss=4.17741 dt=0.0217033 dtf=0.0212886 dtb=0.000151 sps=46.076 sps_per_gpu=46.076 tps=5897.73 tps_per_gpu=5897.73 mfu=0.113995\n\n\n\n[2025-07-23 08:56:52,869502][I][wordplay/trainer:850] step=980 loss=4.1707 dt=0.0206437 dtf=0.0202328 dtb=0.00017225 sps=48.4409 sps_per_gpu=48.4409 tps=6200.44 tps_per_gpu=6200.44 mfu=0.115994\n\n\n\n[2025-07-23 08:56:53,103964][I][wordplay/trainer:850] step=990 loss=4.1891 dt=0.0248825 dtf=0.024147 dtb=0.000182167 sps=40.1888 sps_per_gpu=40.1888 tps=5144.17 tps_per_gpu=5144.17 mfu=0.115511\n\n\n\n[2025-07-23 08:56:53,334522][I][wordplay/trainer:850] step=1000 loss=4.2423 dt=0.0254565 dtf=0.0237365 dtb=0.000168792 sps=39.2826 sps_per_gpu=39.2826 tps=5028.18 tps_per_gpu=5028.18 mfu=0.114825",
    "crumbs": [
      "[02] Large Language Models",
      "[7] Shakespeare Example"
    ]
  },
  {
    "objectID": "02-llms/7-shakespeare-example/index.html#evaluate-model",
    "href": "02-llms/7-shakespeare-example/index.html#evaluate-model",
    "title": "wordplay üéÆ üí¨: Shakespeare ‚úçÔ∏è",
    "section": "Evaluate Model",
    "text": "Evaluate Model\n\nimport time\n\nquery = \"What is an LLM?\"\nt0 = time.perf_counter()\noutputs = trainer.evaluate(\n    query,\n    num_samples=1,\n    max_new_tokens=256,\n    top_k=2,\n    display=False\n)\nlogger.info(f'took: {time.perf_counter() - t0:.4f}s')\nlogger.info(f\"['prompt']: '{query}'\")\nlogger.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n[2025-07-23 08:56:55,840696][I][ipykernel_29374/582817405:12:ezpz.log] took: 2.4147s\n\n\n\n[2025-07-23 08:56:55,849901][I][ipykernel_29374/582817405:13:ezpz.log] ['prompt']: 'What is an LLM?'\n\n\n\n[2025-07-23 08:56:55,853140][I][ipykernel_29374/582817405:14:ezpz.log] ['response']:\n\nWhat is an LLM?ZxxA---'aaaaeeewAAAAA'''qqqqqqqqqqqqaeeqqqqqq''333qqAAA33akkk''qqqqqorrrrrrrrrrqqqqqqq.qe333aaaqqqqqf..qqqqqqq3333333-qqqqbbb''ggSSpMMMqqqqMMqqqqqqqqWW;?;?;?;???;;??MMMM;;;;;;??;;;;;;;;''''';??qqqqqqqW;;''''''''''''';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;'tttttMM",
    "crumbs": [
      "[02] Large Language Models",
      "[7] Shakespeare Example"
    ]
  },
  {
    "objectID": "02-llms/7-shakespeare-example/index.html#footnotes",
    "href": "02-llms/7-shakespeare-example/index.html#footnotes",
    "title": "wordplay üéÆ üí¨: Shakespeare ‚úçÔ∏è",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nin units of A100 bfloat16 peak FLOPS‚Ü©Ô∏é",
    "crumbs": [
      "[02] Large Language Models",
      "[7] Shakespeare Example"
    ]
  },
  {
    "objectID": "02-llms/5-advanced-llms/index.html",
    "href": "02-llms/5-advanced-llms/index.html",
    "title": "Intro to HPC Bootcamp 2025",
    "section": "",
    "text": "CitationBibTeX citation:@online{foreman2025,\n  author = {Foreman, Sam},\n  title = {Intro to {HPC} {Bootcamp} 2025},\n  date = {2025-07-22},\n  url = {https://saforem2.github.io/hpc-bootcamp-2025/02-llms/5-advanced-llms/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2025. ‚ÄúIntro to HPC Bootcamp 2025.‚Äù July 22,\n2025. https://saforem2.github.io/hpc-bootcamp-2025/02-llms/5-advanced-llms/.",
    "crumbs": [
      "[02] Large Language Models",
      "[5] Advanced LLMs"
    ]
  },
  {
    "objectID": "02-llms/1-hands-on-llms/index.html#outline",
    "href": "02-llms/1-hands-on-llms/index.html#outline",
    "title": "What‚Äôs going on under the hood?",
    "section": "Outline",
    "text": "Outline\nDuring this session I will cover: 1. Scientific applications for language models 2. General overview of Transformers 3. Tokenization 4. Model Architecture 5. Pipeline using HuggingFace\n6. Model loading",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/1-hands-on-llms/index.html#modeling-sequential-data",
    "href": "02-llms/1-hands-on-llms/index.html#modeling-sequential-data",
    "title": "What‚Äôs going on under the hood?",
    "section": "Modeling Sequential Data",
    "text": "Modeling Sequential Data\nSequences are variable-length lists with data in subsequent iterations that depends on previous iterations (or tokens).\nMathematically: A sequence is a list of tokens: T = [t_1, t_2, t_3,...,t_N] where each token within the list depends on the others with a particular probability:\nP(t_N | t_{N-1}, ..., t_3, t_2, t_1)\nThe purpose of sequential modeling is to learn these probabilities for possible tokens in a distribution to perform various tasks including: * Sequence generation based on a prompt * Language translation (e.g.¬†English ‚Äì&gt; French) * Property prediction (predicting a property based on an entire sequence) * Identifying mistakes or missing elements in sequential data",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/1-hands-on-llms/index.html#scientific-sequential-data-modeling-examples",
    "href": "02-llms/1-hands-on-llms/index.html#scientific-sequential-data-modeling-examples",
    "title": "What‚Äôs going on under the hood?",
    "section": "Scientific sequential data modeling examples",
    "text": "Scientific sequential data modeling examples\n### Nucleic acid sequences + genomic data\n\n\n\nNucleic acid sequences can be used to predict translation of proteins, mutations, and gene expression levels.\nHere is an image of GenSLM. This is a language model developed by Argonne researchers that can model genomic information in a single model. It was shown to model the evolution of SARS-COV2 without expensive experiments.\n\n\n\nZvyagin et. al 2022. BioRXiv\n\nProtein sequences\nProtein sequences can be used to predict folding structure, protein-protein interactions, chemical/binding properties, protein function and many more properties.\n\n\n\n\n\n\nLin et. al.¬†2023. Science\n\n\nOther applications:\n\nBiomedical text\nSMILES strings\nWeather predictions\nInterfacing with simulations such as molecular dynamics simulation",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/1-hands-on-llms/index.html#overview-of-language-models",
    "href": "02-llms/1-hands-on-llms/index.html#overview-of-language-models",
    "title": "What‚Äôs going on under the hood?",
    "section": "Overview of Language models",
    "text": "Overview of Language models\nWe will now briefly talk about the progression of language models.\n\nTransformers\nThe most common LMs base their design on the Transformer architecture that was introduced in 2017 in the ‚ÄúAttention is all you need‚Äù paper.\n\n\n\nVaswani 2017. Advances in Neural Information Processing Systems\nSince then a multitude of LLM architectures have been designed.\n\n\n\nHuggingFace NLP Course",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/1-hands-on-llms/index.html#coding-example-of-llms-in-action",
    "href": "02-llms/1-hands-on-llms/index.html#coding-example-of-llms-in-action",
    "title": "What‚Äôs going on under the hood?",
    "section": "Coding example of LLMs in action!",
    "text": "Coding example of LLMs in action!\nLet‚Äôs look at an example of running inference with a LLM as a block box to generate text given a prompt and we will also initiate a training loop for an LLM:\nHere, we will use the transformers library which is as part of HuggingFace, a repository of different models, tokenizers and information on how to apply these models\nWarning: Large Language Models are only as good as their training data. They have no ethics, no judgement, or editing ability. We will be using some pretrained models from Hugging Face which used wide samples of internet hosted text. The datasets have not been strictly filtered to restrict all malign content so the generated text may be surprisingly dark or questionable. They do not reflect our core values and are only used for demonstration purposes.\n\n'''\nUncomment below section if running on sophia jupyter notebook\n'''\n# import os\n# os.environ[\"HTTP_PROXY\"]=\"proxy.alcf.anl.gov:3128\"\n# os.environ[\"HTTPS_PROXY\"]=\"proxy.alcf.anl.gov:3128\"\n# os.environ[\"http_proxy\"]=\"proxy.alcf.anl.gov:3128\"\n# os.environ[\"https_proxy\"]=\"proxy.alcf.anl.gov:3128\"\n# os.environ[\"ftp_proxy\"]=\"proxy.alcf.anl.gov:3128\"\n\n'\\nUncomment below section if running on sophia jupyter notebook\\n'\n\n\n\n!pip install transformers\n!pip install pandas\n!pip install torch\n\nRequirement already satisfied: transformers in /opt/homebrew/lib/python3.11/site-packages (4.53.3)\nRequirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.30.0 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.33.4)\nRequirement already satisfied: numpy&gt;=1.17 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from transformers) (1.26.2)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml&gt;=5.1 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2023.10.3)\nRequirement already satisfied: requests in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers&lt;0.22,&gt;=0.21 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors&gt;=0.4.3 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm&gt;=4.27 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub&lt;1.0,&gt;=0.30.0-&gt;transformers) (2025.7.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub&lt;1.0,&gt;=0.30.0-&gt;transformers) (4.14.1)\nRequirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub&lt;1.0,&gt;=0.30.0-&gt;transformers) (1.1.5)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from requests-&gt;transformers) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from requests-&gt;transformers) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from requests-&gt;transformers) (2.1.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from requests-&gt;transformers) (2023.11.17)\nRequirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.1.0)\nRequirement already satisfied: numpy&gt;=1.23.2 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from pandas) (1.26.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.16.0)\nRequirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.7.1)\nRequirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions&gt;=4.10.0 in /opt/homebrew/lib/python3.11/site-packages (from torch) (4.14.1)\nRequirement already satisfied: sympy&gt;=1.13.3 in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.14.0)\nRequirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from torch) (2025.7.0)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from sympy&gt;=1.13.3-&gt;torch) (1.3.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from jinja2-&gt;torch) (2.1.3)\n\n\n\nfrom transformers import AutoTokenizer,AutoModelForCausalLM, AutoConfig\ninput_text = \"My dog really wanted to eat icecream because\"\nfrom transformers import pipeline\ngenerator = pipeline(\"text-generation\", model=\"gpt2\")\npipe = pipeline(\"text-generation\", model=\"gpt2\")\ngenerator(input_text, max_length=20, num_return_sequences=5)\n\nDevice set to use mps:0\nDevice set to use mps:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nBoth `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n\n\n[{'generated_text': 'My dog really wanted to eat icecream because it made her feel good, just like it did when my dad ate spaghetti and I made her cry,\" he said. \"She didn\\'t want to have to do this. It was so hard for her.\" Even with the ice cream, his daughter is still in the dark about what happened to her. She doesn\\'t know what to make of it all. \"She\\'s still a kid and she\\'s not sure what she wants to do,\" said Rizzo. \"He\\'s still trying to figure it out, but she\\'s just a little scared and he\\'s stuck in this limbo.\"\\n\\nIn a way, the situation looks better for her now than it was when she first saw him. When Rizzo got older, she was devastated that he didn\\'t want her to have to grow up. She didn\\'t want to have to go to school. She didn\\'t want to be a teacher, a parent. \"I was happy that he was there to help me, but I never thought I could be on my own. He wouldn\\'t let me be his friend, he wouldn\\'t let me be his brother, that was a big mistake,\" said Rizzo. \"I think he thought I was a little bit crazy, but I think he\\'d have'},\n {'generated_text': 'My dog really wanted to eat icecream because it was so delicious. I was so happy when she told me how much of an icecream it was. She said it\\'s so delicious and I was like, \"Oh my gosh, it\\'s so good! I love it!\" I was so happy.\\n\\nAfter the first week, you were so excited to get into the kitchen. How did you enjoy this?\\n\\nIt\\'s such a great feeling. I was so grateful to be a part of my family. I\\'m always happy about it. I don\\'t know what\\'s going to happen next, but I really love my dog.\\n\\nWho are your favorite people to work with in your life?\\n\\nMy mom. She\\'s also my sister. I think she\\'s great. We\\'re friends. It\\'s great to meet so many of my friends.\\n\\nDo you have any kind of food you would like to share with people?\\n\\nI love having my own pet. I just can\\'t take her away.\\n\\nHow does your dog keep you safe when she\\'s in the fridge?\\n\\nShe\\'s always there with me. He always comes home and I never know what\\'s going to happen next. It\\'s a lot of fun to watch her go and she\\'ll always'},\n {'generated_text': 'My dog really wanted to eat icecream because I\\'ve been looking at him for a while and he\\'s been eating it all day. He just came home from work and he just kept eating it. It\\'s just a really amazing little treat.\"\\n\\nThe food is very close to his dog, who is also a vegetarian. \"He\\'s very good with it and he goes outside, he\\'s eating it and he does have some other treats that he just chews and stuff like that,\" said his dad, Greg, a retired chef and dog trainer.\\n\\n\"It\\'s all to make sure that he\\'s doing well, that he\\'s getting well, and that he can be a good friend. It\\'s just a very good dog. He\\'s doing great.\"\\n\\nThe couple bought the dog a couple of years ago after a pet food store in the village. \"I just love him,\" said Greg. \"He loves being out. He\\'s always been a big part of our family. He\\'s always been there for us. He\\'s always been a big part of our lives.\"\\n\\nGreg said that the family also has a dog called Little Sneezy that was born with a rare form of leukemia. \"The little Sneezy is a very sick dog. We have a special couple of'},\n {'generated_text': 'My dog really wanted to eat icecream because she thought it was really good for her.\\n\\nBut she was wrong. In her mind, she was eating icecream with her two children.'},\n {'generated_text': 'My dog really wanted to eat icecream because it\\'s so good.\"\\n\\nThe family, who live in a small house across the street from the brewery, had been looking for a home for their dog since he was just two years old.\\n\\nOne of their plans was to buy a trailer home for the dogs, but were told they could only get the full size truck.\\n\\n\"I was so upset because my dog was so tiny,\" said Sarah MacLennan, who lives with her husband, Tom, in a family-run family-run business.\\n\\n\"We could only get the truck, and it was going to take about five days to get here.\\n\\n\"We thought, \\'What happened? It was such a waste of money.\\'\"\\n\\nThe family\\'s trailer was never in service at the time.\\n\\n\"We wanted a home, and we wanted a home that was easy to live in,\" said MacLennan.\\n\\nBut they didn\\'t know where to turn.\\n\\n\"We were like, \\'Oh, no, we don\\'t want to sell them this thing on the street. But we don\\'t want to sell them that stuff,\\'\" she said.\\n\\nAt first, MacLennan was devastated about the lack of help.\\n'}]\n\n\nThere are two components that are ‚Äúblack-boxes‚Äù here:\n\nThe method for tokenization\nThe model that generates novel text.",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/1-hands-on-llms/index.html#tokenization-and-embedding-of-sequential-data",
    "href": "02-llms/1-hands-on-llms/index.html#tokenization-and-embedding-of-sequential-data",
    "title": "What‚Äôs going on under the hood?",
    "section": "Tokenization and embedding of sequential data",
    "text": "Tokenization and embedding of sequential data\nHumans can inherently understand language data because they previously learned phonetic sounds.\nMachines don‚Äôt have phonetic knowledge so they need to be told how to break text into standard units to process it.\nThey use a system called ‚Äútokenization‚Äù, where sequences of text are broken into smaller parts, or ‚Äútokens‚Äù, and then fed as input.\n\n\n\nTokenization is a data preprocessing step which transforms the raw text data into a format suitable for machine learning models. Tokenizers break down raw text into smaller units called tokens. These tokens are what is fed into the language models. Based on the type and configuration of the tokenizer, these tokens can be words, subwords, or characters.\nTypes of tokenizers:\n\nCharacter Tokenizers: Split text into individual characters.\nWord Tokenizers: Split text into words based on whitespace or punctuation.\nSubword Tokenizers: Split text into subword units, such as morphemes or character n-grams. Common subword tokenization algorithms include:\nByte-Pair Encoding (BPE),\nSentencePiece,\nWordPiece.\n\n\n\n\nnlpiation\n\nExample of tokenization\nLet‚Äôs look at an example of tokenization using byte-pair encoding.\n\nfrom transformers import AutoTokenizer\n\n# A utility function to tokenize a sequence and print out some information about it.\n\ndef tokenization_summary(tokenizer, sequence):\n\n    # get the vocabulary\n    vocab = tokenizer.vocab\n    # Number of entries to print\n    n = 10\n\n    # Print subset of the vocabulary\n    print(\"Subset of tokenizer.vocab:\")\n    for i, (token, index) in enumerate(tokenizer.vocab.items()):\n        print(f\"{token}: {index}\")\n        if i &gt;= n - 1:\n            break\n\n    print(\"Vocab size of the tokenizer = \", len(vocab))\n    print(\"------------------------------------------\")\n\n    # .tokenize chunks the existing sequence into different tokens based on the rules and vocab of the tokenizer.\n    tokens = tokenizer.tokenize(sequence)\n    print(\"Tokens : \", tokens)\n    print(\"------------------------------------------\")\n\n    # .convert_tokens_to_ids or .encode or .tokenize converts the tokens to their corresponding numerical representation.\n    #  .convert_tokens_to_ids has a 1-1 mapping between tokens and numerical representation\n    # ids = tokenizer.convert_tokens_to_ids(tokens)\n    # print(\"encoded Ids: \", ids)\n\n    # .encode also adds additional information like Start of sequence tokens and End of sequene\n    print(\"tokenized sequence : \", tokenizer.encode(sequence))\n\n    # .tokenizer has additional information about attention_mask.\n    # encode = tokenizer(sequence)\n    # print(\"Encode sequence : \", encode)\n    # print(\"------------------------------------------\")\n\n    # .decode decodes the ids to raw text\n    ids = tokenizer.convert_tokens_to_ids(tokens)\n    decode = tokenizer.decode(ids)\n    print(\"Decode sequence : \", decode)\n\n\ntokenizer_1  =  AutoTokenizer.from_pretrained(\"gpt2\") # GPT-2 uses \"Byte-Pair Encoding (BPE)\"\n\nsequence = \"Counselor, please adjust your Zoom filter to appear as a human, rather than as a cat\"\n\ntokenization_summary(tokenizer_1, sequence)\n\nSubset of tokenizer.vocab:\nƒ†bloody: 15222\nƒ†Lucas: 15257\nƒ†Solution: 28186\nƒ†Homs: 47778\nƒ†Hunt: 12937\nƒ†emerge: 14740\nƒ†::: 7904\nlux: 22564\nƒ†Templ: 31711\nƒ†lin: 9493\nVocab size of the tokenizer =  50257\n------------------------------------------\nTokens :  ['Coun', 'sel', 'or', ',', 'ƒ†please', 'ƒ†adjust', 'ƒ†your', 'ƒ†Zoom', 'ƒ†filter', 'ƒ†to', 'ƒ†appear', 'ƒ†as', 'ƒ†a', 'ƒ†human', ',', 'ƒ†rather', 'ƒ†than', 'ƒ†as', 'ƒ†a', 'ƒ†cat']\n------------------------------------------\ntokenized sequence :  [31053, 741, 273, 11, 3387, 4532, 534, 40305, 8106, 284, 1656, 355, 257, 1692, 11, 2138, 621, 355, 257, 3797]\nDecode sequence :  Counselor, please adjust your Zoom filter to appear as a human, rather than as a cat\n\n\n\n\nToken embedding:\nWords are turned into vectors based on their location within a vocabulary.\nThe strategy of choice for learning language structure from tokenized text is to find a clever way to map each token into a moderate-dimension vector space, adjusting the mapping so that\nSimilar, or associated tokens take up residence nearby each other, and different regions of the space correspond to different position in the sequence. Such a mapping from token ID to a point in a vector space is called a token embedding. The dimension of the vector space is often high (e.g.¬†1024-dimensional), but much smaller than the vocabulary size (30,000‚Äì500,000).\nVarious approaches have been attempted for generating such embeddings, including static algorithms that operate on a corpus of tokenized data as preprocessors for NLP tasks. Transformers, however, adjust their embeddings during training.",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/1-hands-on-llms/index.html#transformer-model-architecture",
    "href": "02-llms/1-hands-on-llms/index.html#transformer-model-architecture",
    "title": "What‚Äôs going on under the hood?",
    "section": "Transformer Model Architecture",
    "text": "Transformer Model Architecture\nNow let‚Äôs look at the base elements that make up a Transformer by dissecting the popular GPT2 model\n\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nprint(model)\n\nGPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)\n\n\nGPT2 is an example of a Transformer Decoder which is used to generate novel text.\nDecoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models. The pretraining of decoder models usually revolves around predicting the next word in the sentence.\nThese models are best suited for tasks involving text generation.\nThe architecture of GPT-2 is inspired by the paper: ‚ÄúGenerating Wikipedia by Summarizing Long Sequences‚Äù which is another arrangement of the transformer block that can do language modeling. This model threw away the encoder and thus is known as the ‚ÄúTransformer-Decoder‚Äù.\n\n\n\nIllustrated GPT2\nKey components of the transformer architecture include:\n\nInput Embeddings: Word embedding or word vectors help us represent words or text as a numeric vector where words with similar meanings have the similar representation.\nPositional Encoding: Injects information about the position of words in a sequence, helping the model understand word order.\nSelf-Attention Mechanism: Allows the model to weigh the importance of different words in a sentence, enabling it to effectively capture contextual information.\nFeedforward Neural Networks: Process information from self-attention layers to generate output for each word/token.\nLayer Normalization and Residual Connections: Aid in stabilizing training and mitigating the vanishing gradient problem.\nTransformer Blocks: Comprised of multiple layers of self-attention and feedforward neural networks, stacked together to form the model.\n\n\nAttention mechanisms\nSince attention mechanisms are arguably the most powerful component of the Transformer, let‚Äôs discuss this in a little more detail.\nSuppose the following sentence is an input sentence we want to translate using an LLM:\n‚ÄùThe animal didn't cross the street because it was too tired‚Äù\nTo understand a full sentence, the model needs to understand what each word means in relation to other words.\nFor example, when we read the sentence: ‚ÄùThe animal didn't cross the street because it was too tired‚Äù we know intuitively that the word \"it\" refers to \"animal\", the state for \"it\" is \"tired\", and the associated action is \"didn't cross\".\nHowever, the model needs a way to learn all of this information in a simple yet generalizable way. What makes Transformers particularly powerful compared to earlier sequential architectures is how it encodes context with the self-attention mechanism.\nAs the model processes each word in the input sequence, attention looks at other positions in the input sequence for clues to a better understanding for this word.\n\n\n\nThe Illustrated Transformer\n\nMulti-head attention\nIn practice, multiple attention heads are used simultaneously.\nThis: * Expands the model‚Äôs ability to focus on different positions. * Prevents the attention to be dominated by the word itself.\n\n\nLet‚Äôs see multi-head attention mechanisms in action!\nWe are going to use the powerful visualization tool bertviz, which allows an interactive experience of the attention mechanisms. Normally these mechanisms are abstracted away but this will allow us to inspect our model in more detail.\n\n!pip install bertviz\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\nRequirement already satisfied: bertviz in /opt/homebrew/lib/python3.11/site-packages (1.4.1)\nRequirement already satisfied: transformers&gt;=2.0 in /opt/homebrew/lib/python3.11/site-packages (from bertviz) (4.53.3)\nRequirement already satisfied: torch&gt;=1.0 in /opt/homebrew/lib/python3.11/site-packages (from bertviz) (2.7.1)\nRequirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from bertviz) (4.66.1)\nRequirement already satisfied: boto3 in /opt/homebrew/lib/python3.11/site-packages (from bertviz) (1.39.11)\nRequirement already satisfied: requests in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from bertviz) (2.31.0)\nRequirement already satisfied: regex in /opt/homebrew/lib/python3.11/site-packages (from bertviz) (2023.10.3)\nRequirement already satisfied: sentencepiece in /opt/homebrew/lib/python3.11/site-packages (from bertviz) (0.2.0)\nRequirement already satisfied: IPython&gt;=7.14 in /opt/homebrew/lib/python3.11/site-packages (from bertviz) (8.21.0)\nRequirement already satisfied: decorator in /opt/homebrew/lib/python3.11/site-packages (from IPython&gt;=7.14-&gt;bertviz) (5.1.1)\nRequirement already satisfied: jedi&gt;=0.16 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from IPython&gt;=7.14-&gt;bertviz) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/homebrew/lib/python3.11/site-packages (from IPython&gt;=7.14-&gt;bertviz) (0.1.6)\nRequirement already satisfied: prompt-toolkit&lt;3.1.0,&gt;=3.0.41 in /opt/homebrew/lib/python3.11/site-packages (from IPython&gt;=7.14-&gt;bertviz) (3.0.43)\nRequirement already satisfied: pygments&gt;=2.4.0 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from IPython&gt;=7.14-&gt;bertviz) (2.17.2)\nRequirement already satisfied: stack-data in /opt/homebrew/lib/python3.11/site-packages (from IPython&gt;=7.14-&gt;bertviz) (0.6.2)\nRequirement already satisfied: traitlets&gt;=5 in /opt/homebrew/lib/python3.11/site-packages (from IPython&gt;=7.14-&gt;bertviz) (5.14.1)\nRequirement already satisfied: pexpect&gt;4.3 in /opt/homebrew/lib/python3.11/site-packages (from IPython&gt;=7.14-&gt;bertviz) (4.9.0)\nRequirement already satisfied: wcwidth in /opt/homebrew/lib/python3.11/site-packages (from prompt-toolkit&lt;3.1.0,&gt;=3.0.41-&gt;IPython&gt;=7.14-&gt;bertviz) (0.2.6)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from jedi&gt;=0.16-&gt;IPython&gt;=7.14-&gt;bertviz) (0.8.3)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /opt/homebrew/lib/python3.11/site-packages (from pexpect&gt;4.3-&gt;IPython&gt;=7.14-&gt;bertviz) (0.7.0)\nRequirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch&gt;=1.0-&gt;bertviz) (3.18.0)\nRequirement already satisfied: typing-extensions&gt;=4.10.0 in /opt/homebrew/lib/python3.11/site-packages (from torch&gt;=1.0-&gt;bertviz) (4.14.1)\nRequirement already satisfied: sympy&gt;=1.13.3 in /opt/homebrew/lib/python3.11/site-packages (from torch&gt;=1.0-&gt;bertviz) (1.14.0)\nRequirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch&gt;=1.0-&gt;bertviz) (3.5)\nRequirement already satisfied: jinja2 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from torch&gt;=1.0-&gt;bertviz) (3.1.2)\nRequirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from torch&gt;=1.0-&gt;bertviz) (2025.7.0)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from sympy&gt;=1.13.3-&gt;torch&gt;=1.0-&gt;bertviz) (1.3.0)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.30.0 in /opt/homebrew/lib/python3.11/site-packages (from transformers&gt;=2.0-&gt;bertviz) (0.33.4)\nRequirement already satisfied: numpy&gt;=1.17 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from transformers&gt;=2.0-&gt;bertviz) (1.26.2)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from transformers&gt;=2.0-&gt;bertviz) (23.2)\nRequirement already satisfied: pyyaml&gt;=5.1 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from transformers&gt;=2.0-&gt;bertviz) (6.0.1)\nRequirement already satisfied: tokenizers&lt;0.22,&gt;=0.21 in /opt/homebrew/lib/python3.11/site-packages (from transformers&gt;=2.0-&gt;bertviz) (0.21.2)\nRequirement already satisfied: safetensors&gt;=0.4.3 in /opt/homebrew/lib/python3.11/site-packages (from transformers&gt;=2.0-&gt;bertviz) (0.5.3)\nRequirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub&lt;1.0,&gt;=0.30.0-&gt;transformers&gt;=2.0-&gt;bertviz) (1.1.5)\nRequirement already satisfied: botocore&lt;1.40.0,&gt;=1.39.11 in /opt/homebrew/lib/python3.11/site-packages (from boto3-&gt;bertviz) (1.39.11)\nRequirement already satisfied: jmespath&lt;2.0.0,&gt;=0.7.1 in /opt/homebrew/lib/python3.11/site-packages (from boto3-&gt;bertviz) (1.0.1)\nRequirement already satisfied: s3transfer&lt;0.14.0,&gt;=0.13.0 in /opt/homebrew/lib/python3.11/site-packages (from boto3-&gt;bertviz) (0.13.1)\nRequirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from botocore&lt;1.40.0,&gt;=1.39.11-&gt;boto3-&gt;bertviz) (2.8.2)\nRequirement already satisfied: urllib3!=2.2.0,&lt;3,&gt;=1.25.4 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from botocore&lt;1.40.0,&gt;=1.39.11-&gt;boto3-&gt;bertviz) (2.1.0)\nRequirement already satisfied: six&gt;=1.5 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.40.0,&gt;=1.39.11-&gt;boto3-&gt;bertviz) (1.16.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from jinja2-&gt;torch&gt;=1.0-&gt;bertviz) (2.1.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from requests-&gt;bertviz) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from requests-&gt;bertviz) (3.6)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/samforeman/Library/Python/3.11/lib/python/site-packages (from requests-&gt;bertviz) (2023.11.17)\nRequirement already satisfied: executing&gt;=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from stack-data-&gt;IPython&gt;=7.14-&gt;bertviz) (1.2.0)\nRequirement already satisfied: asttokens&gt;=2.1.0 in /opt/homebrew/lib/python3.11/site-packages (from stack-data-&gt;IPython&gt;=7.14-&gt;bertviz) (2.4.0)\nRequirement already satisfied: pure-eval in /opt/homebrew/lib/python3.11/site-packages (from stack-data-&gt;IPython&gt;=7.14-&gt;bertviz) (0.2.2)\n\n\nLet‚Äôs load in the model, GPT2 and look at the attention mechanisms.\nHint‚Ä¶ click on the different blocks in the visualization to see the attention\n\nfrom transformers import AutoTokenizer, AutoModel, utils, AutoModelForCausalLM\n\nfrom bertviz import model_view\nutils.logging.set_verbosity_error()  # Suppress standard warnings\n\nmodel_name = 'openai-community/gpt2'\ninput_text = \"No, I am your father\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ninputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\noutputs = model(inputs)  # Run model\nattention = outputs[-1]  # Retrieve attention from model outputs\ntokens = tokenizer.convert_ids_to_tokens(inputs[0])  # Convert input ids to token strings\nmodel_view(attention, tokens)  # Display model view",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/1-hands-on-llms/index.html#pipeline-using-huggingface",
    "href": "02-llms/1-hands-on-llms/index.html#pipeline-using-huggingface",
    "title": "What‚Äôs going on under the hood?",
    "section": "Pipeline using HuggingFace",
    "text": "Pipeline using HuggingFace\nNow, let‚Äôs see a practical application of LLMs using a HuggingFace pipeline for classification.\nThis involves a few steps including: 1. Setting up a prompt 2. Loading in a pretrained model 3. Loading in the tokenizer and tokenizing input text 4. Performing model inference 5. Interpreting inference output\n\n# STEP 0 : Installations and imports\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\nimport torch\nimport torch.nn.functional as F\n\n\n1. Setting up a prompt\nA ‚Äúprompt‚Äù refers to a specific input or query provided to a language model. They guide the text processing and generation by providing the context for the model to generate coherent and relevant text based on the given input.\nThe choice and structure of the prompt depends on the specific task, the context and desired output. Prompts can be ‚Äúdiscrete‚Äù or ‚Äúinstructive‚Äù where they are explicit instructions or questions directed to the language model. They can also be more nuanced by more providing suggestions, directions and contexts to the model.\nWe will use very simple prompts in this tutorial section, but we will learn more about prompt engineering and how it helps in optimizing the performance of the model for a given use case in the following tutorials.\n\n# STEP 1 : Set up the prompt\ninput_text = \"The panoramic view of the ocean was breathtaking.\"\n\n\n\n2. Loading Pretrained Models\nThe AutoModelForSequenceClassification from_pretrained() method instantiates a sequence classification model.\nRefer to https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodels for the list of model classes supported.\n‚Äúfrom_pretrained‚Äù method downloads the pre-trained weights from the Hugging Face Model Hub or the specified URL if the model is not already cached locally. It then loads the weights into the instantiated model, initializing the model parameters with the pre-trained values.\nThe model cache contains:\n\nmodel configuration (config.json)\npretrained model weights (model.safetensors)\ntokenizer information (tokenizer.json, vocab.json, merges.txt, tokenizer.model)\n\n\n# STEP 2 : Load the pretrained model.\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\nconfig = AutoConfig.from_pretrained(model_name)\nprint(config)\n\nDistilBertConfig {\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForSequenceClassification\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"finetuning_task\": \"sst-2\",\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"NEGATIVE\",\n    \"1\": \"POSITIVE\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"NEGATIVE\": 0,\n    \"POSITIVE\": 1\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.53.3\",\n  \"vocab_size\": 30522\n}\n\n\n\n\n\n3. Loading in the tokenizer and tokenizing input text\nHere, we load in a pretrained tokenizer associated with this model.\n\n#STEP 3 : Load the tokenizer and tokenize the input text\ntokenizer  =  AutoTokenizer.from_pretrained(model_name)\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"]\nprint(input_ids)\n\ntensor([[  101,  1996,  6090,  6525,  7712,  3193,  1997,  1996,  4153,  2001,\n          3052, 17904,  1012,   102]])\n\n\n\n\n4. Performing inference and interpreting\nHere, we: * load data into the model, * perform inference to obtain logits, * Convert logits into probabilities * According to probabilities assign label\nThe end result is that we can predict whether the input phrase is positive or negative.\n\n# STEP 5 : Perform inference\noutputs = model(input_ids)\nresult = outputs.logits\nprint(result)\n\n# STEP 6 :  Interpret the output.\nprobabilities = F.softmax(result, dim=-1)\nprint(probabilities)\npredicted_class = torch.argmax(probabilities, dim=-1).item()\nlabels = [\"NEGATIVE\", \"POSITIVE\"]\nout_string = \"[{'label': '\" + str(labels[predicted_class]) + \"', 'score': \" + str(probabilities[0][predicted_class].tolist()) + \"}]\"\nprint(out_string)\n\ntensor([[-4.2767,  4.5486]], grad_fn=&lt;AddmmBackward0&gt;)\ntensor([[1.4695e-04, 9.9985e-01]], grad_fn=&lt;SoftmaxBackward0&gt;)\n[{'label': 'POSITIVE', 'score': 0.9998530149459839}]\n\n\n\n\nSaving and loading models\nModel can be saved and loaded to and from a local model directory.\n\nfrom transformers import AutoModel, AutoModelForCausalLM\n\n# Instantiate and train or fine-tune a model\nmodel = AutoModelForCausalLM.from_pretrained(\"bert-base-uncased\")\n\n# Train or fine-tune the model...\n\n# Save the model to a local directory\ndirectory = \"my_local_model\"\nmodel.save_pretrained(directory)\n\n# Load a pre-trained model from a local directory\nloaded_model = AutoModel.from_pretrained(directory)\n\n[2025-07-23 08:53:20,313] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to mps (auto detect)\n\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nW0723 08:53:20.644000 27000 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n\n\n[2025-07-23 08:53:21,247] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/1-hands-on-llms/index.html#model-hub",
    "href": "02-llms/1-hands-on-llms/index.html#model-hub",
    "title": "What‚Äôs going on under the hood?",
    "section": "Model Hub",
    "text": "Model Hub\nThe Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing.\n\nDownload pre-trained models with the huggingface_hub client library, with Transformers for fine-tuning.\nMake use of Inference API to use models in production settings.\nYou can filter for different models for different tasks, frameworks used, datasets used, and many more.\nYou can select any model, that will show the model card.\nModel card contains information of the model, including the description, usage, limitations etc. Some models also have inference API‚Äôs that can be used directly.\n\nModel Hub Link : https://huggingface.co/docs/hub/en/models-the-hub\nExample of a model card : https://huggingface.co/bert-base-uncased/tree/main",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/1-hands-on-llms/index.html#recommended-reading",
    "href": "02-llms/1-hands-on-llms/index.html#recommended-reading",
    "title": "What‚Äôs going on under the hood?",
    "section": "Recommended reading",
    "text": "Recommended reading\n\n‚ÄúThe Illustrated Transformer‚Äù by Jay Alammar\n‚ÄúVisualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)‚Äù by Jay Alammar\n‚ÄúThe Illustrated GPT-2 (Visualizing Transformer Language Models)‚Äù\n‚ÄúA gentle introduction to positional encoding‚Äù\n‚ÄúLLM Tutorial Workshop (Argonne National Laboratory)‚Äù\n‚ÄúLLM Tutorial Workshop Part 2 (Argonne National Laboratory)‚Äù",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/1-hands-on-llms/index.html#homework",
    "href": "02-llms/1-hands-on-llms/index.html#homework",
    "title": "What‚Äôs going on under the hood?",
    "section": "Homework",
    "text": "Homework\n\nLoad in a generative model using the HuggingFace pipeline and generate text using a batch of prompts.\n\n\nPlay with generative parameters such as temperature, max_new_tokens, and the model itself and explain the effect on the legibility of the model response. Try at least 4 different parameter/model combinations.\nModels that can be used include:\n\ngoogle/gemma-2-2b-it\nmicrosoft/Phi-3-mini-4k-instruct\nmeta-llama/Llama-3.2-1B\nAny model from this list: Text-generation models\ngpt2 if having trouble loading these models in\n\nThis guide should help! Text-generation strategies\n\n\nLoad in 2 models of different parameter size (e.g.¬†GPT2, meta-llama/Llama-2-7b-chat-hf, or distilbert/distilgpt2) and analyze the BertViz for each. How does the attention mechanisms change depending on model size?",
    "crumbs": [
      "[02] Large Language Models",
      "[1] Hands-on LLMs"
    ]
  },
  {
    "objectID": "02-llms/index.html",
    "href": "02-llms/index.html",
    "title": "Introduction to Large Language Models (LLMs)",
    "section": "",
    "text": "CitationBibTeX citation:@online{foreman2025,\n  author = {Foreman, Sam},\n  title = {Introduction to {Large} {Language} {Models} {(LLMs)}},\n  date = {2025-07-15},\n  url = {https://saforem2.github.io/hpc-bootcamp-2025/02-llms/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2025. ‚ÄúIntroduction to Large Language Models\n(LLMs).‚Äù July 15, 2025. https://saforem2.github.io/hpc-bootcamp-2025/02-llms/.",
    "crumbs": [
      "[02] Large Language Models"
    ]
  },
  {
    "objectID": "01-neural-networks/4-representation-learning/index.html",
    "href": "01-neural-networks/4-representation-learning/index.html",
    "title": "Foundation Models",
    "section": "",
    "text": "This notebook is not officially part of the course.\nBut you are welcome to look through it anyways, you can send questions on slack, and we are happy to talk about it.\nAuthor: Corey Adams\nThe previous notebook trained a classifier network which did ok. But what if we didn‚Äôt have a lot of data? In this notebook, we‚Äôll apply that model in a new way with representation learning.\n\nimport os\nimport sys\nif sys.platform == \"darwin\":\n    os.environ[\"TORCH_DEVICE\"] = \"cpu\"\n    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = \"1\"\n\n\nimport numpy, random\nimport torch\nimport torchvision\nfrom torchvision.transforms import v2\n\n\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nbatch_size = 128\n\nHere‚Äôs the Convolutional Neural Network Again:\n\nfrom torch import nn\n\nclass Downsampler(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride=2):\n        super(Downsampler, self).__init__()\n\n        self.norm = nn.InstanceNorm2d(in_channels)\n\n        self.downsample = nn.Conv2d(\n            in_channels=in_channels, \n            out_channels=out_channels,\n            kernel_size = stride,\n            stride = stride,\n        )\n    \n    def forward(self, inputs):\n\n        return self.downsample(self.norm(inputs))\n        \n        \n\nclass ConvNextBlock(nn.Module):\n    \"\"\"This block of operations is loosely based on this paper:\n\n    \"\"\"\n\n\n    def __init__(self, in_channels):\n        super(ConvNextBlock, self).__init__()\n\n        # Depthwise, seperable convolution with a large number of output filters:\n        self.conv1 = nn.Conv2d(in_channels=in_channels, \n                                     out_channels=in_channels, \n                                     groups=in_channels,\n                                     kernel_size=[7,7],\n                                     padding='same' )\n\n        self.norm = nn.InstanceNorm2d(in_channels)\n\n        # Two more convolutions:\n        self.conv2 = nn.Conv2d(in_channels=in_channels, \n                                     out_channels=4*in_channels,\n                                     kernel_size=1)\n\n        self.conv3 = nn.Conv2d(in_channels=4*in_channels, \n                                     out_channels=in_channels,\n                                     kernel_size=1\n                                     )\n\n\n    def forward(self, inputs):\n        x = self.conv1(inputs)\n\n        # The normalization layer:\n        x = self.norm(x)\n\n        x = self.conv2(x)\n\n        # The non-linear activation layer:\n        x = torch.nn.functional.gelu(x)\n\n        x = self.conv3(x)\n\n        # This makes it a residual network:\n        return x + inputs\n    \n\nclass Classifier(nn.Module):\n\n\n    def __init__(self, n_initial_filters, n_stages, blocks_per_stage, n_outputs):\n        super(Classifier, self).__init__()\n\n        # This is a downsampling convolution that will produce patches of output.\n\n        # This is similar to what vision transformers do to tokenize the images.\n        self.stem = nn.Conv2d(in_channels=3,\n                                    out_channels=n_initial_filters,\n                                    kernel_size=1,\n                                    stride=1)\n        \n        self.norm1 = nn.InstanceNorm2d(n_initial_filters)\n\n        current_n_filters = n_initial_filters\n        \n        self.layers = nn.Sequential()\n        for n_blocks in range(n_stages):\n            # Add a convnext block series:\n            for _ in range(blocks_per_stage):\n                self.layers.append(ConvNextBlock(in_channels=current_n_filters))\n            # Add a downsampling layer:\n            self.layers.append(Downsampler(in_channels=current_n_filters, out_channels=2*current_n_filters))\n            # Double the number of filters:\n            current_n_filters = 2*current_n_filters\n\n\n        self.head = nn.Sequential(\n            nn.Flatten(),\n            nn.LayerNorm(current_n_filters),\n            nn.Linear(current_n_filters, n_outputs)\n        )\n\n    def forward(self, inputs):\n\n        x = self.stem(inputs)\n        # Apply a normalization after the initial patching:\n        x = self.norm1(x)\n\n        # Apply the main chunk of the network:\n        x = self.layers(x)\n\n        # Normalize and readout:\n        x = nn.functional.avg_pool2d(x, x.shape[2:])\n        x = self.head(x)\n\n        return x\n\n\ndef create_representation_model(n_features, rank, size):\n\n    model = Classifier(32, 2, 2, n_features)\n\n\n    model.to(torch.get_default_device())\n\n    return model\n\nmodel = create_representation_model(256, 0, 1)\n\nhead = torch.nn.Sequential(\n    nn.Linear(256,128),\n)\n\nhead.to(torch.get_default_device())\n\nfrom torchinfo import summary\n    \nprint(summary(model, input_size=(batch_size, 3, 32, 32)))\nprint(summary(head, input_size=(batch_size, 256)))\n\nThis will download the data if needed:\nWe‚Äôre going to train this on Polaris nodes which have 4 A100s (But only using one node at a time). So, the following helper functions will automatically distribute the code and model to use all 4 GPUs at once:\n(They are all from the DDP Tutorial )\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\nimport ezpz\n\ndef create_data_loaders(transforms, batch_size, rank, seed):\n    # Start up the data loader:\n    # dev = torch.device(\n    #     f\"cuda:{rank}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    dev = ezpz.get_torch_device_type()\n\n    training_data = torchvision.datasets.CIFAR10(\n        root=\"data\",\n        train=True,\n        download=True,\n        transform=transforms\n    )\n\n    training_data, validation_data = torch.utils.data.random_split(\n        training_data,\n        [0.8, 0.2],\n        generator=torch.Generator().manual_seed(55))\n\n    # The dataloader makes our dataset iterable \n    train_dataloader = torch.utils.data.DataLoader(training_data,\n                                                   batch_size=batch_size,\n                                                   shuffle=True,\n                                                   num_workers=8)\n\n    val_dataloader = torch.utils.data.DataLoader(validation_data,\n                                                 batch_size=batch_size,\n                                                 shuffle=True,\n                                                 num_workers=8)\n\n\n    def preprocess(x, y):\n        # CIFAR-10 is *color* images so 3 layers!\n        return x.view(-1, 3, 32, 32).to(dev), y.to(dev)\n\n\n    class WrappedDataLoader:\n        def __init__(self, dl, func):\n            self.dl = dl\n            self.func = func\n\n        def __len__(self):\n            return len(self.dl)\n\n        def __iter__(self):\n            for b in self.dl:\n                yield (self.func(*b))\n\n\n    train_dataloader = WrappedDataLoader(train_dataloader, preprocess)\n    val_dataloader = WrappedDataLoader(val_dataloader, preprocess)\n\n    return train_dataloader, val_dataloader\n\n\n# def demo_basic(rank, world_size, n_epochs):\n#     print(f\"Running basic DDP example on rank {rank}.\")\n#     setup(rank, world_size)\n\n\n#     # create model and move it to GPU with id rank\n#     model = ToyModel().to(rank)\n#     ddp_model = DDP(model, device_ids=[rank])\n\n#     loss_fn = nn.MSELoss()\n#     optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n#     optimizer.zero_grad()\n#     outputs = ddp_model(torch.randn(20, 10))\n#     labels = torch.randn(20, 5).to(rank)\n#     loss_fn(outputs, labels).backward()\n#     optimizer.step()\n\n#     cleanup()\n\n\n# def run_demo(demo_fn, world_size):\n#     mp.spawn(demo_fn,\n#              args=(world_size,5),\n#              nprocs=world_size,\n#              join=True)\n\n\n# import sys, os\n# from multiprocessing import Pool\n# from multiprocessing.reduction import ForkingPickler\n# from types import FunctionType\n# import cloudpickle\n\n# assert sys.version_info &gt;= (3, 8), 'python3.8 or greater required to use reducer_override'\n\n# def reducer_override(obj):\n#     if type(obj) is FunctionType:\n#         return (cloudpickle.loads, (cloudpickle.dumps(obj),))\n#     else:\n#         return NotImplemented\n\n# # Monkeypatch our function reducer into the pickler for multiprocessing.\n# # Without this line, the main block will not work on windows or macOS.\n# # Alterntively, moving the defintionn of foo outside of the if statement\n# # would make the main block work on windows or macOS (when run from\n# # the command line).\n# ForkingPickler.reducer_override = staticmethod(reducer_override)\n\n\n# This method is from the pytorch implementation of SimCLR:\n# https://github.com/sthalles/SimCLR/blob/master/data_aug/contrastive_learning_dataset.py\n\ndef get_simclr_pipeline_transform(size, s=1):\n    \"\"\"Return a set of data augmentation transformations as described in the SimCLR paper.\"\"\"\n    color_jitter = v2.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n    data_transforms = v2.Compose([v2.RandomResizedCrop(size=size, scale=[0.85,1.0]),\n                                          v2.RandomHorizontalFlip(),\n                                          v2.RandomApply([color_jitter], p=0.8),\n                                          v2.RandomGrayscale(p=0.2),\n                                          v2.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n                                          # v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                                          # v2.ToTensor()\n                                        ])\n    return data_transforms\n\n\ntransforms1 = get_simclr_pipeline_transform((32,32))\ntransforms2 = get_simclr_pipeline_transform((32,32))\n\n\ntrain, val = create_data_loaders(v2.ToTensor(), batch_size, 0, seed = 1234)\n\n\nbatch, (X, Y) = next(enumerate(train))\n\n\nX1 = transforms1(X); X2 = transforms2(X)\n\nprint(type(X1))\nprint(type(X2))\n\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n\nplt.imshow(X[0].cpu().permute((1,2,0))); plt.show()\nplt.imshow(X1[0].cpu().permute((1,2,0))); plt.show()\nplt.imshow(X2[0].cpu().permute((1,2,0))); plt.show()\n\n\ndef contrastive_loss(first_images, second_images, rank, world_size = 1, temperature=0.1):\n        # Each image is represented with k parameters,\n        # Assume the batch size is N, so the\n        # inputs have shape (N, k)\n\n        # These are pre-distributed shapes:\n        N = first_images.shape[0]\n        k = first_images.shape[1]\n\n\n        first_images = first_images / torch.norm(first_images,dim=1).reshape((-1,1))\n        second_images = second_images / torch.norm(second_images,dim=1).reshape((-1,1))\n\n        # Take the two tuples, and concatenate them.\n        # Then, reshape into Y = (1, 2N, k) and Z = (2N, 1, k)\n\n        c = torch.concat([first_images, second_images], dim=0)\n\n        # Gather all the c up if the world size &gt; 1:\n        if world_size &gt; 1:\n            gathered_c = torch.distributed.all_gather(tensor=c)\n            gathered_c = gathered_c.reshape((-1, first_images.shape[-1]))\n        else:\n            gathered_c = c\n\n        # Each rank computes only a slice of the global loss matrix, or\n        # the memory usage gets out of control.\n\n        # We calculate the dot product between the local and global tensors:\n        local_reps = c.reshape((c.shape[0], 1, c.shape[1]))\n        all_reps   = gathered_c.reshape((1, gathered_c.shape[0], gathered_c.shape[1]))\n\n\n        # Assume we have n images per rank, for N global images with N = n * world_size\n        # Compute the product of these tensors, which gives shape\n        # (2n, 2N, k)\n        mat =  local_reps*all_reps\n\n        # We need to compute the function (sim(x,y)) for each element in the 2N sequent.\n        # Since the are normalized, we're computing x^T . Y / (||x||*||y||),\n        # but the norms are equal to 1.\n        # So, summing the matrix over the dim = 0 and dim = 1 computes this for each pair.\n\n        sim = torch.sum(mat, dim=-1) / temperature\n\n\n\n        # Now, sim is of shape [2*n, 2*N]\n\n        # This yields a symmetric matrix, diagonal entries equal 1.  Off diagonal are symmetrics and &lt; 1.\n\n        # sim = torch.exp(sim / temperature)\n        # Now, for every entry i in C (concat of both batches), the sum of sim[i] - sim[i][i] is the denominator\n\n        device = sim.device\n\n        # Since we have a non-symmetric matrix, need to build a non-symmetric index:\n        positive = torch.zeros(sim.shape, device=device)\n\n        # We concatenated all the local examples, and compute symmetric positive pairs\n        # So for the first N entries, the index of the positive pair is i + N  (locally)\n        # For the second N entries, the index of the positive pair is i - N (locally)\n        # with a distributed run, we've squashed all the similarity scores together.\n        # to a shape of [2*N, 2*N*Size]\n        # Each 2*N by 2*N block is the local positive indexes, all others are negative.\n        # That means that the index is shifted by global_rank*2*N\n\n        access_index_x = torch.arange(2*N)\n        # For the first N, the y-index is equal to x + 2*N\n        # For the second N\n        access_index_y = torch.arange(2*N)\n        # Shift by +/- N:\n        access_index_y[0:N] = access_index_y[0:N] + N\n        access_index_y[N:]  = access_index_y[N:] - N\n\n        access_index_y +=  rank * 2*N\n\n        # print(\"access_index_y: \", access_index_y, flush=True)\n\n        positive[access_index_x, access_index_y] = 1\n\n        # For the negative, we invert the positive and have to 0 out the self-index entries\n        negative = 1 - positive\n\n        # THESE WORK IF IT'S NOT DISTRIBUTED\n        # positive = torch.tile(torch.eye(N, device=device), (2,2))\n        # # Unsure if this line is needed?\n        # positive = positive - torch.eye(2*N, device=device)\n        #\n        # negative = - (torch.eye(2*N, device=device) - 1)\n\n        with torch.no_grad():\n            # Here, we can compute the top-k metrics for this batch, since we have the global state:\n            # We want the top 5 entries but the self-sim is obviously perfect.\n            # So take the top 6 and reject the first.\n            topk = torch.topk(sim, k=6, dim=-1, sorted=True)\n\n            # Top 1 is just an equality check:\n            top1_acc = topk.indices[:,1] == access_index_y.to(topk.indices.device)\n            top1_acc = torch.mean(top1_acc.to(torch.float))\n          \n            # Top 5 is a little more complicated:\n            # Compute the index distance to the correct index, abs value:\n            top5_acc_dist = torch.abs(topk.indices[:,1:] - access_index_y.to(topk.indices.device).reshape(-1,1))\n            # Get the minumum value, and see if it is less than 5:\n            min_values, _ = torch.min(top5_acc_dist, dim=-1)\n            top5_acc =  min_values &lt; 5.\n            # Average over the batch dimension:\n            top5_acc = torch.mean(top5_acc.to(torch.float))\n\n\n        negative_examples = sim * negative\n        positive_examples = sim * positive\n\n        # Now, positive/negative examples is the temperature normalized similarity.\n        # we need to sum across the whole batch dimension to compute it per-example:\n\n\n        # Compute the alignment, summed over the entire global batch:\n        alignment = torch.sum(positive_examples, dim=-1)\n\n        # Compute the exp, which we'll eventually sum and log:\n        exp = torch.sum(torch.exp(negative_examples), dim=-1)\n\n        # print(\"Alignment: \", alignment, flush=True)\n        # print(\"exp: \",       exp, flush=True)\n\n\n        # And compute the logsumexp of the negative examples:\n        log_sum_exp = torch.log(exp )\n\n\n        # Additionally, we can compute the \"floor\" of the loss at this batch size:\n        # floor = torch.log(1.*N) - 1.\n\n        loss_metrics = {\n            \"alignment\"   : torch.mean(alignment),\n            \"log_sum_exp\" : torch.mean(log_sum_exp),\n            \"top1\"        : top1_acc,\n            \"top5\"        : top5_acc,\n            # \"floor\"       : floor,\n        }\n\n        loss = torch.mean( - alignment + log_sum_exp)\n        return loss, loss_metrics\n\n\ndef train_one_epoch(dataloader, t1, t2, model, head, loss_fn, optimizer, rank, size, progress_bar):\n    model.train()\n    head.train()\n    for (batch, (X, _)) in enumerate(dataloader):\n        # forward pass\n        X1 = t1(X); X2 = t2(X)\n        pred1 = head(model(X1))\n        pred2 = head(model(X2))\n        loss, metrics = loss_fn(pred1, pred2, rank, size)\n\n        # print(metrics)\n        \n        # backward pass calculates gradients\n        loss.backward()\n        \n        # take one step with these gradients\n        optimizer.step()\n        \n        # resets the gradients \n        optimizer.zero_grad()\n\n        # progress_bar.refresh()\n        cpu_metrics = { key : f\"{metrics[key].detach().cpu().numpy():.2f}\" for key in metrics.keys()}\n        cpu_metrics[\"loss\"] = f\"{loss.detach().cpu().numpy():.2f}\"\n        progress_bar.update()\n        progress_bar.set_postfix(cpu_metrics)\n        # progress_bar.description = f\"Train loss: {loss.cpu():.2f} top5: {metrics['top5'].cpu():.2f}\"\n        # break\n\n\ndef validate_one_epoch(dataloader, t1, t2, model, head, loss_fn, rank, size, progress_bar):\n    model.train()\n    head.train()\n    n = 0.\n    sum_metrics = None\n    for (batch, (X, _)) in enumerate(dataloader):\n        # forward pass\n        X1 = t1(X); X2 = t2(X)\n        pred1 = head(model(X1))\n        pred2 = head(model(X2))\n        loss, metrics = loss_fn(pred1, pred2, rank, size)\n\n        # print(metrics)\n        \n        # backward pass calculates gradients\n        loss.backward()\n        \n        # take one step with these gradients\n        optimizer.step()\n        \n        # resets the gradients \n        optimizer.zero_grad()\n\n        # progress_bar.refresh()\n        cpu_metrics = { key : metrics[key].detach().cpu().numpy() for key in metrics.keys()}\n        if sum_metrics is None: \n            sum_metrics = cpu_metrics\n        else:\n            for key in sum_metrics.keys():\n                sum_metrics[key] += cpu_metrics[key]\n        progress_bar.update()\n        n += 1.\n        # progress_bar.description = f\"Train loss: {loss.cpu():.2f} top5: {sum_metrics['top5'].cpu():.2f}\"\n        # break\n    \n    for key in sum_metrics:\n        sum_metrics[key] = sum_metrics[key] / n\n    return sum_metrics\n\n\noptimizer = torch.optim.AdamW(list(model.parameters()) + list(head.parameters()), lr=0.001)\n\n\nfrom tqdm.notebook import tqdm\n\n\n# for j in range(1):\n#     # with tqdm(total=len(train), position=0, leave=True, desc=f\"Train Epoch {j}\") as train_bar1:\n#\n#     #     train_one_epoch(train, transforms1, transforms2, model, head, contrastive_loss, optimizer, 0, 1, train_bar1)\n#\n#     with tqdm(total=len(val), position=0, leave=True, desc=f\"Validate Epoch {j}\") as val_bar:\n#         metrics = validate_one_epoch(val, transforms1, transforms2, model, head, contrastive_loss, 0, 1, val_bar)\n#         print_metrics = {\n#             key : f\"{key}={metrics[key]:.2f}\" for key in metrics.keys()\n#         }\n#         print_metrics = \"; \".join(print_metrics.values())\n#         print(f\"Validate epoch {j}: \", print_metrics)\n\n\n# Now, we retrain the classification head without touching the representation. This is called fine tuning.\n\n\ndef evaluate(dataloader, model, head, loss_fn, val_bar):\n    # Set the model to evaluation mode - some NN pieces behave differently during training\n    # Unnecessary in this situation but added for best practices\n    model.eval()\n    size = len(dataloader)\n    num_batches = len(dataloader)\n    loss, correct = 0, 0\n\n    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n    with torch.no_grad():\n        # loop over all of the batches\n        for X, y in dataloader:\n\n            pred = head(model(X))\n            loss += loss_fn(pred, y).item()\n            # how many are correct in this batch? Tracking for accuracy \n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n            val_bar.update()\n            \n    loss /= num_batches\n    correct /= (size*batch_size)\n    \n    accuracy = 100*correct\n    return accuracy, loss\n\n\ndef fine_tune(dataloader, rep_model, head, loss_fn, optimizer, progress_bar):\n    head.train()\n    model.eval()\n    for batch1, (X, Y) in enumerate(dataloader):\n        # forward pass\n        # Calling detach blocks all gradients into the representation model!\n        rep = rep_model(X).detach()\n        pred = head(rep)\n        loss = loss_fn(pred, Y)\n        \n        \n        # backward pass calculates gradients\n        loss.backward()\n        \n        # take one step with these gradients\n        optimizer.step()\n        \n        # resets the gradients \n        optimizer.zero_grad()\n\n        correct = (pred.argmax(1) == Y).type(torch.float).mean().item()\n                    \n        # progress_bar.refresh()\n        cpu_metrics = {}\n        cpu_metrics[\"acc\"] = f\"{correct:.2f}\"\n        cpu_metrics[\"loss\"] = f\"{loss.detach().cpu().numpy():.2f}\"\n        progress_bar.update()\n        progress_bar.set_postfix(cpu_metrics)\n        # progress_bar.description = f\"Train loss: {loss.cpu():.2f} top5: {metrics['top5'].cpu():.2f}\"\n        # break\n\n\nclassification_head = nn.Linear(256, 10)  # .cuda()\nclassification_loss = loss_fn = nn.CrossEntropyLoss()\nfine_tune_optimizer = torch.optim.AdamW(classification_head.parameters(), lr=0.01)\nprint(fine_tune_optimizer)\n\n\n# for j in range(5):\n#     with tqdm(total=len(train), position=0, leave=True, desc=f\"Fine Tune Epoch {j}\") as train_bar1:\n#\n#         fine_tune(train, model, classification_head, classification_loss, fine_tune_optimizer, train_bar1)\n#     with tqdm(total=len(val), position=0, leave=True, desc=f\"Validate Epoch {j}\") as val_bar:\n#         acc, loss = evaluate(val, model, classification_head, classification_loss, val_bar)\n#         print(f\"Epoch {j}: validation loss: {loss:.3f}, accuracy: {acc:.3f}\")\n\n\n\n\n\n\nCitationBibTeX citation:@online{foreman2025,\n  author = {Foreman, Sam},\n  title = {Foundation {Models}},\n  date = {2025-07-22},\n  url = {https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/4-representation-learning/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2025. ‚ÄúFoundation Models.‚Äù July 22, 2025. https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/4-representation-learning/.",
    "crumbs": [
      "[01] Neural Networks",
      "[4] Representation Learning"
    ]
  },
  {
    "objectID": "01-neural-networks/2-advanced/index.html",
    "href": "01-neural-networks/2-advanced/index.html",
    "title": "Advanced Topics in Neural Networks",
    "section": "",
    "text": "CitationBibTeX citation:@online{foreman2025,\n  author = {Foreman, Sam},\n  title = {Advanced {Topics} in {Neural} {Networks}},\n  date = {2025-07-17},\n  url = {https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/2-advanced/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2025. ‚ÄúAdvanced Topics in Neural Networks.‚Äù\nJuly 17, 2025. https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/2-advanced/.",
    "crumbs": [
      "[01] Neural Networks",
      "[2] Advanced"
    ]
  },
  {
    "objectID": "01-neural-networks/0-intro/index.html",
    "href": "01-neural-networks/0-intro/index.html",
    "title": "Introduction to Neural Networks",
    "section": "",
    "text": "This tutorial covers the basics of neural networks (aka ‚Äúdeep learning‚Äù), which is a technique within machine learning that tends to outperform other techniques when dealing with a large amount of data.\nThe MNIST dataset contains thousands of examples of handwritten numbers, with each digit labeled 0-9.\nWe‚Äôll start with the MNIST problem in this notebook:\nüìì Fitting MNIST with a multi-layer perceptron (MLP)\nNext week, we‚Äôll learn about other types of neural networks.",
    "crumbs": [
      "[01] Neural Networks",
      "[0] Intro to NNs"
    ]
  },
  {
    "objectID": "01-neural-networks/0-intro/index.html#references",
    "href": "01-neural-networks/0-intro/index.html#references",
    "title": "Introduction to Neural Networks",
    "section": "References:",
    "text": "References:\n\nHere are some recommendations for further reading:\n\ntensorflow.org tutorials\nkeras.io tutorials\nCS231n: Convolutional Neural Networks for Visual Recognition\nDeep Learning Specialization, Andrew Ng\nPyTorch Challenge, Udacity\nDeep Learning with Python\nKeras Blog\nHands-on ML book with notebooks.",
    "crumbs": [
      "[01] Neural Networks",
      "[0] Intro to NNs"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html",
    "title": "Artificial intelligence in a nutshell",
    "section": "",
    "text": "In this notebook, we will talk about:",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#how-to-run-this-notebook-on-polaris",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#how-to-run-this-notebook-on-polaris",
    "title": "Artificial intelligence in a nutshell",
    "section": "How to run this notebook on Polaris",
    "text": "How to run this notebook on Polaris\n\nGo to https://jupyter.alcf.anl.gov, and click ‚ÄúLogin Polaris‚Äù\nAfter login, select ALCFAITP project and ALCFAITP queue during the lecture (use debug queue outside of the lecture)\nLoad the notebook and select ‚Äúdatascience/conda-2023-01-10‚Äù python kernel\n\n\n\n\n\n\n\nFigure¬†1\n\n\n\nHow to run this notebook on Google Colab\n\nGo to https://colab.research.google.com/, sign in or sign up\n‚ÄúFile‚Äù-&gt; ‚Äúopen notebook‚Äù\nchoose 01_intro_AI_on_Supercomputer/01_linear_regression_sgd.ipynb from the list\n\n\n\n\nGoogle Colab",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#what-is-ai-training",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#what-is-ai-training",
    "title": "Artificial intelligence in a nutshell",
    "section": "What is AI training?",
    "text": "What is AI training?\nTwo ways of learning English:\n\nthrough learning rules;\nthrough hearing a lot of speakings\n\n\n\n\n\n\n\nFigure¬†2: Data Driven Learning\n\n\n\nI learned English in my middle school, and memorized a lot of grammar rules in my mind. Every time when I speak, I try to follow the grammar rules as much as I can. But I always break the rules.\nHowever, my daugher learned English differently. She learns speaking by hearing a lot of speaking from TV, teachers, classmates, and her older brother. The fact is that, she seldomly breaks grammar rules. This way of learning by observing patterns is very powerful! This is the essence of AI or data driven science.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#how-does-large-language-model-work",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#how-does-large-language-model-work",
    "title": "Artificial intelligence in a nutshell",
    "section": "How does large language model work?",
    "text": "How does large language model work?\nLarge Language Models, like GPT, function by pre-training on extensive datasets to learn language patterns, utilizing transformer architecture for contextual understanding, and can be fine-tuned for specific tasks, enabling them to generate coherent and contextually relevant text based on provided inputs.\n\n\n\n\n\n\nFigure¬†3\n\n\n\nMore complicated example:\n\n \n\nYou can do this on https://chat.openai.com",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#simplest-ai-model-example-linear-regression",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#simplest-ai-model-example-linear-regression",
    "title": "Artificial intelligence in a nutshell",
    "section": "Simplest AI model example: linear regression",
    "text": "Simplest AI model example: linear regression\nThis example is adopted from Bethany Lusch, ALCF.\nLinear regression is the simplest example learning from existing data for future prediction.\n\n\n\n\n\n\nFigure¬†4: Linear regression in Excel\n\n\n\nWe‚Äôre going to review the math involved in this process to help understand how training an AI works.\nFirst we will load some tools that others wrote and we can use to help us work.\n\nPandas: a toolkit for working with row vs. column data, like excel sheets, and CSV (Comma Seperated Values) files.\nNumpy: a toolkit for managing arrays, vectors, matrices, etc, doing math with them, slicing them up, and many other handy things.\nMatplotlib: a toolkit for plotting data\n\n\nimport ambivalent\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use(ambivalent.STYLES['ambivalent'])\nsns.set_context(\"notebook\")\nplt.rcParams[\"figure.figsize\"] = [6.4, 4.8]\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport IPython.display as ipydis\nimport time\n\n\nDataset\nWe used a realestate dataset from Kaggle to produce this reduced dataset. This dataset contains the sale price and above ground square feet of many houses. We can use this data for our linear regression.\nWe use Pandas to read the data file which is stored as Comma Separated Values (CSV). and print the column labels. CSV files are similar to excel sheets.\n\n! [ -e ./slimmed_realestate_data.csv ] || wget https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/01_intro_AI_on_Supercomputer/slimmed_realestate_data.csv\ndata = pd.read_csv('slimmed_realestate_data.csv')\nprint(data.columns)\n\nIndex(['Unnamed: 0', 'SalePrice', 'GrLivArea'], dtype='object')\n\n\nNow pandas provides some helpful tools for us to inspect our data. It provides a plot() function that, behind the scenes, is calling into the Matplotlib library and calling the function matplotlib.pyplot.plot(). In this case, we simply tell it the names of the columns we want as our x and y values and the style ('.' tells matplotlib to use a small dot to represent each data point).\n\ndata.plot(x='GrLivArea', y='SalePrice',style='.')\n\n\n\n\n\n\n\n\n\n\nTheory of linear regression\nThe goal of learning regression is to find a line that is closest to all the points.\nThe slope and intercept of such a line y = m x + b can be found as:\nm = { n (\\Sigma xy) - (\\Sigma x) (\\Sigma y) \\over n (\\Sigma x^2) - (\\Sigma x)^2 } \nb = { (\\Sigma y) (\\Sigma x^2) - (\\Sigma x) (\\Sigma xy) \\over n (\\Sigma x^2) - (\\Sigma x)^2 } \nDetails derivation of this can be found here.\nWe‚Äôll break this calculation into a few steps to help make it easier.\nFirst lets define x and y. x will be our above ground square footage and y will be sale price. In our equations we have a few different values we need, such as n which is just the number of points we have:\n\nn = len(data)\n\nThen we need our x and y by selecting only the column we care about for each one. Note about data formats: data is a Pandas DataFrame object which has rows and columns; data['GrLivArea'] is a Pandas Series object which only has rows; then we also convert from Pandas data formats (in this case a Series) to Numpy data formats using the to_numpy() function which is part of the Pandas Series object.\n\nx = data['GrLivArea'].to_numpy()\ny = data['SalePrice'].to_numpy()\n\nNow we will calculate \\Sigma xy, \\Sigma x, \\Sigma y, and \\Sigma x^2:\n\nsum_xy = np.sum(x*y)\nsum_x = np.sum(x)\nsum_y = np.sum(y)\nsum_x2 = np.sum(x*x)\n\nThe denominator in the equation for m and b are the same so we can calculate that once:\n\ndenominator = n * sum_x2 - sum_x * sum_x\n\nThen we can calculate our fit values:\n\nm = (n * sum_xy - sum_x * sum_y) / denominator\nb = (sum_y * sum_x2 - sum_x * sum_xy) / denominator\nprint('y = %f * x + %f' % (m,b))\n\n# saving these for later comparison\nm_calc = m\nb_calc = b\n\ny = 87.688145 * x + 34754.077892\n\n\nNow we can plot the fit results with our data to see how we did.\nFirst we define a plotting function because we‚Äôre going to do this often and we want to reuse our code:\n\ndef plot_data(x,y,m,b,plt = plt):\n   # plot our data points with 'bo' = blue circles\n   plt.plot(x,y,'bo')\n   # create the line based on our linear fit\n   # first we need to make x points\n   # the 'arange' function generates points between two limits (min,max)\n   linear_x = np.arange(x.min(),x.max())\n   # now we use our fit parameters to calculate the y points based on our x points\n   linear_y = linear_x * m + b\n   # plot the linear points using 'r-' = red line\n   plt.plot(linear_x,linear_y,'r-',label='fit')\n\nNow can use this function to plot our results:\n\nplot_data(x,y,m,b)\n\n\n\n\n\n\n\n\n\n\nTraining through Stochastic Gradient Descent (SGD)\nSGD is a common method in AI for training deep neural networks on large datasets. It is an iterative method for optimizing a loss function that we get to define. We will use this simple linear regression to demonstrate how it works.\n\nThe model\nIn AI, neural networks are often referred to as a model because, once fully trained, they should model (AKA predict) the behavior of our system. In our example, the system is how house prices vary based on house size. We know our system is roughly driven by a linear function:\n \\hat{y_i}(x_i) = m * x_i + b \nWe just need to figure out m and b. Let‚Äôs create a function that calculates our model given x, m, and b.\n\ndef model(x,m,b):\n   return m * x + b\n\n\n\nThe Loss Function\nA loss function, or objective function, is something we define and is based on what we want to achieve. In the method of SGD, it is our goal to minimize (or make close to zero) the values calculated from the loss function. In our example, we ideally want the prediction of our model to be equal to the actual data, though we will settle for ‚Äúas close as possible‚Äù.\nSo we will select our loss function to be the Mean Squared Error function:\n L(y_i,\\hat{y_i}) = (y_i - \\hat{y_i}(x_i))^2 \nwhere y_i is our i^{th} entry in the data['SalePrice'] vector and \\hat{y_i} is the prediction based on evaluting m * x_i + b.\nThis function looks like the figure below when we plot it with x=y_i - \\hat{y_i}(x_i) and we we want to be down near y_i - \\hat{y_i}(x_i) = 0 which indicates that our y_i is as close as possible to \\hat{y_i}.\n\n\n\n\n\n\nFigure¬†5: Loss function for linear regression\n\n\n\nHere we crate a function that calculates this for us.\n\ndef loss(x,y,m,b):\n   y_predicted = model(x,m,b)\n   return np.power( y - y_predicted, 2 )\n\n\n\nMinimizing the Loss Function\nWe want to use the loss function in order to guide how to update m and b to better model our system. In calculus we learn to minimize a function with respect to a variable you calculate the partial derivative with respect to the variable you want to vary.\n { \\partial L \\over \\partial m } = 0 \nThe location of the solution to this is the minimum as shown in the figure above. We can write down the partial derivative of the loss function as:\n { \\partial L \\over \\partial m } = -2 x_i (y_i - \\hat{y_i}(x_i))   { \\partial L \\over \\partial b } = -2 (y_i - \\hat{y_i}(x_i)) \nWe can use this to calculate an adjustment to m and b that will reduce the loss function, effectively improving our fitting parameters. This is done using this equation:\n m' = m - \\eta { \\partial L \\over \\partial m }  b' = b - \\eta { \\partial L \\over \\partial b }\nHere our original m and b are adjusted by the partial derivative multiplied by some small factor, \\eta, called the learning rate. This learning rate is very important in our process and must be tuned for every problem.\nIn our example, the selection of the learning rate essentially defines how close we can get to the minimum, AKA the best fit solution. This figure shows what happens when we pick a large learning rate. We first select a starting point in our loss function (typically randomly), then every update from m/b to m'/b' results in a shift to somewhere else on our loss function (following the red arrows). In this example, our learning rate (\\eta) has been selected too large such that we bounce back and forth around the minimum, never reaching it.\n\n\n\nIf we select a smaller learning we can see better behavior in the next figure.\n\n\n\nThough, keep in mind, too small a learning rate results is so little progress toward the minimum that you may never reach it!\nA pit fall of SGD that one must be aware of is when your loss function is complex, with many minima. The next figure shows such a case, in which we select a small learning rate and our starting point happens to be near a local minimum that is not the lowest minimum. As shown, we do reach a minimum, but it isn‚Äôt the lowest minimum in our loss function. It could be that we randomly select a starting point near the minimum we care about, but we should build methods that are more robust against randomly getting the right answer.\n\n\n\nThen, if we increase our learning rate too much, we bounce around again.\n\n\n\nWhat we want to do in this situation is start with a large learning rate and slowly reduce its size as we progress. That is shown in this next figure.\n\n\n\nAs you can see, this process is not perfect and could still land in a local minimum, but it is important to be aware of these behaviors as you utilize SGD in machine learning.\nSo let‚Äôs continue, we‚Äôll build functions we can use to update our fit parameters, m and b.\n\ndef updated_m(x,y,m,b,learning_rate):\n   dL_dm = - 2 * x * (y - model(x,m,b))\n   dL_dm = np.mean(dL_dm)\n   return m - learning_rate * dL_dm\n\ndef updated_b(x,y,m,b,learning_rate):\n   dL_db = - 2 * (y - model(x,m,b))\n   dL_db = np.mean(dL_db)\n   return b - learning_rate * dL_db",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#mini-batch-training",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#mini-batch-training",
    "title": "Artificial intelligence in a nutshell",
    "section": "Mini Batch Training",
    "text": "Mini Batch Training\nIn AI, datasets are often very large and cannot be processed all at once as is done in the loop above. The data is instead randomly sampled in smaller batches where each batch contains batch_size inputs.\nHow can you change the loop above to sample the dataset in smaller batches?\nHint: Our data variable is a Pandas DataFrame object, search for ‚Äúhow to sample a DataFrame‚Äù.\nInstead of using the entire dataset like:\ndata_x = data['GrLivArea'].to_numpy()\ndata_y = data['SalePrice'].to_numpy()\nUse\ndata_batch = data.sample(batch_size)\ndata_x = data_batch['GrLivArea'].to_numpy()\ndata_y = data_batch['SalePrice'].to_numpy()\nYou also have to adjust the loop_N accordingly to make sure that it loop over the entire datasets the same number of times.\nloop_N = 30*len(data)//batch_size\nPlease plot your learning curve for different batch size, such as 32, 64, 128, 256, 512.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#learning-rate-issue-bonus",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#learning-rate-issue-bonus",
    "title": "Artificial intelligence in a nutshell",
    "section": "Learning rate issue (Bonus)",
    "text": "Learning rate issue (Bonus)\nAs described above, if the learning rate is too large, it will affect the convergence. Do your training with (batch_size = 64, learning_rate_m = 1e-7, learning_rate_b = 1e-1). Then linearly increase the batch size and learning rate until you see the training does not converge.\n(64, 1e-7, 1e-1)*1\n(64, 1e-7, 1e-1)*2\n(64, 1e-7, 1e-1)*4\n(64, 1e-7, 1e-1)*8\n...\nHow to submit your homework\n\nFork the github repo to your personal github\nMake change to the 01_linear_regression_sgd.ipynb, and then push to your personal github\nProvide the link of 01_linear_regression_sgd in the personal github.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#minibatch-training",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#minibatch-training",
    "title": "Artificial intelligence in a nutshell",
    "section": "Minibatch training",
    "text": "Minibatch training\n\nprint('previously calculated: y_i = %.2f * x + %.2f    loss: %f\\n=======================================' % (m_calc,b_calc,loss_value))\n\n\nfor bs in 64, 128, 256, 512:\n    m, b, l = train(bs, epochs=30)\n    print(f\"batch size: {bs}, m={m:.4f}, b={b:.4f}, loss={l:.4f}\")\n\npreviously calculated: y_i = 87.69 * x + 34754.08    loss: 1478200827.641291\n=======================================\nbatch size: 64, m=86.8271, b=29775.7009, loss=1516309699.0500\nbatch size: 128, m=90.4793, b=31079.8373, loss=1479986793.7151\nbatch size: 256, m=86.3628, b=35960.8373, loss=1478698868.6914\nbatch size: 512, m=88.7783, b=33405.8255, loss=1478142876.1533\n\n\nWe see that eventually, we all get similar results with the minibatch training. Of course, here, we still keep the same learning rate. A gene",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/6-linear-regression/index.html#learning-rate",
    "href": "00-intro-AI-HPC/6-linear-regression/index.html#learning-rate",
    "title": "Artificial intelligence in a nutshell",
    "section": "Learning rate",
    "text": "Learning rate\n\nfor i in 1, 2, 4, 8:\n    bs, lrm, lrb = np.array([64, 1e-7, 1e-1])*i\n    bs = int(bs)\n    m, b, l = train(int(bs), epochs=30, learning_rate_m = lrm, learning_rate_b = lrb)\n    print(f\"batch size: {bs}, m={m:.4f}, b={b:.4f}, loss={l:.4f}\")\n\nbatch size: 64, m=89.6547, b=34165.1186, loss=1483595671.6631\nbatch size: 128, m=88.0039, b=34035.0533, loss=1477884365.8506\nbatch size: 256, m=83.3510, b=37836.6704, loss=1492092921.4820\nbatch size: 512, m=144954.2815, b=-98364039.5467, loss=16908967819933548.0000\n\n\nWe can see that, if we increase the batch size and the learning rate proportionally, at certain point, it does not converge for example for the case batch size = 512. To increase the learning rate proportional to the batch size is a general practice. However, if the learning rate is too large, it will continue to move around without finding a local minimum. One trick, people can do is to start with a smaller learning rate in the first few steps / epochs, and once the optimization becomes stable, increase the learning rate proportional to the batch size.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[6] Linear Regression"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/4-nersc/index.html",
    "href": "00-intro-AI-HPC/4-nersc/index.html",
    "title": "Overview of the Argonne Leadership Computing Facility (ALCF)",
    "section": "",
    "text": "CitationBibTeX citation:@online{foreman2025,\n  author = {Foreman, Sam},\n  title = {Overview of the {Argonne} {Leadership} {Computing} {Facility}\n    {(ALCF)}},\n  date = {2025-07-15},\n  url = {https://saforem2.github.io/hpc-bootcamp-2025/00-intro-AI-HPC/4-nersc/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2025. ‚ÄúOverview of the Argonne Leadership Computing\nFacility (ALCF).‚Äù July 15, 2025. https://saforem2.github.io/hpc-bootcamp-2025/00-intro-AI-HPC/4-nersc/.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[4] NERSC"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/2-jupyter-notebooks/index.html#logging-in",
    "href": "00-intro-AI-HPC/2-jupyter-notebooks/index.html#logging-in",
    "title": "Jupyter Notebooks on ThetaGPU",
    "section": "Logging in",
    "text": "Logging in\nLogin via: https://jupyter.alcf.anl.gov/\nSelect ‚ÄúLogin Polaris‚Äù and use your ALCF credentials and Multi-Factor Authentication.\nNote: some of the images below show ‚ÄúThetaGPU‚Äù being used, however, similar instructions apply to ‚ÄúPolaris‚Äù or ‚ÄúSophia‚Äù.\n\n\n\nLogin GIF",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[2] Jupyter Notebooks"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/2-jupyter-notebooks/index.html#server-settings-start-up",
    "href": "00-intro-AI-HPC/2-jupyter-notebooks/index.html#server-settings-start-up",
    "title": "Jupyter Notebooks on ThetaGPU",
    "section": "Server settings & start up",
    "text": "Server settings & start up\n\n\n\nserver options\n\n\nYou‚Äôll want to set your server options to the following: * job profile: ThetaGPU Compute Node * Sophia Queue: by-gpu * Project List: ALCFAITP * Runtime: can be 5 to 60 minutes\nWhen you click ‚Äústart‚Äù you are submitting a job to the batch queue and waiting for the job to begin.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[2] Jupyter Notebooks"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/2-jupyter-notebooks/index.html#server-shutdown-logout",
    "href": "00-intro-AI-HPC/2-jupyter-notebooks/index.html#server-shutdown-logout",
    "title": "Jupyter Notebooks on ThetaGPU",
    "section": "Server shutdown & logout",
    "text": "Server shutdown & logout\nALCF‚Äôs guiding policy is:\n\n‚ÄúWe ask that all users follow good etiquette and be excellent to one another.‚Äù\n\nIf you simply close your browser window, or logout without shutting down the jupyter server, your job will continue to occupy the worker node. Be considerate and shutdown your job when you finish.\n\n\n\nShutdown GIF",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[2] Jupyter Notebooks"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/0-compute-systems/index.html",
    "href": "00-intro-AI-HPC/0-compute-systems/index.html",
    "title": "What is a Supercomputer?",
    "section": "",
    "text": "Argonne hosts DOE supercomputers for use by research scientists in need of large computational resources. Supercomputers are composed of many computing nodes (1 node = 1 physical computer) that are connected by a high-speed communications network so that groups of nodes can share information quickly, effectively operating together as a larger computer.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[0] Compute Systems"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/0-compute-systems/index.html#a-compute-node",
    "href": "00-intro-AI-HPC/0-compute-systems/index.html#a-compute-node",
    "title": "What is a Supercomputer?",
    "section": "A Compute Node",
    "text": "A Compute Node\nIf you look inside your Desktop or Laptop you‚Äôll find these parts:\n\n\n\nparts\n\n\nA computing node of a supercomputer is very similar, each has simliar parts, but it is designed as a single unit that can be inserted and removed from large closet-sized racks with many others:\n\n\n\nblade\n\n\nIn large supercomputers multiple computer processors (CPUs) and/or graphics processors (GPUs) are combined into a single node. It has a CPU on which the local operating system runs. It has local memory for running software. It may have GPUs for doing intensive calculations. Each node has a high-speed network connection that allows it to communicate with other nodes and to a large shared filesystem.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[0] Compute Systems"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/0-compute-systems/index.html#clusterhpc-computing-hardware-setup",
    "href": "00-intro-AI-HPC/0-compute-systems/index.html#clusterhpc-computing-hardware-setup",
    "title": "What is a Supercomputer?",
    "section": "Cluster/HPC Computing Hardware Setup",
    "text": "Cluster/HPC Computing Hardware Setup\n\n\n\nHardware\n\n\nLarge computer systems typically have worker nodes and login nodes. login nodes are the nodes on which every user arrives when they login to the system. login nodes should not be used for computation, but for compiling code, writing/editing code, and launching jobs on the system. A job is the application that will be launched on the worker nodes of the supercomputer.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[0] Compute Systems"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/0-compute-systems/index.html#supercomputers-are-big",
    "href": "00-intro-AI-HPC/0-compute-systems/index.html#supercomputers-are-big",
    "title": "What is a Supercomputer?",
    "section": "Supercomputers are Big!",
    "text": "Supercomputers are Big!\nThese supercomputers occupy a lot of space in the ACLF data center. Here is our staff at the time (2019) in front of Mira, an IBM supercomputer, that debuted as the third fastest supercomputer in the world in 2012:\n\n\n\n\n\n\nFigure¬†1: ALCF Staff",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[0] Compute Systems"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/0-compute-systems/index.html#alcf-computing-system-overview",
    "href": "00-intro-AI-HPC/0-compute-systems/index.html#alcf-computing-system-overview",
    "title": "What is a Supercomputer?",
    "section": "ALCF Computing System Overview",
    "text": "ALCF Computing System Overview\n\nAurora\nAt this time (October 2024), Aurora is being put through its paces with Argonne and Intel working together to prepare the system for scientific researchers. It is amongst the most power supercomputers in thew world, currently sitting at #2 on the Top500.\nHere you can see one of the many rows of Aurora nodes with their Red & Blue water cooling conduits visible.\n\n\n\n\n\n\nFigure¬†2: Aurora\n\n\n\nIn this photo you see a close up of the 16 nodes installed side-by-side:\n\n\n\n\n\n\nFigure¬†3: Aurora\n\n\n\n\n\nSophia\nInside Sophia, you‚Äôll see repetition, though NVidia placed these fancy plates over the hardware so you only see their logo.\nHowever, each plate covers 1 computer node.\n\n\n\nSophia Racks\nSophia Inside\n\n\n\n\n\n\n\n\n\nSophia is an NVIDIA DGX A100-based system. The DGX A100 comprises eight NVIDIA A100 GPUs that provide a total of 320 gigabytes of memory for training AI datasets, as well as high-speed NVIDIA Mellanox ConnectX-6 network interfaces.\nSophia Machine Specs\n\nSpeed: 3.9 petaflops\nEach Node has:\n\n8 NVIDIA (A100) GPUs each with 40GB onboard memory\n2 AMD EPYC (7742) CPUs\n1 TB DDR4 Memory\n\n24 Total Nodes installed in 7 Racks\n\n\n\nPolaris\n\n\n\nPolaris\n\n\nThe inside of Polaris again shows the nodes stacked up in a closet.\n\n\n\nPolaris-rack\n\n\nPolaris is an NVIDIA A100-based system.\nPolaris Machine Specs * Speed: 44 petaflops * Each Node has: * 4 NVIDIA (A100) GPUs * 1 AMD EPYC (Milan) CPUs * ~560 Total Nodes",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[0] Compute Systems"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to HPC Bootcamp 2025",
    "section": "",
    "text": "00-intro-AI-hpc/\n\n0-compute-systems\n1-shared-resources\n2-jupyter-notebooks\n3-homework\n4-nersc\n5-mcmc-example\n6-linear-regression\n\n01-neural-networks/\n\n0-intro\n1-mnist\n2-advanced\n3-conv-nets\n4-representation-learning\n\n02-llms/\n\n0-intro-to-llms\n1-hands-on-llms\n2-prompt-engineering\n3-rag-tutorial\n4-evaluating-llms\n5-advanced-llms\n6-parallel-training\n\n03-ai-for-science/\n\n\n\n\n\nCitationBibTeX citation:@online{foreman2025,\n  author = {Foreman, Sam},\n  title = {Intro to {HPC} {Bootcamp} 2025},\n  date = {2025-07-15},\n  url = {https://saforem2.github.io/intro-hpc-bootcamp-2025},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2025. ‚ÄúIntro to HPC Bootcamp 2025.‚Äù July 15,\n2025. https://saforem2.github.io/intro-hpc-bootcamp-2025."
  },
  {
    "objectID": "00-intro-AI-HPC/1-shared-resources/index.html",
    "href": "00-intro-AI-HPC/1-shared-resources/index.html",
    "title": "Shared Resources",
    "section": "",
    "text": "Supercomputers contain many computer nodes and not every application will use ALL of them. Therefore, we use programs called schedulers that allow users to schedule a job based on how many nodes they need for a specified time.\nA job is defined by a user and requires these parameters to be defined:",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[1] Shared Resources"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/1-shared-resources/index.html#polaris-activity-webpage",
    "href": "00-intro-AI-HPC/1-shared-resources/index.html#polaris-activity-webpage",
    "title": "Shared Resources",
    "section": "Polaris Activity Webpage",
    "text": "Polaris Activity Webpage\n\nWe have a page that shows all the current activity on Polaris. The top of the page shows a graphical representation of all nodes. Colors indicate a job running on that node. Below the graphic there is a table that lists running and queued jobs.\n\nrunning refers to jobs running right now on comptuer nodes. If you hover on a colored node in the graphic, it will highlight all the nodes used by the same job AND the job in the running table below.\nqueued jobs are waiting for an opening in which to run.\n\nüìä Live Status of Jobs on Polaris\n\n\n\npolaris-activity",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[1] Shared Resources"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/3-homework/index.html",
    "href": "00-intro-AI-HPC/3-homework/index.html",
    "title": "GitHub Introduction: Homework Submissions",
    "section": "",
    "text": "You will submit weekly homeworks via this Homework Form The form expects you to submit a URL to a file on github.com. You will need an account on GitHub to do this.\nYou will also need to setup SSH Keys on github and in your ALCF account. * The SSH Key setup is described here * Setup git on Polaris (or target machine):\nNavigate to the main page of this repository and follow the steps below. If you need help with logging into ALCF, please have a look XXX",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[3] Homework"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/3-homework/index.html#fork-the-repo-on-github",
    "href": "00-intro-AI-HPC/3-homework/index.html#fork-the-repo-on-github",
    "title": "GitHub Introduction: Homework Submissions",
    "section": "Fork the repo on GitHub",
    "text": "Fork the repo on GitHub\nThis repository is controled by the ALCF account space, but you can make a copy in to your own github account space by forking the repository as show here. Simply go to the main repository page and click the fork button on the top right.\n\n\n\nfork repo",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[3] Homework"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/3-homework/index.html#clone-the-repo-in-your-home-directory",
    "href": "00-intro-AI-HPC/3-homework/index.html#clone-the-repo-in-your-home-directory",
    "title": "GitHub Introduction: Homework Submissions",
    "section": "Clone the repo in your Home directory",
    "text": "Clone the repo in your Home directory\nAfter you have forked the repo, login to Polaris, and clone the repository in to your home directory.\n\n\n\nclone repo\n\n\nThe command looks like git clone git@github.com:&lt;your-github-account&gt;/ai-science-training-series.git and it will clone the repo into a directory named ai-science-training-series.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[3] Homework"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/3-homework/index.html#do-your-homework",
    "href": "00-intro-AI-HPC/3-homework/index.html#do-your-homework",
    "title": "GitHub Introduction: Homework Submissions",
    "section": "Do your homework",
    "text": "Do your homework\nSome homework can be done via Jupyter notebooks, and others will be source code.\nWhen you sign in to our Jupyter server, you see the contents of your home directory. There you can open your repository and create a new file with an appropriate name, like YEAR-MONTH-DAY_Name_SessionNumber.\n\n\n\ncreate homework",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[3] Homework"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/3-homework/index.html#upload-your-homework-to-your-github-repo",
    "href": "00-intro-AI-HPC/3-homework/index.html#upload-your-homework-to-your-github-repo",
    "title": "GitHub Introduction: Homework Submissions",
    "section": "Upload your homework to your github repo",
    "text": "Upload your homework to your github repo\nNow you need to: - add your new homework file to the list of tracked files in your repository - commit your file to the repository - push your commit to the web server at github\n\n\n\ncommit homework",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[3] Homework"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/3-homework/index.html#sync-your-fork",
    "href": "00-intro-AI-HPC/3-homework/index.html#sync-your-fork",
    "title": "GitHub Introduction: Homework Submissions",
    "section": "Sync Your Fork",
    "text": "Sync Your Fork\nWe will periodically update the content in the ‚Äúupstream‚Äù repository at: https://github.com/argonne-lcf/ai-science-training-series\nYou‚Äôll want to execute these commmands each week, just prior to the start of class, to update your personal ‚Äúfork‚Äù with our latest updates.\nTo do this run these commands inside the directory ai-science-training-series where you cloned your personal fork:\nYou only need to do this once after you first clone your personal fork:\ngit remote add upstream git@github.com:argonne-lcf/ai-science-training-series.git\nThen, every week, prior to class run this:\ngit fetch upstream main\ngit merge upstream/main\ngit push\nThis will keep your fork up to date with the ACLF repository.",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[3] Homework"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/5-mcmc-example/index.html#example-estimate-pi",
    "href": "00-intro-AI-HPC/5-mcmc-example/index.html#example-estimate-pi",
    "title": "Computing \\pi in Parallel with Markov Chain Monte Carlo (MCMC) and MPI",
    "section": "Example: Estimate \\pi",
    "text": "Example: Estimate \\pi\nWe can calculate the value of \\pi using a MPI parallelized version of the Monte Carlo method. The basic idea is to estimate \\pi by randomly sampling points within a square and determining how many fall inside a quarter circle inscribed within that square.\n\n\n\n\\pi\n\n\nThe ratio between the area of the circle and the square is\n\\frac{N_\\text{in}}{N_\\text{total}} = \\frac{\\pi r^2}{4r^2} = \\frac{\\pi}{4}\nTherefore, we can calculate \\pi using \\pi = \\frac{4N_\\text{in}}{N_\\text{total}}\n\nimport ambivalent\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use(ambivalent.STYLES['ambivalent'])\nsns.set_context(\"notebook\")\nplt.rcParams[\"figure.figsize\"] = [6.4, 4.8]\n\nfrom IPython.display import display, clear_output\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport time\nfig, ax = plt.subplots()\n#ax = fig.add_subplot(111)\ncircle = plt.Circle(( 0. , 0. ), 0.5 )\nplt.xlim(-0.5, 0.5)\nplt.ylim(-0.5, 0.5)\nax.add_patch(circle)\nax.set_aspect('equal')\nN = 500\nNin = 0\nt0 = time.time()\nfor i in range(1, N+1):\n    x = random.uniform(-0.5, 0.5)\n    y = random.uniform(-0.5, 0.5)\n    if (np.sqrt(x*x + y*y) &lt; 0.5):\n        Nin += 1\n        plt.plot([x], [y], 'o', color='r', markersize=3)\n    else:\n        plt.plot([x], [y], 'o', color='b', markersize=3)\n    display(fig)\n    plt.xlabel(\"$\\pi$ = %3.4f \\n N_in / N_total = %5d/%5d\" %(Nin*4.0/i, Nin, i))\n    clear_output(wait=True)\n\nres = np.array(Nin, dtype='d')\nt1 = time.time()\nprint(f\"Pi = {res/float(N/4.0)}\")\nprint(\"Time: %s\" %(t1 - t0))\n\nPi = 3.2\nTime: 26.85680103302002\n\n\n\n\n\n\n\n\n\n\nMPI example\n\nfrom mpi4py import MPI\nimport numpy as np\nimport random\nimport time\ncomm = MPI.COMM_WORLD\n\nN = 5000000\nNin = 0\nt0 = time.time()\nfor i in range(comm.rank, N, comm.size):\n    x = random.uniform(-0.5, 0.5)\n    y = random.uniform(-0.5, 0.5)\n    if (np.sqrt(x*x + y*y) &lt; 0.5):\n        Nin += 1\nres = np.array(Nin, dtype='d')\nres_tot = np.array(Nin, dtype='d')\ncomm.Allreduce(res, res_tot, op=MPI.SUM)\nt1 = time.time()\nif comm.rank==0:\n    print(res_tot/float(N/4.0))\n    print(\"Time: %s\" %(t1 - t0))\n\n3.1418344\nTime: 3.0448648929595947\n\n\n\n\nRunning \\pi example on Google Colab\n\nGo to https://colab.research.google.com/, sign in or sign up\n‚ÄúFile‚Äù-&gt; ‚Äúopen notebook‚Äù\nchoose 01_intro_AI_on_Supercomputer/00_mpi.ipynb from the list \n\n\n! wget https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/01_intro_AI_on_Supercomputer/mpi_pi.py\n! pip install mpi4py\n\n--2025-07-23 08:51:33--  https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/01_intro_AI_on_Supercomputer/mpi_pi.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 555 [text/plain]\nSaving to: ‚Äòmpi_pi.py.38‚Äô\n\nmpi_pi.py.38          0%[                    ]       0  --.-KB/s               mpi_pi.py.38        100%[===================&gt;]     555  --.-KB/s    in 0s      \n\n2025-07-23 08:51:33 (48.1 MB/s) - ‚Äòmpi_pi.py.38‚Äô saved [555/555]\n\nRequirement already satisfied: mpi4py in /opt/homebrew/lib/python3.11/site-packages (4.1.0)\n\n\n\n! mpirun -np 1 --allow-run-as-root python mpi_pi.py\n\nNumber of processes: 1\nPi = 3.1426048\nTime: 2.837927\n\n\n\n! mpirun -np 2 --allow-run-as-root --oversubscribe python mpi_pi.py\n\nNumber of processes: 2\nPi = 3.14232\nTime: 1.446787\n\n\n\n! mpirun -np 4 --allow-run-as-root --oversubscribe python mpi_pi.py\n\nNumber of processes: 4\nPi = 3.140768\nTime: 0.812512\n\n\n\n\nRunning \\pi on Polaris\nssh &lt;username&gt;@polaris.alcf.anl.gov\nqsub -A ALCFAITP -l select=1 -q ALCFAITP -l walltime=0:30:00 -l filesystems=home:eagle\n# choose debug queue outside of the class\n# qsub -A ALCFAITP -l select=1 -q debug -l walltime=0:30:00 -l filesystems=home:eagle\n\nmodule load conda/2023-10-04\nconda activate /soft/datascience/ALCFAITP/2023-10-04\ngit clone git@github.com:argonne-lcf/ai-science-training-series.git\ncd ai-science-training-series/01_intro_AI_on_Supercomputer/\nmpirun -np 1 python mpi_pi.py   # 3.141988,   8.029037714004517  s\nmpirun -np 2 python mpi_pi.py   # 3.1415096   4.212774038314819  s\nmpirun -np 4 python mpi_pi.py   # 3.1425632   2.093632459640503  s\nmpirun -np 8 python mpi_pi.py   # 3.1411632   1.0610620975494385 s",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[5] MCMC Example"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/5-mcmc-example/index.html#parallel-computing-in-ai",
    "href": "00-intro-AI-HPC/5-mcmc-example/index.html#parallel-computing-in-ai",
    "title": "Computing \\pi in Parallel with Markov Chain Monte Carlo (MCMC) and MPI",
    "section": "Parallel computing in AI",
    "text": "Parallel computing in AI\nThe parallel computing in AI is usually called distributed training.\nDistributed training is the process of training I models across multiple GPUs or other accelerators, with the goal of speeding up the training process and enabling the training of larger models on larger datasets.\nThere are two ways of parallelization in distributed training.\n\nData parallelism:\n\nEach worker (GPU) has a complete set of model\ndifferent workers work on different subsets of data.\n\nModel parallelism\n\nThe model is splitted into different parts and stored on different workers\nDifferent workers work on computation involved in different parts of the model\n\n\n\n\n\n\n\n\nFigure¬†1: PI\n\n\n\n\n\n\n\n\n\nFigure¬†2: 3D LLM",
    "crumbs": [
      "[00] Intro to AI and HPC",
      "[5] MCMC Example"
    ]
  },
  {
    "objectID": "00-intro-AI-HPC/index.html#contents",
    "href": "00-intro-AI-HPC/index.html#contents",
    "title": "Introduction to AI on Supercomputers",
    "section": "Contents",
    "text": "Contents\n\n0-compute-systems\n1-shared-resources\n2-jupyter-notebooks\n3-homework\n4-nersc\n\nLink to original slides: https://drive.google.com/file/d/1PH6HlXPhsVB1wDcEkfRSZrDQTqWBi7aH/view?usp=sharing",
    "crumbs": [
      "[00] Intro to AI and HPC"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html",
    "href": "01-neural-networks/1-mnist/index.html",
    "title": "Intro to NNs: MNIST",
    "section": "",
    "text": "This tutorial will serve as a gentle introduction to neural networks and deep learning through a hands-on classification problem using the MNIST dataset.\nIn particular, we will introduce neural networks and how to train and improve their learning capabilities. We will use the PyTorch Python library.\nThe MNIST dataset contains thousands of examples of handwritten numbers, with each digit labeled 0-9.\nimport ambivalent\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use(ambivalent.STYLES['ambivalent'])\nsns.set_context(\"notebook\")\nplt.rcParams[\"figure.figsize\"] = [6.4, 4.8]\n# %matplotlib inline\n\nimport torch\nimport torchvision\nfrom torch import nn\n\nimport numpy \nimport matplotlib.pyplot as plt\nimport time",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#the-mnist-dataset",
    "href": "01-neural-networks/1-mnist/index.html#the-mnist-dataset",
    "title": "Intro to NNs: MNIST",
    "section": "The MNIST dataset",
    "text": "The MNIST dataset\nWe will now download the dataset that contains handwritten digits. MNIST is a popular dataset, so we can download it via the PyTorch library.\nNote:\n\nx is for the inputs (images of handwritten digits)\ny is for the labels or outputs (digits 0-9)\nWe are given ‚Äútraining‚Äù and ‚Äútest‚Äù datasets.\n\nTraining datasets are used to fit the model.\nTest datasets are saved until the end, when we are satisfied with our model, to estimate how well our model generalizes to new data.\n\n\nNote that downloading it the first time might take some time.\nThe data is split as follows:\n\n60,000 training examples, 10,000 test examples\ninputs: 1 x 28 x 28 pixels\noutputs (labels): one integer per example\n\n\ntraining_data = torchvision.datasets.MNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=torchvision.transforms.ToTensor()\n)\n\ntest_data = torchvision.datasets.MNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=torchvision.transforms.ToTensor()\n)\n\n\ntrain_size = int(0.8 * len(training_data))  # 80% for training\nval_size = len(training_data) - train_size  # Remaining 20% for validation\ntraining_data, validation_data = torch.utils.data.random_split(\n    training_data,\n    [train_size, val_size],\n    generator=torch.Generator().manual_seed(55)\n)\n\n\nprint(\n    'MNIST data loaded: train:',\n    len(training_data),\n    ' examples, validation: ',\n    len(validation_data),\n    'examples, test:',\n    len(test_data),\n    'examples'\n)\nprint('Input shape', training_data[0][0].shape)\n\nMNIST data loaded: train: 48000  examples, validation:  12000 examples, test: 10000 examples\nInput shape torch.Size([1, 28, 28])\n\n\nLet‚Äôs take a closer look. Here are the first 10 training digits:\n\npltsize=1\nplt.figure(figsize=(10*pltsize, pltsize))\n\nfor i in range(10):\n    plt.subplot(1,10,i+1)\n    plt.axis('off')\n    plt.imshow(numpy.reshape(training_data[i][0], (28, 28)), cmap=\"gray\")\n    plt.title('Class: '+str(training_data[i][1]))",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#generalities",
    "href": "01-neural-networks/1-mnist/index.html#generalities",
    "title": "Intro to NNs: MNIST",
    "section": "Generalities:",
    "text": "Generalities:\nTo train our classifier, we need (besides the data):\n\nA model that depend on parameters \\mathbf{\\theta}. Here we are going to use neural networks.\nA loss function J(\\mathbf{\\theta}) to measure the capabilities of the model.\nAn optimization method.",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#linear-model",
    "href": "01-neural-networks/1-mnist/index.html#linear-model",
    "title": "Intro to NNs: MNIST",
    "section": "Linear Model",
    "text": "Linear Model\nLet‚Äôs begin with a simple linear model: linear regression, like last week.\nWe add one complication: each example is a vector (flattened image), so the ‚Äúslope‚Äù multiplication becomes a dot product. If the target output is a vector as well, then the multiplication becomes matrix multiplication.\nNote, like before, we consider multiple examples at once, adding another dimension to the input.\n\n\n\n\n\n\nFigure¬†2: Linear model for classification\n\n\n\nThe linear layers in PyTorch perform a basic xW + b.\nThese ‚Äúfully connected‚Äù layers connect each input to each output with some weight parameter.\nWe wouldn‚Äôt expect a simple linear model f(x) = xW+b directly outputting the class label and minimizing mean squared error to work well - the model would output labels like 3.55 and 2.11 instead of skipping to integers.\nWe now need:\n\nA loss function J(\\theta) where \\theta is the list of parameters (here W and b). Last week, we used mean squared error (MSE), but this week let‚Äôs make two changes that make more sense for classification:\n\nChange the output to be a length-10 vector of class probabilities (0 to 1, adding to 1).\nCross entropy as the loss function, which is typical for classification. You can read more here.\n\nAn optimization method or optimizer such as the stochastic gradient descent (sgd) method, the Adam optimizer, RMSprop, Adagrad etc. Let‚Äôs start with stochastic gradient descent (sgd), like last week. For far more information about more advanced optimizers than basic SGD, with some cool animations, see https://ruder.io/optimizing-gradient-descent/ or https://distill.pub/2017/momentum/.\nA learning rate. As we learned last week, the learning rate controls how far we move during each step.\n\n\nclass LinearClassifier(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # First, we need to convert the input image to a vector by using \n        # nn.Flatten(). For MNIST, it means the second dimension 28*28 becomes 784.\n        self.flatten = nn.Flatten()\n        # Here, we add a fully connected (\"dense\") layer that has 28 x 28 = 784 input nodes \n        #(one for each pixel in the input image) and 10 output nodes (for probabilities of each class).\n        self.layer_1 = nn.Linear(28*28, 10)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.layer_1(x)\n        return x\n\n\nlinear_model = LinearClassifier()\nprint(linear_model)\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(linear_model.parameters(), lr=0.05)\n\nLinearClassifier(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (layer_1): Linear(in_features=784, out_features=10, bias=True)\n)",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#learning",
    "href": "01-neural-networks/1-mnist/index.html#learning",
    "title": "Intro to NNs: MNIST",
    "section": "Learning",
    "text": "Learning\nNow we are ready to train our first model.\nA training step is comprised of:\n\nA forward pass: the input is passed through the network\nBackpropagation: A backward pass to compute the gradient \\frac{\\partial J}{\\partial \\mathbf{W}} of the loss function with respect to the parameters of the network.\nWeight updates \\mathbf{W} = \\mathbf{W} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W}} where \\alpha is the learning rate.\n\nHow many steps do we take?\n\nThe batch size corresponds to the number of training examples in one pass (forward + backward).\n\nA smaller batch size allows the model to learn from individual examples but takes longer to train.\nA larger batch size requires fewer steps but may result in the model not capturing the nuances in the data.\n\nThe higher the batch size, the more memory you will require.\nAn epoch means one pass through the whole training data (looping over the batches). Using few epochs can lead to underfitting and using too many can lead to overfitting.\nThe choice of batch size and learning rate are important for performance, generalization and accuracy in deep learning.\n\n\nbatch_size = 128\n\n# The dataloader makes our dataset iterable \ntrain_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\nval_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n\n\ndef train_one_epoch(dataloader, model, loss_fn, optimizer):\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # forward pass\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        # backward pass calculates gradients\n        loss.backward()\n        # take one step with these gradients\n        optimizer.step()\n        # resets the gradients \n        optimizer.zero_grad()\n\n\ndef evaluate(dataloader, model, loss_fn):\n    # Set the model to evaluation mode - some NN pieces behave differently during training\n    # Unnecessary in this situation but added for best practices\n    model.eval()\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    loss, correct = 0, 0\n\n    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n    with torch.no_grad():\n        # loop over all of the batches\n        for X, y in dataloader:\n            pred = model(X)\n            loss += loss_fn(pred, y).item()\n            # how many are correct in this batch? Tracking for accuracy \n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    loss /= num_batches\n    correct /= size\n\n    accuracy = 100*correct\n    return accuracy, loss\n\n\n%%time\n\nepochs = 5\ntrain_acc_all = []\nval_acc_all = []\nfor j in range(epochs):\n    train_one_epoch(train_dataloader, linear_model, loss_fn, optimizer)\n\n    # checking on the training loss and accuracy once per epoch\n    acc, loss = evaluate(train_dataloader, linear_model, loss_fn)\n    train_acc_all.append(acc)\n    print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n\n    # checking on the validation loss and accuracy once per epoch\n    val_acc, val_loss = evaluate(val_dataloader, linear_model, loss_fn)\n    val_acc_all.append(val_acc)\n    print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n\nEpoch 0: training loss: 0.5020372807979584, accuracy: 87.50833333333333\nEpoch 0: val. loss: 0.4944046434569866, val. accuracy: 87.575\nEpoch 1: training loss: 0.4218081012964249, accuracy: 89.02916666666667\nEpoch 1: val. loss: 0.4124946540340464, val. accuracy: 88.9\nEpoch 2: training loss: 0.38784585122267407, accuracy: 89.66458333333334\nEpoch 2: val. loss: 0.3779184737421097, val. accuracy: 89.44166666666666\nEpoch 3: training loss: 0.3678611751397451, accuracy: 90.1375\nEpoch 3: val. loss: 0.3577300492436328, val. accuracy: 89.89166666666667\nEpoch 4: training loss: 0.3542598653634389, accuracy: 90.41041666666668\nEpoch 4: val. loss: 0.3441218469054141, val. accuracy: 90.28333333333333\nCPU times: user 13.1 s, sys: 893 ms, total: 14 s\nWall time: 14.3 s\n\n\n\npltsize=1\nplt.figure(figsize=(10*pltsize, 10 * pltsize))\nplt.plot(range(epochs), train_acc_all,label = 'Training Acc.' )\nplt.plot(range(epochs), val_acc_all, label = 'Validation Acc.' )\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\n\n\n\n\n\n\n\n\n# Visualize how the model is doing on the first 10 examples\npltsize=1\nplt.figure(figsize=(10*pltsize, pltsize))\nlinear_model.eval()\nbatch = next(iter(train_dataloader))\npredictions = linear_model(batch[0])\n\nfor i in range(10):\n    plt.subplot(1,10,i+1)\n    plt.axis('off')\n    plt.imshow(batch[0][i,0,:,:], cmap=\"gray\")\n    plt.title('%d' % predictions[i,:].argmax())\n\n\n\n\n\n\n\n\nExercise: How can you improve the accuracy? Some things you might consider: increasing the number of epochs, changing the learning rate, etc.",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#prediction",
    "href": "01-neural-networks/1-mnist/index.html#prediction",
    "title": "Intro to NNs: MNIST",
    "section": "Prediction",
    "text": "Prediction\nLet‚Äôs see how our model generalizes to the unseen test data.\n\n#For HW: cell to change batch size\n#create dataloader for test data\n# The dataloader makes our dataset iterable\n\nbatch_size_test = 256 \ntest_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test)\n\n\nacc_test, loss_test = evaluate(test_dataloader, linear_model, loss_fn)\nprint(\"Test loss: %.4f, test accuracy: %.2f%%\" % (loss_test, acc_test))\n\nTest loss: 0.3328, test accuracy: 90.74%\n\n\nWe can now take a closer look at the results.\nLet‚Äôs define a helper function to show the failure cases of our classifier.\n\ndef show_failures(model, dataloader, maxtoshow=10):\n    model.eval()\n    batch = next(iter(dataloader))\n    predictions = model(batch[0])\n\n    rounded = predictions.argmax(1)\n    errors = rounded!=batch[1]\n    print('Showing max', maxtoshow, 'first failures. '\n          'The predicted class is shown first and the correct class in parentheses.')\n    ii = 0\n    plt.figure(figsize=(maxtoshow, 1))\n    for i in range(batch[0].shape[0]):\n        if ii&gt;=maxtoshow:\n            break\n        if errors[i]:\n            plt.subplot(1, maxtoshow, ii+1)\n            plt.axis('off')\n            plt.imshow(batch[0][i,0,:,:], cmap=\"gray\")\n            plt.title(\"%d (%d)\" % (rounded[i], batch[1][i]))\n            ii = ii + 1\n\nHere are the first 10 images from the test data that this small model classified to a wrong class:\n\nshow_failures(linear_model, test_dataloader)\n\nShowing max 10 first failures. The predicted class is shown first and the correct class in parentheses.",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#multilayer-model",
    "href": "01-neural-networks/1-mnist/index.html#multilayer-model",
    "title": "Intro to NNs: MNIST",
    "section": "Multilayer Model",
    "text": "Multilayer Model\nOur linear model isn‚Äôt enough for high accuracy on this dataset. To improve the model, we often need to add more layers and nonlinearities.\n\n\n\n\n\n\nFigure¬†3: Shallow neural network\n\n\n\nThe output of this NN can be written as \\begin{equation}\\label{eq: NN1d}\n  \\hat{u}(x) = \\sigma_2(\\sigma_1(\\mathbf{x}\\mathbf{W}_1 + \\mathbf{b}_1)\\mathbf{W}_2 + \\mathbf{b}_2),\n\\end{equation} where \\mathbf{x} is the input, \\mathbf{W}_j are the weights of the neural network, \\sigma_j the (nonlinear) activation functions, and \\mathbf{b}_j its biases. The activation function introduces the nonlinearity and makes it possible to learn more complex tasks. Desirable properties in an activation function include being differentiable, bounded, and monotonic.\nImage source: PragatiBaheti\n\n\n\n\n\n\nFigure¬†4: Activation function\n\n\n\nAdding more layers to obtain a deep neural network:\n\n\n\n\n\n\nFigure¬†5",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#important-things-to-know",
    "href": "01-neural-networks/1-mnist/index.html#important-things-to-know",
    "title": "Intro to NNs: MNIST",
    "section": "Important things to know",
    "text": "Important things to know\nDeep Neural networks can be overly flexible/complicated and ‚Äúoverfit‚Äù your data, just like fitting overly complicated polynomials:\n\n\n\n\n\n\nFigure¬†6: Bias-variance tradeoff\n\n\n\nVizualization wrt to the accuracy and loss (Image source: Baeldung):\n\n\n\n\n\n\nFigure¬†7: Visualization of accuracy and loss\n\n\n\nTo improve the generalization of our model on previously unseen data, we employ a technique known as regularization, which constrains our optimization problem in order to discourage complex models.\n\nDropout is the commonly used regularization technique. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\nPenalizing the loss function by adding a term such as \\lambda ||\\mathbf{W}||^2 is alsp a commonly used regularization technique. This helps ‚Äúcontrol‚Äù the magnitude of the weights of the network.\n\n\nVanishing gradients\n\nGradients become small as they propagate backward through the layers.\n\n\nSquashing activation functions like sigmoid or tanh could cause this.\n\nExploding gradients\n\nGradients grow exponentially usually due to ‚Äúpoor‚Äù weight initialization.\n\n\nWe can now implement a deep network in PyTorch.\nnn.Dropout() performs the Dropout operation mentioned earlier:\n\n#For HW: cell to change activation\nclass NonlinearClassifier(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.layers_stack = nn.Sequential(\n            nn.Linear(28*28, 50),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(50, 50),\n            nn.ReLU(),\n           # nn.Dropout(0.2),\n            nn.Linear(50, 50),\n            nn.ReLU(),\n           # nn.Dropout(0.2),\n            nn.Linear(50, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.layers_stack(x)\n\n        return x\n\n\n#### For HW: cell to change learning rate\nnonlinear_model = NonlinearClassifier()\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.05)\n\n\n%%time\n\nepochs = 5\ntrain_acc_all = []\nval_acc_all = []\nfor j in range(epochs):\n    train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)\n\n    # checking on the training loss and accuracy once per epoch\n    acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n    train_acc_all.append(acc)\n    print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n\n    # checking on the validation loss and accuracy once per epoch\n    val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n    val_acc_all.append(val_acc)\n    print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n\nEpoch 0: training loss: 0.7137413622538249, accuracy: 79.11875\nEpoch 0: val. loss: 0.7098101504305576, val. accuracy: 78.85833333333333\nEpoch 1: training loss: 0.4042141046126684, accuracy: 88.45416666666667\nEpoch 1: val. loss: 0.40058832219306456, val. accuracy: 88.47500000000001\nEpoch 2: training loss: 0.31061181537310284, accuracy: 91.10208333333333\nEpoch 2: val. loss: 0.30865114467575194, val. accuracy: 90.71666666666667\nEpoch 3: training loss: 0.2650692541003227, accuracy: 92.3875\nEpoch 3: val. loss: 0.26638953720635555, val. accuracy: 92.10000000000001\nEpoch 4: training loss: 0.21981178426742554, accuracy: 93.63333333333334\nEpoch 4: val. loss: 0.2235655054608558, val. accuracy: 93.41666666666667\nCPU times: user 13.2 s, sys: 937 ms, total: 14.2 s\nWall time: 13.8 s\n\n\n\npltsize=1\nplt.figure(figsize=(10*pltsize, 10 * pltsize))\nplt.plot(range(epochs), train_acc_all,label = 'Training Acc.' )\nplt.plot(range(epochs), val_acc_all, label = 'Validation Acc.' )\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\n\n\n\n\n\n\n\n\nshow_failures(nonlinear_model, test_dataloader)\n\nShowing max 10 first failures. The predicted class is shown first and the correct class in parentheses.",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#recap",
    "href": "01-neural-networks/1-mnist/index.html#recap",
    "title": "Intro to NNs: MNIST",
    "section": "Recap",
    "text": "Recap\nTo train and validate a neural network model, you need:\n\nData split into training/validation/test sets,\nA model with parameters to learn\nAn appropriate loss function\nAn optimizer (with tunable parameters such as learning rate, weight decay etc.) used to learn the parameters of the model.",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#homework",
    "href": "01-neural-networks/1-mnist/index.html#homework",
    "title": "Intro to NNs: MNIST",
    "section": "Homework",
    "text": "Homework\n\nCompare the quality of your model when using different:\n\n\nbatch sizes\nlearning rates\nactivation functions\n\n\nBonus: What is a learning rate scheduler?\n\nIf you have time, experiment with how to improve the model.\nNote: training and validation data can be used to compare models, but test data should be saved until the end as a final check of generalization.",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/1-mnist/index.html#homework-solution",
    "href": "01-neural-networks/1-mnist/index.html#homework-solution",
    "title": "Intro to NNs: MNIST",
    "section": "Homework solution",
    "text": "Homework solution\nMake the following changes to the cells with the comment ‚Äú#For HW‚Äù\n\n#####################To modify the batch size##########################\nbatch_size = 32 # 64, 128, 256, 512\n\n# The dataloader makes our dataset iterable \ntrain_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\nval_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n##############################################################################\n\n\n##########################To change the learning rate##########################\noptimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.01) #modify the value of lr\n##############################################################################\n\n\n##########################To change activation##########################\n###### Go to https://pytorch.org/docs/main/nn.html#non-linear-activations-weighted-sum-nonlinearity for more activations ######\nclass NonlinearClassifier(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.layers_stack = nn.Sequential(\n            nn.Linear(28*28, 50),\n            nn.Sigmoid(), #nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(50, 50),\n            nn.Tanh(), #nn.ReLU(),\n           # nn.Dropout(0.2),\n            nn.Linear(50, 50),\n            nn.ReLU(),\n           # nn.Dropout(0.2),\n            nn.Linear(50, 10)\n        )\n        \n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.layers_stack(x)\n\n        return x\n##############################################################################\n\nBonus question: A learning rate scheduler is an essential deep learning technique used to dynamically adjust the learning rate during training. This strategic can significantly impact the convergence speed and overall performance of a neural network. See below on how to incorporate it to your training.\n\nnonlinear_model = NonlinearClassifier()\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.1)\n\n# Step learning rate scheduler: reduce by a factor of 0.1 every 2 epochs (only for illustrative purposes)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\n\n%%time\n\nepochs = 6\ntrain_acc_all = []\nval_acc_all = []\nfor j in range(epochs):\n    train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)\n    #step the scheduler\n    scheduler.step()\n\n    # Print the current learning rate\n    current_lr = optimizer.param_groups[0]['lr']\n    print(f\"Epoch {j+1}/{epochs}, Learning Rate: {current_lr}\")\n\n    # checking on the training loss and accuracy once per epoch\n    acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n    train_acc_all.append(acc)\n    print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n\n    # checking on the validation loss and accuracy once per epoch\n    val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n    val_acc_all.append(val_acc)\n    print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n\nEpoch 1/6, Learning Rate: 0.1\nEpoch 0: training loss: 0.3832669168462356, accuracy: 88.91041666666666\nEpoch 0: val. loss: 0.37728913243611656, val. accuracy: 88.95833333333333\nEpoch 2/6, Learning Rate: 0.010000000000000002\nEpoch 1: training loss: 0.26695636385430893, accuracy: 92.06041666666667\nEpoch 1: val. loss: 0.2603557359526555, val. accuracy: 92.25\nEpoch 3/6, Learning Rate: 0.010000000000000002\nEpoch 2: training loss: 0.23194983277097345, accuracy: 93.17291666666667\nEpoch 2: val. loss: 0.2255863452454408, val. accuracy: 93.27499999999999\nEpoch 4/6, Learning Rate: 0.0010000000000000002\nEpoch 3: training loss: 0.22610340382779637, accuracy: 93.38958333333333\nEpoch 3: val. loss: 0.22041747208933035, val. accuracy: 93.375\nEpoch 5/6, Learning Rate: 0.0010000000000000002\nEpoch 4: training loss: 0.2239053814721604, accuracy: 93.40833333333333\nEpoch 4: val. loss: 0.21813481837511062, val. accuracy: 93.39166666666667\nEpoch 6/6, Learning Rate: 0.00010000000000000003\nEpoch 5: training loss: 0.22323055649052065, accuracy: 93.41041666666666\nEpoch 5: val. loss: 0.217593674937884, val. accuracy: 93.39166666666667\nCPU times: user 19.7 s, sys: 3.56 s, total: 23.2 s\nWall time: 21.3 s",
    "crumbs": [
      "[01] Neural Networks",
      "[1] MNIST Example"
    ]
  },
  {
    "objectID": "01-neural-networks/3-conv-nets/index.html",
    "href": "01-neural-networks/3-conv-nets/index.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "Up until transformers, convolutions were the state of the art in computer vision.\nIn many ways and applications they still are!\nLarge Language Models, which are what we‚Äôll focus on the rest of the series after this lecture, are really good at ordered, *tokenized data. But there is lots of data that isn‚Äôt implicitly ordered like images, and their more general cousins graphs.\nToday‚Äôs lecture focuses on computer vision models, and particularly on convolutional neural networks. There are a ton of applications you can do with these, and not nearly enough time to get into them. Check out the extra references file to see some publications to get you started if you want to learn more.\nTip: this notebook is much faster on the GPU!",
    "crumbs": [
      "[01] Neural Networks",
      "[3] Conv. Nets"
    ]
  },
  {
    "objectID": "01-neural-networks/3-conv-nets/index.html#convolutional-networks-a-brief-historical-context",
    "href": "01-neural-networks/3-conv-nets/index.html#convolutional-networks-a-brief-historical-context",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Networks: A brief historical context",
    "text": "Convolutional Networks: A brief historical context\n\n\n\nImageNet Accuracy by Yearh\n\n\n\nimport ambivalent\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use(ambivalent.STYLES['ambivalent'])\nsns.set_context(\"notebook\")\n\n\n# Data\nyears = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017]\nimage_net_error_rates = [28, 26, 16, 12, 7, 5, 3, 2]\ndata = {\n    2010: 28,\n    2011: 26,\n    2012: 16,\n    2013: 12,\n    2014: 7,\n    2015: 3,\n    2016: 2.3,\n    2017: 2.1\n}\nhuman_error_rate = 5\n\n# Create bar plot\nplt.bar(years, image_net_error_rates, color='blue')\n\n# Add human error rate line\nplt.axhline(y=human_error_rate, color='red', linestyle='--', label='Human error rate')\n\n# Labels and title\nplt.xlabel('Year')\nplt.ylabel('ImageNet Visual Recognition Error Rate (%)')\nplt.title('ImageNet Error Rates Over Time')\nplt.legend()\n\n# Display plot\nplt.show()\n\n\n\n\n\n\n\n\nreference\n\nimport torch, torchvision",
    "crumbs": [
      "[01] Neural Networks",
      "[3] Conv. Nets"
    ]
  },
  {
    "objectID": "01-neural-networks/3-conv-nets/index.html#convolutional-building-blocks",
    "href": "01-neural-networks/3-conv-nets/index.html#convolutional-building-blocks",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Building Blocks",
    "text": "Convolutional Building Blocks\nWe‚Äôre going to go through some examples of building blocks for convolutional networks. To help illustate some of these, let‚Äôs use an image for examples:\n\nfrom PIL import Image\n# wget line useful in Google Colab\n! wget https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/03_advanced_neural_networks/ALCF-Staff.jpg\nalcf_image = Image.open(\"ALCF-Staff.jpg\")\n\n--2025-07-23 06:24:02--  https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/03_advanced_neural_networks/ALCF-Staff.jpg\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 417835 (408K) [image/jpeg]\nSaving to: ‚ÄòALCF-Staff.jpg.25‚Äô\n\nALCF-Staff.jpg.25     0%[                    ]       0  --.-KB/s               ALCF-Staff.jpg.25   100%[===================&gt;] 408.04K  --.-KB/s    in 0.03s   \n\n2025-07-23 06:24:02 (12.0 MB/s) - ‚ÄòALCF-Staff.jpg.25‚Äô saved [417835/417835]\n\n\n\n\nfrom matplotlib import pyplot as plt\nfigure = plt.figure(figsize=(20,20))\nplt.imshow(alcf_image)\nplt.show()\n\n\n\n\n\n\n\n\n\nConvolutions\nConvolutions are a restriction of - and a specialization of - dense linear layers. A convolution of an image produces another image, and each output pixel is a function of only it‚Äôs local neighborhood of points. This is called an inductive bias and is a big reason why convolutions work for image data: neighboring pixels are correlated and you can operate on just those pixels at a time.\nSee examples of convolutions here\n\n\n\nimage-2.png\n\n\n\n\n\nimage.png\n\n\n\n# Let's apply a convolution to the ALCF Staff photo:\nalcf_tensor = torchvision.transforms.ToTensor()(alcf_image)\n\n# Reshape the tensor to have a batch size of 1:\nalcf_tensor = alcf_tensor.reshape((1,) + alcf_tensor.shape)\n\n# Create a random convolution:\n# shape is: (channels_in, channels_out, kernel_x, kernel_y)\nconv_random = torch.rand((3,3,15,15))\n\nalcf_rand = torch.nn.functional.conv2d(alcf_tensor, conv_random)\nalcf_rand = (1./alcf_rand.max()) * alcf_rand\nprint(alcf_rand.shape)\nalcf_rand = alcf_rand.reshape(alcf_rand.shape[1:])\n\nprint(alcf_tensor.shape)\n\nrand_image = alcf_rand.permute((1,2,0)).cpu()\n\nfigure = plt.figure(figsize=(20,20))\n\nplt.imshow(rand_image)\n\ntorch.Size([1, 3, 1111, 1986])\ntorch.Size([1, 3, 1125, 2000])\n\n\n\n\n\n\n\n\n\n\n\nNormalization\n Reference: Normalizations\nNormalization is the act of transforming the mean and moment of your data to standard values (usually 0.0 and 1.0). It‚Äôs particularly useful in machine learning since it stabilizes training, and allows higher learning rates.\n\n\n\nBatch Normalization accelerates training\n\n\nReference: Batch Norm\n\n# Let's apply a normalization to the ALCF Staff photo:\nalcf_tensor = torchvision.transforms.ToTensor()(alcf_image)\n\n# Reshape the tensor to have a batch size of 1:\nalcf_tensor = alcf_tensor.reshape((1,) + alcf_tensor.shape)\n\n\nalcf_rand = torch.nn.functional.normalize(alcf_tensor)\nalcf_rand = alcf_rand.reshape(alcf_rand.shape[1:])\n\nprint(alcf_tensor.shape)\n\nrand_image = alcf_rand.permute((1,2,0)).cpu()\n\nfigure = plt.figure(figsize=(20,20))\n\nplt.imshow(rand_image)\n\ntorch.Size([1, 3, 1125, 2000])\n\n\n\n\n\n\n\n\n\n\n\nDownsampling (And upsampling)\nDownsampling is a critical component of convolutional and many vision models. Because of the local-only nature of convolutional filters, learning large-range features can be too slow for convergence. Downsampling of layers can bring information from far away closer, effectively changing what it means to be ‚Äúlocal‚Äù as the input to a convolution.\n\n\n\nConvolutional Pooling\n\n\nReference\n\n# Let's apply a normalization to the ALCF Staff photo:\nalcf_tensor = torchvision.transforms.ToTensor()(alcf_image)\n\n# Reshape the tensor to have a batch size of 1:\nalcf_tensor = alcf_tensor.reshape((1,) + alcf_tensor.shape)\n\n\nalcf_rand = torch.nn.functional.max_pool2d(alcf_tensor, 2)\nalcf_rand = alcf_rand.reshape(alcf_rand.shape[1:])\n\nprint(alcf_tensor.shape)\n\nrand_image = alcf_rand.permute((1,2,0)).cpu()\n\nfigure = plt.figure(figsize=(20,20))\n\nplt.imshow(rand_image)\n\ntorch.Size([1, 3, 1125, 2000])\n\n\n\n\n\n\n\n\n\n\n\nResidual Connections\nOne issue, quickly encountered when making convolutional networks deeper and deeper, is the ‚ÄúVanishing Gradients‚Äù problem. As layers were stacked on top of each other, the size of updates dimished at the earlier layers of a convolutional network. The paper ‚ÄúDeep Residual Learning for Image Recognition‚Äù solved this by introduction ‚Äúresidual connections‚Äù as skip layers.\nReference: Deep Residual Learning for Image Recognition\n\n\n\nResidual Layer\n\n\nCompare the performance of the models before and after the introduction of these layers:\n\n\n\nResnet Performance vs.¬†Plain network performance\n\n\nIf you have time to read only one paper on computer vision, make it this one! Resnet was the first model to beat human accuracy on ImageNet and is one of the most impactful papers in AI ever published.",
    "crumbs": [
      "[01] Neural Networks",
      "[3] Conv. Nets"
    ]
  },
  {
    "objectID": "01-neural-networks/3-conv-nets/index.html#building-a-convnet",
    "href": "01-neural-networks/3-conv-nets/index.html#building-a-convnet",
    "title": "Convolutional Neural Networks",
    "section": "Building a ConvNet",
    "text": "Building a ConvNet\nIn this section we‚Äôll build and apply a conv net to the mnist dataset. The layers here are loosely based off of the ConvNext architecture. Why? Because we‚Äôre getting into LLM‚Äôs soon, and this ConvNet uses LLM features. ConvNext is an update to the ResNet architecture that outperforms it.\nConvNext\nThe dataset here is CIFAR-10 - slightly harder than MNIST but still relatively easy and computationally tractable.\n\nfrom torchvision.transforms import v2\ntraining_data = torchvision.datasets.CIFAR10(\n    # Polaris: root=\"/lus/eagle/projects/datasets/CIFAR-10/\",\n    # Polaris: download=False,\n    root=\"data\",\n    download=True,\n    train=True,\n    transform=v2.Compose([\n        v2.ToTensor(),\n        v2.RandomHorizontalFlip(),\n        v2.RandomResizedCrop(size=32, scale=[0.85,1.0], antialias=False),\n        v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    ])\n)\n\ntest_data = torchvision.datasets.CIFAR10(\n    # Polaris: root=\"/lus/eagle/projects/datasets/CIFAR-10/\",\n    # Polaris: download=False,\n    root=\"data\",\n    download=True,\n    train=False,\n    transform=torchvision.transforms.ToTensor()\n)\n\ntraining_data, validation_data = torch.utils.data.random_split(training_data, [0.8, 0.2], generator=torch.Generator().manual_seed(55))\n\nbatch_size = 128\n\n# The dataloader makes our dataset iterable \ntrain_dataloader = torch.utils.data.DataLoader(training_data, \n    batch_size=batch_size, \n    pin_memory=True,\n    shuffle=True, \n    num_workers=2)\nval_dataloader = torch.utils.data.DataLoader(validation_data, \n    batch_size=batch_size, \n    pin_memory=True,\n    shuffle=False, \n    num_workers=2)\n\n/Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\n\n\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n\nbatch, (X, Y) = next(enumerate(train_dataloader))\nplt.imshow(X[0].cpu().permute((1,2,0))); plt.show()\n\n/Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n\n\n\nThis code below is important as our models get bigger: this is wrapping the pytorch data loaders to put the data onto the GPU!\n\ndev = torch.device(\n    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n\ndef preprocess(x, y):\n    # CIFAR-10 is *color* images so 3 layers!\n    return x.view(-1, 3, 32, 32).to(dev), y.to(dev)\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        for b in self.dl:\n            yield (self.func(*b))\n\n\ntrain_dataloader = WrappedDataLoader(train_dataloader, preprocess)\nval_dataloader = WrappedDataLoader(val_dataloader, preprocess)\n\n\nfrom torch import nn\n\n\nclass Downsampler(nn.Module):\n\n    def __init__(self, in_channels, out_channels, shape, stride=2):\n        super(Downsampler, self).__init__()\n\n        self.norm = nn.LayerNorm([in_channels, *shape])\n\n        self.downsample = nn.Conv2d(\n            in_channels=in_channels, \n            out_channels=out_channels,\n            kernel_size = stride,\n            stride = stride,\n        )\n\n    def forward(self, inputs):\n\n\n        return self.downsample(self.norm(inputs))\n\n\n\nclass ConvNextBlock(nn.Module):\n    \"\"\"This block of operations is loosely based on this paper:\n\n    \"\"\"\n\n\n    def __init__(self, in_channels, shape):\n        super(ConvNextBlock, self).__init__()\n\n        # Depthwise, seperable convolution with a large number of output filters:\n        self.conv1 = nn.Conv2d(in_channels=in_channels, \n                                     out_channels=in_channels, \n                                     groups=in_channels,\n                                     kernel_size=[7,7],\n                                     padding='same' )\n\n        self.norm = nn.LayerNorm([in_channels, *shape])\n\n        # Two more convolutions:\n        self.conv2 = nn.Conv2d(in_channels=in_channels, \n                                     out_channels=4*in_channels,\n                                     kernel_size=1)\n\n        self.conv3 = nn.Conv2d(in_channels=4*in_channels, \n                                     out_channels=in_channels,\n                                     kernel_size=1\n                                     )\n\n\n    def forward(self, inputs):\n        x = self.conv1(inputs)\n\n        # The normalization layer:\n        x = self.norm(x)\n\n        x = self.conv2(x)\n\n        # The non-linear activation layer:\n        x = torch.nn.functional.gelu(x)\n\n        x = self.conv3(x)\n\n        # This makes it a residual network:\n        return x + inputs\n\n\nclass Classifier(nn.Module):\n\n\n    def __init__(self, n_initial_filters, n_stages, blocks_per_stage):\n        super(Classifier, self).__init__()\n\n        # This is a downsampling convolution that will produce patches of output.\n\n        # This is similar to what vision transformers do to tokenize the images.\n        self.stem = nn.Conv2d(in_channels=3,\n                                    out_channels=n_initial_filters,\n                                    kernel_size=1,\n                                    stride=1)\n\n        current_shape = [32, 32]\n\n        self.norm1 = nn.LayerNorm([n_initial_filters,*current_shape])\n        # self.norm1 = WrappedLayerNorm()\n\n        current_n_filters = n_initial_filters\n\n        self.layers = nn.Sequential()\n        for i, n_blocks in enumerate(range(n_stages)):\n            # Add a convnext block series:\n            for _ in range(blocks_per_stage):\n                self.layers.append(ConvNextBlock(in_channels=current_n_filters, shape=current_shape))\n            # Add a downsampling layer:\n            if i != n_stages - 1:\n                # Skip downsampling if it's the last layer!\n                self.layers.append(Downsampler(\n                    in_channels=current_n_filters, \n                    out_channels=2*current_n_filters,\n                    shape = current_shape,\n                    )\n                )\n                # Double the number of filters:\n                current_n_filters = 2*current_n_filters\n                # Cut the shape in half:\n                current_shape = [ cs // 2 for cs in current_shape]\n\n\n\n        self.head = nn.Sequential(\n            nn.Flatten(),\n            nn.LayerNorm(current_n_filters),\n            nn.Linear(current_n_filters, 10)\n        )\n        # self.norm2 = nn.InstanceNorm2d(current_n_filters)\n        # # This brings it down to one channel / class\n        # self.bottleneck = nn.Conv2d(in_channels=current_n_filters, out_channels=10, \n        #                                   kernel_size=1, stride=1)\n\n    def forward(self, inputs):\n\n        x = self.stem(inputs)\n        # Apply a normalization after the initial patching:\n        x = self.norm1(x)\n\n        # Apply the main chunk of the network:\n        x = self.layers(x)\n\n        # Normalize and readout:\n        x = nn.functional.avg_pool2d(x, x.shape[2:])\n        x = self.head(x)\n\n        return x\n\n\n\n        # x = self.norm2(x)\n        # x = self.bottleneck(x)\n\n        # # Average pooling of the remaining spatial dimensions (and reshape) makes this label-like:\n        # return nn.functional.avg_pool2d(x, kernel_size=x.shape[-2:]).reshape((-1,10))\n\n\n!pip install torchinfo # if not on Polaris\n\nRequirement already satisfied: torchinfo in /opt/homebrew/lib/python3.11/site-packages (1.8.0)\n\n\n\nmodel = Classifier(32, 4, 2).to(device=dev)\n\nfrom torchinfo import summary\n\nprint(summary(model, input_size=(batch_size, 3, 32, 32)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nClassifier                               [128, 10]                 --\n‚îú‚îÄConv2d: 1-1                            [128, 32, 32, 32]         128\n‚îú‚îÄLayerNorm: 1-2                         [128, 32, 32, 32]         65,536\n‚îú‚îÄSequential: 1-3                        [128, 256, 4, 4]          --\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-1                [128, 32, 32, 32]         --\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-1                  [128, 32, 32, 32]         1,600\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-2               [128, 32, 32, 32]         65,536\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-3                  [128, 128, 32, 32]        4,224\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-4                  [128, 32, 32, 32]         4,128\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-2                [128, 32, 32, 32]         --\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-5                  [128, 32, 32, 32]         1,600\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-6               [128, 32, 32, 32]         65,536\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-7                  [128, 128, 32, 32]        4,224\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-8                  [128, 32, 32, 32]         4,128\n‚îÇ    ‚îî‚îÄDownsampler: 2-3                  [128, 64, 16, 16]         --\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-9               [128, 32, 32, 32]         65,536\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-10                 [128, 64, 16, 16]         8,256\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-4                [128, 64, 16, 16]         --\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-11                 [128, 64, 16, 16]         3,200\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-12              [128, 64, 16, 16]         32,768\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-13                 [128, 256, 16, 16]        16,640\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-14                 [128, 64, 16, 16]         16,448\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-5                [128, 64, 16, 16]         --\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-15                 [128, 64, 16, 16]         3,200\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-16              [128, 64, 16, 16]         32,768\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-17                 [128, 256, 16, 16]        16,640\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-18                 [128, 64, 16, 16]         16,448\n‚îÇ    ‚îî‚îÄDownsampler: 2-6                  [128, 128, 8, 8]          --\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-19              [128, 64, 16, 16]         32,768\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-20                 [128, 128, 8, 8]          32,896\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-7                [128, 128, 8, 8]          --\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-21                 [128, 128, 8, 8]          6,400\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-22              [128, 128, 8, 8]          16,384\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-23                 [128, 512, 8, 8]          66,048\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-24                 [128, 128, 8, 8]          65,664\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-8                [128, 128, 8, 8]          --\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-25                 [128, 128, 8, 8]          6,400\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-26              [128, 128, 8, 8]          16,384\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-27                 [128, 512, 8, 8]          66,048\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-28                 [128, 128, 8, 8]          65,664\n‚îÇ    ‚îî‚îÄDownsampler: 2-9                  [128, 256, 4, 4]          --\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-29              [128, 128, 8, 8]          16,384\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-30                 [128, 256, 4, 4]          131,328\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-10               [128, 256, 4, 4]          --\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-31                 [128, 256, 4, 4]          12,800\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-32              [128, 256, 4, 4]          8,192\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-33                 [128, 1024, 4, 4]         263,168\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-34                 [128, 256, 4, 4]          262,400\n‚îÇ    ‚îî‚îÄConvNextBlock: 2-11               [128, 256, 4, 4]          --\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-35                 [128, 256, 4, 4]          12,800\n‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-36              [128, 256, 4, 4]          8,192\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-37                 [128, 1024, 4, 4]         263,168\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-38                 [128, 256, 4, 4]          262,400\n‚îú‚îÄSequential: 1-4                        [128, 10]                 --\n‚îÇ    ‚îî‚îÄFlatten: 2-12                     [128, 256]                --\n‚îÇ    ‚îî‚îÄLayerNorm: 2-13                   [128, 256]                512\n‚îÇ    ‚îî‚îÄLinear: 2-14                      [128, 10]                 2,570\n==========================================================================================\nTotal params: 2,047,114\nTrainable params: 2,047,114\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 10.34\n==========================================================================================\nInput size (MB): 1.57\nForward/backward pass size (MB): 1036.27\nParams size (MB): 8.19\nEstimated Total Size (MB): 1046.03\n==========================================================================================\n\n\n\ndef evaluate(dataloader, model, loss_fn, val_bar):\n    # Set the model to evaluation mode - some NN pieces behave differently during training\n    # Unnecessary in this situation but added for best practices\n    model.eval()\n    size = len(dataloader)\n    num_batches = len(dataloader)\n    loss, correct = 0, 0\n\n    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n    with torch.no_grad():\n        # loop over all of the batches\n        for X, y in dataloader:\n\n            pred = model(X)\n            loss += loss_fn(pred, y).item()\n            # how many are correct in this batch? Tracking for accuracy \n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n            val_bar.update()\n\n    loss /= num_batches\n    correct /= (size*batch_size)\n\n    accuracy = 100*correct\n    return accuracy, loss\n\n\ndef train_one_epoch(dataloader, model, loss_fn, optimizer, progress_bar):\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # forward pass\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # backward pass calculates gradients\n        loss.backward()\n\n        # take one step with these gradients\n        optimizer.step()\n\n        # resets the gradients \n        optimizer.zero_grad()\n\n        progress_bar.update()\n\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=0.01)\n\n\nimport time\nimport ezpz\nfrom tqdm.notebook import tqdm\n\ndef train_step(x, y):\n    t0 = time.perf_counter()\n    # Forward pass\n    pred = model(x)\n    loss = loss_fn(pred, y)\n    t1 = time.perf_counter()\n\n    # Backward pass\n    loss.backward()\n    t2 = time.perf_counter()\n\n    # Update weights\n    optimizer.step()\n    t3 = time.perf_counter()\n\n    # Reset gradients\n    optimizer.zero_grad()\n    t4 = time.perf_counter()\n\n    return loss.item(), {\n        \"dtf\": t1 - t0,\n        \"dtb\": t2 - t1,\n        \"dtu\": t3 - t2,\n        \"dtz\": t4 - t3,\n    }\n\nlogger = ezpz.get_logger(\"3-conv-nets\")\nhistory = ezpz.History()\nfor i in range(5):\n    t0 = time.perf_counter()\n    x, y = next(iter(train_dataloader))\n    t1 = time.perf_counter()\n    loss, dt = train_step(x, y)\n    logger.info(\n        history.update(\n            {\n                \"iter\": i,\n                \"loss\": loss,\n                \"dtd\": t1 - t0,\n                **dt,\n            }\n        )\n    )\n\n[2025-07-23 06:24:21,843047][I][ezpz/__init__:265:ezpz] Setting logging level to 'INFO' on 'RANK == 0'\n\n\n\n[2025-07-23 06:24:21,845110][I][ezpz/__init__:266:ezpz] Setting logging level to 'CRITICAL' on all others 'RANK != 0'\n\n\n\n[2025-07-23 06:24:33,188255][I][ipykernel_96008/1183336067:38:3-conv-nets] iter=0 loss=2.449330 dtd=3.884327 dtf=0.695731 dtb=6.756479 dtu=0.003305 dtz=0.000880\n\n\n\n[2025-07-23 06:24:46,169640][I][ipykernel_96008/1183336067:38:3-conv-nets] iter=1 loss=2.505571 dtd=5.080432 dtf=0.830492 dtb=7.063211 dtu=0.003747 dtz=0.001182\n\n\n\n[2025-07-23 06:24:57,719810][I][ipykernel_96008/1183336067:38:3-conv-nets] iter=2 loss=2.467676 dtd=4.502937 dtf=0.732804 dtb=6.307935 dtu=0.002281 dtz=0.000987\n\n\n\n[2025-07-23 06:25:10,172282][I][ipykernel_96008/1183336067:38:3-conv-nets] iter=3 loss=2.449162 dtd=4.041015 dtf=1.419110 dtb=6.986501 dtu=0.002535 dtz=0.001335\n\n\n\n[2025-07-23 06:25:21,662560][I][ipykernel_96008/1183336067:38:3-conv-nets] iter=4 loss=2.369287 dtd=4.403100 dtf=0.716576 dtb=6.363842 dtu=0.002715 dtz=0.001452\n\n\n\n# epochs = 1\n# for j in range(epochs):\n#     with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Train Epoch {j}\") as train_bar:\n#         train_one_epoch(train_dataloader, model, loss_fn, optimizer, train_bar)\n#\n#     # checking on the training & validation loss & accuracy \n#     # for training data - only once every 5 epochs (takes a while) \n#     if j % 5 == 0:\n#         with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Validate (train) Epoch {j}\") as train_eval:\n#             acc, loss = evaluate(train_dataloader, model, loss_fn, train_eval)\n#             print(f\"Epoch {j}: training loss: {loss:.3f}, accuracy: {acc:.3f}\")\n#\n#     with tqdm(total=len(val_dataloader), position=0, leave=True, desc=f\"Validate Epoch {j}\") as val_bar:\n#         acc_val, loss_val = evaluate(val_dataloader, model, loss_fn, val_bar)\n#         print(f\"Epoch {j}: validation loss: {loss_val:.3f}, accuracy: {acc_val:.3f}\")",
    "crumbs": [
      "[01] Neural Networks",
      "[3] Conv. Nets"
    ]
  },
  {
    "objectID": "01-neural-networks/3-conv-nets/index.html#homework-1",
    "href": "01-neural-networks/3-conv-nets/index.html#homework-1",
    "title": "Convolutional Neural Networks",
    "section": "Homework 1:",
    "text": "Homework 1:\nIn this notebook, we‚Äôve learned about some basic convolutional networks and trained one on CIFAR-10 images. It did ‚Ä¶ OK. There is significant overfitting of this model. There are some ways to address that, but we didn‚Äôt have time to get into that in this session.\nMeanwhile, your homework (part 1) for this week is to try to train the model again but with a different architecture. Change one or more of the following: - The number of convolutions between downsampling - The number of filters in each layer - The initial ‚Äúpatchify‚Äù layer - Another hyper-parameter of your choosing\nAnd compare your final validation accuracy to the accuracy shown here. Can you beat the validation accuracy shown?\nFor full credit on the homework, you need to show (via text, or make a plot) the training and validation data sets‚Äô performance (loss and accuracy) for all the epochs you train. You also need to explain, in several sentences, what you changed in the network and why you think it makes a difference.",
    "crumbs": [
      "[01] Neural Networks",
      "[3] Conv. Nets"
    ]
  },
  {
    "objectID": "01-neural-networks/index.html",
    "href": "01-neural-networks/index.html",
    "title": "Introduction to Neural Networks",
    "section": "",
    "text": "01-neural-networks/\n\n0-intro\n1-mnist\n2-advanced\n3-conv-nets\n4-representation-learning\n\n\n\n\n\n\nCitationBibTeX citation:@online{foreman2025,\n  author = {Foreman, Sam},\n  title = {Introduction to {Neural} {Networks}},\n  date = {2025-07-15},\n  url = {https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2025. ‚ÄúIntroduction to Neural Networks.‚Äù July\n15, 2025. https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/.",
    "crumbs": [
      "[01] Neural Networks"
    ]
  },
  {
    "objectID": "02-llms/0-intro-to-llms/index.html#brief-overview",
    "href": "02-llms/0-intro-to-llms/index.html#brief-overview",
    "title": "Intro to HPC Bootcamp 2025",
    "section": "Brief overview",
    "text": "Brief overview\n\nScientific applications for language models\nGeneral overview of Transformers\nTokenization\nModel Architecture\nPipeline using HuggingFace\nModel loading",
    "crumbs": [
      "[02] Large Language Models",
      "[0] Intro to LLMs"
    ]
  },
  {
    "objectID": "02-llms/0-intro-to-llms/index.html#sophia-setup",
    "href": "02-llms/0-intro-to-llms/index.html#sophia-setup",
    "title": "Intro to HPC Bootcamp 2025",
    "section": "Sophia Setup",
    "text": "Sophia Setup\n\nIf you are using ALCF, first log in. From a terminal run the following command:\nssh username@sophia.alcf.anl.gov\nAlthough we already cloned the repo before, you‚Äôll want the updated version. \nNow that we have the updated notebooks, we can open them. If you are using ALCF JupyterHub or Google Colab, you can be reminded of the steps here\nReminder: Change the notebook‚Äôs kernel to datascience/conda-2024-08-08 (you may need to change kernel each time you open a notebook for the first time):\n\nselect Kernel in the menu bar\nselect Change kernel‚Ä¶\nselect datascience/conda-2024-08-08 from the drop-down menu",
    "crumbs": [
      "[02] Large Language Models",
      "[0] Intro to LLMs"
    ]
  },
  {
    "objectID": "02-llms/0-intro-to-llms/index.html#google-colab-setup",
    "href": "02-llms/0-intro-to-llms/index.html#google-colab-setup",
    "title": "Intro to HPC Bootcamp 2025",
    "section": "Google colab setup",
    "text": "Google colab setup\nIn case you have trouble accessing Sophia, all notebook material can be run in google colab.\nJust:\n\nGo to this link: Colab\nClick on File/Open notebook\nNagivate to the GitHub tab and find argonne-lcf/ai-science-training-series\nClick on 04_intro_to_llms/IntroLLMs.ipynb",
    "crumbs": [
      "[02] Large Language Models",
      "[0] Intro to LLMs"
    ]
  },
  {
    "objectID": "02-llms/0-intro-to-llms/index.html#references",
    "href": "02-llms/0-intro-to-llms/index.html#references",
    "title": "Intro to HPC Bootcamp 2025",
    "section": "References",
    "text": "References\nI strongly recommend reading ‚ÄúThe Illustrated Transformer‚Äù by Jay AlammarAlammar also has a useful post dedicated more generally to Sequence-to-Sequence modeling ‚ÄúVisualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention), which illustrates the attention mechanism in the context of a more generic language translation model.",
    "crumbs": [
      "[02] Large Language Models",
      "[0] Intro to LLMs"
    ]
  },
  {
    "objectID": "02-llms/0-intro-to-llms/index.html#homework-solutions",
    "href": "02-llms/0-intro-to-llms/index.html#homework-solutions",
    "title": "Intro to HPC Bootcamp 2025",
    "section": "Homework solutions",
    "text": "Homework solutions\nSolutions to homework problems are posted in IntroLLMHWSols.ipynb To see BertViz attention mechanisms, simply open the notebook in google colab.",
    "crumbs": [
      "[02] Large Language Models",
      "[0] Intro to LLMs"
    ]
  },
  {
    "objectID": "02-llms/4-evaluating-llms/index.html",
    "href": "02-llms/4-evaluating-llms/index.html",
    "title": "Intro to HPC Bootcamp 2025",
    "section": "",
    "text": "Evaluating LLMs and Potential Pitfalls\nIntro to AI-Driven Science on Supercomputers @ ALCF 2024\nContact: Marieme Ngom (mngom@anl.gov), Bethany Lusch (blusch@anl.gov), Sandeep Madireddy (smadireddy@anl.gov)\nOverview of LLMs Evaluation\nPotential Pitfalls of LLMs\nLink to breakout rooms forms\nOther helpful links: - OpenAI tokenizer - Chatbot Arena - Chatbot Guardrails Arena\nHomework\nWhat do you think is a particularly good use case for LLMs for science? How would you evaluate it? Your answer does not need to be in paragraphs. When you submit your homework form, you can link to a file in your Github repo where you wrote your answer.\n\n\n\n\n\nCitationBibTeX citation:@online{foreman2025,\n  author = {Foreman, Sam},\n  title = {Intro to {HPC} {Bootcamp} 2025},\n  date = {2025-07-22},\n  url = {https://saforem2.github.io/hpc-bootcamp-2025/02-llms/4-evaluating-llms/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2025. ‚ÄúIntro to HPC Bootcamp 2025.‚Äù July 22,\n2025. https://saforem2.github.io/hpc-bootcamp-2025/02-llms/4-evaluating-llms/.",
    "crumbs": [
      "[02] Large Language Models",
      "[4] Evaluating LLMs"
    ]
  },
  {
    "objectID": "02-llms/6-parallel-training/index.html#hands-on",
    "href": "02-llms/6-parallel-training/index.html#hands-on",
    "title": "Intro to HPC Bootcamp 2025",
    "section": "üëã Hands On",
    "text": "üëã Hands On\n\nSubmit interactive job:\nqsub -A ALCFAITP -q by-node -l select=1 -l walltime=01:00:00,filesystems=eagle:home -I\nOn Sophia:\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\nClone repos:\n\nsaforem2/wordplay:\ngit clone https://github.com/saforem2/wordplay\ncd wordplay\nsaforem2/ezpz:\ngit clone https://github.com/saforem2/ezpz deps/ezpz\n\nSetup python:\nexport PBS_O_WORKDIR=$(pwd) && source deps/ezpz/src/ezpz/bin/utils.sh\nezpz_setup_python\nezpz_setup_job\nInstall {ezpz, wordplay}:\npython3 -m pip install -e deps/ezpz --require-virtualenv\npython3 -m pip install -e . --require-virtualenv\nSetup (or disable) wandb:\n# to setup:\nwandb login\n# to disable:\nexport WANDB_DISABLED=1\nTest Distributed Setup:\nmpirun -n \"${NGPUS}\" python3 -m ezpz.test_dist\nSee: ezpz/test_dist.py\nPrepare Data:\npython3 data/shakespeare_char/prepare.py\nLaunch Training:\nmpirun -n \"${NGPUS}\" python3 -m wordplay \\\n    train.backend=DDP \\\n    train.eval_interval=100 \\\n    data=shakespeare \\\n    train.dtype=bf16 \\\n    model.batch_size=64 \\\n    model.block_size=1024 \\\n    train.max_iters=1000 \\\n    train.log_interval=10 \\\n    train.compile=false",
    "crumbs": [
      "[02] Large Language Models",
      "[6] Parallel Training"
    ]
  },
  {
    "objectID": "02-llms/6-parallel-training/index.html#homework",
    "href": "02-llms/6-parallel-training/index.html#homework",
    "title": "Intro to HPC Bootcamp 2025",
    "section": "üéí Homework",
    "text": "üéí Homework\nSubmit proof that you were able to successfully follow the above instructions and launch a distributed data parallel training run.\nWhere proof can be any of:\n\nThe contents printed out to your terminal during the run\nA path to a logfile containing the output from a run on the ALCF filesystems\nA screenshot of:\n\nthe text printed out from the run\na graph from the W&B Run\nanything that shows that you clearly were able to run the example\n\nurl to a W&B Run or W&B Report\netc.",
    "crumbs": [
      "[02] Large Language Models",
      "[6] Parallel Training"
    ]
  },
  {
    "objectID": "03-ai-for-science/index.html#applications",
    "href": "03-ai-for-science/index.html#applications",
    "title": "AI for Science",
    "section": "Applications",
    "text": "Applications\n\nGenSLM\nMProt-DPO\nAeris\nl2hmc-qcd\n‚Ä¶",
    "crumbs": [
      "[03] AI for Science"
    ]
  }
]