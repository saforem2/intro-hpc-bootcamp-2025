<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.17">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sam Foreman">
<meta name="author" content="Marieme Ngom">
<meta name="author" content="Huihuo Zheng">
<meta name="author" content="Bethany Lusch">
<meta name="author" content="Taylor Childers">
<meta name="dcterms.date" content="2025-07-17">
<meta name="description" content="Introduction to Neural Networks with MNIST dataset">

<title>Intro to NNs: MNIST – Sam Foreman</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../content/01-neural-networks/2-advanced/index.html" rel="next">
<link href="../../../content/01-neural-networks/0-intro/index.html" rel="prev">
<link href="../../../assets/favicon-sf.svg" rel="icon" type="image/svg+xml">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-1ba5600575b524666f3a728b03aeaa80.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-0434ab73b344d2590fb99d65eafd6ba5.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-805fb4b31aff4615b0fdac002984a0cb.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-085e09623a58b77b70dd0ae33b98aedd.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../../site_libs/quarto-contrib/iconify-3.0.0/iconify-icon.min.js"></script>
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "?",
    "H"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js" integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;family=IBM+Plex+Sans+Condensed:ital,wght@0,400;0,500;0,600;0,700&amp;family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans&amp;family=IBM+Plex+Sans+Condensed&amp;family=IBM+Plex+Mono&amp;display=swap" rel="stylesheet">
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-TC329HJ');</script>
<!-- End Google Tag Manager -->
<link rel="preconnect" href="https://fonts.googleapis.com">

  <script defer="" src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../static/fonts/MIosevkaQp/MIosevkaQp.css">
<link rel="stylesheet" href="../../../static/fonts/MIosevkaterm/MIosevkaTerm.css">
<meta property="og:title" content="Intro to NNs: MNIST – Sam Foreman">
<meta property="og:description" content="Introduction to Neural Networks with MNIST dataset">
<meta property="og:image" content="https://samforeman.me/content/01-neural-networks/1-mnist/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta property="og:site_name" content="Sam Foreman">
<meta name="twitter:title" content="Intro to NNs: MNIST – Sam Foreman">
<meta name="twitter:description" content="Introduction to Neural Networks with MNIST dataset">
<meta name="twitter:image" content="https://samforeman.me/content/01-neural-networks/1-mnist/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta name="twitter:site" content="saforem2">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="Intro to NNs: MNIST">
<meta name="citation_author" content="Sam Foreman">
<meta name="citation_author" content="Marieme Ngom">
<meta name="citation_author" content="Huihuo Zheng">
<meta name="citation_author" content="Bethany Lusch">
<meta name="citation_author" content="Taylor Childers">
<meta name="citation_publication_date" content="2025-07-17">
<meta name="citation_cover_date" content="2025-07-17">
<meta name="citation_year" content="2025">
<meta name="citation_online_date" content="2025-07-17">
<meta name="citation_fulltext_html_url" content="https://samforeman.me/content/01-neural-networks/1-mnist/">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=HiPerRAG: High-performance retrieval augmented generation for scientific insights;,citation_author=Ozan Gokdemir;,citation_author=Carlo Siebenschuh;,citation_author=Alexander Brace;,citation_author=Azton Wells;,citation_author=Brian Hsu;,citation_author=Kyle Hippe;,citation_author=Priyanka V. Setty;,citation_author=Aswathy Ajith;,citation_author=J. Gregory Pauloski;,citation_author=Varuni Sastry;,citation_author=Sam Foreman;,citation_author=Huihuo Zheng;,citation_author=Heng Ma;,citation_author=Bharat Kale;,citation_author=Nicholas Chia;,citation_author=Thomas Gibbs;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Francis J. Alexander;,citation_author=Anima Anandkumar;,citation_author=Ian Foster;,citation_author=Rick Stevens;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://arxiv.org/abs/2505.04846;">
<meta name="citation_reference" content="citation_title=MOFA: Discovering materials for carbon capture with a GenAI- and simulation-based workflow;,citation_author=Xiaoli Yan;,citation_author=Nathaniel Hudson;,citation_author=Hyun Park;,citation_author=Daniel Grzenda;,citation_author=J. Gregory Pauloski;,citation_author=Marcus Schwarting;,citation_author=Haochen Pan;,citation_author=Hassan Harb;,citation_author=Samuel Foreman;,citation_author=Chris Knight;,citation_author=Tom Gibbs;,citation_author=Kyle Chard;,citation_author=Santanu Chaudhuri;,citation_author=Emad Tajkhorshid;,citation_author=Ian Foster;,citation_author=Mohamad Moosavi;,citation_author=Logan Ward;,citation_author=E. A. Huerta;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://arxiv.org/abs/2501.10651;">
<meta name="citation_reference" content="citation_title=MProt-DPO: Breaking the ExaFLOPS barrier for multimodal protein design workflows with direct preference optimization;,citation_abstract=We present a scalable, end-to-end workflow for protein design. By augmenting protein sequences with natural language descriptions of their biochemical properties, we train generative models that can be preferentially aligned with protein fitness landscapes. Through complex experimental- and simulation-based observations, we integrate these measures as preferred parameters for generating new protein variants and demonstrate our workflow on five diverse supercomputers. We achieve &amp;amp;amp;gt;1 ExaFLOPS sustained performance in mixed precision on each supercomputer and a maximum sustained performance of 4.11 ExaFLOPS and peak performance of 5.57 ExaFLOPS. We establish the scientific performance of our model on two tasks: (1) across a predetermined benchmark dataset of deep mutational scanning experiments to optimize the fitness-determining mutations in the yeast protein HIS7, and (2) in optimizing the design of the enzyme malate dehydrogenase to achieve lower activation barriers (and therefore increased catalytic rates) using simulation data. Our implementation thus sets high watermarks for multimodal protein design workflows.;,citation_author=Gautham Dharuman;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Sam Foreman;,citation_author=Väinö Hatanpää;,citation_author=Varuni K. Sastry;,citation_author=Huihuo Zheng;,citation_author=Logan Ward;,citation_author=Servesh Muralidharan;,citation_author=Archit Vasan;,citation_author=Bharat Kale;,citation_author=Carla M. Mann;,citation_author=Heng Ma;,citation_author=Yun-Hsuan Cheng;,citation_author=Yuliana Zamora;,citation_author=Shengchao Liu;,citation_author=Chaowei Xiao;,citation_author=Murali Emani;,citation_author=Tom Gibbs;,citation_author=Mahidhar Tatineni;,citation_author=Deepak Canchi;,citation_author=Jerome Mitchell;,citation_author=Koichi Yamada;,citation_author=Maria Garzaran;,citation_author=Michael E. Papka;,citation_author=Ian Foster;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://doi.org/10.1109/SC41406.2024.00013;,citation_doi=10.1109/SC41406.2024.00013;,citation_isbn=9798350352917;,citation_conference_title=Proceedings of the international conference for high performance computing, networking, storage, and analysis;,citation_conference=IEEE Press;,citation_series_title=SC ’24;">
<meta name="citation_reference" content="citation_title=Quality measures for dynamic graph generative models;,citation_author=Ryien Hosseini;,citation_author=Filippo Simini;,citation_author=Venkatram Vishwanath;,citation_author=Rebecca Willett;,citation_author=Henry Hoffmann;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://openreview.net/forum?id=8bjspmAMBk;,citation_conference_title=The thirteenth international conference on learning representations;">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=RG-inspired machine learning for lattice field theory;,citation_author=Sam Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_volume=175;,citation_conference_title=EPJ web of conferences;,citation_conference=EDP Sciences;">
<meta name="citation_reference" content="citation_title=Large energy density in three-plate nanocapacitors due to coulomb blockade;,citation_author=A Hubler;,citation_author=S Foreman;,citation_author=J Liu;,citation_author=L Wortsmann;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=10;,citation_volume=123;,citation_journal_title=Journal of Applied Physics;,citation_publisher=AIP Publishing;">
<meta name="citation_reference" content="citation_title=Examples of renormalization group transformations for image sets;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=5;,citation_volume=98;,citation_journal_title=Physical Review E;,citation_publisher=American Physical Society;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the Ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.22323/1.334.0245;,citation_volume=LATTICE2018;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=Lattice 2018;">
<meta name="citation_reference" content="citation_title=Learning better physics: A machine learning approach to lattice gauge theory;,citation_author=Samuel Alfred Foreman;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_dissertation_institution=University of Iowa;">
<meta name="citation_reference" content="citation_title=Machine learning and neural networks for field theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=HMC with normalizing flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_journal_title=arXiv preprint arXiv:2112.01586;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy storage in quantum resonators;,citation_author=Jiaqi Liu;,citation_author=Alfred W Hubler;,citation_author=Samuel Alfred Foreman;,citation_author=Katharina Ott;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Applications of machine learning to lattice quantum field theory;,citation_author=Denis Boyda;,citation_author=Salvatore Calı̀;,citation_author=Sam Foreman;,citation_author=Lena Funcke;,citation_author=Daniel C Hackett;,citation_author=Yin Lin;,citation_author=Gert Aarts;,citation_author=Andrei Alexandru;,citation_author=Xiao-Yong Jin;,citation_author=Biagio Lucini;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_journal_title=arXiv preprint arXiv:2202.05838;">
<meta name="citation_reference" content="citation_title=Lattice QCD and particle physics;,citation_author=Andreas S Kronfeld;,citation_author=Tanmoy Bhattacharya;,citation_author=Thomas Blum;,citation_author=Norman H Christ;,citation_author=Carleton DeTar;,citation_author=William Detmold;,citation_author=Robert Edwards;,citation_author=Anna Hasenfratz;,citation_author=Huey-Wen Lin;,citation_author=Swagato Mukherjee;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2207.07641;,citation_journal_title=arXiv preprint arXiv:2207.07641;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=6;,citation_volume=37;,citation_journal_title=The International Journal of High Performance Computing Applications;,citation_publisher=SAGE Publications Sage UK: London, England;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=The international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=A comprehensive performance study of large language models on novel AI accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04607;,citation_journal_title=arXiv preprint arXiv:2310.04607;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04610;,citation_journal_title=arXiv preprint arXiv:2310.04610;">
<meta name="citation_reference" content="citation_title=Protein generation via genome-scale language models with bio-physical scoring;,citation_author=Gautham Dharuman;,citation_author=Logan Ward;,citation_author=Heng Ma;,citation_author=Priyanka V Setty;,citation_author=Ozan Gokdemir;,citation_author=Sam Foreman;,citation_author=Murali Emani;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Kristopher Keipert;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=Proceedings of the SC’23 workshops of the international conference on high performance computing, network, storage, and analysis;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2312.08936;,citation_journal_title=arXiv preprint arXiv:2312.08936;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 computational frontier CompF03 topical group report: Machine learning;,citation_author=Phiala Shanahan;,citation_author=Kazuhiro Terao;,citation_author=Daniel Whiteson;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;,citation_journal_title=arXiv preprint arXiv:2209.07559;">
<meta name="citation_reference" content="citation_title=Thorough characterization and analysis of large transformer model training at-scale;,citation_author=Scott Cheng;,citation_author=Jun-Liang Lin;,citation_author=Murali Emani;,citation_author=Siddhisanket Raskar;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Venkatram Vishwanath;,citation_author=Mahmut Taylan Kandemir;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=8;,citation_journal_title=Proceedings of the ACM on Measurement and Analysis of Computing Systems;,citation_publisher=ACM New York, NY, USA;">
<meta name="citation_reference" content="citation_title=Communities through energy justice projects;,citation_author=Mary Ann Leung;,citation_author=Katharine Cahill;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois Curfman McInnes;,citation_author=Suzanne Parete-Koon;,citation_author=Subil Abraham;,citation_author=Lacy Beach Barrier;,citation_author=Gladys Chen;,citation_author=Lizanne DeStefano;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science;">
<meta name="citation_reference" content="citation_title=Applications of a foundation model approach for weather and climate;,citation_author=Troy Arcomano;,citation_author=Alexander Wikner;,citation_author=Romit Maulik;,citation_author=Veerabhadra Rao Kotamarthi;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_volume=2023;,citation_conference_title=AGU fall meeting abstracts;">
<meta name="citation_reference" content="citation_title=Toward a holistic performance evaluation of large language models across diverse ai accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_author=Sanjif Shanmugavelu;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 IEEE international parallel and distributed processing symposium workshops (IPDPSW);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Intro to HPC bootcamp: Engaging new communities through energy justice projects;,citation_author=Suzanne Parete-Koon;,citation_author=Michael Sandoval;,citation_author=Kellen Leland;,citation_author=Subil Abraham;,citation_author=Mary Ann Leung;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois McInnes;,citation_author=Sreeranjani Ramprakash;,citation_author=Lacy Beach Barrier;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science Education;,citation_publisher=Oak Ridge National Laboratory (ORNL), Oak Ridge, TN (United States);">
<meta name="citation_reference" content="citation_title=MProt-DPO: Breaking the ExaFLOPS barrier for multimodal protein design workflows with direct preference optimization;,citation_author=Gautham Dharuman;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Sam Foreman;,citation_author=Väinä Hatanpää;,citation_author=Varuni K Sastry;,citation_author=Huihuo Zheng;,citation_author=Logan Ward;,citation_author=Servesh Muralidharan;,citation_author=Archit Vasan;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 SC24: International conference for high performance computing, networking, storage and analysis SC;,citation_conference=IEEE Computer Society;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=Connor Holmes;,citation_author=Martin Cai;,citation_author=Adam Ghanem;,citation_author=Zhongzhu Zhou;,citation_author=Yuxiong He;,citation_author=Pete Luferenko;,citation_author=Divya Kumar;,citation_author=Jonathan Weyn;,citation_author=Ruixiong Zhang;,citation_author=Sylwester Klocek;,citation_author=Volodymyr Vragov;,citation_author=Mohammed AlQuraishi;,citation_author=Gustaf Ahdritz;,citation_author=Christina Floristean;,citation_author=Cristina Negri;,citation_author=Rao Kotamarthi;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_author=Sam Foreman;,citation_author=Kyle Hippe;,citation_author=Troy Arcomano;,citation_author=Romit Maulik;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot;,citation_author=Murali Emani;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Prasanna Balaprakash;,citation_author=Gina Tourassi;,citation_author=John Gounley;,citation_author=Heidi Hanson;,citation_author=Thomas E Potok;,citation_author=Massimiliano Lupo Pasini;,citation_author=Kate Evans;,citation_author=Dan Lu;,citation_author=Dalton Lunga;,citation_author=Junqi Yin;,citation_author=Sajal Dash;,citation_author=Feiyi Wang;,citation_author=Mallikarjun Shankar;,citation_author=Isaac Lyngaas;,citation_author=Xiao Wang;,citation_author=Guojing Cong;,citation_author=Pei Zhang;,citation_author=Ming Fan;,citation_author=Siyan Liu;,citation_author=Adolfy Hoisie;,citation_author=Shinjae Yoo;,citation_author=Yihui Ren;,citation_author=William Tang;,citation_author=Kyle Felker;,citation_author=Alexey Svyatkovskiy;,citation_author=Hang Liu;,citation_author=Ashwin Aji;,citation_author=Angela Dalton;,citation_author=Michael Schulte;,citation_author=Karl Schulz;,citation_author=Yuntian Deng;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Anima Anandkumar;,citation_author=Rick Stevens;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04610;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=The climate risk &amp;amp;amp; resilience portal (ClimRR) metadata and data dictionary;,citation_author=C. Burdi;,citation_author=Wall. T Branham;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://dub.sh/ClimRR-Metadata;">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2105.03418;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy Justice Analysis of Climate Data with ClimRR;,citation_author=Sam Foreman;,citation_publication_date=2023-08-07;,citation_cover_date=2023-08-07;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/climate-analysis;,citation_language=en;">
<meta name="citation_reference" content="citation_author=Sam Foreman;,citation_publication_date=2023-08-19;,citation_cover_date=2023-08-19;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/l2hmc-qcd;,citation_language=en;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James Osborn;,citation_publication_date=00;,citation_cover_date=00;,citation_year=0;,citation_conference_title=40th international symposium on lattice field theory (lattice 2023) (batavia, IL, united states, 07/31/2023 - 08/04/2023);">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=S. Foreman;,citation_author=X. Jin;,citation_author=J. Osborn;,citation_publication_date=2022-07;,citation_cover_date=2022-07;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_conference_title=The 38th international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Mastering language models;,citation_author=Samuel Montgomery;,citation_publication_date=2023-10;,citation_cover_date=2023-10;,citation_year=2023;,citation_fulltext_html_url=https://towardsdatascience.com/mastering-language-models-32e1d891511a
           ;,citation_journal_title=Medium;,citation_publisher=Towards Data Science;">
<meta name="citation_reference" content="citation_title=Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond;,citation_author=Jingfeng Yang;,citation_author=Hongye Jin;,citation_author=Ruixiang Tang;,citation_author=Xiaotian Han;,citation_author=Qizhang Feng;,citation_author=Haoming Jiang;,citation_author=Bing Yin;,citation_author=Xia Hu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2304.13712;">
<meta name="citation_reference" content="citation_title=Training tips for the transformer model;,citation_author=Martin Popel;,citation_author=Ondřej Bojar;,citation_publication_date=2018-04;,citation_cover_date=2018-04;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.2478%2Fpralin-2018-0002;,citation_issue=1;,citation_doi=10.2478/pralin-2018-0002;,citation_volume=110;,citation_journal_title=The Prague Bulletin of Mathematical Linguistics;,citation_publisher=Charles University in Prague, Karolinum Press;">
<meta name="citation_reference" content="citation_title=Attention is all you need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N. Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03762;">
<meta name="citation_reference" content="citation_title=Tree of thoughts: Deliberate problem solving with large language models;,citation_author=Shunyu Yao;,citation_author=Dian Yu;,citation_author=Jeffrey Zhao;,citation_author=Izhak Shafran;,citation_author=Thomas L. Griffiths;,citation_author=Yuan Cao;,citation_author=Karthik Narasimhan;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2305.10601;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_abstract=We seek to transform how new and emergent variants of pandemiccausing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.Competing Interest StatementThe authors have declared no competing interest.;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot-Sasson;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.biorxiv.org/content/early/2022/11/23/2022.10.10.511571;,citation_doi=10.1101/2022.10.10.511571;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
</head>

<body class="nav-sidebar floating nav-fixed quarto-dark"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = true;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Sam Foreman</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/saforem2/intro-hpc-bootcamp-2025"> 
<span class="menu-text"><span class="icon" style="font-size: 1.25rem; color:var(--bs-nav-link-color);"><iconify-icon role="img" inline="" icon="ph:github-logo" aria-label="Icon github-logo from ph Iconify.design set." title="Icon github-logo from ph Iconify.design set."></iconify-icon></span></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../index.xml"> 
<span class="menu-text"><span class="icon" style="font-size: 1.25rem; color:var(--bs-nav-link-color);"><iconify-icon role="img" inline="" icon="ph:rss" aria-label="Icon rss from ph Iconify.design set." title="Icon rss from ph Iconify.design set."></iconify-icon></span></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../content/01-neural-networks/index.html">[01] Neural Networks</a></li><li class="breadcrumb-item"><a href="../../../content/01-neural-networks/1-mnist/index.html">[1] MNIST Example</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="../../../index.html" class="sidebar-logo-link">
      </a>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../content/00-intro-AI-HPC/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[00] Intro to AI and HPC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../content/00-intro-AI-HPC/0-compute-systems/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] Compute Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../content/00-intro-AI-HPC/1-shared-resources/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[1] Shared Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../content/00-intro-AI-HPC/2-jupyter-notebooks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[2] Jupyter Notebooks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../content/00-intro-AI-HPC/3-homework/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[3] Homework</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../content/00-intro-AI-HPC/4-nersc/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[4] NERSC</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../content/00-intro-AI-HPC/5-mcmc-example/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[5] MCMC Example</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../content/00-intro-AI-HPC/6-linear-regression/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[6] Linear Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../content/01-neural-networks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[01] Neural Networks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../content/01-neural-networks/0-intro/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] Intro to NNs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../content/01-neural-networks/1-mnist/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">[1] MNIST Example</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../content/01-neural-networks/2-advanced/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[2] Advanced</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../content/01-neural-networks/3-conv-nets/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[3] Conv. Nets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../content/01-neural-networks/4-representation-learning/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[4] Representation Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../content/02-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[02] Large Language Models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../content/02-llms/0-intro-to-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] Intro to LLMs</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-mnist-dataset" id="toc-the-mnist-dataset" class="nav-link active" data-scroll-target="#the-mnist-dataset">The MNIST dataset</a></li>
  <li><a href="#generalities" id="toc-generalities" class="nav-link" data-scroll-target="#generalities">Generalities:</a></li>
  <li><a href="#linear-model" id="toc-linear-model" class="nav-link" data-scroll-target="#linear-model">Linear Model</a></li>
  <li><a href="#learning" id="toc-learning" class="nav-link" data-scroll-target="#learning">Learning</a></li>
  <li><a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction">Prediction</a></li>
  <li><a href="#multilayer-model" id="toc-multilayer-model" class="nav-link" data-scroll-target="#multilayer-model">Multilayer Model</a></li>
  <li><a href="#important-things-to-know" id="toc-important-things-to-know" class="nav-link" data-scroll-target="#important-things-to-know">Important things to know</a></li>
  <li><a href="#recap" id="toc-recap" class="nav-link" data-scroll-target="#recap">Recap</a></li>
  <li><a href="#homework" id="toc-homework" class="nav-link" data-scroll-target="#homework">Homework</a></li>
  <li><a href="#homework-solution" id="toc-homework-solution" class="nav-link" data-scroll-target="#homework-solution">Homework solution</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/blob/main/content/01-neural-networks/1-mnist/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/edit/main/content/01-neural-networks/1-mnist/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.md"><i class="bi bi-file-code"></i>Github (GFM)</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TC329HJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../content/01-neural-networks/index.html">[01] Neural Networks</a></li><li class="breadcrumb-item"><a href="../../../content/01-neural-networks/1-mnist/index.html">[1] MNIST Example</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Intro to NNs: MNIST</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>

<div>
  <div class="description">
    Introduction to Neural Networks with MNIST dataset
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Sam Foreman <a href="mailto:foremans@anl.gov" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-9981-0876" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://alcf.anl.gov/about/people/sam-foreman">
            </a><a href="https://www.anl.gov/">ANL</a>
            
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Marieme Ngom <a href="mailto:mngom@anl.gov" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://alcf.anl.gov/about/people/marieme-ngom">
            </a><a href="https://www.anl.gov/">ANL</a>
            
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Huihuo Zheng <a href="mailto:huihuo.zheng@anl.gov" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://alcf.anl.gov/about/people/huihuo-zheng">
            </a><a href="https://www.anl.gov/">ANL</a>
            
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Bethany Lusch <a href="mailto:blusch@anl.gov" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-9521-9990" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://alcf.anl.gov/about/people/bethany-lusch">
            </a><a href="https://www.anl.gov/">ANL</a>
            
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Taylor Childers <a href="mailto:jchilders@anl.gov" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-0492-613X" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://alcf.anl.gov/about/people/taylor-childers">
            </a><a href="https://www.anl.gov/">ANL</a>
            
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 17, 2025</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">July 22, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<p>Author: Marieme Ngom, adapting materials from Bethany Lusch, Asad Khan, Prasanna Balaprakash, Taylor Childers, Corey Adams, Kyle Felker, and Tanwi Mallick.</p>
<p>This tutorial will serve as a gentle introduction to neural networks and deep learning through a hands-on classification problem using the MNIST dataset.</p>
<p>In particular, we will introduce neural networks and how to train and improve their learning capabilities. We will use the PyTorch Python library.</p>
<p>The <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a> contains thousands of examples of handwritten numbers, with each digit labeled 0-9.</p>
<div id="fig-mnist-task" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mnist-task-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../images/mnist_task.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: MNIST sample"><img src="../images/mnist_task.png" class="img-fluid figure-img" width="400"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mnist-task-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: MNIST sample
</figcaption>
</figure>
</div>
<div id="79321b26" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> torch</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> torchvision</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="im">import</span> numpy </span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="im">import</span> time</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="the-mnist-dataset" class="level2">
<h2 class="anchored" data-anchor-id="the-mnist-dataset">The MNIST dataset</h2>
<p>We will now download the dataset that contains handwritten digits. MNIST is a popular dataset, so we can download it via the PyTorch library. Note: - x is for the inputs (images of handwritten digits) and y is for the labels or outputs (digits 0-9) - We are given “training” and “test” datasets. Training datasets are used to fit the model. Test datasets are saved until the end, when we are satisfied with our model, to estimate how well our model generalizes to new data.</p>
<p>Note that downloading it the first time might take some time. The data is split as follows: - 60,000 training examples, 10,000 test examples - inputs: 1 x 28 x 28 pixels - outputs (labels): one integer per example</p>
<div id="daa2b123" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>training_data <span class="op">=</span> torchvision.datasets.MNIST(</span>
<span id="cb2-2"><a href="#cb2-2"></a>    root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb2-3"><a href="#cb2-3"></a>    train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-4"><a href="#cb2-4"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-5"><a href="#cb2-5"></a>    transform<span class="op">=</span>torchvision.transforms.ToTensor()</span>
<span id="cb2-6"><a href="#cb2-6"></a>)</span>
<span id="cb2-7"><a href="#cb2-7"></a></span>
<span id="cb2-8"><a href="#cb2-8"></a>test_data <span class="op">=</span> torchvision.datasets.MNIST(</span>
<span id="cb2-9"><a href="#cb2-9"></a>    root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb2-10"><a href="#cb2-10"></a>    train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb2-11"><a href="#cb2-11"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-12"><a href="#cb2-12"></a>    transform<span class="op">=</span>torchvision.transforms.ToTensor()</span>
<span id="cb2-13"><a href="#cb2-13"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="685d292e" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(training_data))  <span class="co"># 80% for training</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>val_size <span class="op">=</span> <span class="bu">len</span>(training_data) <span class="op">-</span> train_size  <span class="co"># Remaining 20% for validation</span></span>
<span id="cb3-3"><a href="#cb3-3"></a>training_data, validation_data <span class="op">=</span> torch.utils.data.random_split(</span>
<span id="cb3-4"><a href="#cb3-4"></a>    training_data,</span>
<span id="cb3-5"><a href="#cb3-5"></a>    [train_size, val_size],</span>
<span id="cb3-6"><a href="#cb3-6"></a>    generator<span class="op">=</span>torch.Generator().manual_seed(<span class="dv">55</span>)</span>
<span id="cb3-7"><a href="#cb3-7"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="a0f805f5" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="bu">print</span>(<span class="st">'MNIST data loaded: train:'</span>,<span class="bu">len</span>(training_data),<span class="st">' examples, validation: '</span>, <span class="bu">len</span>(validation_data), <span class="st">'examples, test:'</span>,<span class="bu">len</span>(test_data), <span class="st">'examples'</span>)</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="bu">print</span>(<span class="st">'Input shape'</span>, training_data[<span class="dv">0</span>][<span class="dv">0</span>].shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>MNIST data loaded: train: 48000  examples, validation:  12000 examples, test: 10000 examples
Input shape torch.Size([1, 28, 28])</code></pre>
</div>
</div>
<p>Let’s take a closer look. Here are the first 10 training digits:</p>
<div id="1368aef1" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>pltsize<span class="op">=</span><span class="dv">1</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span><span class="op">*</span>pltsize, pltsize))</span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb6-5"><a href="#cb6-5"></a>    plt.subplot(<span class="dv">1</span>,<span class="dv">10</span>,i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb6-6"><a href="#cb6-6"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb6-7"><a href="#cb6-7"></a>    plt.imshow(numpy.reshape(training_data[i][<span class="dv">0</span>], (<span class="dv">28</span>, <span class="dv">28</span>)), cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb6-8"><a href="#cb6-8"></a>    plt.title(<span class="st">'Class: '</span><span class="op">+</span><span class="bu">str</span>(training_data[i][<span class="dv">1</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-6-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="index_files/figure-html/cell-6-output-1.png" width="763" height="102" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="generalities" class="level2">
<h2 class="anchored" data-anchor-id="generalities">Generalities:</h2>
<p>To train our classifier, we need (besides the data): - A model that depend on parameters <span class="math inline">\mathbf{\theta}</span>. Here we are going to use neural networks. - A loss function <span class="math inline">J(\mathbf{\theta})</span> to measure the capabilities of the model. - An optimization method.</p>
</section>
<section id="linear-model" class="level2">
<h2 class="anchored" data-anchor-id="linear-model">Linear Model</h2>
<p>Let’s begin with a simple linear model: linear regression, like last week. We add one complication: each example is a vector (flattened image), so the “slope” multiplication becomes a dot product. If the target output is a vector as well, then the multiplication becomes matrix multiplication.</p>
<p>Note, like before, we consider multiple examples at once, adding another dimension to the input.</p>
<div id="fig-linear-model" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-linear-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../images/LinearModel_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;2: Linear model for classification"><img src="../images/LinearModel_1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-linear-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Linear model for classification
</figcaption>
</figure>
</div>
<p>The linear layers in PyTorch perform a basic <span class="math inline">xW + b</span>. These “fully connected” layers connect each input to each output with some weight parameter. We wouldn’t expect a simple linear model <span class="math inline">f(x) = xW+b</span> directly outputting the class label and minimizing mean squared error to work well - the model would output labels like 3.55 and 2.11 instead of skipping to integers.</p>
<p>We now need: - A loss function <span class="math inline">J(\theta)</span> where <span class="math inline">\theta</span> is the list of parameters (here W and b). Last week, we used mean squared error (MSE), but this week let’s make two changes that make more sense for classification: - Change the output to be a length-10 vector of class probabilities (0 to 1, adding to 1). - Cross entropy as the loss function, which is typical for classification. You can read more <a href="https://gombru.github.io/2018/05/23/cross_entropy_loss/">here</a>.</p>
<ul>
<li><p>An optimization method or optimizer such as the stochastic gradient descent (sgd) method, the Adam optimizer, RMSprop, Adagrad etc. Let’s start with stochastic gradient descent (sgd), like last week. For far more information about more advanced optimizers than basic SGD, with some cool animations, see https://ruder.io/optimizing-gradient-descent/ or https://distill.pub/2017/momentum/.</p></li>
<li><p>A learning rate. As we learned last week, the learning rate controls how far we move during each step.</p></li>
</ul>
<div id="7e467f33" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">class</span> LinearClassifier(nn.Module):</span>
<span id="cb7-2"><a href="#cb7-2"></a></span>
<span id="cb7-3"><a href="#cb7-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb7-4"><a href="#cb7-4"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-5"><a href="#cb7-5"></a>        </span>
<span id="cb7-6"><a href="#cb7-6"></a>        <span class="co"># First, we need to convert the input image to a vector by using </span></span>
<span id="cb7-7"><a href="#cb7-7"></a>        <span class="co"># nn.Flatten(). For MNIST, it means the second dimension 28*28 becomes 784.</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb7-9"><a href="#cb7-9"></a>        </span>
<span id="cb7-10"><a href="#cb7-10"></a>        <span class="co"># Here, we add a fully connected ("dense") layer that has 28 x 28 = 784 input nodes </span></span>
<span id="cb7-11"><a href="#cb7-11"></a>        <span class="co">#(one for each pixel in the input image) and 10 output nodes (for probabilities of each class).</span></span>
<span id="cb7-12"><a href="#cb7-12"></a>        <span class="va">self</span>.layer_1 <span class="op">=</span> nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">10</span>)</span>
<span id="cb7-13"><a href="#cb7-13"></a>        </span>
<span id="cb7-14"><a href="#cb7-14"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-15"><a href="#cb7-15"></a></span>
<span id="cb7-16"><a href="#cb7-16"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb7-17"><a href="#cb7-17"></a>        x <span class="op">=</span> <span class="va">self</span>.layer_1(x)</span>
<span id="cb7-18"><a href="#cb7-18"></a></span>
<span id="cb7-19"><a href="#cb7-19"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="35a01b62" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>linear_model <span class="op">=</span> LinearClassifier()</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="bu">print</span>(linear_model)</span>
<span id="cb8-3"><a href="#cb8-3"></a></span>
<span id="cb8-4"><a href="#cb8-4"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb8-5"><a href="#cb8-5"></a>optimizer <span class="op">=</span> torch.optim.SGD(linear_model.parameters(), lr<span class="op">=</span><span class="fl">0.05</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>LinearClassifier(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (layer_1): Linear(in_features=784, out_features=10, bias=True)
)</code></pre>
</div>
</div>
</section>
<section id="learning" class="level2">
<h2 class="anchored" data-anchor-id="learning">Learning</h2>
<p>Now we are ready to train our first model. A training step is comprised of: - A forward pass: the input is passed through the network - Backpropagation: A backward pass to compute the gradient <span class="math inline">\frac{\partial J}{\partial \mathbf{W}}</span> of the loss function with respect to the parameters of the network. - Weight updates $ = - $ where <span class="math inline">\alpha</span> is the learning rate.</p>
<p>How many steps do we take? - The batch size corresponds to the number of training examples in one pass (forward + backward). A smaller batch size allows the model to learn from individual examples but takes longer to train. A larger batch size requires fewer steps but may result in the model not capturing the nuances in the data. The higher the batch size, the more memory you will require.<br>
- An epoch means one pass through the whole training data (looping over the batches). Using few epochs can lead to underfitting and using too many can lead to overfitting. - The choice of batch size and learning rate are important for performance, generalization and accuracy in deep learning.</p>
<div id="e103fd99" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb10-2"><a href="#cb10-2"></a></span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="co"># The dataloader makes our dataset iterable </span></span>
<span id="cb10-4"><a href="#cb10-4"></a>train_dataloader <span class="op">=</span> torch.utils.data.DataLoader(training_data, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb10-5"><a href="#cb10-5"></a>val_dataloader <span class="op">=</span> torch.utils.data.DataLoader(validation_data, batch_size<span class="op">=</span>batch_size)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="86d25408" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="kw">def</span> train_one_epoch(dataloader, model, loss_fn, optimizer):</span>
<span id="cb11-2"><a href="#cb11-2"></a>    model.train()</span>
<span id="cb11-3"><a href="#cb11-3"></a>    <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb11-4"><a href="#cb11-4"></a>        <span class="co"># forward pass</span></span>
<span id="cb11-5"><a href="#cb11-5"></a>        pred <span class="op">=</span> model(X)</span>
<span id="cb11-6"><a href="#cb11-6"></a>        loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb11-7"><a href="#cb11-7"></a>        </span>
<span id="cb11-8"><a href="#cb11-8"></a>        <span class="co"># backward pass calculates gradients</span></span>
<span id="cb11-9"><a href="#cb11-9"></a>        loss.backward()</span>
<span id="cb11-10"><a href="#cb11-10"></a>        </span>
<span id="cb11-11"><a href="#cb11-11"></a>        <span class="co"># take one step with these gradients</span></span>
<span id="cb11-12"><a href="#cb11-12"></a>        optimizer.step()</span>
<span id="cb11-13"><a href="#cb11-13"></a>        </span>
<span id="cb11-14"><a href="#cb11-14"></a>        <span class="co"># resets the gradients </span></span>
<span id="cb11-15"><a href="#cb11-15"></a>        optimizer.zero_grad()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="b1848102" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="kw">def</span> evaluate(dataloader, model, loss_fn):</span>
<span id="cb12-2"><a href="#cb12-2"></a>    <span class="co"># Set the model to evaluation mode - some NN pieces behave differently during training</span></span>
<span id="cb12-3"><a href="#cb12-3"></a>    <span class="co"># Unnecessary in this situation but added for best practices</span></span>
<span id="cb12-4"><a href="#cb12-4"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb12-5"><a href="#cb12-5"></a>    size <span class="op">=</span> <span class="bu">len</span>(dataloader.dataset)</span>
<span id="cb12-6"><a href="#cb12-6"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb12-7"><a href="#cb12-7"></a>    loss, correct <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb12-8"><a href="#cb12-8"></a></span>
<span id="cb12-9"><a href="#cb12-9"></a>    <span class="co"># We can save computation and memory by not calculating gradients here - we aren't optimizing </span></span>
<span id="cb12-10"><a href="#cb12-10"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-11"><a href="#cb12-11"></a>        <span class="co"># loop over all of the batches</span></span>
<span id="cb12-12"><a href="#cb12-12"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> dataloader:</span>
<span id="cb12-13"><a href="#cb12-13"></a>            pred <span class="op">=</span> model(X)</span>
<span id="cb12-14"><a href="#cb12-14"></a>            loss <span class="op">+=</span> loss_fn(pred, y).item()</span>
<span id="cb12-15"><a href="#cb12-15"></a>            <span class="co"># how many are correct in this batch? Tracking for accuracy </span></span>
<span id="cb12-16"><a href="#cb12-16"></a>            correct <span class="op">+=</span> (pred.argmax(<span class="dv">1</span>) <span class="op">==</span> y).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>().item()</span>
<span id="cb12-17"><a href="#cb12-17"></a></span>
<span id="cb12-18"><a href="#cb12-18"></a>    loss <span class="op">/=</span> num_batches</span>
<span id="cb12-19"><a href="#cb12-19"></a>    correct <span class="op">/=</span> size</span>
<span id="cb12-20"><a href="#cb12-20"></a>    </span>
<span id="cb12-21"><a href="#cb12-21"></a>    accuracy <span class="op">=</span> <span class="dv">100</span><span class="op">*</span>correct</span>
<span id="cb12-22"><a href="#cb12-22"></a>    <span class="cf">return</span> accuracy, loss</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="9c4b529c" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="op">%%</span>time</span>
<span id="cb13-2"><a href="#cb13-2"></a></span>
<span id="cb13-3"><a href="#cb13-3"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb13-4"><a href="#cb13-4"></a>train_acc_all <span class="op">=</span> []</span>
<span id="cb13-5"><a href="#cb13-5"></a>val_acc_all <span class="op">=</span> []</span>
<span id="cb13-6"><a href="#cb13-6"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb13-7"><a href="#cb13-7"></a>    train_one_epoch(train_dataloader, linear_model, loss_fn, optimizer)</span>
<span id="cb13-8"><a href="#cb13-8"></a>    </span>
<span id="cb13-9"><a href="#cb13-9"></a>    <span class="co"># checking on the training loss and accuracy once per epoch</span></span>
<span id="cb13-10"><a href="#cb13-10"></a>    acc, loss <span class="op">=</span> evaluate(train_dataloader, linear_model, loss_fn)</span>
<span id="cb13-11"><a href="#cb13-11"></a>    train_acc_all.append(acc)</span>
<span id="cb13-12"><a href="#cb13-12"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">: training loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">, accuracy: </span><span class="sc">{</span>acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-13"><a href="#cb13-13"></a>    </span>
<span id="cb13-14"><a href="#cb13-14"></a>    <span class="co"># checking on the validation loss and accuracy once per epoch</span></span>
<span id="cb13-15"><a href="#cb13-15"></a>    val_acc, val_loss <span class="op">=</span> evaluate(val_dataloader, linear_model, loss_fn)</span>
<span id="cb13-16"><a href="#cb13-16"></a>    val_acc_all.append(val_acc)</span>
<span id="cb13-17"><a href="#cb13-17"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">: val. loss: </span><span class="sc">{</span>val_loss<span class="sc">}</span><span class="ss">, val. accuracy: </span><span class="sc">{</span>val_acc<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0: training loss: 0.5028048004309337, accuracy: 87.58333333333333
Epoch 0: val. loss: 0.49476587328505006, val. accuracy: 87.73333333333333
Epoch 1: training loss: 0.4222778307596842, accuracy: 88.98125
Epoch 1: val. loss: 0.41275818772772527, val. accuracy: 88.81666666666666
Epoch 2: training loss: 0.3881796567837397, accuracy: 89.63125
Epoch 2: val. loss: 0.3781177671031749, val. accuracy: 89.47500000000001
Epoch 3: training loss: 0.3681236910422643, accuracy: 90.1375
Epoch 3: val. loss: 0.3578927379022253, val. accuracy: 89.98333333333333
Epoch 4: training loss: 0.3544792279402415, accuracy: 90.40208333333334
Epoch 4: val. loss: 0.34426142243628804, val. accuracy: 90.20833333333333
CPU times: user 11.1 s, sys: 724 ms, total: 11.9 s
Wall time: 11.5 s</code></pre>
</div>
</div>
<div id="8cbd6a6b" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>pltsize<span class="op">=</span><span class="dv">1</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span><span class="op">*</span>pltsize, <span class="dv">10</span> <span class="op">*</span> pltsize))</span>
<span id="cb15-3"><a href="#cb15-3"></a>plt.plot(<span class="bu">range</span>(epochs), train_acc_all,label <span class="op">=</span> <span class="st">'Training Acc.'</span> )</span>
<span id="cb15-4"><a href="#cb15-4"></a>plt.plot(<span class="bu">range</span>(epochs), val_acc_all, label <span class="op">=</span> <span class="st">'Validation Acc.'</span> )</span>
<span id="cb15-5"><a href="#cb15-5"></a>plt.xlabel(<span class="st">'Epoch #'</span>)</span>
<span id="cb15-6"><a href="#cb15-6"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb15-7"><a href="#cb15-7"></a>plt.legend()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-13-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="index_files/figure-html/cell-13-output-1.png" width="821" height="799" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<div id="ced7094f" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># Visualize how the model is doing on the first 10 examples</span></span>
<span id="cb16-2"><a href="#cb16-2"></a>pltsize<span class="op">=</span><span class="dv">1</span></span>
<span id="cb16-3"><a href="#cb16-3"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span><span class="op">*</span>pltsize, pltsize))</span>
<span id="cb16-4"><a href="#cb16-4"></a>linear_model.<span class="bu">eval</span>()</span>
<span id="cb16-5"><a href="#cb16-5"></a>batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb16-6"><a href="#cb16-6"></a>predictions <span class="op">=</span> linear_model(batch[<span class="dv">0</span>])</span>
<span id="cb16-7"><a href="#cb16-7"></a></span>
<span id="cb16-8"><a href="#cb16-8"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb16-9"><a href="#cb16-9"></a>    plt.subplot(<span class="dv">1</span>,<span class="dv">10</span>,i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb16-10"><a href="#cb16-10"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb16-11"><a href="#cb16-11"></a>    plt.imshow(batch[<span class="dv">0</span>][i,<span class="dv">0</span>,:,:], cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb16-12"><a href="#cb16-12"></a>    plt.title(<span class="st">'</span><span class="sc">%d</span><span class="st">'</span> <span class="op">%</span> predictions[i,:].argmax())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-14-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="index_files/figure-html/cell-14-output-1.png" width="763" height="102" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Exercise: How can you improve the accuracy? Some things you might consider: increasing the number of epochs, changing the learning rate, etc.</p>
</section>
<section id="prediction" class="level2">
<h2 class="anchored" data-anchor-id="prediction">Prediction</h2>
<p>Let’s see how our model generalizes to the unseen test data.</p>
<div id="a285a352" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="co">#For HW: cell to change batch size</span></span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="co">#create dataloader for test data</span></span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="co"># The dataloader makes our dataset iterable</span></span>
<span id="cb17-4"><a href="#cb17-4"></a></span>
<span id="cb17-5"><a href="#cb17-5"></a>batch_size_test <span class="op">=</span> <span class="dv">256</span> </span>
<span id="cb17-6"><a href="#cb17-6"></a>test_dataloader <span class="op">=</span> torch.utils.data.DataLoader(test_data, batch_size<span class="op">=</span>batch_size_test)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="dd7c3cc9" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>acc_test, loss_test <span class="op">=</span> evaluate(test_dataloader, linear_model, loss_fn)</span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="bu">print</span>(<span class="st">"Test loss: </span><span class="sc">%.4f</span><span class="st">, test accuracy: </span><span class="sc">%.2f%%</span><span class="st">"</span> <span class="op">%</span> (loss_test, acc_test))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test loss: 0.3324, test accuracy: 90.78%</code></pre>
</div>
</div>
<p>We can now take a closer look at the results.</p>
<p>Let’s define a helper function to show the failure cases of our classifier.</p>
<div id="529ecf06" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="kw">def</span> show_failures(model, dataloader, maxtoshow<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb20-2"><a href="#cb20-2"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb20-3"><a href="#cb20-3"></a>    batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader))</span>
<span id="cb20-4"><a href="#cb20-4"></a>    predictions <span class="op">=</span> model(batch[<span class="dv">0</span>])</span>
<span id="cb20-5"><a href="#cb20-5"></a>    </span>
<span id="cb20-6"><a href="#cb20-6"></a>    rounded <span class="op">=</span> predictions.argmax(<span class="dv">1</span>)</span>
<span id="cb20-7"><a href="#cb20-7"></a>    errors <span class="op">=</span> rounded<span class="op">!=</span>batch[<span class="dv">1</span>]</span>
<span id="cb20-8"><a href="#cb20-8"></a>    <span class="bu">print</span>(<span class="st">'Showing max'</span>, maxtoshow, <span class="st">'first failures. '</span></span>
<span id="cb20-9"><a href="#cb20-9"></a>          <span class="st">'The predicted class is shown first and the correct class in parentheses.'</span>)</span>
<span id="cb20-10"><a href="#cb20-10"></a>    ii <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-11"><a href="#cb20-11"></a>    plt.figure(figsize<span class="op">=</span>(maxtoshow, <span class="dv">1</span>))</span>
<span id="cb20-12"><a href="#cb20-12"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch[<span class="dv">0</span>].shape[<span class="dv">0</span>]):</span>
<span id="cb20-13"><a href="#cb20-13"></a>        <span class="cf">if</span> ii<span class="op">&gt;=</span>maxtoshow:</span>
<span id="cb20-14"><a href="#cb20-14"></a>            <span class="cf">break</span></span>
<span id="cb20-15"><a href="#cb20-15"></a>        <span class="cf">if</span> errors[i]:</span>
<span id="cb20-16"><a href="#cb20-16"></a>            plt.subplot(<span class="dv">1</span>, maxtoshow, ii<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb20-17"><a href="#cb20-17"></a>            plt.axis(<span class="st">'off'</span>)</span>
<span id="cb20-18"><a href="#cb20-18"></a>            plt.imshow(batch[<span class="dv">0</span>][i,<span class="dv">0</span>,:,:], cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb20-19"><a href="#cb20-19"></a>            plt.title(<span class="st">"</span><span class="sc">%d</span><span class="st"> (</span><span class="sc">%d</span><span class="st">)"</span> <span class="op">%</span> (rounded[i], batch[<span class="dv">1</span>][i]))</span>
<span id="cb20-20"><a href="#cb20-20"></a>            ii <span class="op">=</span> ii <span class="op">+</span> <span class="dv">1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Here are the first 10 images from the test data that this small model classified to a wrong class:</p>
<div id="03ae7b41" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>show_failures(linear_model, test_dataloader)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Showing max 10 first failures. The predicted class is shown first and the correct class in parentheses.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-18-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="index_files/figure-html/cell-18-output-2.png" width="763" height="102" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="multilayer-model" class="level2">
<h2 class="anchored" data-anchor-id="multilayer-model">Multilayer Model</h2>
<p>Our linear model isn’t enough for high accuracy on this dataset. To improve the model, we often need to add more layers and nonlinearities.</p>
<div id="fig-shallow-nn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-shallow-nn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../images/shallow_nn.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;3: Shallow neural network"><img src="../images/shallow_nn.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-shallow-nn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Shallow neural network
</figcaption>
</figure>
</div>
<p>The output of this NN can be written as <span class="math display">\begin{equation}\label{eq: NN1d}
  \hat{u}(x) = \sigma_2(\sigma_1(\mathbf{x}\mathbf{W}_1 + \mathbf{b}_1)\mathbf{W}_2 + \mathbf{b}_2),
\end{equation}</span> where <span class="math inline">\mathbf{x}</span> is the input, <span class="math inline">\mathbf{W}_j</span> are the weights of the neural network, <span class="math inline">\sigma_j</span> the (nonlinear) activation functions, and <span class="math inline">\mathbf{b}_j</span> its biases. The activation function introduces the nonlinearity and makes it possible to learn more complex tasks. Desirable properties in an activation function include being differentiable, bounded, and monotonic.</p>
<p>Image source: <a href="https://www.v7labs.com/blog/neural-networks-activation-functions">PragatiBaheti</a></p>
<div id="fig-activation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-activation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../images/activation.jpeg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;4: Activation function"><img src="../images/activation.jpeg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-activation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Activation function
</figcaption>
</figure>
</div>
<p>Adding more layers to obtain a deep neural network:</p>
<div id="fig-nn-annotated" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-annotated-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../images/deep_nn_annotated.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;5: "><img src="../images/deep_nn_annotated.jpg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-nn-annotated-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5
</figcaption>
</figure>
</div>
</section>
<section id="important-things-to-know" class="level2">
<h2 class="anchored" data-anchor-id="important-things-to-know">Important things to know</h2>
<p>Deep Neural networks can be overly flexible/complicated and “overfit” your data, just like fitting overly complicated polynomials:</p>
<div id="fig-bias-variance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../images/bias_vs_variance.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;6: Bias-variance tradeoff"><img src="../images/bias_vs_variance.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Bias-variance tradeoff
</figcaption>
</figure>
</div>
<p>Vizualization wrt to the accuracy and loss (Image source: <a href="https://www.baeldung.com/cs/ml-underfitting-overfitting">Baeldung</a>):</p>
<div id="fig-acc-under-over" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-acc-under-over-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/acc_under_over.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;7: Visualization of accuracy and loss"><img src="./images/acc_under_over.webp" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-acc-under-over-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Visualization of accuracy and loss
</figcaption>
</figure>
</div>
<p>To improve the generalization of our model on previously unseen data, we employ a technique known as regularization, which constrains our optimization problem in order to discourage complex models.</p>
<ul>
<li>Dropout is the commonly used regularization technique. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.</li>
<li>Penalizing the loss function by adding a term such as <span class="math inline">\lambda ||\mathbf{W}||^2</span> is alsp a commonly used regularization technique. This helps “control” the magnitude of the weights of the network.</li>
</ul>
<dl>
<dt>Vanishing gradients</dt>
<dd>
Gradients become small as they propagate backward through the layers.
</dd>
<dd>
Squashing activation functions like sigmoid or tanh could cause this.
</dd>
<dt>Exploding gradients</dt>
<dd>
Gradients grow exponentially usually due to “poor” weight initialization.
</dd>
</dl>
<p>We can now implement a deep network in PyTorch.</p>
<p><code>nn.Dropout()</code> performs the Dropout operation mentioned earlier:</p>
<div id="633391e1" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="co">#For HW: cell to change activation</span></span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="kw">class</span> NonlinearClassifier(nn.Module):</span>
<span id="cb23-3"><a href="#cb23-3"></a></span>
<span id="cb23-4"><a href="#cb23-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb23-5"><a href="#cb23-5"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb23-6"><a href="#cb23-6"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb23-7"><a href="#cb23-7"></a>        <span class="va">self</span>.layers_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb23-8"><a href="#cb23-8"></a>            nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">50</span>),</span>
<span id="cb23-9"><a href="#cb23-9"></a>            nn.ReLU(),</span>
<span id="cb23-10"><a href="#cb23-10"></a>            nn.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb23-11"><a href="#cb23-11"></a>            nn.Linear(<span class="dv">50</span>, <span class="dv">50</span>),</span>
<span id="cb23-12"><a href="#cb23-12"></a>            nn.ReLU(),</span>
<span id="cb23-13"><a href="#cb23-13"></a>           <span class="co"># nn.Dropout(0.2),</span></span>
<span id="cb23-14"><a href="#cb23-14"></a>            nn.Linear(<span class="dv">50</span>, <span class="dv">50</span>),</span>
<span id="cb23-15"><a href="#cb23-15"></a>            nn.ReLU(),</span>
<span id="cb23-16"><a href="#cb23-16"></a>           <span class="co"># nn.Dropout(0.2),</span></span>
<span id="cb23-17"><a href="#cb23-17"></a>            nn.Linear(<span class="dv">50</span>, <span class="dv">10</span>)</span>
<span id="cb23-18"><a href="#cb23-18"></a>        )</span>
<span id="cb23-19"><a href="#cb23-19"></a></span>
<span id="cb23-20"><a href="#cb23-20"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb23-21"><a href="#cb23-21"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb23-22"><a href="#cb23-22"></a>        x <span class="op">=</span> <span class="va">self</span>.layers_stack(x)</span>
<span id="cb23-23"><a href="#cb23-23"></a></span>
<span id="cb23-24"><a href="#cb23-24"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="ab3232e8" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="co">#### For HW: cell to change learning rate</span></span>
<span id="cb24-2"><a href="#cb24-2"></a>nonlinear_model <span class="op">=</span> NonlinearClassifier()</span>
<span id="cb24-3"><a href="#cb24-3"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb24-4"><a href="#cb24-4"></a>optimizer <span class="op">=</span> torch.optim.SGD(nonlinear_model.parameters(), lr<span class="op">=</span><span class="fl">0.05</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="9ac10273" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a><span class="op">%%</span>time</span>
<span id="cb25-2"><a href="#cb25-2"></a></span>
<span id="cb25-3"><a href="#cb25-3"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb25-4"><a href="#cb25-4"></a>train_acc_all <span class="op">=</span> []</span>
<span id="cb25-5"><a href="#cb25-5"></a>val_acc_all <span class="op">=</span> []</span>
<span id="cb25-6"><a href="#cb25-6"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb25-7"><a href="#cb25-7"></a>    train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)</span>
<span id="cb25-8"><a href="#cb25-8"></a></span>
<span id="cb25-9"><a href="#cb25-9"></a>    <span class="co"># checking on the training loss and accuracy once per epoch</span></span>
<span id="cb25-10"><a href="#cb25-10"></a>    acc, loss <span class="op">=</span> evaluate(train_dataloader, nonlinear_model, loss_fn)</span>
<span id="cb25-11"><a href="#cb25-11"></a>    train_acc_all.append(acc)</span>
<span id="cb25-12"><a href="#cb25-12"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">: training loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">, accuracy: </span><span class="sc">{</span>acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-13"><a href="#cb25-13"></a></span>
<span id="cb25-14"><a href="#cb25-14"></a>    <span class="co"># checking on the validation loss and accuracy once per epoch</span></span>
<span id="cb25-15"><a href="#cb25-15"></a>    val_acc, val_loss <span class="op">=</span> evaluate(val_dataloader, nonlinear_model, loss_fn)</span>
<span id="cb25-16"><a href="#cb25-16"></a>    val_acc_all.append(val_acc)</span>
<span id="cb25-17"><a href="#cb25-17"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">: val. loss: </span><span class="sc">{</span>val_loss<span class="sc">}</span><span class="ss">, val. accuracy: </span><span class="sc">{</span>val_acc<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0: training loss: 0.7014495553970337, accuracy: 78.69166666666668
Epoch 0: val. loss: 0.6949471463548377, val. accuracy: 78.45833333333333
Epoch 1: training loss: 0.37350349485874174, accuracy: 89.54375
Epoch 1: val. loss: 0.3613164477208827, val. accuracy: 89.60000000000001
Epoch 2: training loss: 0.2938619179725647, accuracy: 91.50416666666666
Epoch 2: val. loss: 0.2854197383561033, val. accuracy: 91.39166666666667
Epoch 3: training loss: 0.2545405149261157, accuracy: 92.41875
Epoch 3: val. loss: 0.24941304825404856, val. accuracy: 92.43333333333334
Epoch 4: training loss: 0.20977706436316174, accuracy: 93.925
Epoch 4: val. loss: 0.20628496584423045, val. accuracy: 93.85833333333333
CPU times: user 11.6 s, sys: 875 ms, total: 12.5 s
Wall time: 12.1 s</code></pre>
</div>
</div>
<div id="86203d51" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a>pltsize<span class="op">=</span><span class="dv">1</span></span>
<span id="cb27-2"><a href="#cb27-2"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span><span class="op">*</span>pltsize, <span class="dv">10</span> <span class="op">*</span> pltsize))</span>
<span id="cb27-3"><a href="#cb27-3"></a>plt.plot(<span class="bu">range</span>(epochs), train_acc_all,label <span class="op">=</span> <span class="st">'Training Acc.'</span> )</span>
<span id="cb27-4"><a href="#cb27-4"></a>plt.plot(<span class="bu">range</span>(epochs), val_acc_all, label <span class="op">=</span> <span class="st">'Validation Acc.'</span> )</span>
<span id="cb27-5"><a href="#cb27-5"></a>plt.xlabel(<span class="st">'Epoch #'</span>)</span>
<span id="cb27-6"><a href="#cb27-6"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb27-7"><a href="#cb27-7"></a>plt.legend()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-22-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="index_files/figure-html/cell-22-output-1.png" width="808" height="799" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<div id="51233fff" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>show_failures(nonlinear_model, test_dataloader)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Showing max 10 first failures. The predicted class is shown first and the correct class in parentheses.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-23-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="index_files/figure-html/cell-23-output-2.png" width="763" height="102" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="recap" class="level2">
<h2 class="anchored" data-anchor-id="recap">Recap</h2>
<p>To train and validate a neural network model, you need: - Data split into training/validation/test sets, - A model with parameters to learn, - An appropriate loss function, - An optimizer (with tunable parameters such as learning rate, weight decay etc.) used to learn the parameters of the model.</p>
</section>
<section id="homework" class="level2">
<h2 class="anchored" data-anchor-id="homework">Homework</h2>
<ol type="1">
<li>Compare the quality of your model when using different:</li>
</ol>
<ul>
<li>batch sizes,</li>
<li>learning rates,</li>
<li>activation functions.</li>
</ul>
<ol start="3" type="1">
<li>Bonus: What is a learning rate scheduler?</li>
</ol>
<p>If you have time, experiment with how to improve the model. Note: training and validation data can be used to compare models, but test data should be saved until the end as a final check of generalization.</p>
</section>
<section id="homework-solution" class="level2">
<h2 class="anchored" data-anchor-id="homework-solution">Homework solution</h2>
<p>Make the following changes to the cells with the comment “#For HW”</p>
<div id="73c97049" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="co">#####################To modify the batch size##########################</span></span>
<span id="cb30-2"><a href="#cb30-2"></a>batch_size <span class="op">=</span> <span class="dv">32</span> <span class="co"># 64, 128, 256, 512</span></span>
<span id="cb30-3"><a href="#cb30-3"></a></span>
<span id="cb30-4"><a href="#cb30-4"></a><span class="co"># The dataloader makes our dataset iterable </span></span>
<span id="cb30-5"><a href="#cb30-5"></a>train_dataloader <span class="op">=</span> torch.utils.data.DataLoader(training_data, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb30-6"><a href="#cb30-6"></a>val_dataloader <span class="op">=</span> torch.utils.data.DataLoader(validation_data, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb30-7"><a href="#cb30-7"></a><span class="co">##############################################################################</span></span>
<span id="cb30-8"><a href="#cb30-8"></a></span>
<span id="cb30-9"><a href="#cb30-9"></a></span>
<span id="cb30-10"><a href="#cb30-10"></a><span class="co">##########################To change the learning rate##########################</span></span>
<span id="cb30-11"><a href="#cb30-11"></a>optimizer <span class="op">=</span> torch.optim.SGD(nonlinear_model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>) <span class="co">#modify the value of lr</span></span>
<span id="cb30-12"><a href="#cb30-12"></a><span class="co">##############################################################################</span></span>
<span id="cb30-13"><a href="#cb30-13"></a></span>
<span id="cb30-14"><a href="#cb30-14"></a></span>
<span id="cb30-15"><a href="#cb30-15"></a><span class="co">##########################To change activation##########################</span></span>
<span id="cb30-16"><a href="#cb30-16"></a><span class="co">###### Go to https://pytorch.org/docs/main/nn.html#non-linear-activations-weighted-sum-nonlinearity for more activations ######</span></span>
<span id="cb30-17"><a href="#cb30-17"></a><span class="kw">class</span> NonlinearClassifier(nn.Module):</span>
<span id="cb30-18"><a href="#cb30-18"></a></span>
<span id="cb30-19"><a href="#cb30-19"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb30-20"><a href="#cb30-20"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb30-21"><a href="#cb30-21"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb30-22"><a href="#cb30-22"></a>        <span class="va">self</span>.layers_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb30-23"><a href="#cb30-23"></a>            nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">50</span>),</span>
<span id="cb30-24"><a href="#cb30-24"></a>            nn.Sigmoid(), <span class="co">#nn.ReLU(),</span></span>
<span id="cb30-25"><a href="#cb30-25"></a>            nn.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb30-26"><a href="#cb30-26"></a>            nn.Linear(<span class="dv">50</span>, <span class="dv">50</span>),</span>
<span id="cb30-27"><a href="#cb30-27"></a>            nn.Tanh(), <span class="co">#nn.ReLU(),</span></span>
<span id="cb30-28"><a href="#cb30-28"></a>           <span class="co"># nn.Dropout(0.2),</span></span>
<span id="cb30-29"><a href="#cb30-29"></a>            nn.Linear(<span class="dv">50</span>, <span class="dv">50</span>),</span>
<span id="cb30-30"><a href="#cb30-30"></a>            nn.ReLU(),</span>
<span id="cb30-31"><a href="#cb30-31"></a>           <span class="co"># nn.Dropout(0.2),</span></span>
<span id="cb30-32"><a href="#cb30-32"></a>            nn.Linear(<span class="dv">50</span>, <span class="dv">10</span>)</span>
<span id="cb30-33"><a href="#cb30-33"></a>        )</span>
<span id="cb30-34"><a href="#cb30-34"></a>        </span>
<span id="cb30-35"><a href="#cb30-35"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb30-36"><a href="#cb30-36"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb30-37"><a href="#cb30-37"></a>        x <span class="op">=</span> <span class="va">self</span>.layers_stack(x)</span>
<span id="cb30-38"><a href="#cb30-38"></a></span>
<span id="cb30-39"><a href="#cb30-39"></a>        <span class="cf">return</span> x</span>
<span id="cb30-40"><a href="#cb30-40"></a><span class="co">##############################################################################</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Bonus question: A learning rate scheduler is an essential deep learning technique used to dynamically adjust the learning rate during training. This strategic can significantly impact the convergence speed and overall performance of a neural network.See below on how to incorporate it to your training.</p>
<div id="187db9af" class="cell" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a>nonlinear_model <span class="op">=</span> NonlinearClassifier()</span>
<span id="cb31-2"><a href="#cb31-2"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb31-3"><a href="#cb31-3"></a>optimizer <span class="op">=</span> torch.optim.SGD(nonlinear_model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb31-4"><a href="#cb31-4"></a></span>
<span id="cb31-5"><a href="#cb31-5"></a><span class="co"># Step learning rate scheduler: reduce by a factor of 0.1 every 2 epochs (only for illustrative purposes)</span></span>
<span id="cb31-6"><a href="#cb31-6"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.StepLR(optimizer, step_size<span class="op">=</span><span class="dv">2</span>, gamma<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="7888bd14" class="cell" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="op">%%</span>time</span>
<span id="cb32-2"><a href="#cb32-2"></a></span>
<span id="cb32-3"><a href="#cb32-3"></a>epochs <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb32-4"><a href="#cb32-4"></a>train_acc_all <span class="op">=</span> []</span>
<span id="cb32-5"><a href="#cb32-5"></a>val_acc_all <span class="op">=</span> []</span>
<span id="cb32-6"><a href="#cb32-6"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb32-7"><a href="#cb32-7"></a>    train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)</span>
<span id="cb32-8"><a href="#cb32-8"></a>    <span class="co">#step the scheduler</span></span>
<span id="cb32-9"><a href="#cb32-9"></a>    scheduler.step()</span>
<span id="cb32-10"><a href="#cb32-10"></a></span>
<span id="cb32-11"><a href="#cb32-11"></a>    <span class="co"># Print the current learning rate</span></span>
<span id="cb32-12"><a href="#cb32-12"></a>    current_lr <span class="op">=</span> optimizer.param_groups[<span class="dv">0</span>][<span class="st">'lr'</span>]</span>
<span id="cb32-13"><a href="#cb32-13"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>j<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss">, Learning Rate: </span><span class="sc">{</span>current_lr<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-14"><a href="#cb32-14"></a>        </span>
<span id="cb32-15"><a href="#cb32-15"></a>    <span class="co"># checking on the training loss and accuracy once per epoch</span></span>
<span id="cb32-16"><a href="#cb32-16"></a>    acc, loss <span class="op">=</span> evaluate(train_dataloader, nonlinear_model, loss_fn)</span>
<span id="cb32-17"><a href="#cb32-17"></a>    train_acc_all.append(acc)</span>
<span id="cb32-18"><a href="#cb32-18"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">: training loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">, accuracy: </span><span class="sc">{</span>acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-19"><a href="#cb32-19"></a>    </span>
<span id="cb32-20"><a href="#cb32-20"></a>    <span class="co"># checking on the validation loss and accuracy once per epoch</span></span>
<span id="cb32-21"><a href="#cb32-21"></a>    val_acc, val_loss <span class="op">=</span> evaluate(val_dataloader, nonlinear_model, loss_fn)</span>
<span id="cb32-22"><a href="#cb32-22"></a>    val_acc_all.append(val_acc)</span>
<span id="cb32-23"><a href="#cb32-23"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">: val. loss: </span><span class="sc">{</span>val_loss<span class="sc">}</span><span class="ss">, val. accuracy: </span><span class="sc">{</span>val_acc<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/6, Learning Rate: 0.1
Epoch 0: training loss: 0.3398292361845573, accuracy: 90.09791666666666
Epoch 0: val. loss: 0.3296065635085106, val. accuracy: 90.14999999999999
Epoch 2/6, Learning Rate: 0.010000000000000002
Epoch 1: training loss: 0.2392725999119381, accuracy: 92.8125
Epoch 1: val. loss: 0.23178244512776533, val. accuracy: 92.95
Epoch 3/6, Learning Rate: 0.010000000000000002
Epoch 2: training loss: 0.22416235791519284, accuracy: 93.36458333333333
Epoch 2: val. loss: 0.21866890810926756, val. accuracy: 93.38333333333333
Epoch 4/6, Learning Rate: 0.0010000000000000002
Epoch 3: training loss: 0.21785686564693849, accuracy: 93.48541666666667
Epoch 3: val. loss: 0.2132312989284595, val. accuracy: 93.53333333333333
Epoch 5/6, Learning Rate: 0.0010000000000000002
Epoch 4: training loss: 0.2159107316955924, accuracy: 93.57916666666667
Epoch 4: val. loss: 0.21136430992682775, val. accuracy: 93.58333333333333
Epoch 6/6, Learning Rate: 0.00010000000000000003
Epoch 5: training loss: 0.2152757442779839, accuracy: 93.59375
Epoch 5: val. loss: 0.21074153574804466, val. accuracy: 93.65
CPU times: user 17.5 s, sys: 3.25 s, total: 20.8 s
Wall time: 18.8 s</code></pre>
</div>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{foreman2025,
  author = {Foreman, Sam and Foreman, Sam and Ngom, Marieme and Zheng,
    Huihuo and Lusch, Bethany and Childers, Taylor},
  title = {Intro to {NNs:} {MNIST}},
  date = {2025-07-17},
  url = {https://samforeman.me/content/01-neural-networks/1-mnist/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-foreman2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Foreman, Sam, Sam Foreman, Marieme Ngom, Huihuo Zheng, Bethany Lusch,
and Taylor Childers. 2025. <span>“Intro to NNs: MNIST.”</span> July 17,
2025. <a href="https://samforeman.me/content/01-neural-networks/1-mnist/">https://samforeman.me/content/01-neural-networks/1-mnist/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/samforeman\.me");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="dark">
<input type="hidden" id="giscus-alt-theme" value="light">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "saforem2/intro-hpc-bootcamp-2025";
    script.dataset.repoId = "";
    script.dataset.category = "General";
    script.dataset.categoryId = "";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../content/01-neural-networks/0-intro/index.html" class="pagination-link" aria-label="[0] Intro to NNs">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">[0] Intro to NNs</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../content/01-neural-networks/2-advanced/index.html" class="pagination-link" aria-label="[2] Advanced">
        <span class="nav-page-text">[2] Advanced</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb34" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb34-1"><a href="#cb34-1"></a><span class="co">---</span></span>
<span id="cb34-2"><a href="#cb34-2"></a><span class="an">title:</span><span class="co"> "Intro to NNs: MNIST"</span></span>
<span id="cb34-3"><a href="#cb34-3"></a><span class="an">description:</span><span class="co"> "Introduction to Neural Networks with MNIST dataset"</span></span>
<span id="cb34-4"><a href="#cb34-4"></a><span class="an">date:</span><span class="co"> 2025-07-17</span></span>
<span id="cb34-5"><a href="#cb34-5"></a><span class="an">date-modified:</span><span class="co"> last-modified</span></span>
<span id="cb34-6"><a href="#cb34-6"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb34-7"><a href="#cb34-7"></a><span class="an">author:</span></span>
<span id="cb34-8"><a href="#cb34-8"></a><span class="co">  - name: Sam Foreman</span></span>
<span id="cb34-9"><a href="#cb34-9"></a><span class="co">    id: sf</span></span>
<span id="cb34-10"><a href="#cb34-10"></a><span class="co">    orcid: 0000-0002-9981-0876</span></span>
<span id="cb34-11"><a href="#cb34-11"></a><span class="co">    email: foremans@anl.gov</span></span>
<span id="cb34-12"><a href="#cb34-12"></a><span class="co">    affiliation:</span></span>
<span id="cb34-13"><a href="#cb34-13"></a><span class="co">      - name: '[ANL](https://www.anl.gov/)'</span></span>
<span id="cb34-14"><a href="#cb34-14"></a><span class="co">        city: Lemont</span></span>
<span id="cb34-15"><a href="#cb34-15"></a><span class="co">        state: IL</span></span>
<span id="cb34-16"><a href="#cb34-16"></a><span class="co">        url: https://alcf.anl.gov/about/people/sam-foreman</span></span>
<span id="cb34-17"><a href="#cb34-17"></a><span class="co">  - name: Marieme Ngom</span></span>
<span id="cb34-18"><a href="#cb34-18"></a><span class="co">    id: mn</span></span>
<span id="cb34-19"><a href="#cb34-19"></a><span class="co">    # orcid: 0000-0002-9981-0876</span></span>
<span id="cb34-20"><a href="#cb34-20"></a><span class="co">    email: mngom@anl.gov</span></span>
<span id="cb34-21"><a href="#cb34-21"></a><span class="co">    affiliation:</span></span>
<span id="cb34-22"><a href="#cb34-22"></a><span class="co">      - name: '[ANL](https://www.anl.gov/)'</span></span>
<span id="cb34-23"><a href="#cb34-23"></a><span class="co">        city: Lemont</span></span>
<span id="cb34-24"><a href="#cb34-24"></a><span class="co">        state: IL</span></span>
<span id="cb34-25"><a href="#cb34-25"></a><span class="co">        url: https://alcf.anl.gov/about/people/marieme-ngom</span></span>
<span id="cb34-26"><a href="#cb34-26"></a><span class="co">  - name: Huihuo Zheng</span></span>
<span id="cb34-27"><a href="#cb34-27"></a><span class="co">    id: hz</span></span>
<span id="cb34-28"><a href="#cb34-28"></a><span class="co">    # orcid: 0000-0002-9981-0876</span></span>
<span id="cb34-29"><a href="#cb34-29"></a><span class="co">    email: huihuo.zheng@anl.gov</span></span>
<span id="cb34-30"><a href="#cb34-30"></a><span class="co">    affiliation:</span></span>
<span id="cb34-31"><a href="#cb34-31"></a><span class="co">      - name: '[ANL](https://www.anl.gov/)'</span></span>
<span id="cb34-32"><a href="#cb34-32"></a><span class="co">        city: Lemont</span></span>
<span id="cb34-33"><a href="#cb34-33"></a><span class="co">        state: IL</span></span>
<span id="cb34-34"><a href="#cb34-34"></a><span class="co">        url: https://alcf.anl.gov/about/people/huihuo-zheng</span></span>
<span id="cb34-35"><a href="#cb34-35"></a><span class="co">  - name: Bethany Lusch</span></span>
<span id="cb34-36"><a href="#cb34-36"></a><span class="co">    id: bl</span></span>
<span id="cb34-37"><a href="#cb34-37"></a><span class="co">    email: blusch@anl.gov</span></span>
<span id="cb34-38"><a href="#cb34-38"></a><span class="co">    orcid: 0000-0002-9521-9990</span></span>
<span id="cb34-39"><a href="#cb34-39"></a><span class="co">    # orcid: 0000-0002-9981-0876</span></span>
<span id="cb34-40"><a href="#cb34-40"></a><span class="co">    affiliation:</span></span>
<span id="cb34-41"><a href="#cb34-41"></a><span class="co">      - name: '[ANL](https://www.anl.gov/)'</span></span>
<span id="cb34-42"><a href="#cb34-42"></a><span class="co">        city: Lemont</span></span>
<span id="cb34-43"><a href="#cb34-43"></a><span class="co">        state: IL</span></span>
<span id="cb34-44"><a href="#cb34-44"></a><span class="co">        url: https://alcf.anl.gov/about/people/bethany-lusch</span></span>
<span id="cb34-45"><a href="#cb34-45"></a><span class="co">  - name: Taylor Childers</span></span>
<span id="cb34-46"><a href="#cb34-46"></a><span class="co">    id: tc</span></span>
<span id="cb34-47"><a href="#cb34-47"></a><span class="co">    orcid: 0000-0002-0492-613X</span></span>
<span id="cb34-48"><a href="#cb34-48"></a><span class="co">    email: jchilders@anl.gov</span></span>
<span id="cb34-49"><a href="#cb34-49"></a><span class="co">    affiliation:</span></span>
<span id="cb34-50"><a href="#cb34-50"></a><span class="co">      - name: '[ANL](https://www.anl.gov/)'</span></span>
<span id="cb34-51"><a href="#cb34-51"></a><span class="co">        city: Lemont</span></span>
<span id="cb34-52"><a href="#cb34-52"></a><span class="co">        state: IL</span></span>
<span id="cb34-53"><a href="#cb34-53"></a><span class="co">        url: https://alcf.anl.gov/about/people/taylor-childers</span></span>
<span id="cb34-54"><a href="#cb34-54"></a><span class="co">---</span></span>
<span id="cb34-55"><a href="#cb34-55"></a></span>
<span id="cb34-56"><a href="#cb34-56"></a>Author: Marieme Ngom, adapting materials from Bethany Lusch, Asad Khan, Prasanna Balaprakash, Taylor Childers, Corey Adams, Kyle Felker, and Tanwi Mallick.</span>
<span id="cb34-57"><a href="#cb34-57"></a></span>
<span id="cb34-58"><a href="#cb34-58"></a>This tutorial will serve as a gentle introduction to neural networks and deep learning through a hands-on classification problem using the MNIST dataset. </span>
<span id="cb34-59"><a href="#cb34-59"></a></span>
<span id="cb34-60"><a href="#cb34-60"></a>In particular, we will introduce neural networks and how to train and improve their learning capabilities.  We will use the PyTorch Python library.</span>
<span id="cb34-61"><a href="#cb34-61"></a></span>
<span id="cb34-62"><a href="#cb34-62"></a>The <span class="co">[</span><span class="ot">MNIST dataset</span><span class="co">](http://yann.lecun.com/exdb/mnist/)</span> contains thousands of examples of handwritten numbers, with each digit labeled 0-9.</span>
<span id="cb34-63"><a href="#cb34-63"></a></span>
<span id="cb34-64"><a href="#cb34-64"></a>::: {#fig-mnist-task}</span>
<span id="cb34-65"><a href="#cb34-65"></a></span>
<span id="cb34-66"><a href="#cb34-66"></a><span class="al">![](../images/mnist_task.png)</span>{width=400px}</span>
<span id="cb34-67"><a href="#cb34-67"></a></span>
<span id="cb34-68"><a href="#cb34-68"></a>MNIST sample</span>
<span id="cb34-69"><a href="#cb34-69"></a>:::</span>
<span id="cb34-70"><a href="#cb34-70"></a></span>
<span id="cb34-71"><a href="#cb34-71"></a></span>
<span id="cb34-72"><a href="#cb34-72"></a></span>
<span id="cb34-75"><a href="#cb34-75"></a><span class="in">```{python}</span></span>
<span id="cb34-76"><a href="#cb34-76"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb34-77"><a href="#cb34-77"></a></span>
<span id="cb34-78"><a href="#cb34-78"></a><span class="im">import</span> torch</span>
<span id="cb34-79"><a href="#cb34-79"></a><span class="im">import</span> torchvision</span>
<span id="cb34-80"><a href="#cb34-80"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb34-81"><a href="#cb34-81"></a></span>
<span id="cb34-82"><a href="#cb34-82"></a><span class="im">import</span> numpy </span>
<span id="cb34-83"><a href="#cb34-83"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb34-84"><a href="#cb34-84"></a><span class="im">import</span> time</span>
<span id="cb34-85"><a href="#cb34-85"></a><span class="in">```</span></span>
<span id="cb34-86"><a href="#cb34-86"></a></span>
<span id="cb34-87"><a href="#cb34-87"></a><span class="fu">## The MNIST dataset</span></span>
<span id="cb34-88"><a href="#cb34-88"></a></span>
<span id="cb34-89"><a href="#cb34-89"></a>We will now download the dataset that contains handwritten digits. MNIST is a popular dataset, so we can download it via the PyTorch library. Note:</span>
<span id="cb34-90"><a href="#cb34-90"></a><span class="ss">- </span>x is for the inputs (images of handwritten digits) and y is for the labels or outputs (digits 0-9)</span>
<span id="cb34-91"><a href="#cb34-91"></a><span class="ss">- </span>We are given "training" and "test" datasets. Training datasets are used to fit the model. Test datasets are saved until the end, when we are satisfied with our model, to estimate how well our model generalizes to new data.</span>
<span id="cb34-92"><a href="#cb34-92"></a></span>
<span id="cb34-93"><a href="#cb34-93"></a>Note that downloading it the first time might take some time.</span>
<span id="cb34-94"><a href="#cb34-94"></a>The data is split as follows:</span>
<span id="cb34-95"><a href="#cb34-95"></a><span class="ss">- </span>60,000 training examples, 10,000 test examples</span>
<span id="cb34-96"><a href="#cb34-96"></a><span class="ss">- </span>inputs: 1 x 28 x 28 pixels</span>
<span id="cb34-97"><a href="#cb34-97"></a><span class="ss">- </span>outputs (labels): one integer per example </span>
<span id="cb34-98"><a href="#cb34-98"></a></span>
<span id="cb34-101"><a href="#cb34-101"></a><span class="in">```{python}</span></span>
<span id="cb34-102"><a href="#cb34-102"></a>training_data <span class="op">=</span> torchvision.datasets.MNIST(</span>
<span id="cb34-103"><a href="#cb34-103"></a>    root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb34-104"><a href="#cb34-104"></a>    train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb34-105"><a href="#cb34-105"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb34-106"><a href="#cb34-106"></a>    transform<span class="op">=</span>torchvision.transforms.ToTensor()</span>
<span id="cb34-107"><a href="#cb34-107"></a>)</span>
<span id="cb34-108"><a href="#cb34-108"></a></span>
<span id="cb34-109"><a href="#cb34-109"></a>test_data <span class="op">=</span> torchvision.datasets.MNIST(</span>
<span id="cb34-110"><a href="#cb34-110"></a>    root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb34-111"><a href="#cb34-111"></a>    train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb34-112"><a href="#cb34-112"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb34-113"><a href="#cb34-113"></a>    transform<span class="op">=</span>torchvision.transforms.ToTensor()</span>
<span id="cb34-114"><a href="#cb34-114"></a>)</span>
<span id="cb34-115"><a href="#cb34-115"></a><span class="in">```</span></span>
<span id="cb34-116"><a href="#cb34-116"></a></span>
<span id="cb34-119"><a href="#cb34-119"></a><span class="in">```{python}</span></span>
<span id="cb34-120"><a href="#cb34-120"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(training_data))  <span class="co"># 80% for training</span></span>
<span id="cb34-121"><a href="#cb34-121"></a>val_size <span class="op">=</span> <span class="bu">len</span>(training_data) <span class="op">-</span> train_size  <span class="co"># Remaining 20% for validation</span></span>
<span id="cb34-122"><a href="#cb34-122"></a>training_data, validation_data <span class="op">=</span> torch.utils.data.random_split(</span>
<span id="cb34-123"><a href="#cb34-123"></a>    training_data,</span>
<span id="cb34-124"><a href="#cb34-124"></a>    [train_size, val_size],</span>
<span id="cb34-125"><a href="#cb34-125"></a>    generator<span class="op">=</span>torch.Generator().manual_seed(<span class="dv">55</span>)</span>
<span id="cb34-126"><a href="#cb34-126"></a>)</span>
<span id="cb34-127"><a href="#cb34-127"></a><span class="in">```</span></span>
<span id="cb34-128"><a href="#cb34-128"></a></span>
<span id="cb34-131"><a href="#cb34-131"></a><span class="in">```{python}</span></span>
<span id="cb34-132"><a href="#cb34-132"></a><span class="bu">print</span>(<span class="st">'MNIST data loaded: train:'</span>,<span class="bu">len</span>(training_data),<span class="st">' examples, validation: '</span>, <span class="bu">len</span>(validation_data), <span class="st">'examples, test:'</span>,<span class="bu">len</span>(test_data), <span class="st">'examples'</span>)</span>
<span id="cb34-133"><a href="#cb34-133"></a><span class="bu">print</span>(<span class="st">'Input shape'</span>, training_data[<span class="dv">0</span>][<span class="dv">0</span>].shape)</span>
<span id="cb34-134"><a href="#cb34-134"></a><span class="in">```</span></span>
<span id="cb34-135"><a href="#cb34-135"></a></span>
<span id="cb34-136"><a href="#cb34-136"></a>Let's take a closer look. Here are the first 10 training digits:</span>
<span id="cb34-137"><a href="#cb34-137"></a></span>
<span id="cb34-140"><a href="#cb34-140"></a><span class="in">```{python}</span></span>
<span id="cb34-141"><a href="#cb34-141"></a>pltsize<span class="op">=</span><span class="dv">1</span></span>
<span id="cb34-142"><a href="#cb34-142"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span><span class="op">*</span>pltsize, pltsize))</span>
<span id="cb34-143"><a href="#cb34-143"></a></span>
<span id="cb34-144"><a href="#cb34-144"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb34-145"><a href="#cb34-145"></a>    plt.subplot(<span class="dv">1</span>,<span class="dv">10</span>,i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb34-146"><a href="#cb34-146"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb34-147"><a href="#cb34-147"></a>    plt.imshow(numpy.reshape(training_data[i][<span class="dv">0</span>], (<span class="dv">28</span>, <span class="dv">28</span>)), cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb34-148"><a href="#cb34-148"></a>    plt.title(<span class="st">'Class: '</span><span class="op">+</span><span class="bu">str</span>(training_data[i][<span class="dv">1</span>]))</span>
<span id="cb34-149"><a href="#cb34-149"></a><span class="in">```</span></span>
<span id="cb34-150"><a href="#cb34-150"></a></span>
<span id="cb34-151"><a href="#cb34-151"></a><span class="fu">## Generalities:</span></span>
<span id="cb34-152"><a href="#cb34-152"></a>To train our classifier, we need (besides the data):</span>
<span id="cb34-153"><a href="#cb34-153"></a><span class="ss">- </span>A model that depend on parameters $\mathbf{\theta}$. Here we are going to use neural networks.</span>
<span id="cb34-154"><a href="#cb34-154"></a><span class="ss">- </span>A loss function $J(\mathbf{\theta})$ to measure the capabilities of the model.</span>
<span id="cb34-155"><a href="#cb34-155"></a><span class="ss">- </span>An optimization method.</span>
<span id="cb34-156"><a href="#cb34-156"></a></span>
<span id="cb34-157"><a href="#cb34-157"></a><span class="fu">## Linear Model</span></span>
<span id="cb34-158"><a href="#cb34-158"></a>Let's begin with a simple linear model: linear regression, like last week. </span>
<span id="cb34-159"><a href="#cb34-159"></a>We add one complication: each example is a vector (flattened image), so the "slope" multiplication becomes a dot product. If the target output is a vector as well, then the multiplication becomes matrix multiplication. </span>
<span id="cb34-160"><a href="#cb34-160"></a></span>
<span id="cb34-161"><a href="#cb34-161"></a>Note, like before, we consider multiple examples at once, adding another dimension to the input. </span>
<span id="cb34-162"><a href="#cb34-162"></a></span>
<span id="cb34-163"><a href="#cb34-163"></a></span>
<span id="cb34-164"><a href="#cb34-164"></a>::: {#fig-linear-model}</span>
<span id="cb34-165"><a href="#cb34-165"></a></span>
<span id="cb34-166"><a href="#cb34-166"></a><span class="al">![](../images/LinearModel_1.png)</span></span>
<span id="cb34-167"><a href="#cb34-167"></a></span>
<span id="cb34-168"><a href="#cb34-168"></a>Linear model for classification</span>
<span id="cb34-169"><a href="#cb34-169"></a>:::</span>
<span id="cb34-170"><a href="#cb34-170"></a> </span>
<span id="cb34-171"><a href="#cb34-171"></a></span>
<span id="cb34-172"><a href="#cb34-172"></a></span>
<span id="cb34-173"><a href="#cb34-173"></a>The linear layers in PyTorch perform a basic $xW + b$. These "fully connected" layers connect each input to each output with some weight parameter. We wouldn't expect a simple linear model $f(x) = xW+b$ directly outputting the class label and minimizing mean squared error to work well - the model would output labels like 3.55 and 2.11 instead of skipping to integers.</span>
<span id="cb34-174"><a href="#cb34-174"></a></span>
<span id="cb34-175"><a href="#cb34-175"></a>We now need:</span>
<span id="cb34-176"><a href="#cb34-176"></a><span class="ss">- </span>A loss function $J(\theta)$ where $\theta$ is the list of parameters (here W and b). Last week, we used mean squared error (MSE), but this week let's make two changes that make more sense for classification:</span>
<span id="cb34-177"><a href="#cb34-177"></a><span class="ss">    - </span>Change the output to be a length-10 vector of class probabilities (0 to 1, adding to 1).</span>
<span id="cb34-178"><a href="#cb34-178"></a><span class="ss">    - </span>Cross entropy as the loss function, which is typical for classification. You can read more <span class="co">[</span><span class="ot">here</span><span class="co">](https://gombru.github.io/2018/05/23/cross_entropy_loss/)</span>. </span>
<span id="cb34-179"><a href="#cb34-179"></a></span>
<span id="cb34-180"><a href="#cb34-180"></a><span class="ss">- </span>An optimization method or optimizer such as the stochastic gradient descent (sgd) method, the Adam optimizer, RMSprop, Adagrad etc. Let's start with stochastic gradient descent (sgd), like last week. For far more information about more advanced optimizers than basic SGD, with some cool animations, see https://ruder.io/optimizing-gradient-descent/ or https://distill.pub/2017/momentum/.</span>
<span id="cb34-181"><a href="#cb34-181"></a></span>
<span id="cb34-182"><a href="#cb34-182"></a><span class="ss">- </span>A learning rate. As we learned last week, the learning rate controls how far we move during each step. </span>
<span id="cb34-183"><a href="#cb34-183"></a></span>
<span id="cb34-186"><a href="#cb34-186"></a><span class="in">```{python}</span></span>
<span id="cb34-187"><a href="#cb34-187"></a><span class="kw">class</span> LinearClassifier(nn.Module):</span>
<span id="cb34-188"><a href="#cb34-188"></a></span>
<span id="cb34-189"><a href="#cb34-189"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb34-190"><a href="#cb34-190"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-191"><a href="#cb34-191"></a>        </span>
<span id="cb34-192"><a href="#cb34-192"></a>        <span class="co"># First, we need to convert the input image to a vector by using </span></span>
<span id="cb34-193"><a href="#cb34-193"></a>        <span class="co"># nn.Flatten(). For MNIST, it means the second dimension 28*28 becomes 784.</span></span>
<span id="cb34-194"><a href="#cb34-194"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb34-195"><a href="#cb34-195"></a>        </span>
<span id="cb34-196"><a href="#cb34-196"></a>        <span class="co"># Here, we add a fully connected ("dense") layer that has 28 x 28 = 784 input nodes </span></span>
<span id="cb34-197"><a href="#cb34-197"></a>        <span class="co">#(one for each pixel in the input image) and 10 output nodes (for probabilities of each class).</span></span>
<span id="cb34-198"><a href="#cb34-198"></a>        <span class="va">self</span>.layer_1 <span class="op">=</span> nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">10</span>)</span>
<span id="cb34-199"><a href="#cb34-199"></a>        </span>
<span id="cb34-200"><a href="#cb34-200"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-201"><a href="#cb34-201"></a></span>
<span id="cb34-202"><a href="#cb34-202"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb34-203"><a href="#cb34-203"></a>        x <span class="op">=</span> <span class="va">self</span>.layer_1(x)</span>
<span id="cb34-204"><a href="#cb34-204"></a></span>
<span id="cb34-205"><a href="#cb34-205"></a>        <span class="cf">return</span> x</span>
<span id="cb34-206"><a href="#cb34-206"></a><span class="in">```</span></span>
<span id="cb34-207"><a href="#cb34-207"></a></span>
<span id="cb34-210"><a href="#cb34-210"></a><span class="in">```{python}</span></span>
<span id="cb34-211"><a href="#cb34-211"></a>linear_model <span class="op">=</span> LinearClassifier()</span>
<span id="cb34-212"><a href="#cb34-212"></a><span class="bu">print</span>(linear_model)</span>
<span id="cb34-213"><a href="#cb34-213"></a></span>
<span id="cb34-214"><a href="#cb34-214"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb34-215"><a href="#cb34-215"></a>optimizer <span class="op">=</span> torch.optim.SGD(linear_model.parameters(), lr<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb34-216"><a href="#cb34-216"></a><span class="in">```</span></span>
<span id="cb34-217"><a href="#cb34-217"></a></span>
<span id="cb34-218"><a href="#cb34-218"></a><span class="fu">## Learning</span></span>
<span id="cb34-219"><a href="#cb34-219"></a></span>
<span id="cb34-220"><a href="#cb34-220"></a>Now we are ready to train our first model. A training step is comprised of:</span>
<span id="cb34-221"><a href="#cb34-221"></a><span class="ss">- </span>A forward pass: the input is passed through the network</span>
<span id="cb34-222"><a href="#cb34-222"></a><span class="ss">- </span>Backpropagation: A backward pass to compute the gradient $\frac{\partial J}{\partial \mathbf{W}}$ of the loss function with respect to the parameters of the network.</span>
<span id="cb34-223"><a href="#cb34-223"></a><span class="ss">- </span>Weight updates $\mathbf{W} = \mathbf{W} - \alpha \frac{\partial J}{\partial \mathbf{W}} $ where $\alpha$ is the learning rate.</span>
<span id="cb34-224"><a href="#cb34-224"></a></span>
<span id="cb34-225"><a href="#cb34-225"></a>How many steps do we take?</span>
<span id="cb34-226"><a href="#cb34-226"></a><span class="ss">- </span>The batch size corresponds to the number of training examples in one pass (forward + backward). A smaller batch size allows the model to learn from individual examples but takes longer to train. A larger batch size requires fewer steps but may result in the model not capturing the nuances in the data. The higher the batch size, the more memory you will require.  </span>
<span id="cb34-227"><a href="#cb34-227"></a><span class="ss">- </span>An epoch means one pass through the whole training data (looping over the batches). Using few epochs can lead to underfitting and using too many can lead to overfitting.</span>
<span id="cb34-228"><a href="#cb34-228"></a><span class="ss">- </span>The choice of batch size and learning rate are important for performance, generalization and accuracy in deep learning.</span>
<span id="cb34-229"><a href="#cb34-229"></a></span>
<span id="cb34-230"><a href="#cb34-230"></a></span>
<span id="cb34-231"><a href="#cb34-231"></a></span>
<span id="cb34-232"><a href="#cb34-232"></a></span>
<span id="cb34-233"><a href="#cb34-233"></a></span>
<span id="cb34-236"><a href="#cb34-236"></a><span class="in">```{python}</span></span>
<span id="cb34-237"><a href="#cb34-237"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb34-238"><a href="#cb34-238"></a></span>
<span id="cb34-239"><a href="#cb34-239"></a><span class="co"># The dataloader makes our dataset iterable </span></span>
<span id="cb34-240"><a href="#cb34-240"></a>train_dataloader <span class="op">=</span> torch.utils.data.DataLoader(training_data, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb34-241"><a href="#cb34-241"></a>val_dataloader <span class="op">=</span> torch.utils.data.DataLoader(validation_data, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb34-242"><a href="#cb34-242"></a><span class="in">```</span></span>
<span id="cb34-243"><a href="#cb34-243"></a></span>
<span id="cb34-246"><a href="#cb34-246"></a><span class="in">```{python}</span></span>
<span id="cb34-247"><a href="#cb34-247"></a><span class="kw">def</span> train_one_epoch(dataloader, model, loss_fn, optimizer):</span>
<span id="cb34-248"><a href="#cb34-248"></a>    model.train()</span>
<span id="cb34-249"><a href="#cb34-249"></a>    <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb34-250"><a href="#cb34-250"></a>        <span class="co"># forward pass</span></span>
<span id="cb34-251"><a href="#cb34-251"></a>        pred <span class="op">=</span> model(X)</span>
<span id="cb34-252"><a href="#cb34-252"></a>        loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb34-253"><a href="#cb34-253"></a>        </span>
<span id="cb34-254"><a href="#cb34-254"></a>        <span class="co"># backward pass calculates gradients</span></span>
<span id="cb34-255"><a href="#cb34-255"></a>        loss.backward()</span>
<span id="cb34-256"><a href="#cb34-256"></a>        </span>
<span id="cb34-257"><a href="#cb34-257"></a>        <span class="co"># take one step with these gradients</span></span>
<span id="cb34-258"><a href="#cb34-258"></a>        optimizer.step()</span>
<span id="cb34-259"><a href="#cb34-259"></a>        </span>
<span id="cb34-260"><a href="#cb34-260"></a>        <span class="co"># resets the gradients </span></span>
<span id="cb34-261"><a href="#cb34-261"></a>        optimizer.zero_grad()</span>
<span id="cb34-262"><a href="#cb34-262"></a><span class="in">```</span></span>
<span id="cb34-263"><a href="#cb34-263"></a></span>
<span id="cb34-266"><a href="#cb34-266"></a><span class="in">```{python}</span></span>
<span id="cb34-267"><a href="#cb34-267"></a><span class="kw">def</span> evaluate(dataloader, model, loss_fn):</span>
<span id="cb34-268"><a href="#cb34-268"></a>    <span class="co"># Set the model to evaluation mode - some NN pieces behave differently during training</span></span>
<span id="cb34-269"><a href="#cb34-269"></a>    <span class="co"># Unnecessary in this situation but added for best practices</span></span>
<span id="cb34-270"><a href="#cb34-270"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb34-271"><a href="#cb34-271"></a>    size <span class="op">=</span> <span class="bu">len</span>(dataloader.dataset)</span>
<span id="cb34-272"><a href="#cb34-272"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb34-273"><a href="#cb34-273"></a>    loss, correct <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb34-274"><a href="#cb34-274"></a></span>
<span id="cb34-275"><a href="#cb34-275"></a>    <span class="co"># We can save computation and memory by not calculating gradients here - we aren't optimizing </span></span>
<span id="cb34-276"><a href="#cb34-276"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb34-277"><a href="#cb34-277"></a>        <span class="co"># loop over all of the batches</span></span>
<span id="cb34-278"><a href="#cb34-278"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> dataloader:</span>
<span id="cb34-279"><a href="#cb34-279"></a>            pred <span class="op">=</span> model(X)</span>
<span id="cb34-280"><a href="#cb34-280"></a>            loss <span class="op">+=</span> loss_fn(pred, y).item()</span>
<span id="cb34-281"><a href="#cb34-281"></a>            <span class="co"># how many are correct in this batch? Tracking for accuracy </span></span>
<span id="cb34-282"><a href="#cb34-282"></a>            correct <span class="op">+=</span> (pred.argmax(<span class="dv">1</span>) <span class="op">==</span> y).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>().item()</span>
<span id="cb34-283"><a href="#cb34-283"></a></span>
<span id="cb34-284"><a href="#cb34-284"></a>    loss <span class="op">/=</span> num_batches</span>
<span id="cb34-285"><a href="#cb34-285"></a>    correct <span class="op">/=</span> size</span>
<span id="cb34-286"><a href="#cb34-286"></a>    </span>
<span id="cb34-287"><a href="#cb34-287"></a>    accuracy <span class="op">=</span> <span class="dv">100</span><span class="op">*</span>correct</span>
<span id="cb34-288"><a href="#cb34-288"></a>    <span class="cf">return</span> accuracy, loss</span>
<span id="cb34-289"><a href="#cb34-289"></a><span class="in">```</span></span>
<span id="cb34-290"><a href="#cb34-290"></a></span>
<span id="cb34-293"><a href="#cb34-293"></a><span class="in">```{python}</span></span>
<span id="cb34-294"><a href="#cb34-294"></a><span class="op">%%</span>time</span>
<span id="cb34-295"><a href="#cb34-295"></a></span>
<span id="cb34-296"><a href="#cb34-296"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb34-297"><a href="#cb34-297"></a>train_acc_all <span class="op">=</span> []</span>
<span id="cb34-298"><a href="#cb34-298"></a>val_acc_all <span class="op">=</span> []</span>
<span id="cb34-299"><a href="#cb34-299"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb34-300"><a href="#cb34-300"></a>    train_one_epoch(train_dataloader, linear_model, loss_fn, optimizer)</span>
<span id="cb34-301"><a href="#cb34-301"></a>    </span>
<span id="cb34-302"><a href="#cb34-302"></a>    <span class="co"># checking on the training loss and accuracy once per epoch</span></span>
<span id="cb34-303"><a href="#cb34-303"></a>    acc, loss <span class="op">=</span> evaluate(train_dataloader, linear_model, loss_fn)</span>
<span id="cb34-304"><a href="#cb34-304"></a>    train_acc_all.append(acc)</span>
<span id="cb34-305"><a href="#cb34-305"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">: training loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">, accuracy: </span><span class="sc">{</span>acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-306"><a href="#cb34-306"></a>    </span>
<span id="cb34-307"><a href="#cb34-307"></a>    <span class="co"># checking on the validation loss and accuracy once per epoch</span></span>
<span id="cb34-308"><a href="#cb34-308"></a>    val_acc, val_loss <span class="op">=</span> evaluate(val_dataloader, linear_model, loss_fn)</span>
<span id="cb34-309"><a href="#cb34-309"></a>    val_acc_all.append(val_acc)</span>
<span id="cb34-310"><a href="#cb34-310"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">: val. loss: </span><span class="sc">{</span>val_loss<span class="sc">}</span><span class="ss">, val. accuracy: </span><span class="sc">{</span>val_acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-311"><a href="#cb34-311"></a><span class="in">```</span></span>
<span id="cb34-312"><a href="#cb34-312"></a></span>
<span id="cb34-315"><a href="#cb34-315"></a><span class="in">```{python}</span></span>
<span id="cb34-316"><a href="#cb34-316"></a>pltsize<span class="op">=</span><span class="dv">1</span></span>
<span id="cb34-317"><a href="#cb34-317"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span><span class="op">*</span>pltsize, <span class="dv">10</span> <span class="op">*</span> pltsize))</span>
<span id="cb34-318"><a href="#cb34-318"></a>plt.plot(<span class="bu">range</span>(epochs), train_acc_all,label <span class="op">=</span> <span class="st">'Training Acc.'</span> )</span>
<span id="cb34-319"><a href="#cb34-319"></a>plt.plot(<span class="bu">range</span>(epochs), val_acc_all, label <span class="op">=</span> <span class="st">'Validation Acc.'</span> )</span>
<span id="cb34-320"><a href="#cb34-320"></a>plt.xlabel(<span class="st">'Epoch #'</span>)</span>
<span id="cb34-321"><a href="#cb34-321"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb34-322"><a href="#cb34-322"></a>plt.legend()</span>
<span id="cb34-323"><a href="#cb34-323"></a><span class="in">```</span></span>
<span id="cb34-324"><a href="#cb34-324"></a></span>
<span id="cb34-327"><a href="#cb34-327"></a><span class="in">```{python}</span></span>
<span id="cb34-328"><a href="#cb34-328"></a><span class="co"># Visualize how the model is doing on the first 10 examples</span></span>
<span id="cb34-329"><a href="#cb34-329"></a>pltsize<span class="op">=</span><span class="dv">1</span></span>
<span id="cb34-330"><a href="#cb34-330"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span><span class="op">*</span>pltsize, pltsize))</span>
<span id="cb34-331"><a href="#cb34-331"></a>linear_model.<span class="bu">eval</span>()</span>
<span id="cb34-332"><a href="#cb34-332"></a>batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb34-333"><a href="#cb34-333"></a>predictions <span class="op">=</span> linear_model(batch[<span class="dv">0</span>])</span>
<span id="cb34-334"><a href="#cb34-334"></a></span>
<span id="cb34-335"><a href="#cb34-335"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb34-336"><a href="#cb34-336"></a>    plt.subplot(<span class="dv">1</span>,<span class="dv">10</span>,i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb34-337"><a href="#cb34-337"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb34-338"><a href="#cb34-338"></a>    plt.imshow(batch[<span class="dv">0</span>][i,<span class="dv">0</span>,:,:], cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb34-339"><a href="#cb34-339"></a>    plt.title(<span class="st">'</span><span class="sc">%d</span><span class="st">'</span> <span class="op">%</span> predictions[i,:].argmax())</span>
<span id="cb34-340"><a href="#cb34-340"></a><span class="in">```</span></span>
<span id="cb34-341"><a href="#cb34-341"></a></span>
<span id="cb34-342"><a href="#cb34-342"></a>Exercise: How can you improve the accuracy? Some things you might consider: increasing the number of epochs, changing the learning rate, etc. </span>
<span id="cb34-343"><a href="#cb34-343"></a></span>
<span id="cb34-344"><a href="#cb34-344"></a><span class="fu">## Prediction</span></span>
<span id="cb34-345"><a href="#cb34-345"></a></span>
<span id="cb34-346"><a href="#cb34-346"></a>Let's see how our model generalizes to the unseen test data.</span>
<span id="cb34-347"><a href="#cb34-347"></a></span>
<span id="cb34-348"><a href="#cb34-348"></a></span>
<span id="cb34-351"><a href="#cb34-351"></a><span class="in">```{python}</span></span>
<span id="cb34-352"><a href="#cb34-352"></a><span class="co">#For HW: cell to change batch size</span></span>
<span id="cb34-353"><a href="#cb34-353"></a><span class="co">#create dataloader for test data</span></span>
<span id="cb34-354"><a href="#cb34-354"></a><span class="co"># The dataloader makes our dataset iterable</span></span>
<span id="cb34-355"><a href="#cb34-355"></a></span>
<span id="cb34-356"><a href="#cb34-356"></a>batch_size_test <span class="op">=</span> <span class="dv">256</span> </span>
<span id="cb34-357"><a href="#cb34-357"></a>test_dataloader <span class="op">=</span> torch.utils.data.DataLoader(test_data, batch_size<span class="op">=</span>batch_size_test)</span>
<span id="cb34-358"><a href="#cb34-358"></a><span class="in">```</span></span>
<span id="cb34-359"><a href="#cb34-359"></a></span>
<span id="cb34-362"><a href="#cb34-362"></a><span class="in">```{python}</span></span>
<span id="cb34-363"><a href="#cb34-363"></a>acc_test, loss_test <span class="op">=</span> evaluate(test_dataloader, linear_model, loss_fn)</span>
<span id="cb34-364"><a href="#cb34-364"></a><span class="bu">print</span>(<span class="st">"Test loss: </span><span class="sc">%.4f</span><span class="st">, test accuracy: </span><span class="sc">%.2f%%</span><span class="st">"</span> <span class="op">%</span> (loss_test, acc_test))</span>
<span id="cb34-365"><a href="#cb34-365"></a><span class="in">```</span></span>
<span id="cb34-366"><a href="#cb34-366"></a></span>
<span id="cb34-367"><a href="#cb34-367"></a>We can now take a closer look at the results.</span>
<span id="cb34-368"><a href="#cb34-368"></a></span>
<span id="cb34-369"><a href="#cb34-369"></a>Let's define a helper function to show the failure cases of our classifier.</span>
<span id="cb34-370"><a href="#cb34-370"></a></span>
<span id="cb34-373"><a href="#cb34-373"></a><span class="in">```{python}</span></span>
<span id="cb34-374"><a href="#cb34-374"></a><span class="kw">def</span> show_failures(model, dataloader, maxtoshow<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb34-375"><a href="#cb34-375"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb34-376"><a href="#cb34-376"></a>    batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader))</span>
<span id="cb34-377"><a href="#cb34-377"></a>    predictions <span class="op">=</span> model(batch[<span class="dv">0</span>])</span>
<span id="cb34-378"><a href="#cb34-378"></a>    </span>
<span id="cb34-379"><a href="#cb34-379"></a>    rounded <span class="op">=</span> predictions.argmax(<span class="dv">1</span>)</span>
<span id="cb34-380"><a href="#cb34-380"></a>    errors <span class="op">=</span> rounded<span class="op">!=</span>batch[<span class="dv">1</span>]</span>
<span id="cb34-381"><a href="#cb34-381"></a>    <span class="bu">print</span>(<span class="st">'Showing max'</span>, maxtoshow, <span class="st">'first failures. '</span></span>
<span id="cb34-382"><a href="#cb34-382"></a>          <span class="st">'The predicted class is shown first and the correct class in parentheses.'</span>)</span>
<span id="cb34-383"><a href="#cb34-383"></a>    ii <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb34-384"><a href="#cb34-384"></a>    plt.figure(figsize<span class="op">=</span>(maxtoshow, <span class="dv">1</span>))</span>
<span id="cb34-385"><a href="#cb34-385"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch[<span class="dv">0</span>].shape[<span class="dv">0</span>]):</span>
<span id="cb34-386"><a href="#cb34-386"></a>        <span class="cf">if</span> ii<span class="op">&gt;=</span>maxtoshow:</span>
<span id="cb34-387"><a href="#cb34-387"></a>            <span class="cf">break</span></span>
<span id="cb34-388"><a href="#cb34-388"></a>        <span class="cf">if</span> errors[i]:</span>
<span id="cb34-389"><a href="#cb34-389"></a>            plt.subplot(<span class="dv">1</span>, maxtoshow, ii<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb34-390"><a href="#cb34-390"></a>            plt.axis(<span class="st">'off'</span>)</span>
<span id="cb34-391"><a href="#cb34-391"></a>            plt.imshow(batch[<span class="dv">0</span>][i,<span class="dv">0</span>,:,:], cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb34-392"><a href="#cb34-392"></a>            plt.title(<span class="st">"</span><span class="sc">%d</span><span class="st"> (</span><span class="sc">%d</span><span class="st">)"</span> <span class="op">%</span> (rounded[i], batch[<span class="dv">1</span>][i]))</span>
<span id="cb34-393"><a href="#cb34-393"></a>            ii <span class="op">=</span> ii <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb34-394"><a href="#cb34-394"></a><span class="in">```</span></span>
<span id="cb34-395"><a href="#cb34-395"></a></span>
<span id="cb34-396"><a href="#cb34-396"></a>Here are the first 10 images from the test data that this small model classified to a wrong class:</span>
<span id="cb34-397"><a href="#cb34-397"></a></span>
<span id="cb34-398"><a href="#cb34-398"></a></span>
<span id="cb34-401"><a href="#cb34-401"></a><span class="in">```{python}</span></span>
<span id="cb34-402"><a href="#cb34-402"></a>show_failures(linear_model, test_dataloader)</span>
<span id="cb34-403"><a href="#cb34-403"></a><span class="in">```</span></span>
<span id="cb34-404"><a href="#cb34-404"></a></span>
<span id="cb34-405"><a href="#cb34-405"></a><span class="co">&lt;!-- # Exercise:</span></span>
<span id="cb34-406"><a href="#cb34-406"></a><span class="co">- Try changing the loss function,</span></span>
<span id="cb34-407"><a href="#cb34-407"></a><span class="co">- Try changing the optimizer --&gt;</span></span>
<span id="cb34-408"><a href="#cb34-408"></a></span>
<span id="cb34-409"><a href="#cb34-409"></a><span class="fu">## Multilayer Model</span></span>
<span id="cb34-410"><a href="#cb34-410"></a>Our linear model isn't enough for high accuracy on this dataset. To improve the model, we often need to add more layers and nonlinearities.</span>
<span id="cb34-411"><a href="#cb34-411"></a></span>
<span id="cb34-412"><a href="#cb34-412"></a>::: {#fig-shallow-nn}</span>
<span id="cb34-413"><a href="#cb34-413"></a></span>
<span id="cb34-414"><a href="#cb34-414"></a><span class="al">![](../images/shallow_nn.png)</span></span>
<span id="cb34-415"><a href="#cb34-415"></a></span>
<span id="cb34-416"><a href="#cb34-416"></a></span>
<span id="cb34-417"><a href="#cb34-417"></a>Shallow neural network</span>
<span id="cb34-418"><a href="#cb34-418"></a>:::</span>
<span id="cb34-419"><a href="#cb34-419"></a></span>
<span id="cb34-420"><a href="#cb34-420"></a>The output of this NN can be written as</span>
<span id="cb34-421"><a href="#cb34-421"></a>\begin{equation}\label{eq: NN1d}</span>
<span id="cb34-422"><a href="#cb34-422"></a>  \hat{u}(x) = \sigma_2(\sigma_1(\mathbf{x}\mathbf{W}_1 + \mathbf{b}_1)\mathbf{W}_2 + \mathbf{b}_2),</span>
<span id="cb34-423"><a href="#cb34-423"></a>\end{equation}</span>
<span id="cb34-424"><a href="#cb34-424"></a>where $\mathbf{x}$ is the input, $\mathbf{W}_j$ are the weights of the neural network, $\sigma_j$ the (nonlinear) activation functions, and $\mathbf{b}_j$ its biases. The activation function introduces the nonlinearity and makes it possible to learn more complex tasks. Desirable properties in an activation function include being differentiable, bounded, and monotonic.</span>
<span id="cb34-425"><a href="#cb34-425"></a></span>
<span id="cb34-426"><a href="#cb34-426"></a></span>
<span id="cb34-427"><a href="#cb34-427"></a>Image source: <span class="co">[</span><span class="ot">PragatiBaheti</span><span class="co">](https://www.v7labs.com/blog/neural-networks-activation-functions)</span></span>
<span id="cb34-428"><a href="#cb34-428"></a></span>
<span id="cb34-429"><a href="#cb34-429"></a>::: {#fig-activation}</span>
<span id="cb34-430"><a href="#cb34-430"></a></span>
<span id="cb34-431"><a href="#cb34-431"></a><span class="al">![](../images/activation.jpeg)</span></span>
<span id="cb34-432"><a href="#cb34-432"></a></span>
<span id="cb34-433"><a href="#cb34-433"></a>Activation function</span>
<span id="cb34-434"><a href="#cb34-434"></a>:::</span>
<span id="cb34-435"><a href="#cb34-435"></a></span>
<span id="cb34-436"><a href="#cb34-436"></a></span>
<span id="cb34-437"><a href="#cb34-437"></a>Adding more layers to obtain a deep neural network:</span>
<span id="cb34-438"><a href="#cb34-438"></a></span>
<span id="cb34-439"><a href="#cb34-439"></a>::: {#fig-nn-annotated}</span>
<span id="cb34-440"><a href="#cb34-440"></a></span>
<span id="cb34-441"><a href="#cb34-441"></a><span class="al">![](../images/deep_nn_annotated.jpg)</span></span>
<span id="cb34-442"><a href="#cb34-442"></a></span>
<span id="cb34-443"><a href="#cb34-443"></a>:::</span>
<span id="cb34-444"><a href="#cb34-444"></a></span>
<span id="cb34-445"><a href="#cb34-445"></a></span>
<span id="cb34-446"><a href="#cb34-446"></a></span>
<span id="cb34-447"><a href="#cb34-447"></a><span class="fu">## Important things to know</span></span>
<span id="cb34-448"><a href="#cb34-448"></a></span>
<span id="cb34-449"><a href="#cb34-449"></a>Deep Neural networks can be overly flexible/complicated and "overfit" your</span>
<span id="cb34-450"><a href="#cb34-450"></a>data, just like fitting overly complicated polynomials:</span>
<span id="cb34-451"><a href="#cb34-451"></a></span>
<span id="cb34-452"><a href="#cb34-452"></a></span>
<span id="cb34-453"><a href="#cb34-453"></a>::: {#fig-bias-variance}</span>
<span id="cb34-454"><a href="#cb34-454"></a></span>
<span id="cb34-455"><a href="#cb34-455"></a><span class="al">![](../images/bias_vs_variance.png)</span></span>
<span id="cb34-456"><a href="#cb34-456"></a></span>
<span id="cb34-457"><a href="#cb34-457"></a>Bias-variance tradeoff</span>
<span id="cb34-458"><a href="#cb34-458"></a>:::</span>
<span id="cb34-459"><a href="#cb34-459"></a></span>
<span id="cb34-460"><a href="#cb34-460"></a></span>
<span id="cb34-461"><a href="#cb34-461"></a>Vizualization wrt to the accuracy and loss (Image source: <span class="co">[</span><span class="ot">Baeldung</span><span class="co">](https://www.baeldung.com/cs/ml-underfitting-overfitting)</span>):</span>
<span id="cb34-462"><a href="#cb34-462"></a></span>
<span id="cb34-463"><a href="#cb34-463"></a>::: {#fig-acc-under-over}</span>
<span id="cb34-464"><a href="#cb34-464"></a><span class="al">![](./images/acc_under_over.webp)</span></span>
<span id="cb34-465"><a href="#cb34-465"></a></span>
<span id="cb34-466"><a href="#cb34-466"></a>Visualization of accuracy and loss</span>
<span id="cb34-467"><a href="#cb34-467"></a>:::</span>
<span id="cb34-468"><a href="#cb34-468"></a></span>
<span id="cb34-469"><a href="#cb34-469"></a></span>
<span id="cb34-470"><a href="#cb34-470"></a></span>
<span id="cb34-471"><a href="#cb34-471"></a>To improve the generalization of our model on previously unseen data, we employ</span>
<span id="cb34-472"><a href="#cb34-472"></a>a technique known as regularization, which constrains our optimization problem</span>
<span id="cb34-473"><a href="#cb34-473"></a>in order to discourage complex models.</span>
<span id="cb34-474"><a href="#cb34-474"></a></span>
<span id="cb34-475"><a href="#cb34-475"></a><span class="ss">  - </span>Dropout is the commonly used regularization technique.</span>
<span id="cb34-476"><a href="#cb34-476"></a>    The Dropout layer randomly sets input units to 0 with a frequency of rate</span>
<span id="cb34-477"><a href="#cb34-477"></a>    at each step during training time, which helps prevent overfitting.</span>
<span id="cb34-478"><a href="#cb34-478"></a><span class="ss">  - </span>Penalizing the loss function by adding a term such as </span>
<span id="cb34-479"><a href="#cb34-479"></a>    $\lambda ||\mathbf{W}||^2$</span>
<span id="cb34-480"><a href="#cb34-480"></a>    is alsp a commonly used regularization technique.</span>
<span id="cb34-481"><a href="#cb34-481"></a>    This helps "control" the magnitude of the weights of the network.</span>
<span id="cb34-482"><a href="#cb34-482"></a></span>
<span id="cb34-483"><a href="#cb34-483"></a>Vanishing gradients</span>
<span id="cb34-484"><a href="#cb34-484"></a>: Gradients become small as they propagate backward through the layers.</span>
<span id="cb34-485"><a href="#cb34-485"></a>: Squashing activation functions like sigmoid or tanh could cause this.</span>
<span id="cb34-486"><a href="#cb34-486"></a></span>
<span id="cb34-487"><a href="#cb34-487"></a>Exploding gradients</span>
<span id="cb34-488"><a href="#cb34-488"></a>: Gradients grow exponentially usually due to "poor" weight initialization.</span>
<span id="cb34-489"><a href="#cb34-489"></a></span>
<span id="cb34-490"><a href="#cb34-490"></a><span class="co">&lt;!--  &lt;img src="images/test_data_rule.png" width="800" hight="500" align="center"/&gt;</span></span>
<span id="cb34-491"><a href="#cb34-491"></a><span class="co">  --&gt;</span></span>
<span id="cb34-492"><a href="#cb34-492"></a></span>
<span id="cb34-493"><a href="#cb34-493"></a>We can now implement a deep network in PyTorch.</span>
<span id="cb34-494"><a href="#cb34-494"></a></span>
<span id="cb34-495"><a href="#cb34-495"></a><span class="in">`nn.Dropout()`</span> performs the Dropout operation mentioned earlier:</span>
<span id="cb34-496"><a href="#cb34-496"></a></span>
<span id="cb34-499"><a href="#cb34-499"></a><span class="in">```{python}</span></span>
<span id="cb34-500"><a href="#cb34-500"></a><span class="co">#For HW: cell to change activation</span></span>
<span id="cb34-501"><a href="#cb34-501"></a><span class="kw">class</span> NonlinearClassifier(nn.Module):</span>
<span id="cb34-502"><a href="#cb34-502"></a></span>
<span id="cb34-503"><a href="#cb34-503"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb34-504"><a href="#cb34-504"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-505"><a href="#cb34-505"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb34-506"><a href="#cb34-506"></a>        <span class="va">self</span>.layers_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb34-507"><a href="#cb34-507"></a>            nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">50</span>),</span>
<span id="cb34-508"><a href="#cb34-508"></a>            nn.ReLU(),</span>
<span id="cb34-509"><a href="#cb34-509"></a>            nn.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb34-510"><a href="#cb34-510"></a>            nn.Linear(<span class="dv">50</span>, <span class="dv">50</span>),</span>
<span id="cb34-511"><a href="#cb34-511"></a>            nn.ReLU(),</span>
<span id="cb34-512"><a href="#cb34-512"></a>           <span class="co"># nn.Dropout(0.2),</span></span>
<span id="cb34-513"><a href="#cb34-513"></a>            nn.Linear(<span class="dv">50</span>, <span class="dv">50</span>),</span>
<span id="cb34-514"><a href="#cb34-514"></a>            nn.ReLU(),</span>
<span id="cb34-515"><a href="#cb34-515"></a>           <span class="co"># nn.Dropout(0.2),</span></span>
<span id="cb34-516"><a href="#cb34-516"></a>            nn.Linear(<span class="dv">50</span>, <span class="dv">10</span>)</span>
<span id="cb34-517"><a href="#cb34-517"></a>        )</span>
<span id="cb34-518"><a href="#cb34-518"></a></span>
<span id="cb34-519"><a href="#cb34-519"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-520"><a href="#cb34-520"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb34-521"><a href="#cb34-521"></a>        x <span class="op">=</span> <span class="va">self</span>.layers_stack(x)</span>
<span id="cb34-522"><a href="#cb34-522"></a></span>
<span id="cb34-523"><a href="#cb34-523"></a>        <span class="cf">return</span> x</span>
<span id="cb34-524"><a href="#cb34-524"></a><span class="in">```</span></span>
<span id="cb34-525"><a href="#cb34-525"></a></span>
<span id="cb34-528"><a href="#cb34-528"></a><span class="in">```{python}</span></span>
<span id="cb34-529"><a href="#cb34-529"></a><span class="co">#### For HW: cell to change learning rate</span></span>
<span id="cb34-530"><a href="#cb34-530"></a>nonlinear_model <span class="op">=</span> NonlinearClassifier()</span>
<span id="cb34-531"><a href="#cb34-531"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb34-532"><a href="#cb34-532"></a>optimizer <span class="op">=</span> torch.optim.SGD(nonlinear_model.parameters(), lr<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb34-533"><a href="#cb34-533"></a><span class="in">```</span></span>
<span id="cb34-534"><a href="#cb34-534"></a></span>
<span id="cb34-537"><a href="#cb34-537"></a><span class="in">```{python}</span></span>
<span id="cb34-538"><a href="#cb34-538"></a><span class="op">%%</span>time</span>
<span id="cb34-539"><a href="#cb34-539"></a></span>
<span id="cb34-540"><a href="#cb34-540"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb34-541"><a href="#cb34-541"></a>train_acc_all <span class="op">=</span> []</span>
<span id="cb34-542"><a href="#cb34-542"></a>val_acc_all <span class="op">=</span> []</span>
<span id="cb34-543"><a href="#cb34-543"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb34-544"><a href="#cb34-544"></a>    train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)</span>
<span id="cb34-545"><a href="#cb34-545"></a></span>
<span id="cb34-546"><a href="#cb34-546"></a>    <span class="co"># checking on the training loss and accuracy once per epoch</span></span>
<span id="cb34-547"><a href="#cb34-547"></a>    acc, loss <span class="op">=</span> evaluate(train_dataloader, nonlinear_model, loss_fn)</span>
<span id="cb34-548"><a href="#cb34-548"></a>    train_acc_all.append(acc)</span>
<span id="cb34-549"><a href="#cb34-549"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">: training loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">, accuracy: </span><span class="sc">{</span>acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-550"><a href="#cb34-550"></a></span>
<span id="cb34-551"><a href="#cb34-551"></a>    <span class="co"># checking on the validation loss and accuracy once per epoch</span></span>
<span id="cb34-552"><a href="#cb34-552"></a>    val_acc, val_loss <span class="op">=</span> evaluate(val_dataloader, nonlinear_model, loss_fn)</span>
<span id="cb34-553"><a href="#cb34-553"></a>    val_acc_all.append(val_acc)</span>
<span id="cb34-554"><a href="#cb34-554"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">: val. loss: </span><span class="sc">{</span>val_loss<span class="sc">}</span><span class="ss">, val. accuracy: </span><span class="sc">{</span>val_acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-555"><a href="#cb34-555"></a><span class="in">```</span></span>
<span id="cb34-556"><a href="#cb34-556"></a></span>
<span id="cb34-559"><a href="#cb34-559"></a><span class="in">```{python}</span></span>
<span id="cb34-560"><a href="#cb34-560"></a>pltsize<span class="op">=</span><span class="dv">1</span></span>
<span id="cb34-561"><a href="#cb34-561"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span><span class="op">*</span>pltsize, <span class="dv">10</span> <span class="op">*</span> pltsize))</span>
<span id="cb34-562"><a href="#cb34-562"></a>plt.plot(<span class="bu">range</span>(epochs), train_acc_all,label <span class="op">=</span> <span class="st">'Training Acc.'</span> )</span>
<span id="cb34-563"><a href="#cb34-563"></a>plt.plot(<span class="bu">range</span>(epochs), val_acc_all, label <span class="op">=</span> <span class="st">'Validation Acc.'</span> )</span>
<span id="cb34-564"><a href="#cb34-564"></a>plt.xlabel(<span class="st">'Epoch #'</span>)</span>
<span id="cb34-565"><a href="#cb34-565"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb34-566"><a href="#cb34-566"></a>plt.legend()</span>
<span id="cb34-567"><a href="#cb34-567"></a><span class="in">```</span></span>
<span id="cb34-568"><a href="#cb34-568"></a></span>
<span id="cb34-571"><a href="#cb34-571"></a><span class="in">```{python}</span></span>
<span id="cb34-572"><a href="#cb34-572"></a>show_failures(nonlinear_model, test_dataloader)</span>
<span id="cb34-573"><a href="#cb34-573"></a><span class="in">```</span></span>
<span id="cb34-574"><a href="#cb34-574"></a></span>
<span id="cb34-575"><a href="#cb34-575"></a><span class="fu">## Recap</span></span>
<span id="cb34-576"><a href="#cb34-576"></a></span>
<span id="cb34-577"><a href="#cb34-577"></a>To train and validate a neural network model, you need:</span>
<span id="cb34-578"><a href="#cb34-578"></a><span class="ss">- </span>Data split into training/validation/test sets,</span>
<span id="cb34-579"><a href="#cb34-579"></a><span class="ss">- </span>A model with parameters to learn,</span>
<span id="cb34-580"><a href="#cb34-580"></a><span class="ss">- </span>An appropriate loss function,</span>
<span id="cb34-581"><a href="#cb34-581"></a><span class="ss">- </span>An optimizer (with tunable parameters such as learning rate, weight decay etc.) used to learn the parameters of the model.</span>
<span id="cb34-582"><a href="#cb34-582"></a></span>
<span id="cb34-583"><a href="#cb34-583"></a><span class="fu">## Homework</span></span>
<span id="cb34-584"><a href="#cb34-584"></a></span>
<span id="cb34-585"><a href="#cb34-585"></a><span class="ss">1. </span>Compare the quality of your model when using different:</span>
<span id="cb34-586"><a href="#cb34-586"></a><span class="ss"> - </span>batch sizes, </span>
<span id="cb34-587"><a href="#cb34-587"></a><span class="ss"> - </span>learning rates,</span>
<span id="cb34-588"><a href="#cb34-588"></a><span class="ss"> - </span>activation functions.</span>
<span id="cb34-589"><a href="#cb34-589"></a></span>
<span id="cb34-590"><a href="#cb34-590"></a><span class="ss">3. </span>Bonus: What is a learning rate scheduler?</span>
<span id="cb34-591"><a href="#cb34-591"></a></span>
<span id="cb34-592"><a href="#cb34-592"></a>If you have time, experiment with how to improve the model. Note: training and validation data can be used to compare models, but test data should be saved until the end as a final check of generalization. </span>
<span id="cb34-593"><a href="#cb34-593"></a></span>
<span id="cb34-594"><a href="#cb34-594"></a><span class="fu">## Homework solution</span></span>
<span id="cb34-595"><a href="#cb34-595"></a></span>
<span id="cb34-596"><a href="#cb34-596"></a>Make the following changes to the cells with the comment "#For HW"</span>
<span id="cb34-597"><a href="#cb34-597"></a></span>
<span id="cb34-600"><a href="#cb34-600"></a><span class="in">```{python}</span></span>
<span id="cb34-601"><a href="#cb34-601"></a><span class="co">#####################To modify the batch size##########################</span></span>
<span id="cb34-602"><a href="#cb34-602"></a>batch_size <span class="op">=</span> <span class="dv">32</span> <span class="co"># 64, 128, 256, 512</span></span>
<span id="cb34-603"><a href="#cb34-603"></a></span>
<span id="cb34-604"><a href="#cb34-604"></a><span class="co"># The dataloader makes our dataset iterable </span></span>
<span id="cb34-605"><a href="#cb34-605"></a>train_dataloader <span class="op">=</span> torch.utils.data.DataLoader(training_data, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb34-606"><a href="#cb34-606"></a>val_dataloader <span class="op">=</span> torch.utils.data.DataLoader(validation_data, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb34-607"><a href="#cb34-607"></a><span class="co">##############################################################################</span></span>
<span id="cb34-608"><a href="#cb34-608"></a></span>
<span id="cb34-609"><a href="#cb34-609"></a></span>
<span id="cb34-610"><a href="#cb34-610"></a><span class="co">##########################To change the learning rate##########################</span></span>
<span id="cb34-611"><a href="#cb34-611"></a>optimizer <span class="op">=</span> torch.optim.SGD(nonlinear_model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>) <span class="co">#modify the value of lr</span></span>
<span id="cb34-612"><a href="#cb34-612"></a><span class="co">##############################################################################</span></span>
<span id="cb34-613"><a href="#cb34-613"></a></span>
<span id="cb34-614"><a href="#cb34-614"></a></span>
<span id="cb34-615"><a href="#cb34-615"></a><span class="co">##########################To change activation##########################</span></span>
<span id="cb34-616"><a href="#cb34-616"></a><span class="co">###### Go to https://pytorch.org/docs/main/nn.html#non-linear-activations-weighted-sum-nonlinearity for more activations ######</span></span>
<span id="cb34-617"><a href="#cb34-617"></a><span class="kw">class</span> NonlinearClassifier(nn.Module):</span>
<span id="cb34-618"><a href="#cb34-618"></a></span>
<span id="cb34-619"><a href="#cb34-619"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb34-620"><a href="#cb34-620"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-621"><a href="#cb34-621"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb34-622"><a href="#cb34-622"></a>        <span class="va">self</span>.layers_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb34-623"><a href="#cb34-623"></a>            nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">50</span>),</span>
<span id="cb34-624"><a href="#cb34-624"></a>            nn.Sigmoid(), <span class="co">#nn.ReLU(),</span></span>
<span id="cb34-625"><a href="#cb34-625"></a>            nn.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb34-626"><a href="#cb34-626"></a>            nn.Linear(<span class="dv">50</span>, <span class="dv">50</span>),</span>
<span id="cb34-627"><a href="#cb34-627"></a>            nn.Tanh(), <span class="co">#nn.ReLU(),</span></span>
<span id="cb34-628"><a href="#cb34-628"></a>           <span class="co"># nn.Dropout(0.2),</span></span>
<span id="cb34-629"><a href="#cb34-629"></a>            nn.Linear(<span class="dv">50</span>, <span class="dv">50</span>),</span>
<span id="cb34-630"><a href="#cb34-630"></a>            nn.ReLU(),</span>
<span id="cb34-631"><a href="#cb34-631"></a>           <span class="co"># nn.Dropout(0.2),</span></span>
<span id="cb34-632"><a href="#cb34-632"></a>            nn.Linear(<span class="dv">50</span>, <span class="dv">10</span>)</span>
<span id="cb34-633"><a href="#cb34-633"></a>        )</span>
<span id="cb34-634"><a href="#cb34-634"></a>        </span>
<span id="cb34-635"><a href="#cb34-635"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-636"><a href="#cb34-636"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb34-637"><a href="#cb34-637"></a>        x <span class="op">=</span> <span class="va">self</span>.layers_stack(x)</span>
<span id="cb34-638"><a href="#cb34-638"></a></span>
<span id="cb34-639"><a href="#cb34-639"></a>        <span class="cf">return</span> x</span>
<span id="cb34-640"><a href="#cb34-640"></a><span class="co">##############################################################################</span></span>
<span id="cb34-641"><a href="#cb34-641"></a><span class="in">```</span></span>
<span id="cb34-642"><a href="#cb34-642"></a></span>
<span id="cb34-643"><a href="#cb34-643"></a>Bonus question: A learning rate scheduler is an essential deep learning technique used to dynamically adjust the learning rate during training. This strategic can significantly impact the convergence speed and overall performance of a neural network.See below on how to incorporate it to your training.</span>
<span id="cb34-644"><a href="#cb34-644"></a></span>
<span id="cb34-647"><a href="#cb34-647"></a><span class="in">```{python}</span></span>
<span id="cb34-648"><a href="#cb34-648"></a>nonlinear_model <span class="op">=</span> NonlinearClassifier()</span>
<span id="cb34-649"><a href="#cb34-649"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb34-650"><a href="#cb34-650"></a>optimizer <span class="op">=</span> torch.optim.SGD(nonlinear_model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb34-651"><a href="#cb34-651"></a></span>
<span id="cb34-652"><a href="#cb34-652"></a><span class="co"># Step learning rate scheduler: reduce by a factor of 0.1 every 2 epochs (only for illustrative purposes)</span></span>
<span id="cb34-653"><a href="#cb34-653"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.StepLR(optimizer, step_size<span class="op">=</span><span class="dv">2</span>, gamma<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb34-654"><a href="#cb34-654"></a><span class="in">```</span></span>
<span id="cb34-655"><a href="#cb34-655"></a></span>
<span id="cb34-658"><a href="#cb34-658"></a><span class="in">```{python}</span></span>
<span id="cb34-659"><a href="#cb34-659"></a><span class="op">%%</span>time</span>
<span id="cb34-660"><a href="#cb34-660"></a></span>
<span id="cb34-661"><a href="#cb34-661"></a>epochs <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb34-662"><a href="#cb34-662"></a>train_acc_all <span class="op">=</span> []</span>
<span id="cb34-663"><a href="#cb34-663"></a>val_acc_all <span class="op">=</span> []</span>
<span id="cb34-664"><a href="#cb34-664"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb34-665"><a href="#cb34-665"></a>    train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)</span>
<span id="cb34-666"><a href="#cb34-666"></a>    <span class="co">#step the scheduler</span></span>
<span id="cb34-667"><a href="#cb34-667"></a>    scheduler.step()</span>
<span id="cb34-668"><a href="#cb34-668"></a></span>
<span id="cb34-669"><a href="#cb34-669"></a>    <span class="co"># Print the current learning rate</span></span>
<span id="cb34-670"><a href="#cb34-670"></a>    current_lr <span class="op">=</span> optimizer.param_groups[<span class="dv">0</span>][<span class="st">'lr'</span>]</span>
<span id="cb34-671"><a href="#cb34-671"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>j<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss">, Learning Rate: </span><span class="sc">{</span>current_lr<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-672"><a href="#cb34-672"></a>        </span>
<span id="cb34-673"><a href="#cb34-673"></a>    <span class="co"># checking on the training loss and accuracy once per epoch</span></span>
<span id="cb34-674"><a href="#cb34-674"></a>    acc, loss <span class="op">=</span> evaluate(train_dataloader, nonlinear_model, loss_fn)</span>
<span id="cb34-675"><a href="#cb34-675"></a>    train_acc_all.append(acc)</span>
<span id="cb34-676"><a href="#cb34-676"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">: training loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">, accuracy: </span><span class="sc">{</span>acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-677"><a href="#cb34-677"></a>    </span>
<span id="cb34-678"><a href="#cb34-678"></a>    <span class="co"># checking on the validation loss and accuracy once per epoch</span></span>
<span id="cb34-679"><a href="#cb34-679"></a>    val_acc, val_loss <span class="op">=</span> evaluate(val_dataloader, nonlinear_model, loss_fn)</span>
<span id="cb34-680"><a href="#cb34-680"></a>    val_acc_all.append(val_acc)</span>
<span id="cb34-681"><a href="#cb34-681"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">: val. loss: </span><span class="sc">{</span>val_loss<span class="sc">}</span><span class="ss">, val. accuracy: </span><span class="sc">{</span>val_acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-682"><a href="#cb34-682"></a><span class="in">```</span></span>
<span id="cb34-683"><a href="#cb34-683"></a></span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://samforeman.me">
<p>samforeman.me</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/blob/main/content/01-neural-networks/1-mnist/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/edit/main/content/01-neural-networks/1-mnist/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/saforem2">
      <i class="bi bi-twitter" role="img" aria-label="Sam Foreman Twitter">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/saforem2/intro-hpc-bootcamp-2025">
      <i class="bi bi-github" role="img" aria-label="Sam Foreman GitHub">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>