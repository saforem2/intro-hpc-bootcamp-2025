<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.17">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sam Foreman">
<meta name="author" content="Huihuo Zheng">
<meta name="author" content="Corey Adams">
<meta name="author" content="Bethany Lusch">
<meta name="dcterms.date" content="2025-07-22">

<title>Convolutional Neural Networks – Intro to HPC Bootcamp 2025</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../01-neural-networks/4-representation-learning/index.html" rel="next">
<link href="../../01-neural-networks/2-advanced/index.html" rel="prev">
<link href="../../assets/favicon-sf.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-64a9477eeb01bb4c41113dc2462ec3b8.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-047ee2ca16eaebc148183a87b705dca3.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-c0ef8121a210f2d9f27a40f1bc4c9547.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-5460c54c7821efc39f0a1364daa6c88f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js" integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;family=IBM+Plex+Sans+Condensed:ital,wght@0,400;0,500;0,600;0,700&amp;family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans&amp;family=IBM+Plex+Sans+Condensed&amp;family=IBM+Plex+Mono&amp;display=swap" rel="stylesheet">
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-TC329HJ');</script>
<!-- End Google Tag Manager -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link href="https://iosevka-webfonts.github.io/iosevka/iosevka.css" rel="stylesheet">


<meta property="og:title" content="Convolutional Neural Networks – Intro to HPC Bootcamp 2025">
<meta property="og:description" content="An introduction to High Performance Computing (HPC) and Artificial Intelligence (AI) for scientific applications, with a focus on practical skills and hands-on experience.">
<meta property="og:image" content="https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/3-conv-nets/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta property="og:site_name" content="Intro to HPC Bootcamp 2025">
<meta name="twitter:title" content="Convolutional Neural Networks – Intro to HPC Bootcamp 2025">
<meta name="twitter:description" content="An introduction to High Performance Computing (HPC) and Artificial Intelligence (AI) for scientific applications, with a focus on practical skills and hands-on experience.">
<meta name="twitter:image" content="https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/3-conv-nets/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta name="twitter:site" content="saforem2">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="Convolutional Neural Networks">
<meta name="citation_author" content="Sam Foreman">
<meta name="citation_author" content="Huihuo Zheng">
<meta name="citation_author" content="Corey Adams">
<meta name="citation_author" content="Bethany Lusch">
<meta name="citation_publication_date" content="2025-07-22">
<meta name="citation_cover_date" content="2025-07-22">
<meta name="citation_year" content="2025">
<meta name="citation_online_date" content="2025-07-22">
<meta name="citation_fulltext_html_url" content="https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/3-conv-nets/">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=HiPerRAG: High-performance retrieval augmented generation for scientific insights;,citation_author=Ozan Gokdemir;,citation_author=Carlo Siebenschuh;,citation_author=Alexander Brace;,citation_author=Azton Wells;,citation_author=Brian Hsu;,citation_author=Kyle Hippe;,citation_author=Priyanka V. Setty;,citation_author=Aswathy Ajith;,citation_author=J. Gregory Pauloski;,citation_author=Varuni Sastry;,citation_author=Sam Foreman;,citation_author=Huihuo Zheng;,citation_author=Heng Ma;,citation_author=Bharat Kale;,citation_author=Nicholas Chia;,citation_author=Thomas Gibbs;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Francis J. Alexander;,citation_author=Anima Anandkumar;,citation_author=Ian Foster;,citation_author=Rick Stevens;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://arxiv.org/abs/2505.04846;">
<meta name="citation_reference" content="citation_title=MOFA: Discovering materials for carbon capture with a GenAI- and simulation-based workflow;,citation_author=Xiaoli Yan;,citation_author=Nathaniel Hudson;,citation_author=Hyun Park;,citation_author=Daniel Grzenda;,citation_author=J. Gregory Pauloski;,citation_author=Marcus Schwarting;,citation_author=Haochen Pan;,citation_author=Hassan Harb;,citation_author=Samuel Foreman;,citation_author=Chris Knight;,citation_author=Tom Gibbs;,citation_author=Kyle Chard;,citation_author=Santanu Chaudhuri;,citation_author=Emad Tajkhorshid;,citation_author=Ian Foster;,citation_author=Mohamad Moosavi;,citation_author=Logan Ward;,citation_author=E. A. Huerta;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://arxiv.org/abs/2501.10651;">
<meta name="citation_reference" content="citation_title=MProt-DPO: Breaking the ExaFLOPS barrier for multimodal protein design workflows with direct preference optimization;,citation_abstract=We present a scalable, end-to-end workflow for protein design. By augmenting protein sequences with natural language descriptions of their biochemical properties, we train generative models that can be preferentially aligned with protein fitness landscapes. Through complex experimental- and simulation-based observations, we integrate these measures as preferred parameters for generating new protein variants and demonstrate our workflow on five diverse supercomputers. We achieve &amp;amp;amp;gt;1 ExaFLOPS sustained performance in mixed precision on each supercomputer and a maximum sustained performance of 4.11 ExaFLOPS and peak performance of 5.57 ExaFLOPS. We establish the scientific performance of our model on two tasks: (1) across a predetermined benchmark dataset of deep mutational scanning experiments to optimize the fitness-determining mutations in the yeast protein HIS7, and (2) in optimizing the design of the enzyme malate dehydrogenase to achieve lower activation barriers (and therefore increased catalytic rates) using simulation data. Our implementation thus sets high watermarks for multimodal protein design workflows.;,citation_author=Gautham Dharuman;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Sam Foreman;,citation_author=Väinö Hatanpää;,citation_author=Varuni K. Sastry;,citation_author=Huihuo Zheng;,citation_author=Logan Ward;,citation_author=Servesh Muralidharan;,citation_author=Archit Vasan;,citation_author=Bharat Kale;,citation_author=Carla M. Mann;,citation_author=Heng Ma;,citation_author=Yun-Hsuan Cheng;,citation_author=Yuliana Zamora;,citation_author=Shengchao Liu;,citation_author=Chaowei Xiao;,citation_author=Murali Emani;,citation_author=Tom Gibbs;,citation_author=Mahidhar Tatineni;,citation_author=Deepak Canchi;,citation_author=Jerome Mitchell;,citation_author=Koichi Yamada;,citation_author=Maria Garzaran;,citation_author=Michael E. Papka;,citation_author=Ian Foster;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://doi.org/10.1109/SC41406.2024.00013;,citation_doi=10.1109/SC41406.2024.00013;,citation_isbn=9798350352917;,citation_conference_title=Proceedings of the international conference for high performance computing, networking, storage, and analysis;,citation_conference=IEEE Press;,citation_series_title=SC ’24;">
<meta name="citation_reference" content="citation_title=Quality measures for dynamic graph generative models;,citation_author=Ryien Hosseini;,citation_author=Filippo Simini;,citation_author=Venkatram Vishwanath;,citation_author=Rebecca Willett;,citation_author=Henry Hoffmann;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://openreview.net/forum?id=8bjspmAMBk;,citation_conference_title=The thirteenth international conference on learning representations;">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=RG-inspired machine learning for lattice field theory;,citation_author=Sam Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_volume=175;,citation_conference_title=EPJ web of conferences;,citation_conference=EDP Sciences;">
<meta name="citation_reference" content="citation_title=Large energy density in three-plate nanocapacitors due to coulomb blockade;,citation_author=A Hubler;,citation_author=S Foreman;,citation_author=J Liu;,citation_author=L Wortsmann;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=10;,citation_volume=123;,citation_journal_title=Journal of Applied Physics;,citation_publisher=AIP Publishing;">
<meta name="citation_reference" content="citation_title=Examples of renormalization group transformations for image sets;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=5;,citation_volume=98;,citation_journal_title=Physical Review E;,citation_publisher=American Physical Society;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the Ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.22323/1.334.0245;,citation_volume=LATTICE2018;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Machine learning inspired analysis of the ising model transition;,citation_author=Samuel Foreman;,citation_author=Joel Giedt;,citation_author=Yannick Meurice;,citation_author=Judah Unmuth-Yockey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=Lattice 2018;">
<meta name="citation_reference" content="citation_title=Learning better physics: A machine learning approach to lattice gauge theory;,citation_author=Samuel Alfred Foreman;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_dissertation_institution=University of Iowa;">
<meta name="citation_reference" content="citation_title=Machine learning and neural networks for field theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=HMC with normalizing flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_journal_title=arXiv preprint arXiv:2112.01586;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy storage in quantum resonators;,citation_author=Jiaqi Liu;,citation_author=Alfred W Hubler;,citation_author=Samuel Alfred Foreman;,citation_author=Katharina Ott;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Applications of machine learning to lattice quantum field theory;,citation_author=Denis Boyda;,citation_author=Salvatore Calı̀;,citation_author=Sam Foreman;,citation_author=Lena Funcke;,citation_author=Daniel C Hackett;,citation_author=Yin Lin;,citation_author=Gert Aarts;,citation_author=Andrei Alexandru;,citation_author=Xiao-Yong Jin;,citation_author=Biagio Lucini;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_journal_title=arXiv preprint arXiv:2202.05838;">
<meta name="citation_reference" content="citation_title=Lattice QCD and particle physics;,citation_author=Andreas S Kronfeld;,citation_author=Tanmoy Bhattacharya;,citation_author=Thomas Blum;,citation_author=Norman H Christ;,citation_author=Carleton DeTar;,citation_author=William Detmold;,citation_author=Robert Edwards;,citation_author=Anna Hasenfratz;,citation_author=Huey-Wen Lin;,citation_author=Swagato Mukherjee;,citation_author=others;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2207.07641;,citation_journal_title=arXiv preprint arXiv:2207.07641;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=6;,citation_volume=37;,citation_journal_title=The International Journal of High Performance Computing Applications;,citation_publisher=SAGE Publications Sage UK: London, England;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=The international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=Superconductivity of in and sn samples;,citation_author=George Deamont;,citation_author=Sam Foreman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=A comprehensive performance study of large language models on novel AI accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04607;,citation_journal_title=arXiv preprint arXiv:2310.04607;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04610;,citation_journal_title=arXiv preprint arXiv:2310.04610;">
<meta name="citation_reference" content="citation_title=Protein generation via genome-scale language models with bio-physical scoring;,citation_author=Gautham Dharuman;,citation_author=Logan Ward;,citation_author=Heng Ma;,citation_author=Priyanka V Setty;,citation_author=Ozan Gokdemir;,citation_author=Sam Foreman;,citation_author=Murali Emani;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Kristopher Keipert;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=Proceedings of the SC’23 workshops of the international conference on high performance computing, network, storage, and analysis;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2312.08936;,citation_journal_title=arXiv preprint arXiv:2312.08936;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 computational frontier CompF03 topical group report: Machine learning;,citation_author=Phiala Shanahan;,citation_author=Kazuhiro Terao;,citation_author=Daniel Whiteson;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;,citation_journal_title=arXiv preprint arXiv:2209.07559;">
<meta name="citation_reference" content="citation_title=Thorough characterization and analysis of large transformer model training at-scale;,citation_author=Scott Cheng;,citation_author=Jun-Liang Lin;,citation_author=Murali Emani;,citation_author=Siddhisanket Raskar;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Venkatram Vishwanath;,citation_author=Mahmut Taylan Kandemir;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=8;,citation_journal_title=Proceedings of the ACM on Measurement and Analysis of Computing Systems;,citation_publisher=ACM New York, NY, USA;">
<meta name="citation_reference" content="citation_title=Communities through energy justice projects;,citation_author=Mary Ann Leung;,citation_author=Katharine Cahill;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois Curfman McInnes;,citation_author=Suzanne Parete-Koon;,citation_author=Subil Abraham;,citation_author=Lacy Beach Barrier;,citation_author=Gladys Chen;,citation_author=Lizanne DeStefano;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science;">
<meta name="citation_reference" content="citation_title=Applications of a foundation model approach for weather and climate;,citation_author=Troy Arcomano;,citation_author=Alexander Wikner;,citation_author=Romit Maulik;,citation_author=Veerabhadra Rao Kotamarthi;,citation_author=Sam Foreman;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_volume=2023;,citation_conference_title=AGU fall meeting abstracts;">
<meta name="citation_reference" content="citation_title=Toward a holistic performance evaluation of large language models across diverse ai accelerators;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Varuni Sastry;,citation_author=Zhen Xie;,citation_author=Siddhisanket Raskar;,citation_author=William Arnold;,citation_author=Rajeev Thakur;,citation_author=Venkatram Vishwanath;,citation_author=Michael E Papka;,citation_author=Sanjif Shanmugavelu;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 IEEE international parallel and distributed processing symposium workshops (IPDPSW);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Intro to HPC bootcamp: Engaging new communities through energy justice projects;,citation_author=Suzanne Parete-Koon;,citation_author=Michael Sandoval;,citation_author=Kellen Leland;,citation_author=Subil Abraham;,citation_author=Mary Ann Leung;,citation_author=Rebecca Hartman-Baker;,citation_author=Paige Kinsley;,citation_author=Lois McInnes;,citation_author=Sreeranjani Ramprakash;,citation_author=Lacy Beach Barrier;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=15;,citation_journal_title=Journal of Computational Science Education;,citation_publisher=Oak Ridge National Laboratory (ORNL), Oak Ridge, TN (United States);">
<meta name="citation_reference" content="citation_title=MProt-DPO: Breaking the ExaFLOPS barrier for multimodal protein design workflows with direct preference optimization;,citation_author=Gautham Dharuman;,citation_author=Kyle Hippe;,citation_author=Alexander Brace;,citation_author=Sam Foreman;,citation_author=Väinä Hatanpää;,citation_author=Varuni K Sastry;,citation_author=Huihuo Zheng;,citation_author=Logan Ward;,citation_author=Servesh Muralidharan;,citation_author=Archit Vasan;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=2024 SC24: International conference for high performance computing, networking, storage and analysis SC;,citation_conference=IEEE Computer Society;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=DeepSpeed4Science initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies;,citation_author=Shuaiwen Leon Song;,citation_author=Bonnie Kruft;,citation_author=Minjia Zhang;,citation_author=Conglong Li;,citation_author=Shiyang Chen;,citation_author=Chengming Zhang;,citation_author=Masahiro Tanaka;,citation_author=Xiaoxia Wu;,citation_author=Jeff Rasley;,citation_author=Ammar Ahmad Awan;,citation_author=Connor Holmes;,citation_author=Martin Cai;,citation_author=Adam Ghanem;,citation_author=Zhongzhu Zhou;,citation_author=Yuxiong He;,citation_author=Pete Luferenko;,citation_author=Divya Kumar;,citation_author=Jonathan Weyn;,citation_author=Ruixiong Zhang;,citation_author=Sylwester Klocek;,citation_author=Volodymyr Vragov;,citation_author=Mohammed AlQuraishi;,citation_author=Gustaf Ahdritz;,citation_author=Christina Floristean;,citation_author=Cristina Negri;,citation_author=Rao Kotamarthi;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_author=Sam Foreman;,citation_author=Kyle Hippe;,citation_author=Troy Arcomano;,citation_author=Romit Maulik;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot;,citation_author=Murali Emani;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Prasanna Balaprakash;,citation_author=Gina Tourassi;,citation_author=John Gounley;,citation_author=Heidi Hanson;,citation_author=Thomas E Potok;,citation_author=Massimiliano Lupo Pasini;,citation_author=Kate Evans;,citation_author=Dan Lu;,citation_author=Dalton Lunga;,citation_author=Junqi Yin;,citation_author=Sajal Dash;,citation_author=Feiyi Wang;,citation_author=Mallikarjun Shankar;,citation_author=Isaac Lyngaas;,citation_author=Xiao Wang;,citation_author=Guojing Cong;,citation_author=Pei Zhang;,citation_author=Ming Fan;,citation_author=Siyan Liu;,citation_author=Adolfy Hoisie;,citation_author=Shinjae Yoo;,citation_author=Yihui Ren;,citation_author=William Tang;,citation_author=Kyle Felker;,citation_author=Alexey Svyatkovskiy;,citation_author=Hang Liu;,citation_author=Ashwin Aji;,citation_author=Angela Dalton;,citation_author=Michael Schulte;,citation_author=Karl Schulz;,citation_author=Yuntian Deng;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Anima Anandkumar;,citation_author=Rick Stevens;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2310.04610;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.07682;">
<meta name="citation_reference" content="citation_title=The climate risk &amp;amp;amp; resilience portal (ClimRR) metadata and data dictionary;,citation_author=C. Burdi;,citation_author=Wall. T Branham;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://dub.sh/ClimRR-Metadata;">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2105.03418;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A trainable framework for effective topological sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.01582;">
<meta name="citation_reference" content="citation_title=Energy Justice Analysis of Climate Data with ClimRR;,citation_author=Sam Foreman;,citation_publication_date=2023-08-07;,citation_cover_date=2023-08-07;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/climate-analysis;,citation_language=en;">
<meta name="citation_reference" content="citation_author=Sam Foreman;,citation_publication_date=2023-08-19;,citation_cover_date=2023-08-19;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/l2hmc-qcd;,citation_language=en;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James Osborn;,citation_publication_date=00;,citation_cover_date=00;,citation_year=0;,citation_conference_title=40th international symposium on lattice field theory (lattice 2023) (batavia, IL, united states, 07/31/2023 - 08/04/2023);">
<meta name="citation_reference" content="citation_title=Progress on $(g-2)_\mu$ from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=S. Foreman;,citation_author=X. Jin;,citation_author=J. Osborn;,citation_publication_date=2022-07;,citation_cover_date=2022-07;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_conference_title=The 38th international symposium on lattice field theory;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Mastering language models;,citation_author=Samuel Montgomery;,citation_publication_date=2023-10;,citation_cover_date=2023-10;,citation_year=2023;,citation_fulltext_html_url=https://towardsdatascience.com/mastering-language-models-32e1d891511a
           ;,citation_journal_title=Medium;,citation_publisher=Towards Data Science;">
<meta name="citation_reference" content="citation_title=Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond;,citation_author=Jingfeng Yang;,citation_author=Hongye Jin;,citation_author=Ruixiang Tang;,citation_author=Xiaotian Han;,citation_author=Qizhang Feng;,citation_author=Haoming Jiang;,citation_author=Bing Yin;,citation_author=Xia Hu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2304.13712;">
<meta name="citation_reference" content="citation_title=Training tips for the transformer model;,citation_author=Martin Popel;,citation_author=Ondřej Bojar;,citation_publication_date=2018-04;,citation_cover_date=2018-04;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.2478%2Fpralin-2018-0002;,citation_issue=1;,citation_doi=10.2478/pralin-2018-0002;,citation_volume=110;,citation_journal_title=The Prague Bulletin of Mathematical Linguistics;,citation_publisher=Charles University in Prague, Karolinum Press;">
<meta name="citation_reference" content="citation_title=Attention is all you need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N. Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03762;">
<meta name="citation_reference" content="citation_title=Tree of thoughts: Deliberate problem solving with large language models;,citation_author=Shunyu Yao;,citation_author=Dian Yu;,citation_author=Jeffrey Zhao;,citation_author=Izhak Shafran;,citation_author=Thomas L. Griffiths;,citation_author=Yuan Cao;,citation_author=Karthik Narasimhan;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2305.10601;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_abstract=We seek to transform how new and emergent variants of pandemiccausing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.Competing Interest StatementThe authors have declared no competing interest.;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot-Sasson;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.biorxiv.org/content/early/2022/11/23/2022.10.10.511571;,citation_doi=10.1101/2022.10.10.511571;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
</head>

<body class="nav-sidebar floating quarto-dark"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = true;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../01-neural-networks/index.html">[01] Neural Networks</a></li><li class="breadcrumb-item"><a href="../../01-neural-networks/3-conv-nets/index.html">[3] Conv. Nets</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Intro to HPC Bootcamp 2025</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="../../index.html" title="Home" class="quarto-navigation-tool px-1" aria-label="Home"><i class="bi bi-home"></i></a>
    <a href="https://github.com/saforem2/intro-hpc-bootcamp-2025" title="GitHub" class="quarto-navigation-tool px-1" aria-label="GitHub"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../00-intro-AI-HPC/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[00] Intro to AI and HPC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/0-compute-systems/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] Compute Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/1-shared-resources/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[1] Shared Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/2-jupyter-notebooks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[2] Jupyter Notebooks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/3-homework/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[3] Homework</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/4-nersc/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[4] NERSC</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/5-mcmc-example/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[5] MCMC Example</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../00-intro-AI-HPC/6-linear-regression/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[6] Linear Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../01-neural-networks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[01] Neural Networks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/0-intro/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] Intro to NNs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/1-mnist/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[1] MNIST Example</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/2-advanced/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[2] Advanced</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/3-conv-nets/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">[3] Conv. Nets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../01-neural-networks/4-representation-learning/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[4] Representation Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../02-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[02] Large Language Models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/0-intro-to-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] Intro to LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/1-hands-on-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[1] Hands-on LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/4-evaluating-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[4] Evaluating LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/5-advanced-llms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[5] Advanced LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/6-parallel-training/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[6] Parallel Training</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/7-shakespeare-example/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[7] Shakespeare Example</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../02-llms/8-shakespeare-example-colab/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[8] Shakespeare Example (Colab)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../03-ai-for-science/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[03] AI for Science</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../03-ai-for-science/0-genslm/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[0] GenSLM</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#convolutional-networks-a-brief-historical-context" id="toc-convolutional-networks-a-brief-historical-context" class="nav-link active" data-scroll-target="#convolutional-networks-a-brief-historical-context">Convolutional Networks: A brief historical context</a></li>
  <li><a href="#convolutional-building-blocks" id="toc-convolutional-building-blocks" class="nav-link" data-scroll-target="#convolutional-building-blocks">Convolutional Building Blocks</a>
  <ul class="collapse">
  <li><a href="#convolutions" id="toc-convolutions" class="nav-link" data-scroll-target="#convolutions">Convolutions</a></li>
  <li><a href="#normalization" id="toc-normalization" class="nav-link" data-scroll-target="#normalization">Normalization</a></li>
  <li><a href="#downsampling-and-upsampling" id="toc-downsampling-and-upsampling" class="nav-link" data-scroll-target="#downsampling-and-upsampling">Downsampling (And upsampling)</a></li>
  <li><a href="#residual-connections" id="toc-residual-connections" class="nav-link" data-scroll-target="#residual-connections">Residual Connections</a></li>
  </ul></li>
  <li><a href="#building-a-convnet" id="toc-building-a-convnet" class="nav-link" data-scroll-target="#building-a-convnet">Building a ConvNet</a></li>
  <li><a href="#homework-1" id="toc-homework-1" class="nav-link" data-scroll-target="#homework-1">Homework 1:</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/blob/main/01-neural-networks/3-conv-nets/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/edit/main/01-neural-networks/3-conv-nets/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.md"><i class="bi bi-file-code"></i>Github (GFM)</a></li><li><a href="index.ipynb" download="index.ipynb"><i class="bi bi-journal-code"></i>Jupyter</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TC329HJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../01-neural-networks/index.html">[01] Neural Networks</a></li><li class="breadcrumb-item"><a href="../../01-neural-networks/3-conv-nets/index.html">[3] Conv. Nets</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Convolutional Neural Networks</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Sam Foreman <a href="mailto:foremans@anl.gov" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-9981-0876" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://alcf.anl.gov/about/people/sam-foreman">
            </a><a href="https://www.anl.gov/">ANL</a>
            
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Huihuo Zheng </p>
  </div>
  <div class="quarto-title-meta-contents">
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Corey Adams </p>
  </div>
  <div class="quarto-title-meta-contents">
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Bethany Lusch </p>
  </div>
  <div class="quarto-title-meta-contents">
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 22, 2025</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">July 23, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<p><a href="https://colab.research.google.com/github/saforem2/intro-hpc-bootcamp-2025/blob/main/docs/01-neural-networks/3-conv-nets/index.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid"></a></p>
<p>Up until transformers, convolutions were <em>the</em> state of the art in computer vision.</p>
<p>In many ways and applications they still are!</p>
<p>Large Language Models, which are what we’ll focus on the rest of the series after this lecture, are really good at ordered, *tokenized data. But there is lots of data that isn’t <em>implicitly</em> ordered like <code>images</code>, and their more general cousins <code>graphs</code>.</p>
<p>Today’s lecture focuses on computer vision models, and particularly on convolutional neural networks. There are a ton of applications you can do with these, and not nearly enough time to get into them. Check out the extra references file to see some publications to get you started if you want to learn more.</p>
<p>Tip: this notebook is much faster on the GPU!</p>
<section id="convolutional-networks-a-brief-historical-context" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-networks-a-brief-historical-context">Convolutional Networks: A brief historical context</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./ImageNet.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="ImageNet Accuracy by Yearh"><img src="./ImageNet.png" class="img-fluid figure-img" alt="ImageNet Accuracy by Yearh"></a></p>
<figcaption>ImageNet Accuracy by Yearh</figcaption>
</figure>
</div>
<div id="ee53e332" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> ambivalent</span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a>plt.style.use(ambivalent.STYLES[<span class="st">'ambivalent'</span>])</span>
<span id="cb1-7"><a href="#cb1-7"></a>sns.set_context(<span class="st">"notebook"</span>)</span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="im">from</span> rich <span class="im">import</span> <span class="bu">print</span></span>
<span id="cb1-10"><a href="#cb1-10"></a></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="co"># Data</span></span>
<span id="cb1-12"><a href="#cb1-12"></a>years <span class="op">=</span> [<span class="dv">2010</span>, <span class="dv">2011</span>, <span class="dv">2012</span>, <span class="dv">2013</span>, <span class="dv">2014</span>, <span class="dv">2015</span>, <span class="dv">2016</span>, <span class="dv">2017</span>]</span>
<span id="cb1-13"><a href="#cb1-13"></a>image_net_error_rates <span class="op">=</span> [<span class="dv">28</span>, <span class="dv">26</span>, <span class="dv">16</span>, <span class="dv">12</span>, <span class="dv">7</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">2</span>]</span>
<span id="cb1-14"><a href="#cb1-14"></a>data <span class="op">=</span> {</span>
<span id="cb1-15"><a href="#cb1-15"></a>    <span class="dv">2010</span>: <span class="dv">28</span>,</span>
<span id="cb1-16"><a href="#cb1-16"></a>    <span class="dv">2011</span>: <span class="dv">26</span>,</span>
<span id="cb1-17"><a href="#cb1-17"></a>    <span class="dv">2012</span>: <span class="dv">16</span>,</span>
<span id="cb1-18"><a href="#cb1-18"></a>    <span class="dv">2013</span>: <span class="dv">12</span>,</span>
<span id="cb1-19"><a href="#cb1-19"></a>    <span class="dv">2014</span>: <span class="dv">7</span>,</span>
<span id="cb1-20"><a href="#cb1-20"></a>    <span class="dv">2015</span>: <span class="dv">3</span>,</span>
<span id="cb1-21"><a href="#cb1-21"></a>    <span class="dv">2016</span>: <span class="fl">2.3</span>,</span>
<span id="cb1-22"><a href="#cb1-22"></a>    <span class="dv">2017</span>: <span class="fl">2.1</span></span>
<span id="cb1-23"><a href="#cb1-23"></a>}</span>
<span id="cb1-24"><a href="#cb1-24"></a>human_error_rate <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb1-25"><a href="#cb1-25"></a></span>
<span id="cb1-26"><a href="#cb1-26"></a><span class="co"># Create bar plot</span></span>
<span id="cb1-27"><a href="#cb1-27"></a>plt.bar(years, image_net_error_rates, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb1-28"><a href="#cb1-28"></a></span>
<span id="cb1-29"><a href="#cb1-29"></a><span class="co"># Add human error rate line</span></span>
<span id="cb1-30"><a href="#cb1-30"></a>plt.axhline(y<span class="op">=</span>human_error_rate, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'Human error rate'</span>)</span>
<span id="cb1-31"><a href="#cb1-31"></a></span>
<span id="cb1-32"><a href="#cb1-32"></a><span class="co"># Labels and title</span></span>
<span id="cb1-33"><a href="#cb1-33"></a>plt.xlabel(<span class="st">'Year'</span>)</span>
<span id="cb1-34"><a href="#cb1-34"></a>plt.ylabel(<span class="st">'ImageNet Visual Recognition Error Rate (%)'</span>)</span>
<span id="cb1-35"><a href="#cb1-35"></a>plt.title(<span class="st">'ImageNet Error Rates Over Time'</span>)</span>
<span id="cb1-36"><a href="#cb1-36"></a>plt.legend()</span>
<span id="cb1-37"><a href="#cb1-37"></a></span>
<span id="cb1-38"><a href="#cb1-38"></a><span class="co"># Display plot</span></span>
<span id="cb1-39"><a href="#cb1-39"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-2-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="index_files/figure-html/cell-2-output-1.png" width="1023" height="795" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p><a href="https://www.researchgate.net/publication/332452649_A_Roadmap_for_Foundational_Research_on_Artificial_Intelligence_in_Medical_Imaging_From_the_2018_NIHRSNAACRThe_Academy_Workshop">reference</a></p>
<div id="988c9e07" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> torch, torchvision</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="convolutional-building-blocks" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-building-blocks">Convolutional Building Blocks</h2>
<p>We’re going to go through some examples of building blocks for convolutional networks. To help illustate some of these, let’s use an image for examples:</p>
<div id="c5f60ed6" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="co"># wget line useful in Google Colab</span></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="op">!</span> wget https:<span class="op">//</span>raw.githubusercontent.com<span class="op">/</span>argonne<span class="op">-</span>lcf<span class="op">/</span>ai<span class="op">-</span>science<span class="op">-</span>training<span class="op">-</span>series<span class="op">/</span>main<span class="op">/</span><span class="dv">0</span><span class="er">3_advanced_neural_networks</span><span class="op">/</span>ALCF<span class="op">-</span>Staff.jpg</span>
<span id="cb3-4"><a href="#cb3-4"></a>alcf_image <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">"ALCF-Staff.jpg"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>--2025-07-23 17:53:34--  https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/03_advanced_neural_networks/ALCF-Staff.jpg
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 417835 (408K) [image/jpeg]
Saving to: ‘ALCF-Staff.jpg.29’

ALCF-Staff.jpg.29     0%[                    ]       0  --.-KB/s               ALCF-Staff.jpg.29   100%[===================&gt;] 408.04K  --.-KB/s    in 0.03s   

2025-07-23 17:53:35 (11.7 MB/s) - ‘ALCF-Staff.jpg.29’ saved [417835/417835]
</code></pre>
</div>
</div>
<div id="3ae9bae0" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb5-2"><a href="#cb5-2"></a>figure <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb5-3"><a href="#cb5-3"></a>plt.imshow(alcf_image)</span>
<span id="cb5-4"><a href="#cb5-4"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-5-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="index_files/figure-html/cell-5-output-1.png" width="1555" height="887" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<section id="convolutions" class="level3">
<h3 class="anchored" data-anchor-id="convolutions">Convolutions</h3>
<p>Convolutions are a restriction of - and a specialization of - dense linear layers. A convolution of an image produces another image, and each output pixel is a function of only it’s local neighborhood of points. This is called an <em>inductive bias</em> and is a big reason why convolutions work for image data: neighboring pixels are correlated and you can operate on just those pixels at a time.</p>
<p>See examples of convolutions <a href="https://github.com/vdumoulin/conv_arithmetic">here</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./conv_eqn.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="image-2.png"><img src="./conv_eqn.png" class="img-fluid figure-img" alt="image-2.png"></a></p>
<figcaption>image-2.png</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./conv.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="image.png"><img src="./conv.png" class="img-fluid figure-img" alt="image.png"></a></p>
<figcaption>image.png</figcaption>
</figure>
</div>
<div id="2ec7c5b9" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Let's apply a convolution to the ALCF Staff photo:</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>alcf_tensor <span class="op">=</span> torchvision.transforms.ToTensor()(alcf_image)</span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="co"># Reshape the tensor to have a batch size of 1:</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>alcf_tensor <span class="op">=</span> alcf_tensor.reshape((<span class="dv">1</span>,) <span class="op">+</span> alcf_tensor.shape)</span>
<span id="cb6-6"><a href="#cb6-6"></a></span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="co"># Create a random convolution:</span></span>
<span id="cb6-8"><a href="#cb6-8"></a><span class="co"># shape is: (channels_in, channels_out, kernel_x, kernel_y)</span></span>
<span id="cb6-9"><a href="#cb6-9"></a>conv_random <span class="op">=</span> torch.rand((<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">15</span>,<span class="dv">15</span>))</span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a>alcf_rand <span class="op">=</span> torch.nn.functional.conv2d(alcf_tensor, conv_random)</span>
<span id="cb6-12"><a href="#cb6-12"></a>alcf_rand <span class="op">=</span> (<span class="fl">1.</span><span class="op">/</span>alcf_rand.<span class="bu">max</span>()) <span class="op">*</span> alcf_rand</span>
<span id="cb6-13"><a href="#cb6-13"></a><span class="bu">print</span>(alcf_rand.shape)</span>
<span id="cb6-14"><a href="#cb6-14"></a>alcf_rand <span class="op">=</span> alcf_rand.reshape(alcf_rand.shape[<span class="dv">1</span>:])</span>
<span id="cb6-15"><a href="#cb6-15"></a></span>
<span id="cb6-16"><a href="#cb6-16"></a><span class="bu">print</span>(alcf_tensor.shape)</span>
<span id="cb6-17"><a href="#cb6-17"></a></span>
<span id="cb6-18"><a href="#cb6-18"></a>rand_image <span class="op">=</span> alcf_rand.permute((<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>)).cpu()</span>
<span id="cb6-19"><a href="#cb6-19"></a></span>
<span id="cb6-20"><a href="#cb6-20"></a>figure <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb6-21"><a href="#cb6-21"></a></span>
<span id="cb6-22"><a href="#cb6-22"></a>plt.imshow(rand_image)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1111</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1986</span><span style="font-weight: bold">])</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1125</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2000</span><span style="font-weight: bold">])</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-6-output-3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="index_files/figure-html/cell-6-output-3.png" width="1555" height="883" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="normalization" class="level3">
<h3 class="anchored" data-anchor-id="normalization">Normalization</h3>
<p><img src="./batch_norm.png" class="img-fluid" alt="Batch Norm"> Reference: <a href="https://arxiv.org/pdf/1903.10520.pdf">Normalizations</a></p>
<p>Normalization is the act of transforming the mean and moment of your data to standard values (usually 0.0 and 1.0). It’s particularly useful in machine learning since it stabilizes training, and allows higher learning rates.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./batch_norm_effect.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Batch Normalization accelerates training"><img src="./batch_norm_effect.png" class="img-fluid figure-img" alt="Batch Normalization accelerates training"></a></p>
<figcaption>Batch Normalization accelerates training</figcaption>
</figure>
</div>
<p>Reference: <a href="https://arxiv.org/pdf/1502.03167.pdf">Batch Norm</a></p>
<div id="6d0cb992" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># Let's apply a normalization to the ALCF Staff photo:</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>alcf_tensor <span class="op">=</span> torchvision.transforms.ToTensor()(alcf_image)</span>
<span id="cb7-3"><a href="#cb7-3"></a></span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="co"># Reshape the tensor to have a batch size of 1:</span></span>
<span id="cb7-5"><a href="#cb7-5"></a>alcf_tensor <span class="op">=</span> alcf_tensor.reshape((<span class="dv">1</span>,) <span class="op">+</span> alcf_tensor.shape)</span>
<span id="cb7-6"><a href="#cb7-6"></a></span>
<span id="cb7-7"><a href="#cb7-7"></a></span>
<span id="cb7-8"><a href="#cb7-8"></a>alcf_rand <span class="op">=</span> torch.nn.functional.normalize(alcf_tensor)</span>
<span id="cb7-9"><a href="#cb7-9"></a>alcf_rand <span class="op">=</span> alcf_rand.reshape(alcf_rand.shape[<span class="dv">1</span>:])</span>
<span id="cb7-10"><a href="#cb7-10"></a></span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="bu">print</span>(alcf_tensor.shape)</span>
<span id="cb7-12"><a href="#cb7-12"></a></span>
<span id="cb7-13"><a href="#cb7-13"></a>rand_image <span class="op">=</span> alcf_rand.permute((<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>)).cpu()</span>
<span id="cb7-14"><a href="#cb7-14"></a></span>
<span id="cb7-15"><a href="#cb7-15"></a>figure <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb7-16"><a href="#cb7-16"></a></span>
<span id="cb7-17"><a href="#cb7-17"></a>plt.imshow(rand_image)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1125</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2000</span><span style="font-weight: bold">])</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-7-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="index_files/figure-html/cell-7-output-2.png" width="1555" height="887" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="downsampling-and-upsampling" class="level3">
<h3 class="anchored" data-anchor-id="downsampling-and-upsampling">Downsampling (And upsampling)</h3>
<p>Downsampling is a critical component of convolutional and many vision models. Because of the local-only nature of convolutional filters, learning large-range features can be too slow for convergence. Downsampling of layers can bring information from far away closer, effectively changing what it means to be “local” as the input to a convolution.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./conv_pooling.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Convolutional Pooling"><img src="./conv_pooling.png" class="img-fluid figure-img" alt="Convolutional Pooling"></a></p>
<figcaption>Convolutional Pooling</figcaption>
</figure>
</div>
<p><a href="https://www.researchgate.net/publication/333593451_Application_of_Transfer_Learning_Using_Convolutional_Neural_Network_Method_for_Early_Detection_of_Terry's_Nail">Reference</a></p>
<div id="19950a47" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># Let's apply a normalization to the ALCF Staff photo:</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>alcf_tensor <span class="op">=</span> torchvision.transforms.ToTensor()(alcf_image)</span>
<span id="cb8-3"><a href="#cb8-3"></a></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="co"># Reshape the tensor to have a batch size of 1:</span></span>
<span id="cb8-5"><a href="#cb8-5"></a>alcf_tensor <span class="op">=</span> alcf_tensor.reshape((<span class="dv">1</span>,) <span class="op">+</span> alcf_tensor.shape)</span>
<span id="cb8-6"><a href="#cb8-6"></a></span>
<span id="cb8-7"><a href="#cb8-7"></a></span>
<span id="cb8-8"><a href="#cb8-8"></a>alcf_rand <span class="op">=</span> torch.nn.functional.max_pool2d(alcf_tensor, <span class="dv">2</span>)</span>
<span id="cb8-9"><a href="#cb8-9"></a>alcf_rand <span class="op">=</span> alcf_rand.reshape(alcf_rand.shape[<span class="dv">1</span>:])</span>
<span id="cb8-10"><a href="#cb8-10"></a></span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="bu">print</span>(alcf_tensor.shape)</span>
<span id="cb8-12"><a href="#cb8-12"></a></span>
<span id="cb8-13"><a href="#cb8-13"></a>rand_image <span class="op">=</span> alcf_rand.permute((<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>)).cpu()</span>
<span id="cb8-14"><a href="#cb8-14"></a></span>
<span id="cb8-15"><a href="#cb8-15"></a>figure <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb8-16"><a href="#cb8-16"></a></span>
<span id="cb8-17"><a href="#cb8-17"></a>plt.imshow(rand_image)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1125</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2000</span><span style="font-weight: bold">])</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-8-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="index_files/figure-html/cell-8-output-2.png" width="1546" height="886" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="residual-connections" class="level3">
<h3 class="anchored" data-anchor-id="residual-connections">Residual Connections</h3>
<p>One issue, quickly encountered when making convolutional networks deeper and deeper, is the “Vanishing Gradients” problem. As layers were stacked on top of each other, the size of updates dimished at the earlier layers of a convolutional network. The paper “Deep Residual Learning for Image Recognition” solved this by introduction “residual connections” as skip layers.</p>
<p>Reference: <a href="https://arxiv.org/pdf/1512.03385.pdf">Deep Residual Learning for Image Recognition</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./residual_layer.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Residual Layer"><img src="./residual_layer.png" class="img-fluid figure-img" alt="Residual Layer"></a></p>
<figcaption>Residual Layer</figcaption>
</figure>
</div>
<p>Compare the performance of the models before and after the introduction of these layers:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./resnet_comparison.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Resnet Performance vs.&nbsp;Plain network performance"><img src="./resnet_comparison.png" class="img-fluid figure-img" alt="Resnet Performance vs.&nbsp;Plain network performance"></a></p>
<figcaption>Resnet Performance vs.&nbsp;Plain network performance</figcaption>
</figure>
</div>
<p>If you have time to read only one paper on computer vision, make it this one! Resnet was the first model to beat human accuracy on ImageNet and is one of the most impactful papers in AI ever published.</p>
</section>
</section>
<section id="building-a-convnet" class="level2">
<h2 class="anchored" data-anchor-id="building-a-convnet">Building a ConvNet</h2>
<p>In this section we’ll build and apply a conv net to the mnist dataset. The layers here are loosely based off of the ConvNext architecture. Why? Because we’re getting into LLM’s soon, and this ConvNet uses LLM features. ConvNext is an update to the ResNet architecture that outperforms it.</p>
<p><a href="https://arxiv.org/abs/2201.03545">ConvNext</a></p>
<p>The dataset here is CIFAR-10 - slightly harder than MNIST but still relatively easy and computationally tractable.</p>
<div id="f99871a8" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> v2</span>
<span id="cb9-2"><a href="#cb9-2"></a>training_data <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb9-3"><a href="#cb9-3"></a>    <span class="co"># Polaris: root="/lus/eagle/projects/datasets/CIFAR-10/",</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>    <span class="co"># Polaris: download=False,</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>    root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb9-6"><a href="#cb9-6"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb9-7"><a href="#cb9-7"></a>    train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb9-8"><a href="#cb9-8"></a>    transform<span class="op">=</span>v2.Compose([</span>
<span id="cb9-9"><a href="#cb9-9"></a>        v2.ToTensor(),</span>
<span id="cb9-10"><a href="#cb9-10"></a>        v2.RandomHorizontalFlip(),</span>
<span id="cb9-11"><a href="#cb9-11"></a>        v2.RandomResizedCrop(size<span class="op">=</span><span class="dv">32</span>, scale<span class="op">=</span>[<span class="fl">0.85</span>,<span class="fl">1.0</span>], antialias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb9-12"><a href="#cb9-12"></a>        v2.ColorJitter(brightness<span class="op">=</span><span class="fl">0.2</span>, contrast<span class="op">=</span><span class="fl">0.2</span>, saturation<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb9-13"><a href="#cb9-13"></a>    ])</span>
<span id="cb9-14"><a href="#cb9-14"></a>)</span>
<span id="cb9-15"><a href="#cb9-15"></a></span>
<span id="cb9-16"><a href="#cb9-16"></a>test_data <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb9-17"><a href="#cb9-17"></a>    <span class="co"># Polaris: root="/lus/eagle/projects/datasets/CIFAR-10/",</span></span>
<span id="cb9-18"><a href="#cb9-18"></a>    <span class="co"># Polaris: download=False,</span></span>
<span id="cb9-19"><a href="#cb9-19"></a>    root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb9-20"><a href="#cb9-20"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb9-21"><a href="#cb9-21"></a>    train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb9-22"><a href="#cb9-22"></a>    transform<span class="op">=</span>torchvision.transforms.ToTensor()</span>
<span id="cb9-23"><a href="#cb9-23"></a>)</span>
<span id="cb9-24"><a href="#cb9-24"></a></span>
<span id="cb9-25"><a href="#cb9-25"></a>training_data, validation_data <span class="op">=</span> torch.utils.data.random_split(training_data, [<span class="fl">0.8</span>, <span class="fl">0.2</span>], generator<span class="op">=</span>torch.Generator().manual_seed(<span class="dv">55</span>))</span>
<span id="cb9-26"><a href="#cb9-26"></a></span>
<span id="cb9-27"><a href="#cb9-27"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb9-28"><a href="#cb9-28"></a></span>
<span id="cb9-29"><a href="#cb9-29"></a><span class="co"># The dataloader makes our dataset iterable </span></span>
<span id="cb9-30"><a href="#cb9-30"></a>train_dataloader <span class="op">=</span> torch.utils.data.DataLoader(training_data, </span>
<span id="cb9-31"><a href="#cb9-31"></a>    batch_size<span class="op">=</span>batch_size, </span>
<span id="cb9-32"><a href="#cb9-32"></a>    pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb9-33"><a href="#cb9-33"></a>    shuffle<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb9-34"><a href="#cb9-34"></a>    num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb9-35"><a href="#cb9-35"></a>val_dataloader <span class="op">=</span> torch.utils.data.DataLoader(validation_data, </span>
<span id="cb9-36"><a href="#cb9-36"></a>    batch_size<span class="op">=</span>batch_size, </span>
<span id="cb9-37"><a href="#cb9-37"></a>    pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb9-38"><a href="#cb9-38"></a>    shuffle<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb9-39"><a href="#cb9-39"></a>    num_workers<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.
  warnings.warn(</code></pre>
</div>
</div>
<div id="cacd753e" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="op">%</span>matplotlib inline</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="85b98323" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>batch, (X, Y) <span class="op">=</span> <span class="bu">next</span>(<span class="bu">enumerate</span>(train_dataloader))</span>
<span id="cb12-2"><a href="#cb12-2"></a>plt.imshow(X[<span class="dv">0</span>].cpu().permute((<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>)))<span class="op">;</span> plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/samforeman/projects/saforem2/intro-hpc-bootcamp-2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.
  warnings.warn(warn_msg)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-11-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="index_files/figure-html/cell-11-output-2.png" width="759" height="755" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>This code below is important as our models get bigger: this is wrapping the pytorch data loaders to put the data onto the GPU!</p>
<div id="652fcdee" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>dev <span class="op">=</span> torch.device(</span>
<span id="cb14-2"><a href="#cb14-2"></a>    <span class="st">"cuda"</span>) <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb14-3"><a href="#cb14-3"></a></span>
<span id="cb14-4"><a href="#cb14-4"></a></span>
<span id="cb14-5"><a href="#cb14-5"></a><span class="kw">def</span> preprocess(x, y):</span>
<span id="cb14-6"><a href="#cb14-6"></a>    <span class="co"># CIFAR-10 is *color* images so 3 layers!</span></span>
<span id="cb14-7"><a href="#cb14-7"></a>    <span class="cf">return</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>).to(dev), y.to(dev)</span>
<span id="cb14-8"><a href="#cb14-8"></a></span>
<span id="cb14-9"><a href="#cb14-9"></a></span>
<span id="cb14-10"><a href="#cb14-10"></a><span class="kw">class</span> WrappedDataLoader:</span>
<span id="cb14-11"><a href="#cb14-11"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dl, func):</span>
<span id="cb14-12"><a href="#cb14-12"></a>        <span class="va">self</span>.dl <span class="op">=</span> dl</span>
<span id="cb14-13"><a href="#cb14-13"></a>        <span class="va">self</span>.func <span class="op">=</span> func</span>
<span id="cb14-14"><a href="#cb14-14"></a></span>
<span id="cb14-15"><a href="#cb14-15"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb14-16"><a href="#cb14-16"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.dl)</span>
<span id="cb14-17"><a href="#cb14-17"></a></span>
<span id="cb14-18"><a href="#cb14-18"></a>    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>):</span>
<span id="cb14-19"><a href="#cb14-19"></a>        <span class="cf">for</span> b <span class="kw">in</span> <span class="va">self</span>.dl:</span>
<span id="cb14-20"><a href="#cb14-20"></a>            <span class="cf">yield</span> (<span class="va">self</span>.func(<span class="op">*</span>b))</span>
<span id="cb14-21"><a href="#cb14-21"></a></span>
<span id="cb14-22"><a href="#cb14-22"></a></span>
<span id="cb14-23"><a href="#cb14-23"></a>train_dataloader <span class="op">=</span> WrappedDataLoader(train_dataloader, preprocess)</span>
<span id="cb14-24"><a href="#cb14-24"></a>val_dataloader <span class="op">=</span> WrappedDataLoader(val_dataloader, preprocess)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="369e0a3a" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb15-2"><a href="#cb15-2"></a></span>
<span id="cb15-3"><a href="#cb15-3"></a></span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="kw">class</span> Downsampler(nn.Module):</span>
<span id="cb15-5"><a href="#cb15-5"></a></span>
<span id="cb15-6"><a href="#cb15-6"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, shape, stride<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb15-7"><a href="#cb15-7"></a>        <span class="bu">super</span>(Downsampler, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb15-8"><a href="#cb15-8"></a></span>
<span id="cb15-9"><a href="#cb15-9"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm([in_channels, <span class="op">*</span>shape])</span>
<span id="cb15-10"><a href="#cb15-10"></a></span>
<span id="cb15-11"><a href="#cb15-11"></a>        <span class="va">self</span>.downsample <span class="op">=</span> nn.Conv2d(</span>
<span id="cb15-12"><a href="#cb15-12"></a>            in_channels<span class="op">=</span>in_channels, </span>
<span id="cb15-13"><a href="#cb15-13"></a>            out_channels<span class="op">=</span>out_channels,</span>
<span id="cb15-14"><a href="#cb15-14"></a>            kernel_size <span class="op">=</span> stride,</span>
<span id="cb15-15"><a href="#cb15-15"></a>            stride <span class="op">=</span> stride,</span>
<span id="cb15-16"><a href="#cb15-16"></a>        )</span>
<span id="cb15-17"><a href="#cb15-17"></a></span>
<span id="cb15-18"><a href="#cb15-18"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb15-19"><a href="#cb15-19"></a></span>
<span id="cb15-20"><a href="#cb15-20"></a></span>
<span id="cb15-21"><a href="#cb15-21"></a>        <span class="cf">return</span> <span class="va">self</span>.downsample(<span class="va">self</span>.norm(inputs))</span>
<span id="cb15-22"><a href="#cb15-22"></a></span>
<span id="cb15-23"><a href="#cb15-23"></a></span>
<span id="cb15-24"><a href="#cb15-24"></a></span>
<span id="cb15-25"><a href="#cb15-25"></a><span class="kw">class</span> ConvNextBlock(nn.Module):</span>
<span id="cb15-26"><a href="#cb15-26"></a>    <span class="co">"""This block of operations is loosely based on this paper:</span></span>
<span id="cb15-27"><a href="#cb15-27"></a></span>
<span id="cb15-28"><a href="#cb15-28"></a><span class="co">    """</span></span>
<span id="cb15-29"><a href="#cb15-29"></a></span>
<span id="cb15-30"><a href="#cb15-30"></a></span>
<span id="cb15-31"><a href="#cb15-31"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, shape):</span>
<span id="cb15-32"><a href="#cb15-32"></a>        <span class="bu">super</span>(ConvNextBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb15-33"><a href="#cb15-33"></a></span>
<span id="cb15-34"><a href="#cb15-34"></a>        <span class="co"># Depthwise, seperable convolution with a large number of output filters:</span></span>
<span id="cb15-35"><a href="#cb15-35"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span>in_channels, </span>
<span id="cb15-36"><a href="#cb15-36"></a>                                     out_channels<span class="op">=</span>in_channels, </span>
<span id="cb15-37"><a href="#cb15-37"></a>                                     groups<span class="op">=</span>in_channels,</span>
<span id="cb15-38"><a href="#cb15-38"></a>                                     kernel_size<span class="op">=</span>[<span class="dv">7</span>,<span class="dv">7</span>],</span>
<span id="cb15-39"><a href="#cb15-39"></a>                                     padding<span class="op">=</span><span class="st">'same'</span> )</span>
<span id="cb15-40"><a href="#cb15-40"></a></span>
<span id="cb15-41"><a href="#cb15-41"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm([in_channels, <span class="op">*</span>shape])</span>
<span id="cb15-42"><a href="#cb15-42"></a></span>
<span id="cb15-43"><a href="#cb15-43"></a>        <span class="co"># Two more convolutions:</span></span>
<span id="cb15-44"><a href="#cb15-44"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span>in_channels, </span>
<span id="cb15-45"><a href="#cb15-45"></a>                                     out_channels<span class="op">=</span><span class="dv">4</span><span class="op">*</span>in_channels,</span>
<span id="cb15-46"><a href="#cb15-46"></a>                                     kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-47"><a href="#cb15-47"></a></span>
<span id="cb15-48"><a href="#cb15-48"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">4</span><span class="op">*</span>in_channels, </span>
<span id="cb15-49"><a href="#cb15-49"></a>                                     out_channels<span class="op">=</span>in_channels,</span>
<span id="cb15-50"><a href="#cb15-50"></a>                                     kernel_size<span class="op">=</span><span class="dv">1</span></span>
<span id="cb15-51"><a href="#cb15-51"></a>                                     )</span>
<span id="cb15-52"><a href="#cb15-52"></a></span>
<span id="cb15-53"><a href="#cb15-53"></a></span>
<span id="cb15-54"><a href="#cb15-54"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb15-55"><a href="#cb15-55"></a>        x <span class="op">=</span> <span class="va">self</span>.conv1(inputs)</span>
<span id="cb15-56"><a href="#cb15-56"></a></span>
<span id="cb15-57"><a href="#cb15-57"></a>        <span class="co"># The normalization layer:</span></span>
<span id="cb15-58"><a href="#cb15-58"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb15-59"><a href="#cb15-59"></a></span>
<span id="cb15-60"><a href="#cb15-60"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</span>
<span id="cb15-61"><a href="#cb15-61"></a></span>
<span id="cb15-62"><a href="#cb15-62"></a>        <span class="co"># The non-linear activation layer:</span></span>
<span id="cb15-63"><a href="#cb15-63"></a>        x <span class="op">=</span> torch.nn.functional.gelu(x)</span>
<span id="cb15-64"><a href="#cb15-64"></a></span>
<span id="cb15-65"><a href="#cb15-65"></a>        x <span class="op">=</span> <span class="va">self</span>.conv3(x)</span>
<span id="cb15-66"><a href="#cb15-66"></a></span>
<span id="cb15-67"><a href="#cb15-67"></a>        <span class="co"># This makes it a residual network:</span></span>
<span id="cb15-68"><a href="#cb15-68"></a>        <span class="cf">return</span> x <span class="op">+</span> inputs</span>
<span id="cb15-69"><a href="#cb15-69"></a></span>
<span id="cb15-70"><a href="#cb15-70"></a></span>
<span id="cb15-71"><a href="#cb15-71"></a><span class="kw">class</span> Classifier(nn.Module):</span>
<span id="cb15-72"><a href="#cb15-72"></a></span>
<span id="cb15-73"><a href="#cb15-73"></a></span>
<span id="cb15-74"><a href="#cb15-74"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_initial_filters, n_stages, blocks_per_stage):</span>
<span id="cb15-75"><a href="#cb15-75"></a>        <span class="bu">super</span>(Classifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb15-76"><a href="#cb15-76"></a></span>
<span id="cb15-77"><a href="#cb15-77"></a>        <span class="co"># This is a downsampling convolution that will produce patches of output.</span></span>
<span id="cb15-78"><a href="#cb15-78"></a></span>
<span id="cb15-79"><a href="#cb15-79"></a>        <span class="co"># This is similar to what vision transformers do to tokenize the images.</span></span>
<span id="cb15-80"><a href="#cb15-80"></a>        <span class="va">self</span>.stem <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb15-81"><a href="#cb15-81"></a>                                    out_channels<span class="op">=</span>n_initial_filters,</span>
<span id="cb15-82"><a href="#cb15-82"></a>                                    kernel_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb15-83"><a href="#cb15-83"></a>                                    stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-84"><a href="#cb15-84"></a></span>
<span id="cb15-85"><a href="#cb15-85"></a>        current_shape <span class="op">=</span> [<span class="dv">32</span>, <span class="dv">32</span>]</span>
<span id="cb15-86"><a href="#cb15-86"></a></span>
<span id="cb15-87"><a href="#cb15-87"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm([n_initial_filters,<span class="op">*</span>current_shape])</span>
<span id="cb15-88"><a href="#cb15-88"></a>        <span class="co"># self.norm1 = WrappedLayerNorm()</span></span>
<span id="cb15-89"><a href="#cb15-89"></a></span>
<span id="cb15-90"><a href="#cb15-90"></a>        current_n_filters <span class="op">=</span> n_initial_filters</span>
<span id="cb15-91"><a href="#cb15-91"></a></span>
<span id="cb15-92"><a href="#cb15-92"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.Sequential()</span>
<span id="cb15-93"><a href="#cb15-93"></a>        <span class="cf">for</span> i, n_blocks <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">range</span>(n_stages)):</span>
<span id="cb15-94"><a href="#cb15-94"></a>            <span class="co"># Add a convnext block series:</span></span>
<span id="cb15-95"><a href="#cb15-95"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(blocks_per_stage):</span>
<span id="cb15-96"><a href="#cb15-96"></a>                <span class="va">self</span>.layers.append(ConvNextBlock(in_channels<span class="op">=</span>current_n_filters, shape<span class="op">=</span>current_shape))</span>
<span id="cb15-97"><a href="#cb15-97"></a>            <span class="co"># Add a downsampling layer:</span></span>
<span id="cb15-98"><a href="#cb15-98"></a>            <span class="cf">if</span> i <span class="op">!=</span> n_stages <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb15-99"><a href="#cb15-99"></a>                <span class="co"># Skip downsampling if it's the last layer!</span></span>
<span id="cb15-100"><a href="#cb15-100"></a>                <span class="va">self</span>.layers.append(Downsampler(</span>
<span id="cb15-101"><a href="#cb15-101"></a>                    in_channels<span class="op">=</span>current_n_filters, </span>
<span id="cb15-102"><a href="#cb15-102"></a>                    out_channels<span class="op">=</span><span class="dv">2</span><span class="op">*</span>current_n_filters,</span>
<span id="cb15-103"><a href="#cb15-103"></a>                    shape <span class="op">=</span> current_shape,</span>
<span id="cb15-104"><a href="#cb15-104"></a>                    )</span>
<span id="cb15-105"><a href="#cb15-105"></a>                )</span>
<span id="cb15-106"><a href="#cb15-106"></a>                <span class="co"># Double the number of filters:</span></span>
<span id="cb15-107"><a href="#cb15-107"></a>                current_n_filters <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>current_n_filters</span>
<span id="cb15-108"><a href="#cb15-108"></a>                <span class="co"># Cut the shape in half:</span></span>
<span id="cb15-109"><a href="#cb15-109"></a>                current_shape <span class="op">=</span> [ cs <span class="op">//</span> <span class="dv">2</span> <span class="cf">for</span> cs <span class="kw">in</span> current_shape]</span>
<span id="cb15-110"><a href="#cb15-110"></a></span>
<span id="cb15-111"><a href="#cb15-111"></a></span>
<span id="cb15-112"><a href="#cb15-112"></a></span>
<span id="cb15-113"><a href="#cb15-113"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Sequential(</span>
<span id="cb15-114"><a href="#cb15-114"></a>            nn.Flatten(),</span>
<span id="cb15-115"><a href="#cb15-115"></a>            nn.LayerNorm(current_n_filters),</span>
<span id="cb15-116"><a href="#cb15-116"></a>            nn.Linear(current_n_filters, <span class="dv">10</span>)</span>
<span id="cb15-117"><a href="#cb15-117"></a>        )</span>
<span id="cb15-118"><a href="#cb15-118"></a>        <span class="co"># self.norm2 = nn.InstanceNorm2d(current_n_filters)</span></span>
<span id="cb15-119"><a href="#cb15-119"></a>        <span class="co"># # This brings it down to one channel / class</span></span>
<span id="cb15-120"><a href="#cb15-120"></a>        <span class="co"># self.bottleneck = nn.Conv2d(in_channels=current_n_filters, out_channels=10, </span></span>
<span id="cb15-121"><a href="#cb15-121"></a>        <span class="co">#                                   kernel_size=1, stride=1)</span></span>
<span id="cb15-122"><a href="#cb15-122"></a></span>
<span id="cb15-123"><a href="#cb15-123"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb15-124"><a href="#cb15-124"></a></span>
<span id="cb15-125"><a href="#cb15-125"></a>        x <span class="op">=</span> <span class="va">self</span>.stem(inputs)</span>
<span id="cb15-126"><a href="#cb15-126"></a>        <span class="co"># Apply a normalization after the initial patching:</span></span>
<span id="cb15-127"><a href="#cb15-127"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x)</span>
<span id="cb15-128"><a href="#cb15-128"></a></span>
<span id="cb15-129"><a href="#cb15-129"></a>        <span class="co"># Apply the main chunk of the network:</span></span>
<span id="cb15-130"><a href="#cb15-130"></a>        x <span class="op">=</span> <span class="va">self</span>.layers(x)</span>
<span id="cb15-131"><a href="#cb15-131"></a></span>
<span id="cb15-132"><a href="#cb15-132"></a>        <span class="co"># Normalize and readout:</span></span>
<span id="cb15-133"><a href="#cb15-133"></a>        x <span class="op">=</span> nn.functional.avg_pool2d(x, x.shape[<span class="dv">2</span>:])</span>
<span id="cb15-134"><a href="#cb15-134"></a>        x <span class="op">=</span> <span class="va">self</span>.head(x)</span>
<span id="cb15-135"><a href="#cb15-135"></a></span>
<span id="cb15-136"><a href="#cb15-136"></a>        <span class="cf">return</span> x</span>
<span id="cb15-137"><a href="#cb15-137"></a></span>
<span id="cb15-138"><a href="#cb15-138"></a></span>
<span id="cb15-139"><a href="#cb15-139"></a></span>
<span id="cb15-140"><a href="#cb15-140"></a>        <span class="co"># x = self.norm2(x)</span></span>
<span id="cb15-141"><a href="#cb15-141"></a>        <span class="co"># x = self.bottleneck(x)</span></span>
<span id="cb15-142"><a href="#cb15-142"></a></span>
<span id="cb15-143"><a href="#cb15-143"></a>        <span class="co"># # Average pooling of the remaining spatial dimensions (and reshape) makes this label-like:</span></span>
<span id="cb15-144"><a href="#cb15-144"></a>        <span class="co"># return nn.functional.avg_pool2d(x, kernel_size=x.shape[-2:]).reshape((-1,10))</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="b02dbf50" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="op">!</span>pip install torchinfo <span class="co"># if not on Polaris</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="4cd9b018" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>model <span class="op">=</span> Classifier(<span class="dv">32</span>, <span class="dv">4</span>, <span class="dv">2</span>).to(device<span class="op">=</span>dev)</span>
<span id="cb17-2"><a href="#cb17-2"></a></span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb17-4"><a href="#cb17-4"></a></span>
<span id="cb17-5"><a href="#cb17-5"></a><span class="bu">print</span>(summary(model, input_size<span class="op">=</span>(batch_size, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">==========================================================================================
Layer <span style="font-weight: bold">(</span>typ<span style="color: #00ff00; text-decoration-color: #00ff00; font-weight: bold">e:de</span>pth-idx<span style="font-weight: bold">)</span>                   Output Shape              Param #
==========================================================================================
Classifier                               <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span><span style="font-weight: bold">]</span>                 --
├─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>                            <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>
├─LayerNorm: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>                         <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">65</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">536</span>
├─Sequential: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>                        <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">]</span>          --
│    └─ConvNextBlock: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>                <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span><span style="font-weight: bold">]</span>         --
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>                  <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">600</span>
│    │    └─LayerNorm: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>               <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">65</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">536</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>                  <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span><span style="font-weight: bold">]</span>        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">224</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>                  <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>
│    └─ConvNextBlock: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>                <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span><span style="font-weight: bold">]</span>         --
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>                  <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">600</span>
│    │    └─LayerNorm: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>               <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">65</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">536</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>                  <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span><span style="font-weight: bold">]</span>        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">224</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>                  <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>
│    └─Downsampler: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>                  <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">64</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>         --
│    │    └─LayerNorm: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>               <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">65</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">536</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">64</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>
│    └─ConvNextBlock: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>                <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">64</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>         --
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">64</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">200</span>
│    │    └─LayerNorm: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>              <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">64</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">640</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">64</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">448</span>
│    └─ConvNextBlock: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>                <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">64</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>         --
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">64</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">200</span>
│    │    └─LayerNorm: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>              <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">64</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">640</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">18</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">64</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">448</span>
│    └─Downsampler: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>                  <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">]</span>          --
│    │    └─LayerNorm: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19</span>              <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">64</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">20</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">896</span>
│    └─ConvNextBlock: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>                <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">]</span>          --
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">21</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">400</span>
│    │    └─LayerNorm: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">22</span>              <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">384</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">23</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">512</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">66</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">048</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">24</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">65</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">664</span>
│    └─ConvNextBlock: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>                <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">]</span>          --
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">25</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">400</span>
│    │    └─LayerNorm: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26</span>              <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">384</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">27</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">512</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">66</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">048</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">28</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">65</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">664</span>
│    └─Downsampler: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>                  <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">]</span>          --
│    │    └─LayerNorm: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">29</span>              <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">384</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">30</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">131</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">328</span>
│    └─ConvNextBlock: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>               <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">]</span>          --
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">31</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">800</span>
│    │    └─LayerNorm: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>              <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">192</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">33</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1024</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">263</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">168</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">34</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">262</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">400</span>
│    └─ConvNextBlock: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>               <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">]</span>          --
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">35</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">800</span>
│    │    └─LayerNorm: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">36</span>              <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">192</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">37</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1024</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">]</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">263</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">168</span>
│    │    └─Conv2d: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">38</span>                 <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">]</span>          <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">262</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">400</span>
├─Sequential: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>                        <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span><span style="font-weight: bold">]</span>                 --
│    └─Flatten: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>                     <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span><span style="font-weight: bold">]</span>                --
│    └─LayerNorm: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>                   <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span><span style="font-weight: bold">]</span>                <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">512</span>
│    └─Linear: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>                      <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span><span style="font-weight: bold">]</span>                 <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">570</span>
==========================================================================================
Total params: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">047</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">114</span>
Trainable params: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">047</span>,<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">114</span>
Non-trainable params: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>
Total mult-adds <span style="font-weight: bold">(</span>Units.GIGABYTES<span style="font-weight: bold">)</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10.34</span>
==========================================================================================
Input size <span style="font-weight: bold">(</span>MB<span style="font-weight: bold">)</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.57</span>
Forward/backward pass size <span style="font-weight: bold">(</span>MB<span style="font-weight: bold">)</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1036.27</span>
Params size <span style="font-weight: bold">(</span>MB<span style="font-weight: bold">)</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8.19</span>
Estimated Total Size <span style="font-weight: bold">(</span>MB<span style="font-weight: bold">)</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1046.03</span>
==========================================================================================
</pre>
</div>
</div>
<div id="a700562a" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="kw">def</span> evaluate(dataloader, model, loss_fn, val_bar):</span>
<span id="cb18-2"><a href="#cb18-2"></a>    <span class="co"># Set the model to evaluation mode - some NN pieces behave differently during training</span></span>
<span id="cb18-3"><a href="#cb18-3"></a>    <span class="co"># Unnecessary in this situation but added for best practices</span></span>
<span id="cb18-4"><a href="#cb18-4"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb18-5"><a href="#cb18-5"></a>    size <span class="op">=</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb18-6"><a href="#cb18-6"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb18-7"><a href="#cb18-7"></a>    loss, correct <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb18-8"><a href="#cb18-8"></a></span>
<span id="cb18-9"><a href="#cb18-9"></a>    <span class="co"># We can save computation and memory by not calculating gradients here - we aren't optimizing </span></span>
<span id="cb18-10"><a href="#cb18-10"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb18-11"><a href="#cb18-11"></a>        <span class="co"># loop over all of the batches</span></span>
<span id="cb18-12"><a href="#cb18-12"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> dataloader:</span>
<span id="cb18-13"><a href="#cb18-13"></a></span>
<span id="cb18-14"><a href="#cb18-14"></a>            pred <span class="op">=</span> model(X)</span>
<span id="cb18-15"><a href="#cb18-15"></a>            loss <span class="op">+=</span> loss_fn(pred, y).item()</span>
<span id="cb18-16"><a href="#cb18-16"></a>            <span class="co"># how many are correct in this batch? Tracking for accuracy </span></span>
<span id="cb18-17"><a href="#cb18-17"></a>            correct <span class="op">+=</span> (pred.argmax(<span class="dv">1</span>) <span class="op">==</span> y).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>().item()</span>
<span id="cb18-18"><a href="#cb18-18"></a>            val_bar.update()</span>
<span id="cb18-19"><a href="#cb18-19"></a></span>
<span id="cb18-20"><a href="#cb18-20"></a>    loss <span class="op">/=</span> num_batches</span>
<span id="cb18-21"><a href="#cb18-21"></a>    correct <span class="op">/=</span> (size<span class="op">*</span>batch_size)</span>
<span id="cb18-22"><a href="#cb18-22"></a></span>
<span id="cb18-23"><a href="#cb18-23"></a>    accuracy <span class="op">=</span> <span class="dv">100</span><span class="op">*</span>correct</span>
<span id="cb18-24"><a href="#cb18-24"></a>    <span class="cf">return</span> accuracy, loss</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="b50bb294" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="kw">def</span> train_one_epoch(dataloader, model, loss_fn, optimizer, progress_bar):</span>
<span id="cb19-2"><a href="#cb19-2"></a>    model.train()</span>
<span id="cb19-3"><a href="#cb19-3"></a>    <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb19-4"><a href="#cb19-4"></a>        <span class="co"># forward pass</span></span>
<span id="cb19-5"><a href="#cb19-5"></a>        pred <span class="op">=</span> model(X)</span>
<span id="cb19-6"><a href="#cb19-6"></a>        loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb19-7"><a href="#cb19-7"></a></span>
<span id="cb19-8"><a href="#cb19-8"></a>        <span class="co"># backward pass calculates gradients</span></span>
<span id="cb19-9"><a href="#cb19-9"></a>        loss.backward()</span>
<span id="cb19-10"><a href="#cb19-10"></a></span>
<span id="cb19-11"><a href="#cb19-11"></a>        <span class="co"># take one step with these gradients</span></span>
<span id="cb19-12"><a href="#cb19-12"></a>        optimizer.step()</span>
<span id="cb19-13"><a href="#cb19-13"></a></span>
<span id="cb19-14"><a href="#cb19-14"></a>        <span class="co"># resets the gradients </span></span>
<span id="cb19-15"><a href="#cb19-15"></a>        optimizer.zero_grad()</span>
<span id="cb19-16"><a href="#cb19-16"></a></span>
<span id="cb19-17"><a href="#cb19-17"></a>        progress_bar.update()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="c3c97f03" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb20-2"><a href="#cb20-2"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, weight_decay<span class="op">=</span><span class="fl">0.01</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="ca404dd8" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="im">import</span> time</span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="im">import</span> ezpz</span>
<span id="cb21-3"><a href="#cb21-3"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb21-4"><a href="#cb21-4"></a></span>
<span id="cb21-5"><a href="#cb21-5"></a><span class="kw">def</span> train_step(x, y):</span>
<span id="cb21-6"><a href="#cb21-6"></a>    t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb21-7"><a href="#cb21-7"></a>    <span class="co"># Forward pass</span></span>
<span id="cb21-8"><a href="#cb21-8"></a>    pred <span class="op">=</span> model(x)</span>
<span id="cb21-9"><a href="#cb21-9"></a>    loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb21-10"><a href="#cb21-10"></a>    t1 <span class="op">=</span> time.perf_counter()</span>
<span id="cb21-11"><a href="#cb21-11"></a></span>
<span id="cb21-12"><a href="#cb21-12"></a>    <span class="co"># Backward pass</span></span>
<span id="cb21-13"><a href="#cb21-13"></a>    loss.backward()</span>
<span id="cb21-14"><a href="#cb21-14"></a>    t2 <span class="op">=</span> time.perf_counter()</span>
<span id="cb21-15"><a href="#cb21-15"></a></span>
<span id="cb21-16"><a href="#cb21-16"></a>    <span class="co"># Update weights</span></span>
<span id="cb21-17"><a href="#cb21-17"></a>    optimizer.step()</span>
<span id="cb21-18"><a href="#cb21-18"></a>    t3 <span class="op">=</span> time.perf_counter()</span>
<span id="cb21-19"><a href="#cb21-19"></a></span>
<span id="cb21-20"><a href="#cb21-20"></a>    <span class="co"># Reset gradients</span></span>
<span id="cb21-21"><a href="#cb21-21"></a>    optimizer.zero_grad()</span>
<span id="cb21-22"><a href="#cb21-22"></a>    t4 <span class="op">=</span> time.perf_counter()</span>
<span id="cb21-23"><a href="#cb21-23"></a></span>
<span id="cb21-24"><a href="#cb21-24"></a>    <span class="cf">return</span> loss.item(), {</span>
<span id="cb21-25"><a href="#cb21-25"></a>        <span class="st">"dtf"</span>: t1 <span class="op">-</span> t0,</span>
<span id="cb21-26"><a href="#cb21-26"></a>        <span class="st">"dtb"</span>: t2 <span class="op">-</span> t1,</span>
<span id="cb21-27"><a href="#cb21-27"></a>        <span class="st">"dtu"</span>: t3 <span class="op">-</span> t2,</span>
<span id="cb21-28"><a href="#cb21-28"></a>        <span class="st">"dtz"</span>: t4 <span class="op">-</span> t3,</span>
<span id="cb21-29"><a href="#cb21-29"></a>    }</span>
<span id="cb21-30"><a href="#cb21-30"></a></span>
<span id="cb21-31"><a href="#cb21-31"></a>logger <span class="op">=</span> ezpz.get_logger(<span class="st">"3-conv-nets"</span>)</span>
<span id="cb21-32"><a href="#cb21-32"></a>history <span class="op">=</span> ezpz.History()</span>
<span id="cb21-33"><a href="#cb21-33"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb21-34"><a href="#cb21-34"></a>    t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb21-35"><a href="#cb21-35"></a>    x, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb21-36"><a href="#cb21-36"></a>    t1 <span class="op">=</span> time.perf_counter()</span>
<span id="cb21-37"><a href="#cb21-37"></a>    loss, dt <span class="op">=</span> train_step(x, y)</span>
<span id="cb21-38"><a href="#cb21-38"></a>    logger.info(</span>
<span id="cb21-39"><a href="#cb21-39"></a>        history.update(</span>
<span id="cb21-40"><a href="#cb21-40"></a>            {</span>
<span id="cb21-41"><a href="#cb21-41"></a>                <span class="st">"iter"</span>: i,</span>
<span id="cb21-42"><a href="#cb21-42"></a>                <span class="st">"loss"</span>: loss,</span>
<span id="cb21-43"><a href="#cb21-43"></a>                <span class="st">"dtd"</span>: t1 <span class="op">-</span> t0,</span>
<span id="cb21-44"><a href="#cb21-44"></a>                <span class="op">**</span>dt,</span>
<span id="cb21-45"><a href="#cb21-45"></a>            }</span>
<span id="cb21-46"><a href="#cb21-46"></a>        )</span>
<span id="cb21-47"><a href="#cb21-47"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">[<span style="color: #7f7f7f; text-decoration-color: #7f7f7f">2025-07-23 17:53:51,365606</span>][<span style="color: #008000; text-decoration-color: #008000">I</span>][<span style="color: #008080; text-decoration-color: #008080; font-style: italic">ezpz</span>/<span style="color: #000080; text-decoration-color: #000080">__init__</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #800080; text-decoration-color: #800080">265</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">ezpz</span>]<span style="color: #c0c0c0; text-decoration-color: #c0c0c0"> </span>Setting logging level to <span style="color: #008000; text-decoration-color: #008000">'INFO'</span> on <span style="color: #008000; text-decoration-color: #008000">'RANK == 0'</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">[<span style="color: #7f7f7f; text-decoration-color: #7f7f7f">2025-07-23 17:53:51,366685</span>][<span style="color: #008000; text-decoration-color: #008000">I</span>][<span style="color: #008080; text-decoration-color: #008080; font-style: italic">ezpz</span>/<span style="color: #000080; text-decoration-color: #000080">__init__</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #800080; text-decoration-color: #800080">266</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">ezpz</span>]<span style="color: #c0c0c0; text-decoration-color: #c0c0c0"> </span>Setting logging level to <span style="color: #008000; text-decoration-color: #008000">'CRITICAL'</span> on all others <span style="color: #008000; text-decoration-color: #008000">'RANK != 0'</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">[<span style="color: #7f7f7f; text-decoration-color: #7f7f7f">2025-07-23 17:54:02,803869</span>][<span style="color: #008000; text-decoration-color: #008000">I</span>][<span style="color: #008080; text-decoration-color: #008080; font-style: italic">ipykernel_56729</span>/<span style="color: #000080; text-decoration-color: #000080">1183336067</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #800080; text-decoration-color: #800080">38</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">3-conv-nets</span>]<span style="color: #c0c0c0; text-decoration-color: #c0c0c0"> </span><span style="color: #0000ff; text-decoration-color: #0000ff">iter</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span> <span style="color: #0000ff; text-decoration-color: #0000ff">loss</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.415054</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtd</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3.956162</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtf</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.740205</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtb</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6.733845</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtu</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.002490</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtz</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.001619</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">[<span style="color: #7f7f7f; text-decoration-color: #7f7f7f">2025-07-23 17:54:14,538047</span>][<span style="color: #008000; text-decoration-color: #008000">I</span>][<span style="color: #008080; text-decoration-color: #008080; font-style: italic">ipykernel_56729</span>/<span style="color: #000080; text-decoration-color: #000080">1183336067</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #800080; text-decoration-color: #800080">38</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">3-conv-nets</span>]<span style="color: #c0c0c0; text-decoration-color: #c0c0c0"> </span><span style="color: #0000ff; text-decoration-color: #0000ff">iter</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span> <span style="color: #0000ff; text-decoration-color: #0000ff">loss</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.340536</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtd</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4.138664</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtf</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.783476</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtb</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6.805354</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtu</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.002662</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtz</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.001365</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">[<span style="color: #7f7f7f; text-decoration-color: #7f7f7f">2025-07-23 17:54:25,945551</span>][<span style="color: #008000; text-decoration-color: #008000">I</span>][<span style="color: #008080; text-decoration-color: #008080; font-style: italic">ipykernel_56729</span>/<span style="color: #000080; text-decoration-color: #000080">1183336067</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #800080; text-decoration-color: #800080">38</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">3-conv-nets</span>]<span style="color: #c0c0c0; text-decoration-color: #c0c0c0"> </span><span style="color: #0000ff; text-decoration-color: #0000ff">iter</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span> <span style="color: #0000ff; text-decoration-color: #0000ff">loss</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.335325</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtd</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4.058667</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtf</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.741292</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtb</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6.601992</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtu</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.002268</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtz</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.000930</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">[<span style="color: #7f7f7f; text-decoration-color: #7f7f7f">2025-07-23 17:54:37,424838</span>][<span style="color: #008000; text-decoration-color: #008000">I</span>][<span style="color: #008080; text-decoration-color: #008080; font-style: italic">ipykernel_56729</span>/<span style="color: #000080; text-decoration-color: #000080">1183336067</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #800080; text-decoration-color: #800080">38</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">3-conv-nets</span>]<span style="color: #c0c0c0; text-decoration-color: #c0c0c0"> </span><span style="color: #0000ff; text-decoration-color: #0000ff">iter</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span> <span style="color: #0000ff; text-decoration-color: #0000ff">loss</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.385161</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtd</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4.003525</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtf</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.728265</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtb</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6.742297</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtu</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.002377</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtz</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.000425</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">[<span style="color: #7f7f7f; text-decoration-color: #7f7f7f">2025-07-23 17:54:49,281354</span>][<span style="color: #008000; text-decoration-color: #008000">I</span>][<span style="color: #008080; text-decoration-color: #008080; font-style: italic">ipykernel_56729</span>/<span style="color: #000080; text-decoration-color: #000080">1183336067</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #800080; text-decoration-color: #800080">38</span><span style="color: #0000ff; text-decoration-color: #0000ff">:</span><span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">3-conv-nets</span>]<span style="color: #c0c0c0; text-decoration-color: #c0c0c0"> </span><span style="color: #0000ff; text-decoration-color: #0000ff">iter</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span> <span style="color: #0000ff; text-decoration-color: #0000ff">loss</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.337500</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtd</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4.299756</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtf</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.755723</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtb</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6.794353</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtu</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.002641</span> <span style="color: #0000ff; text-decoration-color: #0000ff">dtz</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.001612</span>
</pre>
</div>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="co"># epochs = 1</span></span>
<span id="cb22-2"><a href="#cb22-2"></a><span class="co"># for j in range(epochs):</span></span>
<span id="cb22-3"><a href="#cb22-3"></a><span class="co">#     with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f"Train Epoch {j}") as train_bar:</span></span>
<span id="cb22-4"><a href="#cb22-4"></a><span class="co">#         train_one_epoch(train_dataloader, model, loss_fn, optimizer, train_bar)</span></span>
<span id="cb22-5"><a href="#cb22-5"></a><span class="co">#</span></span>
<span id="cb22-6"><a href="#cb22-6"></a><span class="co">#     # checking on the training &amp; validation loss &amp; accuracy </span></span>
<span id="cb22-7"><a href="#cb22-7"></a><span class="co">#     # for training data - only once every 5 epochs (takes a while) </span></span>
<span id="cb22-8"><a href="#cb22-8"></a><span class="co">#     if j % 5 == 0:</span></span>
<span id="cb22-9"><a href="#cb22-9"></a><span class="co">#         with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f"Validate (train) Epoch {j}") as train_eval:</span></span>
<span id="cb22-10"><a href="#cb22-10"></a><span class="co">#             acc, loss = evaluate(train_dataloader, model, loss_fn, train_eval)</span></span>
<span id="cb22-11"><a href="#cb22-11"></a><span class="co">#             print(f"Epoch {j}: training loss: {loss:.3f}, accuracy: {acc:.3f}")</span></span>
<span id="cb22-12"><a href="#cb22-12"></a><span class="co">#</span></span>
<span id="cb22-13"><a href="#cb22-13"></a><span class="co">#     with tqdm(total=len(val_dataloader), position=0, leave=True, desc=f"Validate Epoch {j}") as val_bar:</span></span>
<span id="cb22-14"><a href="#cb22-14"></a><span class="co">#         acc_val, loss_val = evaluate(val_dataloader, model, loss_fn, val_bar)</span></span>
<span id="cb22-15"><a href="#cb22-15"></a><span class="co">#         print(f"Epoch {j}: validation loss: {loss_val:.3f}, accuracy: {acc_val:.3f}")</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="homework-1" class="level2">
<h2 class="anchored" data-anchor-id="homework-1">Homework 1:</h2>
<p>In this notebook, we’ve learned about some basic convolutional networks and trained one on CIFAR-10 images. It did … OK. There is significant overfitting of this model. There are some ways to address that, but we didn’t have time to get into that in this session.</p>
<p>Meanwhile, your homework (part 1) for this week is to try to train the model again but with a different architecture. Change one or more of the following: - The number of convolutions between downsampling - The number of filters in each layer - The initial “patchify” layer - Another hyper-parameter of your choosing</p>
<p>And compare your final validation accuracy to the accuracy shown here. Can you beat the validation accuracy shown?</p>
<p>For full credit on the homework, you need to show (via text, or make a plot) the training and validation data sets’ performance (loss and accuracy) for all the epochs you train. You also need to explain, in several sentences, what you changed in the network and why you think it makes a difference.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{foreman2025,
  author = {Foreman, Sam and Foreman, Sam and Zheng, Huihuo and Adams,
    Corey and Lusch, Bethany},
  title = {Convolutional {Neural} {Networks}},
  date = {2025-07-22},
  url = {https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/3-conv-nets/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-foreman2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Foreman, Sam, Sam Foreman, Huihuo Zheng, Corey Adams, and Bethany Lusch.
2025. <span>“Convolutional Neural Networks.”</span> July 22, 2025. <a href="https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/3-conv-nets/">https://saforem2.github.io/hpc-bootcamp-2025/01-neural-networks/3-conv-nets/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/saforem2\.github\.io\/hpc-bootcamp-2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="dark">
<input type="hidden" id="giscus-alt-theme" value="light">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "saforem2/intro-hpc-bootcamp-2025";
    script.dataset.repoId = "R_kgDOPQ7DhQ";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOPQ7Dhc4CtXSR";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../01-neural-networks/2-advanced/index.html" class="pagination-link" aria-label="[2] Advanced">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">[2] Advanced</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../01-neural-networks/4-representation-learning/index.html" class="pagination-link" aria-label="[4] Representation Learning">
        <span class="nav-page-text">[4] Representation Learning</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb23-1"><a href="#cb23-1"></a><span class="co">---</span></span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="an">title:</span><span class="co"> Convolutional Neural Networks</span></span>
<span id="cb23-3"><a href="#cb23-3"></a><span class="co"># jupyter: python3</span></span>
<span id="cb23-4"><a href="#cb23-4"></a><span class="an">author:</span></span>
<span id="cb23-5"><a href="#cb23-5"></a><span class="co">  - name: Sam Foreman</span></span>
<span id="cb23-6"><a href="#cb23-6"></a><span class="co">    id: sf</span></span>
<span id="cb23-7"><a href="#cb23-7"></a><span class="co">    orcid: 0000-0002-9981-0876</span></span>
<span id="cb23-8"><a href="#cb23-8"></a><span class="co">    email: foremans@anl.gov</span></span>
<span id="cb23-9"><a href="#cb23-9"></a><span class="co">    affiliation:</span></span>
<span id="cb23-10"><a href="#cb23-10"></a><span class="co">      - name: '[ANL](https://www.anl.gov/)'</span></span>
<span id="cb23-11"><a href="#cb23-11"></a><span class="co">        city: Lemont</span></span>
<span id="cb23-12"><a href="#cb23-12"></a><span class="co">        state: IL</span></span>
<span id="cb23-13"><a href="#cb23-13"></a><span class="co">        url: https://alcf.anl.gov/about/people/sam-foreman</span></span>
<span id="cb23-14"><a href="#cb23-14"></a><span class="co">  - name: Huihuo Zheng</span></span>
<span id="cb23-15"><a href="#cb23-15"></a><span class="co">    id: hz</span></span>
<span id="cb23-16"><a href="#cb23-16"></a><span class="co">  - name: Corey Adams</span></span>
<span id="cb23-17"><a href="#cb23-17"></a><span class="co">    id: ca</span></span>
<span id="cb23-18"><a href="#cb23-18"></a><span class="co">  - name: Bethany Lusch</span></span>
<span id="cb23-19"><a href="#cb23-19"></a><span class="co">    id: bl</span></span>
<span id="cb23-20"><a href="#cb23-20"></a><span class="co">---</span></span>
<span id="cb23-21"><a href="#cb23-21"></a></span>
<span id="cb23-22"><a href="#cb23-22"></a><span class="co">[</span><span class="al">![](https://colab.research.google.com/assets/colab-badge.svg)</span><span class="co">](https://colab.research.google.com/github/saforem2/intro-hpc-bootcamp-2025/blob/main/docs/01-neural-networks/3-conv-nets/index.ipynb)</span></span>
<span id="cb23-23"><a href="#cb23-23"></a></span>
<span id="cb23-24"><a href="#cb23-24"></a>Up until transformers, convolutions were *the* state of the art in computer</span>
<span id="cb23-25"><a href="#cb23-25"></a>vision.</span>
<span id="cb23-26"><a href="#cb23-26"></a></span>
<span id="cb23-27"><a href="#cb23-27"></a>In many ways and applications they still are!</span>
<span id="cb23-28"><a href="#cb23-28"></a></span>
<span id="cb23-29"><a href="#cb23-29"></a>Large Language Models, which are what we'll focus on the rest of the series after this lecture, are really good at ordered, *tokenized data.  But there is lots of data that isn't _implicitly_ ordered like <span class="in">`images`</span>, and their more general cousins <span class="in">`graphs`</span>.</span>
<span id="cb23-30"><a href="#cb23-30"></a></span>
<span id="cb23-31"><a href="#cb23-31"></a>Today's lecture focuses on computer vision models, and particularly on convolutional neural networks.  There are a ton of applications you can do with these, and not nearly enough time to get into them.  Check out the extra references file to see some publications to get you started if you want to learn more.</span>
<span id="cb23-32"><a href="#cb23-32"></a></span>
<span id="cb23-33"><a href="#cb23-33"></a>Tip: this notebook is much faster on the GPU!</span>
<span id="cb23-34"><a href="#cb23-34"></a></span>
<span id="cb23-35"><a href="#cb23-35"></a></span>
<span id="cb23-36"><a href="#cb23-36"></a><span class="fu">## Convolutional Networks: A brief historical context</span></span>
<span id="cb23-37"><a href="#cb23-37"></a></span>
<span id="cb23-38"><a href="#cb23-38"></a><span class="al">![ImageNet Accuracy by Yearh](./ImageNet.png)</span></span>
<span id="cb23-39"><a href="#cb23-39"></a></span>
<span id="cb23-42"><a href="#cb23-42"></a><span class="in">```{python}</span></span>
<span id="cb23-43"><a href="#cb23-43"></a><span class="im">import</span> ambivalent</span>
<span id="cb23-44"><a href="#cb23-44"></a></span>
<span id="cb23-45"><a href="#cb23-45"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb23-46"><a href="#cb23-46"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb23-47"><a href="#cb23-47"></a></span>
<span id="cb23-48"><a href="#cb23-48"></a>plt.style.use(ambivalent.STYLES[<span class="st">'ambivalent'</span>])</span>
<span id="cb23-49"><a href="#cb23-49"></a>sns.set_context(<span class="st">"notebook"</span>)</span>
<span id="cb23-50"><a href="#cb23-50"></a></span>
<span id="cb23-51"><a href="#cb23-51"></a><span class="im">from</span> ezpz.log.config <span class="im">import</span> STYLES</span>
<span id="cb23-52"><a href="#cb23-52"></a><span class="im">from</span> rich.console <span class="im">import</span> Console</span>
<span id="cb23-53"><a href="#cb23-53"></a><span class="im">from</span> rich.theme <span class="im">import</span> Theme</span>
<span id="cb23-54"><a href="#cb23-54"></a></span>
<span id="cb23-55"><a href="#cb23-55"></a>console <span class="op">=</span> Console(theme<span class="op">=</span>Theme(STYLES))</span>
<span id="cb23-56"><a href="#cb23-56"></a></span>
<span id="cb23-57"><a href="#cb23-57"></a><span class="co"># Data</span></span>
<span id="cb23-58"><a href="#cb23-58"></a>years <span class="op">=</span> [<span class="dv">2010</span>, <span class="dv">2011</span>, <span class="dv">2012</span>, <span class="dv">2013</span>, <span class="dv">2014</span>, <span class="dv">2015</span>, <span class="dv">2016</span>, <span class="dv">2017</span>]</span>
<span id="cb23-59"><a href="#cb23-59"></a>image_net_error_rates <span class="op">=</span> [<span class="dv">28</span>, <span class="dv">26</span>, <span class="dv">16</span>, <span class="dv">12</span>, <span class="dv">7</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">2</span>]</span>
<span id="cb23-60"><a href="#cb23-60"></a>data <span class="op">=</span> {</span>
<span id="cb23-61"><a href="#cb23-61"></a>    <span class="dv">2010</span>: <span class="dv">28</span>,</span>
<span id="cb23-62"><a href="#cb23-62"></a>    <span class="dv">2011</span>: <span class="dv">26</span>,</span>
<span id="cb23-63"><a href="#cb23-63"></a>    <span class="dv">2012</span>: <span class="dv">16</span>,</span>
<span id="cb23-64"><a href="#cb23-64"></a>    <span class="dv">2013</span>: <span class="dv">12</span>,</span>
<span id="cb23-65"><a href="#cb23-65"></a>    <span class="dv">2014</span>: <span class="dv">7</span>,</span>
<span id="cb23-66"><a href="#cb23-66"></a>    <span class="dv">2015</span>: <span class="dv">3</span>,</span>
<span id="cb23-67"><a href="#cb23-67"></a>    <span class="dv">2016</span>: <span class="fl">2.3</span>,</span>
<span id="cb23-68"><a href="#cb23-68"></a>    <span class="dv">2017</span>: <span class="fl">2.1</span></span>
<span id="cb23-69"><a href="#cb23-69"></a>}</span>
<span id="cb23-70"><a href="#cb23-70"></a>human_error_rate <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb23-71"><a href="#cb23-71"></a></span>
<span id="cb23-72"><a href="#cb23-72"></a><span class="co"># Create bar plot</span></span>
<span id="cb23-73"><a href="#cb23-73"></a>plt.bar(years, image_net_error_rates, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb23-74"><a href="#cb23-74"></a></span>
<span id="cb23-75"><a href="#cb23-75"></a><span class="co"># Add human error rate line</span></span>
<span id="cb23-76"><a href="#cb23-76"></a>plt.axhline(y<span class="op">=</span>human_error_rate, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'Human error rate'</span>)</span>
<span id="cb23-77"><a href="#cb23-77"></a></span>
<span id="cb23-78"><a href="#cb23-78"></a><span class="co"># Labels and title</span></span>
<span id="cb23-79"><a href="#cb23-79"></a>plt.xlabel(<span class="st">'Year'</span>)</span>
<span id="cb23-80"><a href="#cb23-80"></a>plt.ylabel(<span class="st">'ImageNet Visual Recognition Error Rate (%)'</span>)</span>
<span id="cb23-81"><a href="#cb23-81"></a>plt.title(<span class="st">'ImageNet Error Rates Over Time'</span>)</span>
<span id="cb23-82"><a href="#cb23-82"></a>plt.legend()</span>
<span id="cb23-83"><a href="#cb23-83"></a></span>
<span id="cb23-84"><a href="#cb23-84"></a><span class="co"># Display plot</span></span>
<span id="cb23-85"><a href="#cb23-85"></a>plt.show()</span>
<span id="cb23-86"><a href="#cb23-86"></a></span>
<span id="cb23-87"><a href="#cb23-87"></a><span class="in">```</span></span>
<span id="cb23-88"><a href="#cb23-88"></a></span>
<span id="cb23-89"><a href="#cb23-89"></a></span>
<span id="cb23-90"><a href="#cb23-90"></a></span>
<span id="cb23-91"><a href="#cb23-91"></a><span class="co">[</span><span class="ot">reference</span><span class="co">](https://www.researchgate.net/publication/332452649_A_Roadmap_for_Foundational_Research_on_Artificial_Intelligence_in_Medical_Imaging_From_the_2018_NIHRSNAACRThe_Academy_Workshop)</span></span>
<span id="cb23-92"><a href="#cb23-92"></a></span>
<span id="cb23-93"><a href="#cb23-93"></a></span>
<span id="cb23-96"><a href="#cb23-96"></a><span class="in">```{python}</span></span>
<span id="cb23-97"><a href="#cb23-97"></a><span class="im">import</span> torch, torchvision</span>
<span id="cb23-98"><a href="#cb23-98"></a><span class="in">```</span></span>
<span id="cb23-99"><a href="#cb23-99"></a></span>
<span id="cb23-100"><a href="#cb23-100"></a><span class="fu">## Convolutional Building Blocks</span></span>
<span id="cb23-101"><a href="#cb23-101"></a></span>
<span id="cb23-102"><a href="#cb23-102"></a>We're going to go through some examples of building blocks for convolutional networks.  To help illustate some of these, let's use an image for examples:</span>
<span id="cb23-103"><a href="#cb23-103"></a></span>
<span id="cb23-106"><a href="#cb23-106"></a><span class="in">```{python}</span></span>
<span id="cb23-107"><a href="#cb23-107"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb23-108"><a href="#cb23-108"></a><span class="co"># wget line useful in Google Colab</span></span>
<span id="cb23-109"><a href="#cb23-109"></a><span class="op">!</span> wget https:<span class="op">//</span>raw.githubusercontent.com<span class="op">/</span>argonne<span class="op">-</span>lcf<span class="op">/</span>ai<span class="op">-</span>science<span class="op">-</span>training<span class="op">-</span>series<span class="op">/</span>main<span class="op">/</span><span class="dv">0</span><span class="er">3_advanced_neural_networks</span><span class="op">/</span>ALCF<span class="op">-</span>Staff.jpg</span>
<span id="cb23-110"><a href="#cb23-110"></a>alcf_image <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">"ALCF-Staff.jpg"</span>)</span>
<span id="cb23-111"><a href="#cb23-111"></a><span class="in">```</span></span>
<span id="cb23-112"><a href="#cb23-112"></a></span>
<span id="cb23-115"><a href="#cb23-115"></a><span class="in">```{python}</span></span>
<span id="cb23-116"><a href="#cb23-116"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb23-117"><a href="#cb23-117"></a>figure <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb23-118"><a href="#cb23-118"></a>plt.imshow(alcf_image)</span>
<span id="cb23-119"><a href="#cb23-119"></a>plt.show()</span>
<span id="cb23-120"><a href="#cb23-120"></a><span class="in">```</span></span>
<span id="cb23-121"><a href="#cb23-121"></a></span>
<span id="cb23-122"><a href="#cb23-122"></a><span class="fu">### Convolutions</span></span>
<span id="cb23-123"><a href="#cb23-123"></a></span>
<span id="cb23-124"><a href="#cb23-124"></a>Convolutions are a restriction of - and a specialization of - dense linear layers.  A convolution of an image produces another image, and each output pixel is a function of only it's local neighborhood of points.  This is called an _inductive bias_ and is a big reason why convolutions work for image data: neighboring pixels are correlated and you can operate on just those pixels at a time.</span>
<span id="cb23-125"><a href="#cb23-125"></a></span>
<span id="cb23-126"><a href="#cb23-126"></a>See examples of convolutions <span class="co">[</span><span class="ot">here</span><span class="co">](https://github.com/vdumoulin/conv_arithmetic)</span></span>
<span id="cb23-127"><a href="#cb23-127"></a></span>
<span id="cb23-128"><a href="#cb23-128"></a><span class="al">![image-2.png](./conv_eqn.png)</span></span>
<span id="cb23-129"><a href="#cb23-129"></a></span>
<span id="cb23-130"><a href="#cb23-130"></a><span class="al">![image.png](./conv.png)</span></span>
<span id="cb23-131"><a href="#cb23-131"></a></span>
<span id="cb23-134"><a href="#cb23-134"></a><span class="in">```{python}</span></span>
<span id="cb23-135"><a href="#cb23-135"></a><span class="co"># Let's apply a convolution to the ALCF Staff photo:</span></span>
<span id="cb23-136"><a href="#cb23-136"></a>alcf_tensor <span class="op">=</span> torchvision.transforms.ToTensor()(alcf_image)</span>
<span id="cb23-137"><a href="#cb23-137"></a></span>
<span id="cb23-138"><a href="#cb23-138"></a><span class="co"># Reshape the tensor to have a batch size of 1:</span></span>
<span id="cb23-139"><a href="#cb23-139"></a>alcf_tensor <span class="op">=</span> alcf_tensor.reshape((<span class="dv">1</span>,) <span class="op">+</span> alcf_tensor.shape)</span>
<span id="cb23-140"><a href="#cb23-140"></a></span>
<span id="cb23-141"><a href="#cb23-141"></a><span class="co"># Create a random convolution:</span></span>
<span id="cb23-142"><a href="#cb23-142"></a><span class="co"># shape is: (channels_in, channels_out, kernel_x, kernel_y)</span></span>
<span id="cb23-143"><a href="#cb23-143"></a>conv_random <span class="op">=</span> torch.rand((<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">15</span>,<span class="dv">15</span>))</span>
<span id="cb23-144"><a href="#cb23-144"></a></span>
<span id="cb23-145"><a href="#cb23-145"></a>alcf_rand <span class="op">=</span> torch.nn.functional.conv2d(alcf_tensor, conv_random)</span>
<span id="cb23-146"><a href="#cb23-146"></a>alcf_rand <span class="op">=</span> (<span class="fl">1.</span><span class="op">/</span>alcf_rand.<span class="bu">max</span>()) <span class="op">*</span> alcf_rand</span>
<span id="cb23-147"><a href="#cb23-147"></a>console.log(alcf_rand.shape)</span>
<span id="cb23-148"><a href="#cb23-148"></a>alcf_rand <span class="op">=</span> alcf_rand.reshape(alcf_rand.shape[<span class="dv">1</span>:])</span>
<span id="cb23-149"><a href="#cb23-149"></a></span>
<span id="cb23-150"><a href="#cb23-150"></a>console.log(alcf_tensor.shape)</span>
<span id="cb23-151"><a href="#cb23-151"></a></span>
<span id="cb23-152"><a href="#cb23-152"></a>rand_image <span class="op">=</span> alcf_rand.permute((<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>)).cpu()</span>
<span id="cb23-153"><a href="#cb23-153"></a></span>
<span id="cb23-154"><a href="#cb23-154"></a>figure <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb23-155"><a href="#cb23-155"></a></span>
<span id="cb23-156"><a href="#cb23-156"></a>plt.imshow(rand_image)</span>
<span id="cb23-157"><a href="#cb23-157"></a><span class="in">```</span></span>
<span id="cb23-158"><a href="#cb23-158"></a></span>
<span id="cb23-159"><a href="#cb23-159"></a><span class="fu">### Normalization</span></span>
<span id="cb23-160"><a href="#cb23-160"></a></span>
<span id="cb23-161"><a href="#cb23-161"></a><span class="al">![Batch Norm](./batch_norm.png)</span></span>
<span id="cb23-162"><a href="#cb23-162"></a>Reference: <span class="co">[</span><span class="ot">Normalizations</span><span class="co">](https://arxiv.org/pdf/1903.10520.pdf)</span></span>
<span id="cb23-163"><a href="#cb23-163"></a></span>
<span id="cb23-164"><a href="#cb23-164"></a>Normalization is the act of transforming the mean and moment of your data to standard values (usually 0.0 and 1.0).  It's particularly useful in machine learning since it stabilizes training, and allows higher learning rates.</span>
<span id="cb23-165"><a href="#cb23-165"></a></span>
<span id="cb23-166"><a href="#cb23-166"></a><span class="al">![Batch Normalization accelerates training](./batch_norm_effect.png)</span></span>
<span id="cb23-167"><a href="#cb23-167"></a></span>
<span id="cb23-168"><a href="#cb23-168"></a>Reference: <span class="co">[</span><span class="ot">Batch Norm</span><span class="co">](https://arxiv.org/pdf/1502.03167.pdf)</span></span>
<span id="cb23-169"><a href="#cb23-169"></a></span>
<span id="cb23-172"><a href="#cb23-172"></a><span class="in">```{python}</span></span>
<span id="cb23-173"><a href="#cb23-173"></a><span class="co"># Let's apply a normalization to the ALCF Staff photo:</span></span>
<span id="cb23-174"><a href="#cb23-174"></a>alcf_tensor <span class="op">=</span> torchvision.transforms.ToTensor()(alcf_image)</span>
<span id="cb23-175"><a href="#cb23-175"></a></span>
<span id="cb23-176"><a href="#cb23-176"></a><span class="co"># Reshape the tensor to have a batch size of 1:</span></span>
<span id="cb23-177"><a href="#cb23-177"></a>alcf_tensor <span class="op">=</span> alcf_tensor.reshape((<span class="dv">1</span>,) <span class="op">+</span> alcf_tensor.shape)</span>
<span id="cb23-178"><a href="#cb23-178"></a></span>
<span id="cb23-179"><a href="#cb23-179"></a></span>
<span id="cb23-180"><a href="#cb23-180"></a>alcf_rand <span class="op">=</span> torch.nn.functional.normalize(alcf_tensor)</span>
<span id="cb23-181"><a href="#cb23-181"></a>alcf_rand <span class="op">=</span> alcf_rand.reshape(alcf_rand.shape[<span class="dv">1</span>:])</span>
<span id="cb23-182"><a href="#cb23-182"></a></span>
<span id="cb23-183"><a href="#cb23-183"></a>console.log(alcf_tensor.shape)</span>
<span id="cb23-184"><a href="#cb23-184"></a></span>
<span id="cb23-185"><a href="#cb23-185"></a>rand_image <span class="op">=</span> alcf_rand.permute((<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>)).cpu()</span>
<span id="cb23-186"><a href="#cb23-186"></a></span>
<span id="cb23-187"><a href="#cb23-187"></a>figure <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb23-188"><a href="#cb23-188"></a></span>
<span id="cb23-189"><a href="#cb23-189"></a>plt.imshow(rand_image)</span>
<span id="cb23-190"><a href="#cb23-190"></a><span class="in">```</span></span>
<span id="cb23-191"><a href="#cb23-191"></a></span>
<span id="cb23-192"><a href="#cb23-192"></a></span>
<span id="cb23-193"><a href="#cb23-193"></a><span class="fu">### Downsampling (And upsampling)</span></span>
<span id="cb23-194"><a href="#cb23-194"></a></span>
<span id="cb23-195"><a href="#cb23-195"></a>Downsampling is a critical component of convolutional and many vision models.  Because of the local-only nature of convolutional filters, learning large-range features can be too slow for convergence.  Downsampling of layers can bring information from far away closer, effectively changing what it means to be "local" as the input to a convolution.</span>
<span id="cb23-196"><a href="#cb23-196"></a></span>
<span id="cb23-197"><a href="#cb23-197"></a><span class="al">![Convolutional Pooling](./conv_pooling.png)</span></span>
<span id="cb23-198"><a href="#cb23-198"></a></span>
<span id="cb23-199"><a href="#cb23-199"></a><span class="co">[</span><span class="ot">Reference</span><span class="co">](https://www.researchgate.net/publication/333593451_Application_of_Transfer_Learning_Using_Convolutional_Neural_Network_Method_for_Early_Detection_of_Terry's_Nail)</span></span>
<span id="cb23-200"><a href="#cb23-200"></a></span>
<span id="cb23-201"><a href="#cb23-201"></a></span>
<span id="cb23-204"><a href="#cb23-204"></a><span class="in">```{python}</span></span>
<span id="cb23-205"><a href="#cb23-205"></a><span class="co"># Let's apply a normalization to the ALCF Staff photo:</span></span>
<span id="cb23-206"><a href="#cb23-206"></a>alcf_tensor <span class="op">=</span> torchvision.transforms.ToTensor()(alcf_image)</span>
<span id="cb23-207"><a href="#cb23-207"></a></span>
<span id="cb23-208"><a href="#cb23-208"></a><span class="co"># Reshape the tensor to have a batch size of 1:</span></span>
<span id="cb23-209"><a href="#cb23-209"></a>alcf_tensor <span class="op">=</span> alcf_tensor.reshape((<span class="dv">1</span>,) <span class="op">+</span> alcf_tensor.shape)</span>
<span id="cb23-210"><a href="#cb23-210"></a></span>
<span id="cb23-211"><a href="#cb23-211"></a></span>
<span id="cb23-212"><a href="#cb23-212"></a>alcf_rand <span class="op">=</span> torch.nn.functional.max_pool2d(alcf_tensor, <span class="dv">2</span>)</span>
<span id="cb23-213"><a href="#cb23-213"></a>alcf_rand <span class="op">=</span> alcf_rand.reshape(alcf_rand.shape[<span class="dv">1</span>:])</span>
<span id="cb23-214"><a href="#cb23-214"></a></span>
<span id="cb23-215"><a href="#cb23-215"></a>console.log(alcf_tensor.shape)</span>
<span id="cb23-216"><a href="#cb23-216"></a></span>
<span id="cb23-217"><a href="#cb23-217"></a>rand_image <span class="op">=</span> alcf_rand.permute((<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>)).cpu()</span>
<span id="cb23-218"><a href="#cb23-218"></a></span>
<span id="cb23-219"><a href="#cb23-219"></a>figure <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb23-220"><a href="#cb23-220"></a></span>
<span id="cb23-221"><a href="#cb23-221"></a>plt.imshow(rand_image)</span>
<span id="cb23-222"><a href="#cb23-222"></a><span class="in">```</span></span>
<span id="cb23-223"><a href="#cb23-223"></a></span>
<span id="cb23-224"><a href="#cb23-224"></a><span class="fu">### Residual Connections</span></span>
<span id="cb23-225"><a href="#cb23-225"></a></span>
<span id="cb23-226"><a href="#cb23-226"></a>One issue, quickly encountered when making convolutional networks deeper and deeper, is the "Vanishing Gradients" problem.  As layers were stacked on top of each other, the size of updates dimished at the earlier layers of a convolutional network.  The paper "Deep Residual Learning for Image Recognition" solved this by introduction "residual connections" as skip layers.</span>
<span id="cb23-227"><a href="#cb23-227"></a></span>
<span id="cb23-228"><a href="#cb23-228"></a></span>
<span id="cb23-229"><a href="#cb23-229"></a>Reference: <span class="co">[</span><span class="ot">Deep Residual Learning for Image Recognition</span><span class="co">](https://arxiv.org/pdf/1512.03385.pdf)</span></span>
<span id="cb23-230"><a href="#cb23-230"></a></span>
<span id="cb23-231"><a href="#cb23-231"></a><span class="al">![Residual Layer](./residual_layer.png)</span></span>
<span id="cb23-232"><a href="#cb23-232"></a></span>
<span id="cb23-233"><a href="#cb23-233"></a></span>
<span id="cb23-234"><a href="#cb23-234"></a>Compare the performance of the models before and after the introduction of these layers:</span>
<span id="cb23-235"><a href="#cb23-235"></a></span>
<span id="cb23-236"><a href="#cb23-236"></a><span class="al">![Resnet Performance vs. Plain network performance](./resnet_comparison.png)</span></span>
<span id="cb23-237"><a href="#cb23-237"></a></span>
<span id="cb23-238"><a href="#cb23-238"></a>If you have time to read only one paper on computer vision, make it this one!  Resnet was the first model to beat human accuracy on ImageNet and is one of the most impactful papers in AI ever published.</span>
<span id="cb23-239"><a href="#cb23-239"></a></span>
<span id="cb23-240"><a href="#cb23-240"></a><span class="fu">## Building a ConvNet</span></span>
<span id="cb23-241"><a href="#cb23-241"></a></span>
<span id="cb23-242"><a href="#cb23-242"></a>In this section we'll build and apply a conv net to the mnist dataset.  The layers here are loosely based off of the ConvNext architecture.  Why?  Because we're getting into LLM's soon, and this ConvNet uses LLM features.  ConvNext is an update to the ResNet architecture that outperforms it.</span>
<span id="cb23-243"><a href="#cb23-243"></a></span>
<span id="cb23-244"><a href="#cb23-244"></a><span class="co">[</span><span class="ot">ConvNext</span><span class="co">](https://arxiv.org/abs/2201.03545)</span></span>
<span id="cb23-245"><a href="#cb23-245"></a></span>
<span id="cb23-246"><a href="#cb23-246"></a>The dataset here is CIFAR-10 - slightly harder than MNIST but still relatively easy and computationally tractable.</span>
<span id="cb23-247"><a href="#cb23-247"></a></span>
<span id="cb23-250"><a href="#cb23-250"></a><span class="in">```{python}</span></span>
<span id="cb23-251"><a href="#cb23-251"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> v2</span>
<span id="cb23-252"><a href="#cb23-252"></a>training_data <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb23-253"><a href="#cb23-253"></a>    <span class="co"># Polaris: root="/lus/eagle/projects/datasets/CIFAR-10/",</span></span>
<span id="cb23-254"><a href="#cb23-254"></a>    <span class="co"># Polaris: download=False,</span></span>
<span id="cb23-255"><a href="#cb23-255"></a>    root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb23-256"><a href="#cb23-256"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb23-257"><a href="#cb23-257"></a>    train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb23-258"><a href="#cb23-258"></a>    transform<span class="op">=</span>v2.Compose([</span>
<span id="cb23-259"><a href="#cb23-259"></a>        v2.ToTensor(),</span>
<span id="cb23-260"><a href="#cb23-260"></a>        v2.RandomHorizontalFlip(),</span>
<span id="cb23-261"><a href="#cb23-261"></a>        v2.RandomResizedCrop(size<span class="op">=</span><span class="dv">32</span>, scale<span class="op">=</span>[<span class="fl">0.85</span>,<span class="fl">1.0</span>], antialias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb23-262"><a href="#cb23-262"></a>        v2.ColorJitter(brightness<span class="op">=</span><span class="fl">0.2</span>, contrast<span class="op">=</span><span class="fl">0.2</span>, saturation<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb23-263"><a href="#cb23-263"></a>    ])</span>
<span id="cb23-264"><a href="#cb23-264"></a>)</span>
<span id="cb23-265"><a href="#cb23-265"></a></span>
<span id="cb23-266"><a href="#cb23-266"></a>test_data <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb23-267"><a href="#cb23-267"></a>    <span class="co"># Polaris: root="/lus/eagle/projects/datasets/CIFAR-10/",</span></span>
<span id="cb23-268"><a href="#cb23-268"></a>    <span class="co"># Polaris: download=False,</span></span>
<span id="cb23-269"><a href="#cb23-269"></a>    root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb23-270"><a href="#cb23-270"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb23-271"><a href="#cb23-271"></a>    train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb23-272"><a href="#cb23-272"></a>    transform<span class="op">=</span>torchvision.transforms.ToTensor()</span>
<span id="cb23-273"><a href="#cb23-273"></a>)</span>
<span id="cb23-274"><a href="#cb23-274"></a></span>
<span id="cb23-275"><a href="#cb23-275"></a>training_data, validation_data <span class="op">=</span> torch.utils.data.random_split(training_data, [<span class="fl">0.8</span>, <span class="fl">0.2</span>], generator<span class="op">=</span>torch.Generator().manual_seed(<span class="dv">55</span>))</span>
<span id="cb23-276"><a href="#cb23-276"></a></span>
<span id="cb23-277"><a href="#cb23-277"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb23-278"><a href="#cb23-278"></a></span>
<span id="cb23-279"><a href="#cb23-279"></a><span class="co"># The dataloader makes our dataset iterable </span></span>
<span id="cb23-280"><a href="#cb23-280"></a>train_dataloader <span class="op">=</span> torch.utils.data.DataLoader(training_data, </span>
<span id="cb23-281"><a href="#cb23-281"></a>    batch_size<span class="op">=</span>batch_size, </span>
<span id="cb23-282"><a href="#cb23-282"></a>    pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb23-283"><a href="#cb23-283"></a>    shuffle<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb23-284"><a href="#cb23-284"></a>    num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb23-285"><a href="#cb23-285"></a>val_dataloader <span class="op">=</span> torch.utils.data.DataLoader(validation_data, </span>
<span id="cb23-286"><a href="#cb23-286"></a>    batch_size<span class="op">=</span>batch_size, </span>
<span id="cb23-287"><a href="#cb23-287"></a>    pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb23-288"><a href="#cb23-288"></a>    shuffle<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb23-289"><a href="#cb23-289"></a>    num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb23-290"><a href="#cb23-290"></a><span class="in">```</span></span>
<span id="cb23-291"><a href="#cb23-291"></a></span>
<span id="cb23-294"><a href="#cb23-294"></a><span class="in">```{python}</span></span>
<span id="cb23-295"><a href="#cb23-295"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb23-296"><a href="#cb23-296"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb23-297"><a href="#cb23-297"></a><span class="in">```</span></span>
<span id="cb23-298"><a href="#cb23-298"></a></span>
<span id="cb23-301"><a href="#cb23-301"></a><span class="in">```{python}</span></span>
<span id="cb23-302"><a href="#cb23-302"></a>batch, (X, Y) <span class="op">=</span> <span class="bu">next</span>(<span class="bu">enumerate</span>(train_dataloader))</span>
<span id="cb23-303"><a href="#cb23-303"></a>plt.imshow(X[<span class="dv">0</span>].cpu().permute((<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>)))<span class="op">;</span> plt.show()</span>
<span id="cb23-304"><a href="#cb23-304"></a><span class="in">```</span></span>
<span id="cb23-305"><a href="#cb23-305"></a></span>
<span id="cb23-306"><a href="#cb23-306"></a>This code below is important as our models get bigger: this is wrapping the pytorch data loaders to put the data onto the GPU!</span>
<span id="cb23-307"><a href="#cb23-307"></a></span>
<span id="cb23-310"><a href="#cb23-310"></a><span class="in">```{python}</span></span>
<span id="cb23-311"><a href="#cb23-311"></a>dev <span class="op">=</span> torch.device(</span>
<span id="cb23-312"><a href="#cb23-312"></a>    <span class="st">"cuda"</span>) <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb23-313"><a href="#cb23-313"></a></span>
<span id="cb23-314"><a href="#cb23-314"></a></span>
<span id="cb23-315"><a href="#cb23-315"></a><span class="kw">def</span> preprocess(x, y):</span>
<span id="cb23-316"><a href="#cb23-316"></a>    <span class="co"># CIFAR-10 is *color* images so 3 layers!</span></span>
<span id="cb23-317"><a href="#cb23-317"></a>    <span class="cf">return</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>).to(dev), y.to(dev)</span>
<span id="cb23-318"><a href="#cb23-318"></a></span>
<span id="cb23-319"><a href="#cb23-319"></a></span>
<span id="cb23-320"><a href="#cb23-320"></a><span class="kw">class</span> WrappedDataLoader:</span>
<span id="cb23-321"><a href="#cb23-321"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dl, func):</span>
<span id="cb23-322"><a href="#cb23-322"></a>        <span class="va">self</span>.dl <span class="op">=</span> dl</span>
<span id="cb23-323"><a href="#cb23-323"></a>        <span class="va">self</span>.func <span class="op">=</span> func</span>
<span id="cb23-324"><a href="#cb23-324"></a></span>
<span id="cb23-325"><a href="#cb23-325"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb23-326"><a href="#cb23-326"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.dl)</span>
<span id="cb23-327"><a href="#cb23-327"></a></span>
<span id="cb23-328"><a href="#cb23-328"></a>    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>):</span>
<span id="cb23-329"><a href="#cb23-329"></a>        <span class="cf">for</span> b <span class="kw">in</span> <span class="va">self</span>.dl:</span>
<span id="cb23-330"><a href="#cb23-330"></a>            <span class="cf">yield</span> (<span class="va">self</span>.func(<span class="op">*</span>b))</span>
<span id="cb23-331"><a href="#cb23-331"></a></span>
<span id="cb23-332"><a href="#cb23-332"></a></span>
<span id="cb23-333"><a href="#cb23-333"></a>train_dataloader <span class="op">=</span> WrappedDataLoader(train_dataloader, preprocess)</span>
<span id="cb23-334"><a href="#cb23-334"></a>val_dataloader <span class="op">=</span> WrappedDataLoader(val_dataloader, preprocess)</span>
<span id="cb23-335"><a href="#cb23-335"></a><span class="in">```</span></span>
<span id="cb23-336"><a href="#cb23-336"></a></span>
<span id="cb23-339"><a href="#cb23-339"></a><span class="in">```{python}</span></span>
<span id="cb23-340"><a href="#cb23-340"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb23-341"><a href="#cb23-341"></a></span>
<span id="cb23-342"><a href="#cb23-342"></a></span>
<span id="cb23-343"><a href="#cb23-343"></a><span class="kw">class</span> Downsampler(nn.Module):</span>
<span id="cb23-344"><a href="#cb23-344"></a></span>
<span id="cb23-345"><a href="#cb23-345"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, shape, stride<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb23-346"><a href="#cb23-346"></a>        <span class="bu">super</span>(Downsampler, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb23-347"><a href="#cb23-347"></a></span>
<span id="cb23-348"><a href="#cb23-348"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm([in_channels, <span class="op">*</span>shape])</span>
<span id="cb23-349"><a href="#cb23-349"></a></span>
<span id="cb23-350"><a href="#cb23-350"></a>        <span class="va">self</span>.downsample <span class="op">=</span> nn.Conv2d(</span>
<span id="cb23-351"><a href="#cb23-351"></a>            in_channels<span class="op">=</span>in_channels, </span>
<span id="cb23-352"><a href="#cb23-352"></a>            out_channels<span class="op">=</span>out_channels,</span>
<span id="cb23-353"><a href="#cb23-353"></a>            kernel_size <span class="op">=</span> stride,</span>
<span id="cb23-354"><a href="#cb23-354"></a>            stride <span class="op">=</span> stride,</span>
<span id="cb23-355"><a href="#cb23-355"></a>        )</span>
<span id="cb23-356"><a href="#cb23-356"></a></span>
<span id="cb23-357"><a href="#cb23-357"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb23-358"><a href="#cb23-358"></a></span>
<span id="cb23-359"><a href="#cb23-359"></a></span>
<span id="cb23-360"><a href="#cb23-360"></a>        <span class="cf">return</span> <span class="va">self</span>.downsample(<span class="va">self</span>.norm(inputs))</span>
<span id="cb23-361"><a href="#cb23-361"></a></span>
<span id="cb23-362"><a href="#cb23-362"></a></span>
<span id="cb23-363"><a href="#cb23-363"></a></span>
<span id="cb23-364"><a href="#cb23-364"></a><span class="kw">class</span> ConvNextBlock(nn.Module):</span>
<span id="cb23-365"><a href="#cb23-365"></a>    <span class="co">"""This block of operations is loosely based on this paper:</span></span>
<span id="cb23-366"><a href="#cb23-366"></a></span>
<span id="cb23-367"><a href="#cb23-367"></a><span class="co">    """</span></span>
<span id="cb23-368"><a href="#cb23-368"></a></span>
<span id="cb23-369"><a href="#cb23-369"></a></span>
<span id="cb23-370"><a href="#cb23-370"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, shape):</span>
<span id="cb23-371"><a href="#cb23-371"></a>        <span class="bu">super</span>(ConvNextBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb23-372"><a href="#cb23-372"></a></span>
<span id="cb23-373"><a href="#cb23-373"></a>        <span class="co"># Depthwise, seperable convolution with a large number of output filters:</span></span>
<span id="cb23-374"><a href="#cb23-374"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span>in_channels, </span>
<span id="cb23-375"><a href="#cb23-375"></a>                                     out_channels<span class="op">=</span>in_channels, </span>
<span id="cb23-376"><a href="#cb23-376"></a>                                     groups<span class="op">=</span>in_channels,</span>
<span id="cb23-377"><a href="#cb23-377"></a>                                     kernel_size<span class="op">=</span>[<span class="dv">7</span>,<span class="dv">7</span>],</span>
<span id="cb23-378"><a href="#cb23-378"></a>                                     padding<span class="op">=</span><span class="st">'same'</span> )</span>
<span id="cb23-379"><a href="#cb23-379"></a></span>
<span id="cb23-380"><a href="#cb23-380"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm([in_channels, <span class="op">*</span>shape])</span>
<span id="cb23-381"><a href="#cb23-381"></a></span>
<span id="cb23-382"><a href="#cb23-382"></a>        <span class="co"># Two more convolutions:</span></span>
<span id="cb23-383"><a href="#cb23-383"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span>in_channels, </span>
<span id="cb23-384"><a href="#cb23-384"></a>                                     out_channels<span class="op">=</span><span class="dv">4</span><span class="op">*</span>in_channels,</span>
<span id="cb23-385"><a href="#cb23-385"></a>                                     kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-386"><a href="#cb23-386"></a></span>
<span id="cb23-387"><a href="#cb23-387"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">4</span><span class="op">*</span>in_channels, </span>
<span id="cb23-388"><a href="#cb23-388"></a>                                     out_channels<span class="op">=</span>in_channels,</span>
<span id="cb23-389"><a href="#cb23-389"></a>                                     kernel_size<span class="op">=</span><span class="dv">1</span></span>
<span id="cb23-390"><a href="#cb23-390"></a>                                     )</span>
<span id="cb23-391"><a href="#cb23-391"></a></span>
<span id="cb23-392"><a href="#cb23-392"></a></span>
<span id="cb23-393"><a href="#cb23-393"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb23-394"><a href="#cb23-394"></a>        x <span class="op">=</span> <span class="va">self</span>.conv1(inputs)</span>
<span id="cb23-395"><a href="#cb23-395"></a></span>
<span id="cb23-396"><a href="#cb23-396"></a>        <span class="co"># The normalization layer:</span></span>
<span id="cb23-397"><a href="#cb23-397"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb23-398"><a href="#cb23-398"></a></span>
<span id="cb23-399"><a href="#cb23-399"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</span>
<span id="cb23-400"><a href="#cb23-400"></a></span>
<span id="cb23-401"><a href="#cb23-401"></a>        <span class="co"># The non-linear activation layer:</span></span>
<span id="cb23-402"><a href="#cb23-402"></a>        x <span class="op">=</span> torch.nn.functional.gelu(x)</span>
<span id="cb23-403"><a href="#cb23-403"></a></span>
<span id="cb23-404"><a href="#cb23-404"></a>        x <span class="op">=</span> <span class="va">self</span>.conv3(x)</span>
<span id="cb23-405"><a href="#cb23-405"></a></span>
<span id="cb23-406"><a href="#cb23-406"></a>        <span class="co"># This makes it a residual network:</span></span>
<span id="cb23-407"><a href="#cb23-407"></a>        <span class="cf">return</span> x <span class="op">+</span> inputs</span>
<span id="cb23-408"><a href="#cb23-408"></a></span>
<span id="cb23-409"><a href="#cb23-409"></a></span>
<span id="cb23-410"><a href="#cb23-410"></a><span class="kw">class</span> Classifier(nn.Module):</span>
<span id="cb23-411"><a href="#cb23-411"></a></span>
<span id="cb23-412"><a href="#cb23-412"></a></span>
<span id="cb23-413"><a href="#cb23-413"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_initial_filters, n_stages, blocks_per_stage):</span>
<span id="cb23-414"><a href="#cb23-414"></a>        <span class="bu">super</span>(Classifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb23-415"><a href="#cb23-415"></a></span>
<span id="cb23-416"><a href="#cb23-416"></a>        <span class="co"># This is a downsampling convolution that will produce patches of output.</span></span>
<span id="cb23-417"><a href="#cb23-417"></a></span>
<span id="cb23-418"><a href="#cb23-418"></a>        <span class="co"># This is similar to what vision transformers do to tokenize the images.</span></span>
<span id="cb23-419"><a href="#cb23-419"></a>        <span class="va">self</span>.stem <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb23-420"><a href="#cb23-420"></a>                                    out_channels<span class="op">=</span>n_initial_filters,</span>
<span id="cb23-421"><a href="#cb23-421"></a>                                    kernel_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb23-422"><a href="#cb23-422"></a>                                    stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-423"><a href="#cb23-423"></a></span>
<span id="cb23-424"><a href="#cb23-424"></a>        current_shape <span class="op">=</span> [<span class="dv">32</span>, <span class="dv">32</span>]</span>
<span id="cb23-425"><a href="#cb23-425"></a></span>
<span id="cb23-426"><a href="#cb23-426"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm([n_initial_filters,<span class="op">*</span>current_shape])</span>
<span id="cb23-427"><a href="#cb23-427"></a>        <span class="co"># self.norm1 = WrappedLayerNorm()</span></span>
<span id="cb23-428"><a href="#cb23-428"></a></span>
<span id="cb23-429"><a href="#cb23-429"></a>        current_n_filters <span class="op">=</span> n_initial_filters</span>
<span id="cb23-430"><a href="#cb23-430"></a></span>
<span id="cb23-431"><a href="#cb23-431"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.Sequential()</span>
<span id="cb23-432"><a href="#cb23-432"></a>        <span class="cf">for</span> i, n_blocks <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">range</span>(n_stages)):</span>
<span id="cb23-433"><a href="#cb23-433"></a>            <span class="co"># Add a convnext block series:</span></span>
<span id="cb23-434"><a href="#cb23-434"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(blocks_per_stage):</span>
<span id="cb23-435"><a href="#cb23-435"></a>                <span class="va">self</span>.layers.append(ConvNextBlock(in_channels<span class="op">=</span>current_n_filters, shape<span class="op">=</span>current_shape))</span>
<span id="cb23-436"><a href="#cb23-436"></a>            <span class="co"># Add a downsampling layer:</span></span>
<span id="cb23-437"><a href="#cb23-437"></a>            <span class="cf">if</span> i <span class="op">!=</span> n_stages <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb23-438"><a href="#cb23-438"></a>                <span class="co"># Skip downsampling if it's the last layer!</span></span>
<span id="cb23-439"><a href="#cb23-439"></a>                <span class="va">self</span>.layers.append(Downsampler(</span>
<span id="cb23-440"><a href="#cb23-440"></a>                    in_channels<span class="op">=</span>current_n_filters, </span>
<span id="cb23-441"><a href="#cb23-441"></a>                    out_channels<span class="op">=</span><span class="dv">2</span><span class="op">*</span>current_n_filters,</span>
<span id="cb23-442"><a href="#cb23-442"></a>                    shape <span class="op">=</span> current_shape,</span>
<span id="cb23-443"><a href="#cb23-443"></a>                    )</span>
<span id="cb23-444"><a href="#cb23-444"></a>                )</span>
<span id="cb23-445"><a href="#cb23-445"></a>                <span class="co"># Double the number of filters:</span></span>
<span id="cb23-446"><a href="#cb23-446"></a>                current_n_filters <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>current_n_filters</span>
<span id="cb23-447"><a href="#cb23-447"></a>                <span class="co"># Cut the shape in half:</span></span>
<span id="cb23-448"><a href="#cb23-448"></a>                current_shape <span class="op">=</span> [ cs <span class="op">//</span> <span class="dv">2</span> <span class="cf">for</span> cs <span class="kw">in</span> current_shape]</span>
<span id="cb23-449"><a href="#cb23-449"></a></span>
<span id="cb23-450"><a href="#cb23-450"></a></span>
<span id="cb23-451"><a href="#cb23-451"></a></span>
<span id="cb23-452"><a href="#cb23-452"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Sequential(</span>
<span id="cb23-453"><a href="#cb23-453"></a>            nn.Flatten(),</span>
<span id="cb23-454"><a href="#cb23-454"></a>            nn.LayerNorm(current_n_filters),</span>
<span id="cb23-455"><a href="#cb23-455"></a>            nn.Linear(current_n_filters, <span class="dv">10</span>)</span>
<span id="cb23-456"><a href="#cb23-456"></a>        )</span>
<span id="cb23-457"><a href="#cb23-457"></a>        <span class="co"># self.norm2 = nn.InstanceNorm2d(current_n_filters)</span></span>
<span id="cb23-458"><a href="#cb23-458"></a>        <span class="co"># # This brings it down to one channel / class</span></span>
<span id="cb23-459"><a href="#cb23-459"></a>        <span class="co"># self.bottleneck = nn.Conv2d(in_channels=current_n_filters, out_channels=10, </span></span>
<span id="cb23-460"><a href="#cb23-460"></a>        <span class="co">#                                   kernel_size=1, stride=1)</span></span>
<span id="cb23-461"><a href="#cb23-461"></a></span>
<span id="cb23-462"><a href="#cb23-462"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb23-463"><a href="#cb23-463"></a></span>
<span id="cb23-464"><a href="#cb23-464"></a>        x <span class="op">=</span> <span class="va">self</span>.stem(inputs)</span>
<span id="cb23-465"><a href="#cb23-465"></a>        <span class="co"># Apply a normalization after the initial patching:</span></span>
<span id="cb23-466"><a href="#cb23-466"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x)</span>
<span id="cb23-467"><a href="#cb23-467"></a></span>
<span id="cb23-468"><a href="#cb23-468"></a>        <span class="co"># Apply the main chunk of the network:</span></span>
<span id="cb23-469"><a href="#cb23-469"></a>        x <span class="op">=</span> <span class="va">self</span>.layers(x)</span>
<span id="cb23-470"><a href="#cb23-470"></a></span>
<span id="cb23-471"><a href="#cb23-471"></a>        <span class="co"># Normalize and readout:</span></span>
<span id="cb23-472"><a href="#cb23-472"></a>        x <span class="op">=</span> nn.functional.avg_pool2d(x, x.shape[<span class="dv">2</span>:])</span>
<span id="cb23-473"><a href="#cb23-473"></a>        x <span class="op">=</span> <span class="va">self</span>.head(x)</span>
<span id="cb23-474"><a href="#cb23-474"></a></span>
<span id="cb23-475"><a href="#cb23-475"></a>        <span class="cf">return</span> x</span>
<span id="cb23-476"><a href="#cb23-476"></a></span>
<span id="cb23-477"><a href="#cb23-477"></a></span>
<span id="cb23-478"><a href="#cb23-478"></a></span>
<span id="cb23-479"><a href="#cb23-479"></a>        <span class="co"># x = self.norm2(x)</span></span>
<span id="cb23-480"><a href="#cb23-480"></a>        <span class="co"># x = self.bottleneck(x)</span></span>
<span id="cb23-481"><a href="#cb23-481"></a></span>
<span id="cb23-482"><a href="#cb23-482"></a>        <span class="co"># # Average pooling of the remaining spatial dimensions (and reshape) makes this label-like:</span></span>
<span id="cb23-483"><a href="#cb23-483"></a>        <span class="co"># return nn.functional.avg_pool2d(x, kernel_size=x.shape[-2:]).reshape((-1,10))</span></span>
<span id="cb23-484"><a href="#cb23-484"></a><span class="in">```</span></span>
<span id="cb23-485"><a href="#cb23-485"></a></span>
<span id="cb23-488"><a href="#cb23-488"></a><span class="in">```{python}</span></span>
<span id="cb23-489"><a href="#cb23-489"></a><span class="co">#| output: false</span></span>
<span id="cb23-490"><a href="#cb23-490"></a><span class="op">!</span>pip install torchinfo <span class="co"># if not on Polaris</span></span>
<span id="cb23-491"><a href="#cb23-491"></a><span class="in">```</span></span>
<span id="cb23-492"><a href="#cb23-492"></a></span>
<span id="cb23-495"><a href="#cb23-495"></a><span class="in">```{python}</span></span>
<span id="cb23-496"><a href="#cb23-496"></a>model <span class="op">=</span> Classifier(<span class="dv">32</span>, <span class="dv">4</span>, <span class="dv">2</span>).to(device<span class="op">=</span>dev)</span>
<span id="cb23-497"><a href="#cb23-497"></a></span>
<span id="cb23-498"><a href="#cb23-498"></a><span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb23-499"><a href="#cb23-499"></a></span>
<span id="cb23-500"><a href="#cb23-500"></a>console.log(summary(model, input_size<span class="op">=</span>(batch_size, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)))</span>
<span id="cb23-501"><a href="#cb23-501"></a><span class="in">```</span></span>
<span id="cb23-502"><a href="#cb23-502"></a></span>
<span id="cb23-505"><a href="#cb23-505"></a><span class="in">```{python}</span></span>
<span id="cb23-506"><a href="#cb23-506"></a><span class="kw">def</span> evaluate(dataloader, model, loss_fn, val_bar):</span>
<span id="cb23-507"><a href="#cb23-507"></a>    <span class="co"># Set the model to evaluation mode - some NN pieces behave differently during training</span></span>
<span id="cb23-508"><a href="#cb23-508"></a>    <span class="co"># Unnecessary in this situation but added for best practices</span></span>
<span id="cb23-509"><a href="#cb23-509"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb23-510"><a href="#cb23-510"></a>    size <span class="op">=</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb23-511"><a href="#cb23-511"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb23-512"><a href="#cb23-512"></a>    loss, correct <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb23-513"><a href="#cb23-513"></a></span>
<span id="cb23-514"><a href="#cb23-514"></a>    <span class="co"># We can save computation and memory by not calculating gradients here - we aren't optimizing </span></span>
<span id="cb23-515"><a href="#cb23-515"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb23-516"><a href="#cb23-516"></a>        <span class="co"># loop over all of the batches</span></span>
<span id="cb23-517"><a href="#cb23-517"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> dataloader:</span>
<span id="cb23-518"><a href="#cb23-518"></a></span>
<span id="cb23-519"><a href="#cb23-519"></a>            pred <span class="op">=</span> model(X)</span>
<span id="cb23-520"><a href="#cb23-520"></a>            loss <span class="op">+=</span> loss_fn(pred, y).item()</span>
<span id="cb23-521"><a href="#cb23-521"></a>            <span class="co"># how many are correct in this batch? Tracking for accuracy </span></span>
<span id="cb23-522"><a href="#cb23-522"></a>            correct <span class="op">+=</span> (pred.argmax(<span class="dv">1</span>) <span class="op">==</span> y).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>().item()</span>
<span id="cb23-523"><a href="#cb23-523"></a>            val_bar.update()</span>
<span id="cb23-524"><a href="#cb23-524"></a></span>
<span id="cb23-525"><a href="#cb23-525"></a>    loss <span class="op">/=</span> num_batches</span>
<span id="cb23-526"><a href="#cb23-526"></a>    correct <span class="op">/=</span> (size<span class="op">*</span>batch_size)</span>
<span id="cb23-527"><a href="#cb23-527"></a></span>
<span id="cb23-528"><a href="#cb23-528"></a>    accuracy <span class="op">=</span> <span class="dv">100</span><span class="op">*</span>correct</span>
<span id="cb23-529"><a href="#cb23-529"></a>    <span class="cf">return</span> accuracy, loss</span>
<span id="cb23-530"><a href="#cb23-530"></a><span class="in">```</span></span>
<span id="cb23-531"><a href="#cb23-531"></a></span>
<span id="cb23-534"><a href="#cb23-534"></a><span class="in">```{python}</span></span>
<span id="cb23-535"><a href="#cb23-535"></a><span class="kw">def</span> train_one_epoch(dataloader, model, loss_fn, optimizer, progress_bar):</span>
<span id="cb23-536"><a href="#cb23-536"></a>    model.train()</span>
<span id="cb23-537"><a href="#cb23-537"></a>    <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb23-538"><a href="#cb23-538"></a>        <span class="co"># forward pass</span></span>
<span id="cb23-539"><a href="#cb23-539"></a>        pred <span class="op">=</span> model(X)</span>
<span id="cb23-540"><a href="#cb23-540"></a>        loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb23-541"><a href="#cb23-541"></a></span>
<span id="cb23-542"><a href="#cb23-542"></a>        <span class="co"># backward pass calculates gradients</span></span>
<span id="cb23-543"><a href="#cb23-543"></a>        loss.backward()</span>
<span id="cb23-544"><a href="#cb23-544"></a></span>
<span id="cb23-545"><a href="#cb23-545"></a>        <span class="co"># take one step with these gradients</span></span>
<span id="cb23-546"><a href="#cb23-546"></a>        optimizer.step()</span>
<span id="cb23-547"><a href="#cb23-547"></a></span>
<span id="cb23-548"><a href="#cb23-548"></a>        <span class="co"># resets the gradients </span></span>
<span id="cb23-549"><a href="#cb23-549"></a>        optimizer.zero_grad()</span>
<span id="cb23-550"><a href="#cb23-550"></a></span>
<span id="cb23-551"><a href="#cb23-551"></a>        progress_bar.update()</span>
<span id="cb23-552"><a href="#cb23-552"></a><span class="in">```</span></span>
<span id="cb23-553"><a href="#cb23-553"></a></span>
<span id="cb23-556"><a href="#cb23-556"></a><span class="in">```{python}</span></span>
<span id="cb23-557"><a href="#cb23-557"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb23-558"><a href="#cb23-558"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, weight_decay<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb23-559"><a href="#cb23-559"></a><span class="in">```</span></span>
<span id="cb23-560"><a href="#cb23-560"></a></span>
<span id="cb23-563"><a href="#cb23-563"></a><span class="in">```{python}</span></span>
<span id="cb23-564"><a href="#cb23-564"></a><span class="im">import</span> time</span>
<span id="cb23-565"><a href="#cb23-565"></a><span class="im">import</span> ezpz</span>
<span id="cb23-566"><a href="#cb23-566"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb23-567"><a href="#cb23-567"></a></span>
<span id="cb23-568"><a href="#cb23-568"></a><span class="kw">def</span> train_step(x, y):</span>
<span id="cb23-569"><a href="#cb23-569"></a>    t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb23-570"><a href="#cb23-570"></a>    <span class="co"># Forward pass</span></span>
<span id="cb23-571"><a href="#cb23-571"></a>    pred <span class="op">=</span> model(x)</span>
<span id="cb23-572"><a href="#cb23-572"></a>    loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb23-573"><a href="#cb23-573"></a>    t1 <span class="op">=</span> time.perf_counter()</span>
<span id="cb23-574"><a href="#cb23-574"></a></span>
<span id="cb23-575"><a href="#cb23-575"></a>    <span class="co"># Backward pass</span></span>
<span id="cb23-576"><a href="#cb23-576"></a>    loss.backward()</span>
<span id="cb23-577"><a href="#cb23-577"></a>    t2 <span class="op">=</span> time.perf_counter()</span>
<span id="cb23-578"><a href="#cb23-578"></a></span>
<span id="cb23-579"><a href="#cb23-579"></a>    <span class="co"># Update weights</span></span>
<span id="cb23-580"><a href="#cb23-580"></a>    optimizer.step()</span>
<span id="cb23-581"><a href="#cb23-581"></a>    t3 <span class="op">=</span> time.perf_counter()</span>
<span id="cb23-582"><a href="#cb23-582"></a></span>
<span id="cb23-583"><a href="#cb23-583"></a>    <span class="co"># Reset gradients</span></span>
<span id="cb23-584"><a href="#cb23-584"></a>    optimizer.zero_grad()</span>
<span id="cb23-585"><a href="#cb23-585"></a>    t4 <span class="op">=</span> time.perf_counter()</span>
<span id="cb23-586"><a href="#cb23-586"></a></span>
<span id="cb23-587"><a href="#cb23-587"></a>    <span class="cf">return</span> loss.item(), {</span>
<span id="cb23-588"><a href="#cb23-588"></a>        <span class="st">"dtf"</span>: t1 <span class="op">-</span> t0,</span>
<span id="cb23-589"><a href="#cb23-589"></a>        <span class="st">"dtb"</span>: t2 <span class="op">-</span> t1,</span>
<span id="cb23-590"><a href="#cb23-590"></a>        <span class="st">"dtu"</span>: t3 <span class="op">-</span> t2,</span>
<span id="cb23-591"><a href="#cb23-591"></a>        <span class="st">"dtz"</span>: t4 <span class="op">-</span> t3,</span>
<span id="cb23-592"><a href="#cb23-592"></a>    }</span>
<span id="cb23-593"><a href="#cb23-593"></a></span>
<span id="cb23-594"><a href="#cb23-594"></a>logger <span class="op">=</span> ezpz.get_logger(<span class="st">"3-conv-nets"</span>)</span>
<span id="cb23-595"><a href="#cb23-595"></a>history <span class="op">=</span> ezpz.History()</span>
<span id="cb23-596"><a href="#cb23-596"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb23-597"><a href="#cb23-597"></a>    t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb23-598"><a href="#cb23-598"></a>    x, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb23-599"><a href="#cb23-599"></a>    t1 <span class="op">=</span> time.perf_counter()</span>
<span id="cb23-600"><a href="#cb23-600"></a>    loss, dt <span class="op">=</span> train_step(x, y)</span>
<span id="cb23-601"><a href="#cb23-601"></a>    logger.info(</span>
<span id="cb23-602"><a href="#cb23-602"></a>        history.update(</span>
<span id="cb23-603"><a href="#cb23-603"></a>            {</span>
<span id="cb23-604"><a href="#cb23-604"></a>                <span class="st">"iter"</span>: i,</span>
<span id="cb23-605"><a href="#cb23-605"></a>                <span class="st">"loss"</span>: loss,</span>
<span id="cb23-606"><a href="#cb23-606"></a>                <span class="st">"dtd"</span>: t1 <span class="op">-</span> t0,</span>
<span id="cb23-607"><a href="#cb23-607"></a>                <span class="op">**</span>dt,</span>
<span id="cb23-608"><a href="#cb23-608"></a>            }</span>
<span id="cb23-609"><a href="#cb23-609"></a>        )</span>
<span id="cb23-610"><a href="#cb23-610"></a>    )</span>
<span id="cb23-611"><a href="#cb23-611"></a><span class="in">```</span></span>
<span id="cb23-612"><a href="#cb23-612"></a></span>
<span id="cb23-613"><a href="#cb23-613"></a></span>
<span id="cb23-614"><a href="#cb23-614"></a><span class="in">```python</span></span>
<span id="cb23-615"><a href="#cb23-615"></a><span class="co"># epochs = 1</span></span>
<span id="cb23-616"><a href="#cb23-616"></a><span class="co"># for j in range(epochs):</span></span>
<span id="cb23-617"><a href="#cb23-617"></a><span class="co">#     with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f"Train Epoch {j}") as train_bar:</span></span>
<span id="cb23-618"><a href="#cb23-618"></a><span class="co">#         train_one_epoch(train_dataloader, model, loss_fn, optimizer, train_bar)</span></span>
<span id="cb23-619"><a href="#cb23-619"></a><span class="co">#</span></span>
<span id="cb23-620"><a href="#cb23-620"></a><span class="co">#     # checking on the training &amp; validation loss &amp; accuracy </span></span>
<span id="cb23-621"><a href="#cb23-621"></a><span class="co">#     # for training data - only once every 5 epochs (takes a while) </span></span>
<span id="cb23-622"><a href="#cb23-622"></a><span class="co">#     if j % 5 == 0:</span></span>
<span id="cb23-623"><a href="#cb23-623"></a><span class="co">#         with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f"Validate (train) Epoch {j}") as train_eval:</span></span>
<span id="cb23-624"><a href="#cb23-624"></a><span class="co">#             acc, loss = evaluate(train_dataloader, model, loss_fn, train_eval)</span></span>
<span id="cb23-625"><a href="#cb23-625"></a><span class="co">#             console.log(f"Epoch {j}: training loss: {loss:.3f}, accuracy: {acc:.3f}")</span></span>
<span id="cb23-626"><a href="#cb23-626"></a><span class="co">#</span></span>
<span id="cb23-627"><a href="#cb23-627"></a><span class="co">#     with tqdm(total=len(val_dataloader), position=0, leave=True, desc=f"Validate Epoch {j}") as val_bar:</span></span>
<span id="cb23-628"><a href="#cb23-628"></a><span class="co">#         acc_val, loss_val = evaluate(val_dataloader, model, loss_fn, val_bar)</span></span>
<span id="cb23-629"><a href="#cb23-629"></a><span class="co">#         console.log(f"Epoch {j}: validation loss: {loss_val:.3f}, accuracy: {acc_val:.3f}")</span></span>
<span id="cb23-630"><a href="#cb23-630"></a><span class="in">```</span></span>
<span id="cb23-631"><a href="#cb23-631"></a></span>
<span id="cb23-632"><a href="#cb23-632"></a><span class="fu">## Homework 1:</span></span>
<span id="cb23-633"><a href="#cb23-633"></a></span>
<span id="cb23-634"><a href="#cb23-634"></a>In this notebook, we've learned about some basic convolutional networks and trained one on CIFAR-10 images.</span>
<span id="cb23-635"><a href="#cb23-635"></a>It did ... OK.</span>
<span id="cb23-636"><a href="#cb23-636"></a>There is significant overfitting of this model.</span>
<span id="cb23-637"><a href="#cb23-637"></a>There are some ways to address that, but we didn't have time to get into that in this session.</span>
<span id="cb23-638"><a href="#cb23-638"></a></span>
<span id="cb23-639"><a href="#cb23-639"></a>Meanwhile, your homework (part 1) for this week is to try to train the model</span>
<span id="cb23-640"><a href="#cb23-640"></a>again but with a different architecture.</span>
<span id="cb23-641"><a href="#cb23-641"></a>Change one or more of the following:</span>
<span id="cb23-642"><a href="#cb23-642"></a><span class="ss">- </span>The number of convolutions between downsampling</span>
<span id="cb23-643"><a href="#cb23-643"></a><span class="ss">- </span>The number of filters in each layer</span>
<span id="cb23-644"><a href="#cb23-644"></a><span class="ss">- </span>The initial "patchify" layer</span>
<span id="cb23-645"><a href="#cb23-645"></a><span class="ss">- </span>Another hyper-parameter of your choosing</span>
<span id="cb23-646"><a href="#cb23-646"></a></span>
<span id="cb23-647"><a href="#cb23-647"></a></span>
<span id="cb23-648"><a href="#cb23-648"></a>And compare your final validation accuracy to the accuracy shown here.</span>
<span id="cb23-649"><a href="#cb23-649"></a>Can you beat the validation accuracy shown?</span>
<span id="cb23-650"><a href="#cb23-650"></a></span>
<span id="cb23-651"><a href="#cb23-651"></a>For full credit on the homework, you need to show (via text, or make a plot)</span>
<span id="cb23-652"><a href="#cb23-652"></a>the training and validation data sets' performance (loss and accuracy) for all</span>
<span id="cb23-653"><a href="#cb23-653"></a>the epochs you train.  You also need to explain, in several sentences, what you</span>
<span id="cb23-654"><a href="#cb23-654"></a>changed in the network and why you think it makes a difference.</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://saforem2.github.io/intro-hpc-bootcamp-2025/">
<p>saforem2.github.io/intro-hpc-bootcamp-2025</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/blob/main/01-neural-networks/3-conv-nets/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/edit/main/01-neural-networks/3-conv-nets/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/intro-hpc-bootcamp-2025/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/saforem2">
      <i class="bi bi-twitter" role="img" aria-label="Sam Foreman Twitter">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link active" href="https://github.com/saforem2/intro-hpc-bootcamp-2025" aria-current="page">
      <i class="bi bi-github" role="img" aria-label="Sam Foreman GitHub">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>