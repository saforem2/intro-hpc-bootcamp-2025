---
title: "Introduction to Large Language Models"
description: "A beginner's guide to understanding neural networks, their architecture, and how they function."
categories:
    - ai
    - hpc
date: 2025-07-15
date-modified: last-modified
---

::: {.callout-note title="Authors" collapse="false"}
Content modified from original content written by Archit Vasan , including
materials on LLMs by Varuni Sastri and Carlo Graziani at Argonne, and
discussion/editorial work by Taylor Childers, Bethany Lusch, and Venkat
Vishwanath (Argonne)
:::

Inspiration from the blog posts "The Illustrated Transformer" and "The
Illustrated GPT2" by Jay Alammar, highly recommended reading.

This tutorial covers the some fundamental concepts necessary to to study of
large language models (LLMs).

## Brief overview

* Scientific applications for language models
* General overview of Transformers
* Tokenization
* Model Architecture
* Pipeline using HuggingFace
* Model loading

## References

I strongly recommend reading:

- ["The Illustrated Transformer"](https://jalammar.github.io/illustrated-transformer/)
  by Jay AlammarAlammar also has a useful post dedicated more generally to
  Sequence-to-Sequence modeling 
- [LLM Course by ðŸ¤— HuggingFace](https://huggingface.co/learn/llm-course/chapter1/1)
- ["Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/),
  which illustrates the attention mechanism in the context of a more generic
  language translation model.
- [GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/)
