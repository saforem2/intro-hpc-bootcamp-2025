{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction To Sequential Data Modeling\n",
        "\n",
        "[Sam Foreman](https://samforeman.me)\n",
        "(\\[[ALCF](https://alcf.anl.gov/about/people/sam-foreman)\\](<https://alcf.anl.gov/about/people/sam-foreman>))  \n",
        "2025-07-22\n",
        "\n",
        "Author/Perpetrator: Carlo Graziani, including materials on LLMs by\n",
        "Varuni Sastri, and discussion/editorial work by Taylor Childers, Archit\n",
        "Vasan, Bethany Lusch, and Venkat Vishwanath (Argonne)\n",
        "\n",
        "Word embedding visualizations adapted from Kevin Gimpel (Toyota\n",
        "Technological Institute at Chicago) [Visualizing\n",
        "BERT](https://home.ttic.edu/~kgimpel/viz-bert/viz-bert.html).\n",
        "\n",
        "Some inspiration from the blog post [“The Illustrated\n",
        "Transformer”](https://jalammar.github.io/illustrated-transformer/) by\n",
        "Jay Alammar, highly recommended reading before next week’s deeper dive\n",
        "into Transformer tech by Archit Vasan. Another useful resource is [this\n",
        "ALCF\n",
        "tutorial](https://github.com/argonne-lcf/llm-workshop/blob/main/tutorials/01-llm-101/LLMs101.ipynb).\n",
        "\n",
        "Before you begin, make sure that you have your environment set up and\n",
        "your repo refreshed, as described in previous lessons, and reviewed in\n",
        "the accompanying ‘Readme.md’ file. Make sure that you select the kernel\n",
        "‘datascience/conda-2023-01-10’ at the top-left of the Jupyter notebook.\n",
        "\n",
        "## Modeling Sequential Data\n",
        "\n",
        "The modern terminology for the analysis of sequential data is “large\n",
        "language modeling” (LLM). This usage comes from Natural Language\n",
        "Processing (NLP), and is perhaps unfortunate—especially from the\n",
        "perspective of “AI For Science,” because, as I hope to clarify shortly,\n",
        "this kind of modeling is more generally useful than just for NLP.\n",
        "\n",
        "This session is dedicated to setting out the basics of sequential data\n",
        "modeling, and introducing (and, where possible motivating) a few key\n",
        "elements required for DL approaches to such modeling—principally\n",
        "transformers.\n",
        "\n",
        "## A Basic LLM Pipeline: Text Prediction\n",
        "\n",
        "Let’s download and run a model from the HuggingFace model hub, to start\n",
        "getting a feeling for one very popular LLM activity: text prediction."
      ],
      "id": "52568433-7723-4f39-a2a9-0e2421e37112"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Administrivia: modify this notebook so output text wraps.\n",
        "from IPython.display import HTML, display\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "      white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)\n",
        "\n",
        "# Set various proxies to download models and data\n",
        "# !export HTTP_PROXY=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
        "# !export HTTPS_PROXY=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
        "# !export http_proxy=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
        "# !export https_proxy=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
        "# !export ftp_proxy=\"http://proxy-01.pub.alcf.anl.gov:3128\" "
      ],
      "id": "cell-3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install transformers"
      ],
      "id": "cell-4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In what follows, we will ask an LLM based on the gpt2 model to complete\n",
        "the phrase “I’m late because”"
      ],
      "id": "fa5776d8-5388-4e7e-a443-ea0422afab30"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# In this framework, setting  up a \"pipeline\" involves selecting a (pre-trained) model,\n",
        "# and a task---in this case, text generation \n",
        "generator = pipeline(\"text-generation\", model='gpt2')\n",
        "\n",
        "prompt = \"I'm late because\"\n",
        "\n",
        "# We request 5 completions of the prompt, of length 25 words\n",
        "res = generator(prompt, max_length=25, num_return_sequences=5)\n",
        "\n",
        "# What did we get?\n",
        "for each in res:\n",
        "    print(each['generated_text'])\n",
        "    print('******************\"')"
      ],
      "id": "cell-6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Huh. These seem pretty random. What happens if we predict a single\n",
        "sentence, but extend its length?"
      ],
      "id": "3faf4422-7d04-4dca-9747-64f8dc8408a1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = generator(prompt, max_length=100, num_return_sequences=1)\n",
        "print(res[0]['generated_text'])\n",
        "print('******************\"')"
      ],
      "id": "cell-8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are some notable features here:\n",
        "\n",
        "1.  Each sentence is different from the previous one.\n",
        "2.  The sentences seem fairly disjointed, and go down weird rabbit holes\n",
        "    after (or sometimes during) the first sentence.  \n",
        "3.  The sentences always, or almost always, end in mid-sentence.\n",
        "\n",
        "What explains this behavior?\n",
        "\n",
        "The answer to (1) is that this kind of text prediction (like ChatGPT’s)\n",
        "is *generative*. The text prompt is used to contruct a probability\n",
        "distribution over possible completions, and each generated sentence is a\n",
        "*sample* from that distribution.\n",
        "\n",
        "The answer to (2) is: LLMs split up sentences into words or word\n",
        "fragments, called *tokens*. The way the samples are drawn is\n",
        "sequentially, one token at a time. Suppose the $k$-th token is denoted\n",
        "by the symbol $t_k$ ($t_k$ could be a word, a punctuation mark, a space,\n",
        "etc.). At stage $N$, the $N$-th token is drawn randomly from the\n",
        "distribution $$\n",
        "\\mathrm{Prob}(t_N | \\mathrm{prompt}, t_1, t_2,\\ldots,t_{N-1}),\n",
        "$$ at stage $N+1$ the random draw is from $$\n",
        "\\mathrm{Prob}(t_{N+1} | \\mathrm{prompt}, t_1, t_2,\\ldots,t_{N-1},t_N),\n",
        "$$ and so on.\n",
        "\n",
        "(If you are unfamiliar with this probability notation, know that an\n",
        "expression such as $\\mathrm{Prob}(X)$ should be read as “the probability\n",
        "of occurrence of event $X$”, while the expression $\\mathrm{Prob}(X|Y)$\n",
        "should be read as “the probability of occurrence of the event $X$\n",
        "*given* that event $Y$ has been observed.” This is called a *conditional\n",
        "probability*, and we often say “Probability of $X$ conditioned on $Y$”\n",
        "for $\\mathrm{Prob}(X|Y)$. The observation of $Y$ changes the probability\n",
        "distribution governing the occurrence of $X$. Here, the occurrence of\n",
        "the prompt, and of the first $N-1$ tokens, affects the probability\n",
        "distribution governing the $N$-th token $t_N$.)\n",
        "\n",
        "It follows from this fact that the sentences start growing apart at the\n",
        "first token $t_1$, and, depending on the model (in this case, gpt-2),\n",
        "are usually more strongly influenced by recent tokens than by older\n",
        "ones, so that $t_{90}$ is more strongly conditioned by it’s immediate\n",
        "predecessors $t_{89}$, $t_{88}$, and $t_{87}$ than it is by the prompt,\n",
        "or by $t_1$, $t_2$, or $t_3$. We would say that this model’s\n",
        "*attention*—it’s ability to maintain context over sentences and\n",
        "paragraphs—is quite limited.\n",
        "\n",
        "The answer to (3) is also related to the generation mechanism. With this\n",
        "style of generation, it is not possible to require that the text create\n",
        "exactly 100-token complete sentences. The best that one could do is to\n",
        "run the code so as to stop after the first (or second, or third) “End of\n",
        "Sentence” (EOS) token.\n",
        "\n",
        "\\## Where Do Deep Learning Model Architecture Choices Come From?\n",
        "\n",
        "From the model development view, the elements of a DL architecture are\n",
        "usually the result of *a lot* of trial-and-error by researchers.\n",
        "However, at a deeper level, those choices are dictated by the nature of\n",
        "the data itself: some strategies that are successful for some types of\n",
        "data are nearly pointless for other types.\n",
        "\n",
        "For example, last week Corey Adams lectured on the application of\n",
        "convolutional nets—network architectures based on local convolutional\n",
        "kernels—to image analysis. ConvNets were a remarkable discovery in the\n",
        "field, which arose through the desire to exploit *local 2-D spatial\n",
        "structure* in images—edges, gradients, contrasts, large coherent\n",
        "features, small-scale details, and so on. Convolutional kernels,\n",
        "together with pooling (and some other art) turn out to be exceptionally\n",
        "well-adapted to discovering such structure.\n",
        "\n",
        "On the other hand, convolutions are not as useful if the data does not\n",
        "have that sort of spatial structure. It would be sort of senseless to\n",
        "reach for convnets to model, say, seasonal effects on product sales data\n",
        "across different manufacturing categories, or natural language sequences\n",
        "(although this has been tried).\n",
        "\n",
        "So it makes sense to think about the nature of data when approaching\n",
        "this field. Generally speaking, there are two broad categories of data\n",
        "types that have dominated DL practice: vector data, and sequential data.\n",
        "This course so far has trafficked in vector data, but with LLMs, we move\n",
        "into the domain of sequential data.\n",
        "\n",
        "What distinguishes these two data types?\n",
        "\n",
        "### **Vector Data:**\n",
        "\n",
        "Fixed-length lists of real numbers, that can be visualized as living in\n",
        "a high-dimensional space, one dimension per list element.\n",
        "\n",
        "### Example: **Image Data**"
      ],
      "id": "7bb24d61-bd7e-4830-b79b-c2d5ae12e4c6"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p float=\"center\">"
      ],
      "id": "0616aed2-0a93-40d8-afac-bd3c3d6caa22"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figure>"
      ],
      "id": "b06de0ce-c8d9-462d-9a38-2246e3e98005"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"Figures/CIFAR-10.png\" width=\"500\" />"
      ],
      "id": "3677f6de-cd7d-4dc1-b884-1973411805c6"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figure>"
      ],
      "id": "17be3136-4565-4fbb-8eea-4905912d5764"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "b3a21a36-00ee-4d77-9330-499cd3f995b0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Typical queries and decisions:**\n",
        "\n",
        "-   Image classification\n",
        "-   Inpainting—fill in blank regions\n",
        "-   Segmentation—Identify elements in an image, e.g. cars, people,\n",
        "    clouds…\n",
        "\n",
        "### Example: **Data from simulations**"
      ],
      "id": "4adf9806-ff25-4556-b7c1-f9ea05459b2e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p float=\"center\">"
      ],
      "id": "927f4276-1db1-466e-9fee-f246fca12147"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figure>"
      ],
      "id": "2f727487-f60e-45fb-8467-03e38ca94a16"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"Figures/Simulations.png\" width=\"800\" />"
      ],
      "id": "0687d49b-36c6-4cc2-bdbc-b12685fa73db"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figure>"
      ],
      "id": "2d81fc64-92ae-472d-8674-dee7308ce71e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "5673ca92-4df8-431f-8ead-14ea80aeca9f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Typical queries and decisions:**\n",
        "\n",
        "-   Manifold finding/data reduction, i.e. how many dimensions are\n",
        "    *really* required to describe the data (this is basically what\n",
        "    autoencoders do);\n",
        "-   Emulation—train on simulation data, learn to produce similar output,\n",
        "    or output at simulation settings not yet attempted, at much lower\n",
        "    cost than the original simulators\n",
        "-   Forecasting of weather, economics, pollution…\n",
        "\n",
        "### **Sequential Data:**\n",
        "\n",
        "*Sequences* are variable-length lists, not necessarily real-valued,\n",
        "possibly containing gaps or requiring completion\n",
        "\n",
        "### Example: **Text Documents**"
      ],
      "id": "ef6d817f-4ac6-400b-8ab6-3c6b06e277ae"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p float=\"center\">"
      ],
      "id": "6751541a-7926-4aed-b892-ed843beed220"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figure>"
      ],
      "id": "3b66db72-340f-4f3b-b113-ae5c5ae0bbee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"Figures/Lorem.png\" width=\"500\" />"
      ],
      "id": "cdce0ff1-2995-4ab1-999e-6235b06f71f2"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figure>"
      ],
      "id": "a44ad238-69e1-4527-bd0a-93584afec19e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "e12edb2e-129a-41d9-9ee1-7aa684872c14"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Typical queries and decisions:**\n",
        "\n",
        "-   Translation\n",
        "-   Spell checking and correction\n",
        "-   Text prediction and generation\n",
        "-   Sentiment analysis\n",
        "\n",
        "### Example: **Genetic Sequences**"
      ],
      "id": "9213371f-7286-44d5-8ba0-20fe3241f02d"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p float=\"center\">"
      ],
      "id": "17eefa5b-6902-4247-8fb1-895f87cbf13a"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figure>"
      ],
      "id": "4eb6c154-56fd-4cac-b63d-2f49a5fb4003"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"Figures/RNA-codons.svg.png\" width=\"500\" />"
      ],
      "id": "faedfce4-38fe-4e2b-96fb-d7508fe4702c"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figure>"
      ],
      "id": "4be3e73d-1935-4980-962a-fe0332eb95ab"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "cb7b0097-423e-4c01-9477-b3ae16180a70"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Typical queries and decisions:**\n",
        "\n",
        "-   Prediction of likely variants/mutations from DNA variability\n",
        "-   Realistic DNA sequence synthesis\n",
        "-   Predicting gene expression\n",
        "\n",
        "### Example: **Protein Chains**"
      ],
      "id": "315beb77-bd96-4daa-9027-7b63d6897c43"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p float=\"center\">"
      ],
      "id": "9464a2cb-9cf0-44b9-96f6-84f44efe28ca"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figure>"
      ],
      "id": "89138aae-3da5-4840-98d6-dc90963084dd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"Figures/Protein-Structure-06.png\" width=\"500\" />"
      ],
      "id": "27e38d9d-4e10-431a-9799-d0483e14bf08"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figure>"
      ],
      "id": "288829ea-2e25-4490-b8be-89b2b41ffe96"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "81a8dfea-8f92-4cb3-8502-610efa441e23"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Typical queries and decisions:**\n",
        "\n",
        "-   Predict folding structure\n",
        "-   Predict chemical/binding properties\n",
        "\n",
        "Other examples of scientifically important types of sequential data\n",
        "include chemical compounds (sequences of atoms) and weather states\n",
        "(sequences of spatially-resolved temperatures, pressures, humidities,\n",
        "etc.).\n",
        "\n",
        "It should be clear from the above that sequential data is much richer\n",
        "and more highly-structured, in general, than vector data.\n",
        "\n",
        "Let’s take a break. In the meantime, go back to the LLM pipeline that we\n",
        "built at the beginning, and try seeing what kind of text prediction\n",
        "result you can obtain through “prompt engineering”: try making longer,\n",
        "or more constraining prompts, and see if you can get more sensible\n",
        "results from the model.\n",
        "\n",
        "## Elements of DL Sequence Modeling\n",
        "\n",
        "In 2017, the paper [Attention is All You\n",
        "Need](https://arxiv.org/abs/1706.03762) (Vaswani et al.) introduced the\n",
        "modern *transformer architecture*. Transformers were game-changers in\n",
        "the subject of sequential data modeling, but in important respects they\n",
        "built upon elements that were already in use in existing sequential data\n",
        "architectures, such as recurrent neural networks (RNNs), which they\n",
        "displaced.\n",
        "\n",
        "Let’s look at two of those key elements that are needed by modern\n",
        "sequential models (AKA “LLMs”): tokenization, and word embedding. The\n",
        "third key element—attention—will be covered next week.\n",
        "\n",
        "### Tokenization\n",
        "\n",
        "Recall that unlike vector data, sequential data can have values that are\n",
        "essentially arbitrary—integers, real numbers, natural language words,\n",
        "DNA codons, and so on. LLMs are trained and used in a manner that is\n",
        "largely agnostic of the data type. This is possible because in a step\n",
        "preliminary to training, LLMs convert the data to *tokens*. Tokens are\n",
        "elements of a finite, discrete set—a *vocabulary*, with a certain fixed\n",
        "*vocabulary size*. In the case of English natural language, the\n",
        "vocabulary may be made up of whole words, plus punctuation, spaces, etc.\n",
        "with a vocabulary size approaching 500,000; or it may be made up of\n",
        "sub-words, or word stems, or from other clever coding schemes, with\n",
        "reduced vocabulary size—as low as 30,000.\n",
        "\n",
        "There is clearly a tension represented by tokenization choice: On the\n",
        "one hand, there are great benefits to reducing the vocabulary size,\n",
        "because this can lead to substantial reductions in the number of\n",
        "parameters required by an LLM. On the other hand, the actual\n",
        "distribution of sequence elements is more likely to be exposed to the\n",
        "model by more complex tokenizations, with larger vocabulary sizes.\n",
        "\n",
        "For example, take a phrase such as “The chef added salt to the soup,\n",
        "because it required seasoning.” The LLM needs to be able to parse such a\n",
        "sentence and discern, for example, that the article “it” refers to “the\n",
        "soup”, and not to “the chef” or “salt” (how it does this is the subject\n",
        "of Attention, discussed next week). It should be clear that such\n",
        "relationship are easier to tease out from a word tokenization (“The”,\n",
        "“chef”, “added”…) than from, say, a character-by-character tokenization\n",
        "(“t”, “h”, “e”, “c”…). On the other hand, a character-by-character\n",
        "tokenization has a much smaller vocabulary size ($\\le 40$ including\n",
        "punctuation, spaces, etc.), whereas an English word-based vocabulary\n",
        "would have a vocabulary of about 500,000 (although the majority of these\n",
        "would be rarely-used words).\n",
        "\n",
        "Let’s look at some tokenization examples:"
      ],
      "id": "07f57493-4726-439f-8609-51d862f7d73a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# A utility function to tokenize a sequence and print out some information about it.\n",
        "\n",
        "def tokenization_summary(tokenizer, sequence):\n",
        "\n",
        "    # get the vocabulary\n",
        "    vocab = tokenizer.vocab\n",
        "    # Number of entries to print\n",
        "    n = 10\n",
        "\n",
        "    # Print subset of the vocabulary\n",
        "    print(\"Subset of tokenizer.vocab:\")\n",
        "    for i, (token, index) in enumerate(tokenizer.vocab.items()):\n",
        "        print(f\"{token}: {index}\")\n",
        "        if i >= n - 1:\n",
        "            break\n",
        "\n",
        "    print(\"Vocab size of the tokenizer = \", len(vocab))\n",
        "    print(\"------------------------------------------\")\n",
        "\n",
        "    # .tokenize chunks the existing sequence into different tokens based on the rules and vocab of the tokenizer.\n",
        "    tokens = tokenizer.tokenize(sequence)\n",
        "    print(\"Tokens : \", tokens)\n",
        "    print(\"------------------------------------------\")\n",
        "\n",
        "    # .convert_tokens_to_ids or .encode or .tokenize converts the tokens to their corresponding numerical representation.\n",
        "    #  .convert_tokens_to_ids has a 1-1 mapping between tokens and numerical representation\n",
        "    # ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    # print(\"encoded Ids: \", ids)\n",
        "\n",
        "    # .encode also adds additional information like Start of sequence tokens and End of sequene\n",
        "    print(\"tokenized sequence : \", tokenizer.encode(sequence))\n",
        "\n",
        "    # .tokenizer has additional information about attention_mask.\n",
        "    # encode = tokenizer(sequence)\n",
        "    # print(\"Encode sequence : \", encode)\n",
        "    # print(\"------------------------------------------\")\n",
        "\n",
        "    # .decode decodes the ids to raw text\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    decode = tokenizer.decode(ids)\n",
        "    print(\"Decode sequence : \", decode)\n",
        "\n",
        "\n",
        "tokenizer_1  =  AutoTokenizer.from_pretrained(\"gpt2\") # GPT-2 uses \"Byte-Pair Encoding (BPE)\"\n",
        "\n",
        "sequence = \"Counselor, please adjust your Zoom filter to appear as a human, rather than as a cat\"\n",
        "\n",
        "tokenization_summary(tokenizer_1, sequence)\n"
      ],
      "id": "cell-13"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GPT-2 uses a so-called “Byte-pair encoding” (BPE), which can break up\n",
        "long words into subwords (e.g. “Coun” “sel”lor”). This allows for rare\n",
        "words (the majority of English vocabulary words) to be encoded in a\n",
        "smaller vocabulary (in this case, 50257 token IDs). This tokenization\n",
        "also incorporates initial whitespace before a word into the word for\n",
        "efficiency, so a word will be encoded differently whether it is at the\n",
        "beginning of the sentence (without space) or not. This is the reason for\n",
        "those odd-looking “Ġ” characters in the vocabulary.\n",
        "\n",
        "For a description of how BPE works, see [this\n",
        "article](https://huggingface.co/learn/nlp-course/en/chapter6/5).\n",
        "\n",
        "(Python tip: try typing “help(tokenizer_1)” for more information on what\n",
        "it’s doing and how to use it).\n",
        "\n",
        "Let’s look at a different tokenizer now:"
      ],
      "id": "f7c9c42a-101c-43bc-b377-70d262be52b9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer_2  =  AutoTokenizer.from_pretrained(\"bert-base-cased\") # BERT uses WordPiece encoding\n",
        "\n",
        "tokenization_summary(tokenizer_2, sequence)\n"
      ],
      "id": "cell-15"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model (part of the BERT family) uses WordPiece encoding. This is\n",
        "another clever strategy (described\n",
        "[here](https://huggingface.co/learn/nlp-course/en/chapter6/6)) that\n",
        "reduces the vocabulary size to 28996, by starting with a character-level\n",
        "encoding, then training on a text corpus to find ways to efficiently\n",
        "merge some of these preliminary tokens into others representing longer\n",
        "strings of frequent occurrence. Note that “Counsel” occurs frequently\n",
        "enough to rate its own token, and the algorithm builds “counselor” out\n",
        "of “counsel”+“or” (actually, “##or”, where the “\\##” is a tag indicating\n",
        "that the token is a word piece). Similarly for “Zoom”=“Zoo” + “#m”.\n",
        "\n",
        "Note that one could obtain a simpler tokenizer by using elementary\n",
        "Python ‘string’ operations to split sentences on words, punctuation, and\n",
        "the like, obtaining *very* large vocabulary sizes, but not requiring\n",
        "advance training on corpuses of text as WordPiece and BPE do. In NLP,\n",
        "however, such strategies are considered naive, because very large\n",
        "vocabulary sizes produce cripplingly large LLM parameter sizes. It is\n",
        "better to train a tokenizer on actual data.\n",
        "\n",
        "Note also: there is, to this day, no approach to judging what is an\n",
        "*optimal* tokenization—one that best preserves distributional\n",
        "information contained in the sequence without an exploding vocabulary\n",
        "size.\n",
        "\n",
        "### Token Embedding\n",
        "\n",
        "The strategy of choice for learning language structure from tokenized\n",
        "text is to find a clever way to map each token into a moderate-dimension\n",
        "vector space, adjusting the mapping so that\n",
        "\n",
        "1.  Similar, or associated tokens take up residence nearby each other,\n",
        "    and\n",
        "2.  Different regions of the space correspond to different position in\n",
        "    the sequence.\n",
        "\n",
        "Such a mapping from token ID to a point in a vector space is called a\n",
        "*token embedding*. The dimension of the vector space is often high\n",
        "(e.g. 1024-dimensional), but much smaller than the vocabulary size\n",
        "(30,000–500,000). The choice of this dimension is made based on a\n",
        "compromise between expressiveness and computational cost. Various\n",
        "approaches have been attempted for generating such embeddings, including\n",
        "static algorithms that operate on a corpus of tokenized data as\n",
        "preprocessors for NLP tasks. Transformers, however, adjust their\n",
        "embeddings during training.\n",
        "\n",
        "The point of an embedding is to allow computational NLP architectures to\n",
        "operate on familiar, and easily tensorizable data entities—vectors—which\n",
        "can easily be subjected to the usual sorts of DL operations.\n",
        "\n",
        "Let’s take a look at a visualization of the token embedding associated\n",
        "with a pre-trained BERT transformer model. The following is adapted from\n",
        "[Kevin Gimpel’s\n",
        "visualization](https://home.ttic.edu/~kgimpel/viz-bert/viz-bert.html)."
      ],
      "id": "986834b1-f0be-441f-a714-d8fdfe20efac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "plt.rcParams['figure.figsize'] = [100, 60]\n",
        "\n",
        "# Load BERT.\n",
        "model = BertModel.from_pretrained('bert-large-uncased-whole-word-masking')\n",
        "# Set the model to eval mode.\n",
        "model.eval()\n",
        "# This notebook assumes CPU execution. If you want to use GPUs, put the model on cuda and modify subsequent code blocks.\n",
        "#model.to('cuda')\n",
        "# Load tokenizer.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking')\n",
        "\n",
        "# Save the BERT vocabulary to a file -- by default it will name this file \"vocab.txt\".\n",
        "tokenizer.save_vocabulary(save_directory='.')\n",
        "\n",
        "print(\"The vocabulary size is: \", model.config.vocab_size) # Size of the vocabulary\n"
      ],
      "id": "cell-17"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get BERT's vocabulary embeddings.\n",
        "wordembs = model.get_input_embeddings()\n",
        "\n",
        "# Convert the vocabulary embeddings to numpy.\n",
        "allinds = np.arange(0,model.config.vocab_size,1)\n",
        "inputinds = torch.LongTensor(allinds)\n",
        "bertwordembs = wordembs(inputinds).detach().numpy()\n",
        "print(bertwordembs.shape)\n"
      ],
      "id": "cell-18"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the array bertwordembs contains 30522 vectors of size 1024, the\n",
        "latter being the dimension of the embedding space.\n",
        "\n",
        "The visualization of this data will be carried out by means of a\n",
        "projection to 2 dimensions by an algorithm called “t-SNE”\n",
        "(“t-distributed stochastic neighbor embedding”), which attempts to keep\n",
        "“similar” (in the high-dimensional space) points together, while\n",
        "spreading apart in the 2-dimensional projection points that are distant\n",
        "from each other in the high-dimensional space."
      ],
      "id": "db5f4344-d347-418a-9214-fe952b69a9c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in the vocabulary\n",
        "filename = \"vocab.txt\"\n",
        "with open(filename,'r') as f:\n",
        "    bertwords = np.array([])\n",
        "    for line in f:\n",
        "        bertwords = np.append(bertwords, line.rstrip())\n",
        "\n",
        "# Determine vocabulary to use for t-SNE/visualization. The indices are hard-coded based partially on inspection:\n",
        "bert_char_indices_to_use = np.arange(999, 1063, 1)\n",
        "bert_voc_indices_to_plot = np.append(bert_char_indices_to_use, np.arange(1996, 5932, 1))\n",
        "bert_voc_indices_to_use = np.append(bert_char_indices_to_use, np.arange(1996, 11932, 1))\n",
        "\n",
        "bert_voc_indices_to_use_tensor = torch.LongTensor(bert_voc_indices_to_use)\n",
        "bert_word_embs_to_use = wordembs(bert_voc_indices_to_use_tensor).detach().numpy()\n",
        "bert_words_to_plot = bertwords[bert_voc_indices_to_plot]\n",
        "\n",
        "\n",
        "print(len(bert_voc_indices_to_plot))\n",
        "print(len(bert_voc_indices_to_use))"
      ],
      "id": "cell-20"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That is to say, 10000 words will be used to compute the t-SNE data, but\n",
        "only 4000 of them will be plotted. We’re ready to run the t-SNE\n",
        "projection."
      ],
      "id": "a17f0d36-1053-4bf7-aca7-b0c3c24cf56e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run t-SNE on the BERT vocabulary embeddings we selected:\n",
        "mytsne_words = TSNE(n_components=2,early_exaggeration=12,verbose=2,metric='cosine',init='pca',n_iter=2500)\n",
        "bert_word_embs_to_use_tsne = mytsne_words.fit_transform(bert_word_embs_to_use)"
      ],
      "id": "cell-22"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the transformed BERT vocabulary embeddings:\n",
        "fig = plt.figure()\n",
        "alltexts = list()\n",
        "for i, txt in enumerate(bert_words_to_plot):\n",
        "    plt.scatter(bert_word_embs_to_use_tsne[i,0], bert_word_embs_to_use_tsne[i,1], s=0)\n",
        "    currtext = plt.text(bert_word_embs_to_use_tsne[i,0], bert_word_embs_to_use_tsne[i,1], txt, family='sans-serif')\n",
        "    alltexts.append(currtext)\n",
        "    \n",
        "\n",
        "# Save the plot before adjusting.\n",
        "plt.savefig('viz-bert-voc-tsne10k-viz4k-noadj.pdf', format='pdf')\n",
        "\n",
        "plt.show"
      ],
      "id": "cell-23"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here’s are some close-ups of this projection. The first one has clusters\n",
        "of verbs with similar meanings.\n",
        "\n",
        "<img src=\"Figures/viz-bert-voc-verbs.png\" align=\"center\" width=800/>\n",
        "\n",
        "Next, we see some sub-word clusters, containing the “\\##” tag used by\n",
        "BERT’s WordPiece tokenization scheme. This cluster consists of English\n",
        "suffixes.\n",
        "\n",
        "<img src=\"Figures/viz-bert-voc-suffixes.png\" align=\"center\" width=800/>\n",
        "\n",
        "Here is what appears to be a cluster of suffixes of jurisdiction names:\n",
        "\n",
        "<img src=\"Figures/viz-bert-voc-entities.png\" align=\"center\" width=800/>\n",
        "\n",
        "### Homework\n",
        "\n",
        "1.  **Tokenization**\n",
        "\n",
        "Write a generic Python tokenizer, which takes a set of text lines and\n",
        "tabulates the different words (that is, the tokens will be simply\n",
        "English words), keeping track of the frequency of each word. Use the\n",
        "guidance in the accompanying notebook, ‘Homework_1.ipynb’.\n",
        "\n",
        "1.  **Embedding**\n",
        "\n",
        "Modify the embedding visualization code above to zoom in on various\n",
        "regions of the projections, and identify at least one interesting\n",
        "cluster of tokens."
      ],
      "id": "f93112a0-7ecc-48e8-b3be-d5c0ef52fb26"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "pytorch.venv",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  }
}